{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FirstDeep1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9LkH+OBB8+a9UXcNBy77i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KSY1526/myblog/blob/master/_notebooks/2021-12-26-FirstDeep1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"[처음 시작하는 딥러닝] 1. 신경망 기초 실습\"\n",
        "- author: Seong Yeon Kim \n",
        "- categories: [book, jupyter, Deep Learning, foundation, matrix, math]"
      ],
      "metadata": {
        "id": "XcQRgWFSfenH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 넘파이"
      ],
      "metadata": {
        "id": "clvjQCpjfjHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo7btOMYfYY7",
        "outputId": "49c28cfb-c412-4932-8917-a26350959c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a+b: [1, 2, 3, 4, 5, 6]\n",
            "불가능\n",
            "\n",
            "a+b [5 7 9]\n",
            "a*b [ 4 10 18]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "a = [1,2,3]\n",
        "b = [4,5,6]\n",
        "\n",
        "print('a+b:', a+b)\n",
        "\n",
        "try:\n",
        "    print(a*b)\n",
        "except TypeError:\n",
        "    print('불가능')\n",
        "\n",
        "print()\n",
        "\n",
        "a = np.array([1,2,3])\n",
        "b = np.array([4,5,6])\n",
        "print('a+b', a+b)\n",
        "print('a*b', a*b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R의 벡터 자료형 처럼 파이썬의 리스트 자료형은 더하기, 곱하기가 자유롭지 않습니다.\n",
        "\n",
        "이를 해결하기 위해 C언어 기반 넘파이 패키지를 활용합니다."
      ],
      "metadata": {
        "id": "c5CGeoVjbaLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1,2],[3,4]])\n",
        "print(a)\n",
        "\n",
        "print('axis =0', a.sum(axis = 0))\n",
        "print('axis =1', a.sum(axis = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL6bw2JEf_Uj",
        "outputId": "5b3ef643-972e-4799-8ddf-4bd84354af34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "axis =0 [4 6]\n",
            "axis =1 [3 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "간단한 행렬 연산입니다. axis = 0과 1의 차이점을 나타냅니다."
      ],
      "metadata": {
        "id": "WVn_6cAEbksf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1,2,3],[4,5,6]])\n",
        "b = np.array([10,20,30])\n",
        "\n",
        "print('a+b:\\n', a+b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZBoT3xxgcnx",
        "outputId": "97e6ce75-841e-4917-d851-b2235bc9161d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a+b:\n",
            " [[11 22 33]\n",
            " [14 25 36]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R과 마찬가지로 재활용 규칙이 사용됩니다."
      ],
      "metadata": {
        "id": "OPIdmmK3br41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 신경망 기초 행렬 연산"
      ],
      "metadata": {
        "id": "RyA62fvQwH0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv(func, input_, delta = 0.001):\n",
        "    return (func(input_+delta) - func(input_-delta)) / (2 * delta)\n",
        "\n",
        "def matrix_function_forward_sum(X,W,sigma):    \n",
        "    assert X.shape[1] == W.shape[0] \n",
        "    # assert은 아래 코드가 다음조건이 맞을때만 성립한다는 것을 알려줍니다.\n",
        "\n",
        "    N = np.dot(X,W)\n",
        "\n",
        "    S = sigma(N)\n",
        "\n",
        "    L = np.sum(S)\n",
        "    return L   \n",
        "\n",
        "def matrix_function_backward_sum_1(X,W,sigma):    \n",
        "    assert X.shape[1] == W.shape[0] \n",
        "    # assert은 아래 코드가 다음조건이 맞을때만 성립한다는 것을 알려줍니다.\n",
        "\n",
        "    N = np.dot(X,W)\n",
        "\n",
        "    S = sigma(N) # 입력받은 함수를 적용합니다.\n",
        "\n",
        "    L = np.sum(S)\n",
        "\n",
        "    dLdS = np.ones_like(S) # S배열 차원크기 유지, 값은 모두 1로\n",
        "\n",
        "    dSdN = deriv(sigma, N)\n",
        "\n",
        "    dLdN = dLdS * dSdN\n",
        "    \n",
        "    dNdX = np.transpose(W, (1,0))\n",
        "\n",
        "    dLdX = np.dot(dLdN, dNdX)\n",
        "\n",
        "    return dLdX"
      ],
      "metadata": {
        "id": "f1kHscK7g0cN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "행렬 미분입니다. X는 입력값 행렬, W는 가중치행렬 입니다."
      ],
      "metadata": {
        "id": "06CucRIrbw91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(190204)\n",
        "\n",
        "X = np.random.randn(3,3)\n",
        "W = np.random.randn(3,2)\n",
        "\n",
        "print('X:')\n",
        "print(X)\n",
        "\n",
        "print('L:')\n",
        "print(round(matrix_function_forward_sum(X,W,sigmoid), 4))\n",
        "print()\n",
        "print('dLdX:')\n",
        "print(matrix_function_backward_sum_1(X,W,sigmoid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A25LkxKJbZFS",
        "outputId": "0a8f8fda-5897-4ec7-a243-d122cb4a2f60"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:\n",
            "[[-1.57752816 -0.6664228   0.63910406]\n",
            " [-0.56152218  0.73729959 -1.42307821]\n",
            " [-1.44348429 -0.39128029  0.1539322 ]]\n",
            "L:\n",
            "2.3755\n",
            "\n",
            "dLdX:\n",
            "[[ 0.2488887  -0.37478057  0.01121962]\n",
            " [ 0.12604152 -0.27807404 -0.13945837]\n",
            " [ 0.22992798 -0.36623443 -0.02252592]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X는 입력하는 행렬이고 L은 출력값 dLdX은 X 행렬이 변할 때 L값의 변화량, 즉 미분값입니다.\n",
        "\n",
        "이 미분값은 d(sigma)/d(u) * (XW) * (W)^T으로 구할 수 있습니다. (손으로 증명 가능)"
      ],
      "metadata": {
        "id": "slunzFXGdhnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = X.copy()\n",
        "X1[0,0] += 0.001\n",
        "\n",
        "print(round((matrix_function_forward_sum(X1,W,sigmoid) - matrix_function_forward_sum(X,W,sigmoid))/0.001, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqGQcS7Jcvb0",
        "outputId": "61ae8bb9-e907-4e51-e4fc-6c80eab9abde"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X[0,0] 값을 0.001 증가시켰을때 L 값의 변화량을 구했습니다. dLdX [0,0] 값과 유사한 것을 알 수 있어요."
      ],
      "metadata": {
        "id": "QZaKpi9sejVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 선형회귀 행렬 연산"
      ],
      "metadata": {
        "id": "OmobH4-agK9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_linear_regression(X_batch, y_batch, weights):\n",
        "    # X는 행렬, y는 벡터, weight는 딕셔너리형태(W + B)\n",
        "    assert X_batch.shape[0] == y_batch.shape[0] # 데이터 크기가 같은가\n",
        "    assert X_batch.shape[1] == weights['W'].shape[0] # 가중치 개수가 맞는가.(행렬 연산이 가능한가)\n",
        "    assert weights['B'].shape[0] ==  weights['B'].shape[1] == 1\n",
        "\n",
        "    N = np.dot(X_batch, weights['W'])\n",
        "    P = N + weights['B']\n",
        "\n",
        "    loss = np.mean(np.power(y_batch-P, 2))\n",
        "\n",
        "    forward_info = {}\n",
        "    forward_info['X'] = X_batch\n",
        "    forward_info['N'] = N # XW\n",
        "    forward_info['P'] = P # XW + B\n",
        "    forward_info['y'] = y_batch\n",
        "\n",
        "    return loss, forward_info"
      ],
      "metadata": {
        "id": "i7BlHP0seYxv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력값(X) 파라미터(가중치, W)가 주어졌을때 선형 회귀 값을 구하는 함수입니다.\n",
        "\n",
        "하지만 우리가 원하는건 파라미터(가중치)를 추정하는 일이죠. \n",
        "\n",
        "L = mean(Sigma(y(i) - yhat(i))) 을 최소로 하는 파라미터를 미분을 이용해서 찾아봅시다."
      ],
      "metadata": {
        "id": "Jv0WnJulpKq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_gradients(forward_info, weights):\n",
        "    batch_size = forward_info['X'].shape[0]\n",
        "    dLdP = -2 * (forward_info['y'] - forward_info['P'])\n",
        "    dPdN = np.ones_like(forward_info['N'])\n",
        "    dPdB = np.ones_like(weights['B'])\n",
        "\n",
        "    dLdN = dLdP * dPdN\n",
        "\n",
        "    dNdW = np.transpose(forward_info['X'], (1,0))\n",
        "\n",
        "    dLdW = np.dot(dNdW, dLdN) # X^T가 행렬 곱법칙 때문에 먼저나옴.\n",
        "    dLdB = (dLdP * dPdB).sum(axis = 0)\n",
        "\n",
        "    loss_gradients = {}\n",
        "    loss_gradients['W'] = dLdW\n",
        "    loss_gradients['B'] = dLdB\n",
        "\n",
        "    return loss_gradients"
      ],
      "metadata": {
        "id": "utkxbyRBpH7e"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 구한 신경망 기초 행렬 연산식을 이용해 선형회귀 가중치의 도함수를 구했습니다."
      ],
      "metadata": {
        "id": "PRnJxdH5vXjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 밑바박부터 만드는 신경망"
      ],
      "metadata": {
        "id": "XMS1bBjT1cfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_loss(X, y, weights):\n",
        "    M1 = np.dot(X, weights['W1'])\n",
        "    N1 = M1 + weights['B1']\n",
        "    O1 = sigmoid(N1)\n",
        "    \n",
        "    M2 = np.dot(O1, weights['W2'])\n",
        "    P = M2 + weights['B2']\n",
        "\n",
        "    loss = np.mean(np.power(y - P, 2))\n",
        "\n",
        "    forward_info = {}\n",
        "    forward_info['X1'] = X1 # 입력값\n",
        "    forward_info['M1'] = M1 # X * W(1), 여러개의 선형결합 결과들\n",
        "    forward_info['N1'] = N1 # 상수항 더하기\n",
        "    forward_info['O1'] = O1 # 시그모이드 함수 씌우기\n",
        "    forward_info['M2'] = M2 # 시그모이드 함수 씨운 13개 값 다시 선형결합\n",
        "    forward_info['P'] = P # 다시 상수항 더한 값 최종 Y_HAT으로 생각\n",
        "    forward_info['y'] = y"
      ],
      "metadata": {
        "id": "cf-J-yBsoQmi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "간단한 신경망 구조입니다. 선형결합을 행 개수만큼 하고 나온 여러개의 결과값에 시그모이드 함수를 씌우고 다시 선형결합해서 결과를 냅니다."
      ],
      "metadata": {
        "id": "ZHV-JHJB4lQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_gradients(forward_info, weights):\n",
        "    '''\n",
        "    신경망의 각 파라미터에 대한 손실의 편미분을 계산\n",
        "    '''    \n",
        "    dLdP = -(forward_info['y'] - forward_info['P'])\n",
        "    \n",
        "    dPdM2 = np.ones_like(forward_info['M2']) # P = M2 + B2\n",
        "    dLdM2 = dLdP * dPdM2\n",
        "    dPdB2 = np.ones_like(weights['B2'])\n",
        "    dLdB2 = (dLdP * dPdB2).sum(axis=0)\n",
        "    \n",
        "    dM2dW2 = np.transpose(forward_info['O1'], (1, 0)) # O1 * W2    \n",
        "    dLdW2 = np.dot(dM2dW2, dLdP)\n",
        "\n",
        "    dM2dO1 = np.transpose(weights['W2'], (1, 0)) # 이게 중요\n",
        "    dLdO1 = np.dot(dLdM2, dM2dO1)\n",
        "    \n",
        "    dO1dN1 = sigmoid(forward_info['N1']) * (1- sigmoid(forward_info['N1']))\n",
        "    dLdN1 = dLdO1 * dO1dN1\n",
        "    \n",
        "    dN1dB1 = np.ones_like(weights['B1'])\n",
        "    dN1dM1 = np.ones_like(forward_info['M1'])\n",
        "    \n",
        "    dLdB1 = (dLdN1 * dN1dB1).sum(axis=0)\n",
        "    \n",
        "    dLdM1 = dLdN1 * dN1dM1\n",
        "    \n",
        "    dM1dW1 = np.transpose(forward_info['X'], (1, 0)) \n",
        "\n",
        "    dLdW1 = np.dot(dM1dW1, dLdM1)\n",
        "\n",
        "    loss_gradients = {}\n",
        "    loss_gradients['W2'] = dLdW2\n",
        "    loss_gradients['B2'] = dLdB2.sum(axis=0)\n",
        "    loss_gradients['W1'] = dLdW1\n",
        "    loss_gradients['B1'] = dLdB1.sum(axis=0)\n",
        "    \n",
        "    return loss_gradients"
      ],
      "metadata": {
        "id": "20_UR2I23tI1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "조금 복잡하긴 하지만 직접 합성함수 미분을 해봤습니다."
      ],
      "metadata": {
        "id": "gOo4SUr4FDJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 느낀점"
      ],
      "metadata": {
        "id": "Gsw4CQ4q9X9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드 실습은 많지 않지만 공부시간이 상당히 오래걸렸습니다.\n",
        "\n",
        "다변량 벡터나 행렬을 사용한 합성함수를 미분하는게 여간 힘든게 아니였어요. 수학적 지식, 머신러닝 경험이 조금 필요합니다.\n",
        "\n",
        "직관적으로 이해하기위해 많이 노력했습니다.\n",
        "\n",
        "그래도 딥러닝이 어떤것인지 토대를 배웠는데 정말 흥미로웠어요.\n",
        "\n",
        "그동안 겉핥기 식으로 대충 모형만 보고 패키지만 갔다가 썼는데 밑바닥부터 구현하니 딥러닝의 기초 구조를 알수 있어서 좋았습니다."
      ],
      "metadata": {
        "id": "Gh45WiccGCiW"
      }
    }
  ]
}