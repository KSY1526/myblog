{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2aVqmxyX646RowT/orlu6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KSY1526/myblog/blob/master/_notebooks/2022-02-21-torch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"[DACON] 자연어 처리 문장 쌍 분류하기2 With Pytorch\"\n",
        "- author: Seong Yeon Kim \n",
        "- categories: [jupyter, Deep Learning, Pytorch, DACON, natural language, BERT, tokenizer, Classifier]"
      ],
      "metadata": {
        "id": "uxEtqRxhfG0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지 설치하기"
      ],
      "metadata": {
        "id": "eIwy7aFgfG3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YWx_MS0wAUn",
        "outputId": "81c9bfc8-409c-476d-d85a-b127301c1121"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "구글 드라이브와 연동합니다."
      ],
      "metadata": {
        "id": "CWsmrPB87H60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet-cu101\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece==0.1.85\n",
        "!pip install transformers==2.1.1\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import gluonnlp as nlp\n",
        "\n",
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "# 스케줄러 조정 함수\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zWCr26v07AC",
        "outputId": "50f02066-d81b-452f-ac9c-47ae71bb54b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet-cu101\n",
            "  Downloading mxnet_cu101-1.9.0-py3-none-manylinux2014_x86_64.whl (358.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 358.1 MB 4.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (1.21.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2021.10.8)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.9.0\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.21.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.27)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.7)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595727 sha256=21bfa5933185a4a625340e1bfaf8e75fd58e3e8d80269d47e30a5d2587306f84\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Collecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 11.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "Collecting transformers==2.1.1\n",
            "  Downloading transformers-2.1.1-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (1.21.5)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.3-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 51.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.25.0,>=1.24.3\n",
            "  Downloading botocore-1.24.3-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 7.0 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.3->boto3->transformers==2.1.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.3->boto3->transformers==2.1.1) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.3 botocore-1.24.3 jmespath-0.10.0 s3transfer-0.5.1 sacremoses-0.0.47 transformers-2.1.1 urllib3-1.25.11\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-3u3t0irc\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-3u3t0irc\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.21.3)\n",
            "Requirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet>=1.4.0\n",
            "  Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3 MB 37.5 MB/s \n",
            "\u001b[?25hCollecting onnxruntime==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.1.85)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111)\n",
            "Collecting transformers>=4.8.1\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.27)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.2.3) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.11.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (0.0.47)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.62.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (3.0.7)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (0.5.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.25.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.3->boto3->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.3->boto3->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.2.3) (3.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15449 sha256=5ca56065402e4d6b5d7a3099cd3639179f1e840149ba1a0fbe8273c500f816f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kat_1l4g/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "Successfully built kobert\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, onnxruntime, mxnet, kobert\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 2.1.1\n",
            "    Uninstalling transformers-2.1.1:\n",
            "      Successfully uninstalled transformers-2.1.1\n",
            "Successfully installed huggingface-hub-0.4.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 tokenizers-0.11.5 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필요한 패키지를 설치하고 임포트 합니다."
      ],
      "metadata": {
        "id": "BRpZK0eq7Kkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "rhefLr741Tri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/sentence/'\n",
        "\n",
        "train = pd.read_csv(path + 'train_data.csv')\n",
        "test = pd.read_csv(path + 'test_data.csv')\n",
        "sample_submission = pd.read_csv(path + 'sample_submission.csv')\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "qrEcDybmdmo9",
        "outputId": "9376fb20-5230-47db-fc5a-cda83d39ed5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0ddf36dd-2b03-4813-b0be-4faf598af020\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
              "      <td>씨름의 여자들의 놀이이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
              "      <td>자작극을 벌인 이는 3명이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
              "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
              "      <td>원주민들은 종합대책에 만족했다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
              "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ddf36dd-2b03-4813-b0be-4faf598af020')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ddf36dd-2b03-4813-b0be-4faf598af020 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ddf36dd-2b03-4813-b0be-4faf598af020');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index  ...          label\n",
              "0      0  ...  contradiction\n",
              "1      1  ...  contradiction\n",
              "2      2  ...     entailment\n",
              "3      3  ...        neutral\n",
              "4      4  ...        neutral\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 불러옵니다. 데이터에 대한 설명은 이전에 했으니 생략합니다."
      ],
      "metadata": {
        "id": "q1zfEx7Y7QRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {'entailment' : 0, 'contradiction' : 1, 'neutral' : 2}\n",
        "\n",
        "train_content = []\n",
        "test_content = []\n",
        "\n",
        "for idx in tqdm(train.index):\n",
        "    train_content.append(list([[train.loc[idx, 'premise'], train.loc[idx, 'hypothesis']],\n",
        "                               label_dict[train.loc[idx, 'label']]]))\n",
        "\n",
        "for idx in tqdm(test.index):\n",
        "    test_content.append(list([[test.loc[idx, 'premise'], test.loc[idx, 'hypothesis']]]))\n",
        "\n",
        "\n",
        "dataset_train = train_content[:20000]\n",
        "dataset_valid = train_content[20000:]\n",
        "dataset_test = test_content\n",
        "\n",
        "dataset_train[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsgF2OYe-WeQ",
        "outputId": "c192fbec-4fff-4e13-e710-adca1d9d4330"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24998/24998 [00:01<00:00, 23116.36it/s]\n",
            "100%|██████████| 1666/1666 [00:00<00:00, 39118.13it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이 넓고 평평한 백사장이나 마당에서 모여 서로 힘과 슬기를 겨루는 것이다.',\n",
              "   '씨름의 여자들의 놀이이다.'],\n",
              "  1],\n",
              " [['삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나, 중국 내에서의 여론은 자작극이라는 증거가 충분함에도 불구하고 좋지 않다.',\n",
              "   '자작극을 벌인 이는 3명이다.'],\n",
              "  1],\n",
              " [['이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.',\n",
              "   '예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.'],\n",
              "  0],\n",
              " [['광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 적극 나섰다.',\n",
              "   '원주민들은 종합대책에 만족했다.'],\n",
              "  2],\n",
              " [['진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는 책임 있는 모습을 보여주는 것이 필요하다.',\n",
              "   '이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.'],\n",
              "  2]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력된 데이터를 리스트 형태로 다시 구성합니다. 이때 [[전제, 가설], 라벨] 구성으로 데이터 한 세트를 만듭니다.\n",
        "\n",
        "2만개를 트레인 세트, 나머지 약 5천개를 valid 세트로 구성합니다."
      ],
      "metadata": {
        "id": "F4Ur33XK1X23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로더 구축하기"
      ],
      "metadata": {
        "id": "2fDOCu_V2Zcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 70\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 5e-5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# kobert 패키지 내 BERT 모델과 어휘 집합을 입력받습니다. \n",
        "bertmodel, vocab = get_pytorch_kobert_model(cachedir = '.cache')\n",
        "\n",
        "# kobert 패키지 내 토크나이저를 입력받습니다.\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "# 입력받은 토크나이저를 입력받은 어휘 집합을 이용해 학습합니다.\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)"
      ],
      "metadata": {
        "id": "q3KZEZJPds_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b6b1fe-641c-43bc-d5bc-027c4220c2d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n",
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본적인 하이퍼 파라미터 설정을 미리 합니다. 문장 최대 길이, 에포크, 러닝레이트 등등을 미리 지정합니다.\n",
        "\n",
        "또 kobert 패키지 내 BERT 모델, 토크나이저, 어휘 집합을 입력받습니다."
      ],
      "metadata": {
        "id": "IIIvaNuK7wvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sen_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair, mode = 'train'):\n",
        "        \n",
        "        self.mode = mode\n",
        "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length = max_len,\n",
        "                                                   pad = pad, pair = pair)\n",
        "        # 문장 쌍(pair = True)을 학습하는 트렌스포머를 만듭니다.\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "            self.sentence = [transform(i[sen_idx]) for i in dataset]\n",
        "            self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "        else:\n",
        "            self.sentence = [transform(i[sen_idx]) for i in dataset]\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.mode == 'train':\n",
        "            return (self.sentence[i] + (self.labels[i],))\n",
        "\n",
        "        else:\n",
        "            return self.sentence[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.sentence))\n",
        "        "
      ],
      "metadata": {
        "id": "MV8nZ2EF8x64"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치 내 Dataset 클래스를 상속받아서 데이터 셋을 클래스로 만듭니다.\n",
        "\n",
        "Dataset 클래스를 상속했기 때문에 향후 DataLoader 내에 실을 수 있겠죠.\n",
        "\n",
        "리스트 형태인 데이터 내 문장을 트랜스포머를 이용해서 변환 한 뒤 sentence 내 리스트 형태로 저장합니다.\n",
        "\n",
        "라벨 값 또한 labels 내 리스트 형태로 저장합니다. "
      ],
      "metadata": {
        "id": "hDzZc1r3iTAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (리스트 형태 데이터 셋, 문장위치, 라벨위치, 토크나이저, 문장 최대 길이, pad, pair, 모드)\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, True, mode = 'train')\n",
        "data_valid = BERTDataset(dataset_valid, 0, 1, tok, max_len, True, True, mode = 'train')\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, True, mode = 'test')\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 0)\n",
        "valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size = batch_size, num_workers = 0)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 0)"
      ],
      "metadata": {
        "id": "KBai2q1h06ho"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 정의한 BERTDataset 클래스를 이용해 문장을 트랜스포머로 변형하고, 로더에 실겠습니다."
      ],
      "metadata": {
        "id": "8Uo4bTV5jN9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWRqRJFfqRxR",
        "outputId": "d9d1dde6-6ddc-43fc-b0f7-196a67d007fe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   2, 3088, 6117, 7086, 2658, 5439, 6708, 6080, 4059, 7245, 1442,\n",
              "        6965, 1423, 5939, 1678, 1504, 7096, 6081,  517,   46, 2822, 5712,\n",
              "        7098, 3954, 7227, 5940, 1459, 5439, 4841, 7724, 7828, 2298, 6493,\n",
              "        7178, 7098, 1907, 5804, 6903, 2064, 2720, 5211, 5468, 2948, 5573,\n",
              "         517, 5411, 6095, 5760,  913,  517,   54,    3, 3088, 6117, 7095,\n",
              "        3318, 5939, 1504, 7096, 7100,  517,   54,    3,    1,    1,    1,\n",
              "           1,    1,    1,    1], dtype=int32),\n",
              " array(63, dtype=int32),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0], dtype=int32),\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 데이터 셋은 문장을 트랜스포머를 통해 숫자로 바꾼 값, 전체 문장이 끝나는 위치, 두번째 문장을 1로 표기한 리스트, 라벨값으로 구성됩니다."
      ],
      "metadata": {
        "id": "mIJehIQZqUuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 구축하기"
      ],
      "metadata": {
        "id": "deIs0KxTjKmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size = 768, num_classes = 3, dr_rate = None, params = None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p = dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(),\n",
        "                              attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "\n",
        "        return self.classifier(out)\n"
      ],
      "metadata": {
        "id": "KU1w-UAIjMAq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치 내 nn.Module 클래스를 상속받아 모델을 클래스로 구현했습니다. init 함수로 정의를 하고 forward 함수에서 실행을 합니다.\n",
        "\n",
        "특이한 점은 bert 모델에 입력값으로 input_ids(숫자로 변환된 문장), token_type_ids(두번째 문장을 1로 표기한 리스트), attention_mask(문장부분을 1로, 문장 아닌부분을 0으로 표기한 리스트) 세가지를 받습니다.\n",
        "\n",
        "현재 데이터 로더는 input_ids, token_type_ids는 bert 입력값과 같은형태이나, attention_mask가 아닌 문장이 끝나는 부분을 숫자로 알려줍니다.\n",
        "\n",
        "문장이 끝나는 부분 값을 attention_mask 형식으로 바꾸기 위해 gen_attention_mask 함수를 사용했습니다."
      ],
      "metadata": {
        "id": "1ujeE2T1lfk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate = 0.5).to(device)\n",
        "\n",
        "# 가중치 감퇴를 위한 부분 (오버피팅 방지를 위해)\n",
        "no_decay = ['bias', 'LayerNorm.weight'] # 이 부분은 가중치 감퇴 x\n",
        "optimizer_group_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay' : 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay' : 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_group_parameters, lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# warnup 값을 설정해줍니다.\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "# warnup 값보다 스텝이 작을경우 러닝레이트를 선형증가, 클경우 러닝레이트 임의에 방법으로 업데이트 합니다.\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step,\n",
        "                                            num_training_steps = t_total)"
      ],
      "metadata": {
        "id": "NyBkpnfzlEwK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 정의하고, 자연어 분류문제에 자주 사용되는 AdamW 옵티마이저와 크로스엔트로피 손실함수를 정의합니다.\n",
        "\n",
        "오버피팅 방지를 위해 가중치를 규제하는 부분도 정의했으며, 러닝레이트 또한 스케줄러를 통해 유동적으로 조정해줍니다."
      ],
      "metadata": {
        "id": "6dhd-MNMsLQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습하기"
      ],
      "metadata": {
        "id": "slyV2jDLs042"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(x,y):\n",
        "    max_vals, max_indices = torch.max(x, 1)\n",
        "    train_acc = (max_indices == y).sum().data.cpu().numpy() / max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "sij5Rs57pRwA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 성능 확인을 위해 정확도를 계산해주는 함수를 만들었습니다."
      ],
      "metadata": {
        "id": "i5jwEdefsqQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    valid_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
        "        optimizer.zero_grad() # 옵티마이저 파라미터 배치단위로 초기화\n",
        "        token_ids = token_ids.long().to(device) \n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length = valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Vanishing 또는 Exploding 방지\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step() # 러닝 레이트 업데이트\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(valid_dataloader), total = len(valid_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length = valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        valid_acc += calc_accuracy(out, label)\n",
        "\n",
        "    print(\"epoch {} valid acc {}\".format(e+1, valid_acc / (batch_id+1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miwvdIaxp-qW",
        "outputId": "418eb18c-1c9a-4c16-9cf9-941fd3411a47"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:05<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train acc 0.7162040734824281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:39<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 valid acc 0.7140690928270043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:06<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 train acc 0.8008186900958466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:39<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 valid acc 0.7144646624472574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:06<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 train acc 0.8582268370607029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:38<00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 valid acc 0.7225738396624473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [06:59<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 train acc 0.8917731629392971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:38<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 valid acc 0.7298918776371308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:01<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 train acc 0.9011581469648562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:38<00:00,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 valid acc 0.7172336497890296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 로더에서 배치단위로 값을 꺼내서 모델에 적용하고, 가중치를 업데이트 시켜줬습니다.\n",
        "\n",
        "한 에포크가 끝날때마다 정확도를 지속적으로 확인하는 모습입니다."
      ],
      "metadata": {
        "id": "VCAfxVbtvErP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 이용하기"
      ],
      "metadata": {
        "id": "xbkGsTN3tzLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_id, (token_ids, valid_length, segment_ids) in tqdm(enumerate(test_dataloader), total = len(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length = valid_length\n",
        "        result.append(model(token_ids, valid_length, segment_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDnw9z6WryVh",
        "outputId": "4682de2f-55bd-4dca-8fb6-51fea6a061ef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:12<00:00,  2.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 평가모드로 변환하고 테스트 데이터를 모델에 넣어 3개 라벨의 예측 확률을 출력한 것을 result 안에 넣습니다."
      ],
      "metadata": {
        "id": "ONjBiwPDvRdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_ = []\n",
        "\n",
        "for i in result:\n",
        "    for j in i:\n",
        "        result_.append(int(torch.argmax(j)))\n",
        "\n",
        "out = [list(label_dict.keys())[_] for _ in result_]\n",
        "\n",
        "sample_submission['label'] = out\n",
        "\n",
        "sample_submission.to_csv('sentence_4.csv',index=False)\n"
      ],
      "metadata": {
        "id": "JZ3G0i5_ucH5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 최종적으로 필요한 것은 라벨 이름입니다. 지금 result에 있는 값은 3개 라벨의 각각의 확률값이죠.\n",
        "\n",
        "argmax 함수를 사용해 가장 확률이 높은 값을 추출하고, 앞서 정의한 딕셔너리를 이용해 라벨로 변환하여 파일로 저장합니다."
      ],
      "metadata": {
        "id": "cgtwwZ-JvmB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 느낀점"
      ],
      "metadata": {
        "id": "wH0sQEeXv7Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치 사용법을 익히기 위해 이해하기 쉽고 좋은 코드들을 직접 하나하나 따라치는 중입니다.\n",
        "\n",
        "이번 코드는 딥러닝에 한 분야인 자연어 처리쪽으로, 제가 가장 관심있는 분야인데요. 이전 코드보단 확실히 난이도가 있는 것 같습니다.\n",
        "\n",
        "가중치에 오버피팅, 폭주, 소실을 막기 위해 더 다양한 기술들을 사용하는데 시간이 날때 보다 자세히 관찰하려합니다.\n",
        "\n",
        "GPU에 성능 체감을 처음 하는데 이것때문에 현질을 한 사람의 마음이 바로 이해가 되네요.\n",
        "\n",
        "max_grad_norm 값을 실수로 -1로 했을 뿐인데 모델 성능이 극악으로 안좋아졌습니다. 이것때문에 고생 많이 했는데, 긍정적으로 보면 max_grad_norm 값이 매우 중요하다는 걸 피부로 느꼈네요.\n",
        "\n",
        "다음엔 이미지 분류 문제를 공부할 것 같습니다."
      ],
      "metadata": {
        "id": "JVDUGTI4v8gI"
      }
    }
  ]
}