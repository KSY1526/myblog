{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FirstDeep4.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEBECoSr0o6ovOhANaNWTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KSY1526/myblog/blob/master/_notebooks/2022-01-27-FirstDeep4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"[처음 시작하는 딥러닝] 4. 밑바닥부터 만들어보는 RNN\"\n",
        "- author: Seong Yeon Kim \n",
        "- categories: [book, jupyter, Deep Learning, matrix, math, class]"
      ],
      "metadata": {
        "id": "e2_kW_opu2oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자동미분"
      ],
      "metadata": {
        "id": "OPZKiLlRu2-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iHXjfsuSu1ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d037fe5-d706-4457-d179-eb836dbae424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 1 0]\n",
            "__add__ 함수 사용: [6 7 5 4]\n",
            "+ 연산자 사용: [6 7 5 4]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([2,3,1,0])\n",
        "\n",
        "print(a)\n",
        "print('__add__ 함수 사용:', a.__add__(4))\n",
        "print('+ 연산자 사용:', a + 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이썬에서는 + 연산자를 사용할 때 내부에서  _ _ add_ _  함수가 호출되는 방식입니다.\n",
        "\n",
        "다른 연산자도 마찬가지로 연결된 함수를 불러오는 방식으로 실행됩니다."
      ],
      "metadata": {
        "id": "-n5Z0Z3ppAtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union, List\n",
        "\n",
        "Numberable = Union[float, int] \n",
        "# 정수, 소수 둘다 함께하는 자료형 생성, 파이토치에서는 tensor가 비슷한 역할을 함.\n",
        "\n",
        "def ensure_number(num: Numberable): # -> NumberWithGrad, 자료형으로 출력해줌.\n",
        "    if isinstance(num, NumberWithGrad): # 자료형 확인함수\n",
        "        return num\n",
        "    \n",
        "    else:\n",
        "        return NumberWithGrad(num)\n",
        "\n",
        "class NumberWithGrad(object):\n",
        "    def __init__(self, num: Numberable,\n",
        "                 depends_on : List[Numberable] = None,\n",
        "                 creation_op: str = ''):\n",
        "        self.num = num # 원래 값(숫자) 자체를 저장.\n",
        "        self.grad = None\n",
        "        self.depends_on = depends_on or []\n",
        "        self.creation_op = creation_op\n",
        "\n",
        "    def __add__(self, other: Numberable = None): # NumberWithGrad 출력\n",
        "        return NumberWithGrad(self.num + ensure_number(other).num,\n",
        "                              # 입력된 값 NumberWithGrad 자료형으로 변환 후 더하기.\n",
        "                              depends_on = [self, ensure_number(other)],\n",
        "                              # 3 + 4이면 [3, 4]로 저장(NumberWithGrad 자료형으로)\n",
        "                              creation_op = 'add')\n",
        "    \n",
        "    def __mul__(self, other: Numberable = None): # NumberWithGrad 출력\n",
        "        return NumberWithGrad(self.num * ensure_number(other).num,\n",
        "                              depends_on = [self, ensure_number(other)],\n",
        "                              creation_op = 'mul')\n",
        "        \n",
        "    def backward(self, backward_grad: Numberable = None) -> None:\n",
        "        if backward_grad is None: # 이 함수가 처음 호출될때\n",
        "            self.grad = 1\n",
        "\n",
        "        # 이부분에서 기울기가 누적.\n",
        "        else:\n",
        "            if self.grad is None: # 기울기 정보가 아직 없다면\n",
        "                self.grad = backward_grad # backward_grad로 설정\n",
        "\n",
        "            else:\n",
        "                self.grad += backward_grad # 있다면 기존 기울기에 backward_grad를 더함.\n",
        "\n",
        "\n",
        "        # self.grad를 역방향으로 전달.\n",
        "        # 둘 중 어느 요소를 증가시켜도 출력이 같은 값만큼 증가함.\n",
        "        if self.creation_op == 'add':\n",
        "            self.depends_on[0].backward(self.grad)\n",
        "            self.depends_on[1].backward(self.grad)\n",
        "\n",
        "\n",
        "        if self.creation_op == 'mul':\n",
        "            # 첫번째 요소에 대한 미분 계산\n",
        "            new = self.depends_on[1] * self.grad\n",
        "            # 미분을 역방향으로 전달\n",
        "            self.depends_on[0].backward(new.num)\n",
        "\n",
        "            # 두번째 요소에 대한 미분 계산\n",
        "            new = self.depends_on[0] * self.grad\n",
        "            #미분을 역방향으로 전달.\n",
        "            self.depends_on[1].backward(new.num)\n",
        "\n",
        "a = NumberWithGrad(3)\n",
        "\n",
        "b = a * 4\n",
        "c = b + 5\n",
        "\n",
        "c.backward()\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "id": "F-vGfI8lo-Ky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b716971a-ee15-4ddc-8363-7a9f0f15b4cd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자동 미분을 간단하게 구현한 함수입니다. 저도 이해하는데 꽤 걸렸는데 단기간에 이해하기 쉽진 않습니다.\n",
        "\n",
        "우선 Numberable 자료형을 생성합니다. int, float 자료형을 통합했다고 보면 될 것 같아요. 또 ensure_number 함수는 입력되는 모든 값을 뒤에 나오는 NumberWithGrad 자료형으로 변환해준다고 생각하면 됩니다.\n",
        "\n",
        "NumberWithGrad 클레스도 간단하게 하나의 자료형으로 이해하시는게 좋습니다. a = NumberWithGrad(3)은 3이라는 값을 NumberWithGrad 자료형으로 선언했다고 생각하면 됩니다.\n",
        "\n",
        "이후 a * 4 코드가 진행되면 파이썬에서는 a. __ mul __(4) 함수를 써서 곱하기를 진행하는데, NumberWithGrad 자료형 내부에 __ mul __ 함수가 오버라이딩 되어있습니다.\n",
        "\n",
        "오버라이딩 된 NumberWithGrad 내부 함수는 값 부분에 정상적인 곱하기 연산을 수행하고, depends_on 부분에 연산에 사용된 숫자를 NumberWithGrad 자료형으로 기록, creation_op 부분에 곱하기 연산이 수행됬다는 것을 기록합니다.\n",
        "\n",
        "즉 b = a * 4 연산을 통해 b는 NumberWithGrad 자료형으로 바뀌고 연산에 사용된 숫자와 연산기호가 NumberWithGrad 자료형 내부에 저장됩니다.\n",
        "\n",
        "c = b + 5 부분에서 c도 마찬가지로 NumberWithGrad 자료형으로 저장되며 b와 5가 depends_on 부분에 저장되는데 b 같은 경우 이전 NumberWithGrad 자료형 그대로 저장되는 것을 주목합시다.\n",
        "\n",
        "그리고 c.backward() 함수를 실행하면 backward_grad 입력값에 아무것도 안 넣었기 때문에 첫번째 if문에 걸립니다. c의 grad 값은 1로 선언됩니다.\n",
        "\n",
        "그 뒤 c를 만들기 위해 사용했던 연산기호가 + 이기 때문에 세번째 if문에 걸리게 되고 c를 만들어준 두 연산자가 기록되어 있는 depends_on을 이용해 다시 backward 함수를 실행시킵니다.\n",
        "\n",
        "backward 함수를 실행시킬때 함수 입력값에 c.grad을 넣고 함수를 실행합니다. 상세히 말하면 c.depends_on[0] = b이기 때문에 b.backward(c.grad)를 실행합니다.\n",
        "\n",
        "함수 입력값에 c.grad가 있기 때문에 두번째 if문에 걸리게 되고 b.grad 값에 1을 넣게 됩니다. 그 후 아까 c에서 벌어졌던 연산을 다시 반복합니다.\n",
        "\n",
        "이후 내용을 한번 더 상세하게 설명하면 b를 만들기 위해 사용했던 연산기호가 * 이므로 네번째 if문에 걸리게 되고 여기서는 b.grad에 서로 반대되는 값을 곱해준 것을 backward 함수에 전달합니다.\n",
        "\n",
        "과정이 조금 복잡해서 저도 되집어본다고 생각하고 클레스 구조를 간단히 서술했는데 관심있는 분들은 스스로 코드를 해석해보시면서 제 설명을 참고하시면 보다 쉽게 익힐 수 있을거 같아요. 이해 안되는 부분은 댓글달아주시면 아는 선에서 설명드리겠습니다."
      ],
      "metadata": {
        "id": "dF-6EdyqSClY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = NumberWithGrad(3)\n",
        "\n",
        "b = a * 4\n",
        "c = b + 3\n",
        "d = (a + 2) # a 다시 사용.\n",
        "e = c * d\n",
        "e.backward()\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KmnqS55RJu2",
        "outputId": "7739cd53-ce28-48c1-eba2-9c4f84eeda8a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아까보다 한단계 복잡한 자동미분 예시입니다. 실제 도함수를 통해 계산한 결과와 일치하는 것을 볼 수 있어요.\n",
        "\n",
        "예제는 d = (4*a + 3) * (a + 2) 입니다.\n",
        "\n",
        "자동 미분이 필요한 이유가 순방향 계산 과정의 중간 결과를 재사용 할 수 있습니다. 윗 예제도 a를 두번 써도 정상적으로 실행됩니다.\n",
        "\n",
        "이전에 했던 합성함수 미분법을 활용했을때는 윗 예제와 같은 식을 설명할 수 없기 때문에 자동 미분 방법이 중요합니다."
      ],
      "metadata": {
        "id": "99VMBzSKxP8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요한 라이브러리 임포트, 활성화 함수 선언"
      ],
      "metadata": {
        "id": "i4RZ4XoBT1KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')\n",
        "%matplotlib inline\n",
        "\n",
        "from copy import deepcopy\n",
        "from collections import deque\n",
        "\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "\n",
        "\n",
        "def assert_same_shape(output, output_grad):\n",
        "    assert output.shape == output_grad.shape, \\\n",
        "    '''\n",
        "    두 ndarray의 모양이 같아야 하는데,\n",
        "    첫 번째 ndarray의 모양은 {0}이고\n",
        "    두 번째 ndarray의 모양은 {1}이다.\n",
        "    '''.format(tuple(output_grad.shape), tuple(output.shape))\n",
        "    return None\n",
        "\n",
        "def assert_dim(t, dim):\n",
        "    assert len(t.shape) == dim, \\\n",
        "    '''\n",
        "    이 텐서는 {0}차원이어야 하는데, {1}차원이다.\n",
        "    '''.format(dim, len(t.shape))\n",
        "    return None\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def sigmoid(x: ndarray):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def dsigmoid(x: ndarray):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "\n",
        "def tanh(x: ndarray):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def dtanh(x: ndarray):\n",
        "    return 1 - np.tanh(x) * np.tanh(x)\n",
        "\n",
        "\n",
        "def softmax(x, axis=None):\n",
        "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
        "\n",
        "\n",
        "def batch_softmax(input_array: ndarray):\n",
        "    out = []\n",
        "    for row in input_array:\n",
        "        out.append(softmax(row, axis=1))\n",
        "    return np.stack(out)"
      ],
      "metadata": {
        "id": "rbz_s_qcRrbu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "필요한 라이브러리와 활성화 함수를 선언했습니다."
      ],
      "metadata": {
        "id": "MqIr43lTUwFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 옵티마이저, 손실함수"
      ],
      "metadata": {
        "id": "Wp6qV5okU11z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNOptimizer(object):\n",
        "    def __init__(self, lr: float = 0.01,\n",
        "                 gradient_clipping: bool = True) -> None:\n",
        "        self.lr = lr\n",
        "        self.gradient_clipping = gradient_clipping\n",
        "        self.first = True\n",
        "\n",
        "    def step(self) -> None:\n",
        "\n",
        "        for layer in self.model.layers:\n",
        "            for key in layer.params.keys():\n",
        "\n",
        "                if self.gradient_clipping:\n",
        "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
        "\n",
        "                self._update_rule(param=layer.params[key]['value'],\n",
        "                                  grad=layer.params[key]['deriv'])\n",
        "\n",
        "    def _update_rule(self, **kwargs) -> None:\n",
        "        raise NotImplementedError()"
      ],
      "metadata": {
        "id": "lZOLdooTUujl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNOptimizer 클레스를 통해 RNN에서 사용할 수 있는 옵티마이저의 기본 골격을 만들었습니다.\n",
        "\n",
        "코드 내 상세 내용은 저도 정확히 모르겠습니다."
      ],
      "metadata": {
        "id": "9dxJvjBoWLcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(RNNOptimizer):\n",
        "    def __init__(self,\n",
        "                 lr: float = 0.01,\n",
        "                 gradient_clipping: bool = True) -> None:\n",
        "        super().__init__(lr, gradient_clipping)\n",
        "\n",
        "    def _update_rule(self, **kwargs) -> None:\n",
        "\n",
        "        update = self.lr*kwargs['grad']\n",
        "        kwargs['param'] -= update\n",
        "\n",
        "\n",
        "class AdaGrad(RNNOptimizer):\n",
        "    def __init__(self,\n",
        "                 lr: float = 0.01,\n",
        "                gradient_clipping: bool = True) -> None:\n",
        "        super().__init__(lr, gradient_clipping)\n",
        "        self.eps = 1e-7\n",
        "\n",
        "    def step(self) -> None:\n",
        "        if self.first:\n",
        "            self.sum_squares = {}\n",
        "            for i, layer in enumerate(self.model.layers):\n",
        "                self.sum_squares[i] = {}\n",
        "                for key in layer.params.keys():\n",
        "                    self.sum_squares[i][key] = np.zeros_like(layer.params[key]['value'])\n",
        "            \n",
        "            self.first = False\n",
        "\n",
        "        for i, layer in enumerate(self.model.layers):\n",
        "            for key in layer.params.keys():\n",
        "                \n",
        "                if self.gradient_clipping:\n",
        "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
        "                \n",
        "                self._update_rule(param=layer.params[key]['value'],\n",
        "                                  grad=layer.params[key]['deriv'],\n",
        "                                  sum_square=self.sum_squares[i][key])\n",
        "\n",
        "    def _update_rule(self, **kwargs) -> None:\n",
        "\n",
        "            # 이전 기울기의 제곱의 합을 계산\n",
        "            kwargs['sum_square'] += (self.eps +\n",
        "                                     np.power(kwargs['grad'], 2))\n",
        "\n",
        "            # 이전 5개 기울기의 제곱의 합으로 학습률을 수정\n",
        "            lr = np.divide(self.lr, np.sqrt(kwargs['sum_square']))\n",
        "\n",
        "            # 수정된 학습률을 적용\n",
        "            kwargs['param'] -= lr * kwargs['grad']"
      ],
      "metadata": {
        "id": "7OJxsJB9WFzp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 만든 RNNOptimizer를 상속받아 SGD와 AdaGrad 옵티마이저 클레스를 만들었습니다."
      ],
      "metadata": {
        "id": "E4W41alkWWqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,\n",
        "                prediction: ndarray,\n",
        "                target: ndarray) -> float:\n",
        "\n",
        "        assert_same_shape(prediction, target)\n",
        "\n",
        "        self.prediction = prediction\n",
        "        self.target = target\n",
        "\n",
        "        self.output = self._output()\n",
        "\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self) -> ndarray:\n",
        "\n",
        "        self.input_grad = self._input_grad()\n",
        "\n",
        "        assert_same_shape(self.prediction, self.input_grad)\n",
        "\n",
        "        return self.input_grad\n",
        "\n",
        "    def _output(self) -> float:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _input_grad(self) -> ndarray:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "        \n",
        "class SoftmaxCrossEntropy(Loss):\n",
        "    def __init__(self, eps: float=1e-9) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.single_class = False\n",
        "\n",
        "    def _output(self) -> float:\n",
        "\n",
        "        out = []\n",
        "        for row in self.prediction:\n",
        "            out.append(softmax(row, axis=1))\n",
        "        softmax_preds = np.stack(out)\n",
        "\n",
        "        # 안정적인 계산을 위해 소프트맥스의 출력을 제한\n",
        "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
        "\n",
        "        # 손실을 실제로 계산\n",
        "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
        "            (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
        "\n",
        "        return np.sum(softmax_cross_entropy_loss)\n",
        "\n",
        "    def _input_grad(self) -> np.ndarray:\n",
        "\n",
        "        return self.softmax_preds - self.target"
      ],
      "metadata": {
        "id": "-aTgswHlWd0d"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 부분도 가벼운 마음으로 정리했습니다. 주가 아닌 부분이기 때문에 자세한 설명은 생략할께요."
      ],
      "metadata": {
        "id": "b4S7sIEEaf7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 순환 신경망 구현"
      ],
      "metadata": {
        "id": "VQ66RjqcWeZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLayer(object):\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, weight_scale = None):\n",
        "\n",
        "        self.hidden_size = hidden_size # 은닉 뉴런수\n",
        "        self.output_size = output_size \n",
        "        self.weight_scale = weight_scale\n",
        "        self.start_H = np.zeros((1, hidden_size)) # 이 층에 내부상태 저장\n",
        "        self.first = True # 처음임을 선언\n",
        "\n",
        "\n",
        "    def _init_params(self, input_: ndarray): # input_ = x_seq_in\n",
        "        \n",
        "        self.vocab_size = input_.shape[2] # 사용되는 글자의 가짓수\n",
        "        \n",
        "        if not self.weight_scale:\n",
        "            self.weight_scale = 2 / (self.vocab_size + self.output_size)\n",
        "        \n",
        "        # 밑에 나오는 것은 파라미터를 저장하기 위한 형식입니다.\n",
        "        # 키 value부분은 실제 파라미터, 키 deriv는 그에대한 기울깃값입니다.\n",
        "        self.params = {}\n",
        "        self.params['W_f'] = {}\n",
        "        self.params['B_f'] = {}\n",
        "        self.params['W_v'] = {}\n",
        "        self.params['B_v'] = {}\n",
        "        \n",
        "        self.params['W_f']['value'] = np.random.normal(loc = 0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
        "        self.params['B_f']['value'] = np.random.normal(loc = 0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.hidden_size))\n",
        "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(self.hidden_size, self.output_size))\n",
        "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.output_size))    \n",
        "        \n",
        "        self.params['W_f']['deriv'] = np.zeros_like(self.params['W_f']['value'])\n",
        "        self.params['B_f']['deriv'] = np.zeros_like(self.params['B_f']['value'])\n",
        "        self.params['W_v']['deriv'] = np.zeros_like(self.params['W_v']['value'])\n",
        "        self.params['B_v']['deriv'] = np.zeros_like(self.params['B_v']['value'])\n",
        "        \n",
        "        self.cells = [RNNNode() for x in range(input_.shape[1])] # input_shape[1] : 시간순서\n",
        "        # 시간 순서만큼 RNN 노드를 리스트형태로 만들어 cells에 저장.\n",
        "\n",
        "    \n",
        "    def _clear_gradients(self):\n",
        "        for key in self.params.keys():\n",
        "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
        "\n",
        "    \n",
        "    def forward(self, x_seq_in: ndarray): # 입력값은 배치, 순서(시간), 변수 크기\n",
        "\n",
        "        if self.first: # 처음이면\n",
        "            self._init_params(x_seq_in) # 윗 함수 실행할 것.\n",
        "            self.first = False # 처음 아닌 상태 표시\n",
        "\n",
        "        batch_size = x_seq_in.shape[0] # 배치 크기 입력\n",
        "\n",
        "        H_in = np.copy(self.start_H) # 은닉층 내부 상태 저장. (1, 은닉층)\n",
        "        H_in = np.repeat(H_in, batch_size, axis = 0) # 은닉층 배치크기만큼 확대. (배치크기, 은닉층)\n",
        "\n",
        "        sequence_length = x_seq_in.shape[1] # 순서형 자료 개수 입력\n",
        "\n",
        "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
        "\n",
        "        for t in range(sequence_length): # 순서형 자료 개수만큼\n",
        "            x_in = x_seq_in[:, t, :] # 특정 시점에 입력값들.\n",
        "\n",
        "            # RNN 노드 내 forward함수 실행. H_in 값이 지속적으로 업데이트 됩니다.\n",
        "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
        "            x_seq_out[:, t, :] = y_out # 출력층에 값을 넣어줍니다.\n",
        "\n",
        "        self.start_H = H_in.mean(axis = 0, keepdims = True) \n",
        "        # 마지막 은닉층 값을 기억합니다.\n",
        "\n",
        "        return x_seq_out\n",
        "\n",
        "\n",
        "    def backward(self, x_seq_out_grad: ndarray):\n",
        "        \n",
        "        batch_size = x_seq_out_grad.shape[0]\n",
        "\n",
        "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        sequence_length = x_seq_out_grad.shape[1]\n",
        "        \n",
        "        x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size))\n",
        "\n",
        "\n",
        "        for t in reversed(range(sequence_length)):\n",
        "            x_out_grad = x_seq_out_grad[:, t, :]\n",
        "\n",
        "            grad_out, h_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
        "\n",
        "            x_seq_in_grad[:, t, :] = grad_out\n",
        "\n",
        "        return x_seq_in_grad"
      ],
      "metadata": {
        "id": "afLK4u0yarRH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN 층을 직접 구현했습니다. 내용이 상당히 방대해 이해하기 쉽진 않습니다.\n",
        "\n",
        "쉽게 정리하면 입력되는 형태는 (배치크기, 순서, 특성 크기) 인 3차원 입니다. \n",
        "\n",
        "그 후 층 내부에서는 처음에 배치단위별로 한 순서씩 모든 특성크기가 입력 됩니다.\n",
        "\n",
        "층 내부 구조는 입력층, 히든층, 출력층으로 되어있으며 입력층은 입력된 특성크기 + 히든층 크기 로 구성되어있습니다.\n",
        "\n",
        "히든층은 사용자가 정의하며, 출력층은 원하는 결과물에 따라 개수를 정해주면 됩니다.\n",
        "\n",
        "이때 히든층은 지속적으로 업데이트되어 출력됩니다. 더 자세한 설명은 RNN 노드 부분에서 하겠습니다."
      ],
      "metadata": {
        "id": "TqQB5H_U7_uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNNode(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x_in: ndarray, H_in: ndarray,\n",
        "                params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
        "                # params_dic 예시로 [B_f][value, ndarray] 로 생각하면 됩니다.\n",
        "        \n",
        "        self.X_in = x_in # (배치크기, 변수 크기)\n",
        "        self.H_in = H_in # (배치크기, 은닉층 크기)\n",
        "\n",
        "        self.Z = np.column_stack((x_in, H_in))\n",
        "        # column_stack 함수는 두 넘파이 배열을 입력받아 열을 기준(여기선 배치마다)으로 병합합니다.\n",
        "        # 즉 Z는 (배치크기, 변수 + 은닉층 크기) 입니다. \n",
        "        # 이 말은 입력값은 입력된 변수 + 이전 은닉층 값이라는 거죠. 이 부분이 핵심입니다.\n",
        "\n",
        "        self.H_int = np.dot(self.Z, params_dict['W_f']['value'] + params_dict['B_f']['value'])\n",
        "        # W_f는 (은닉층 + 변수, 은닉층), B_f는 (1, 은닉층), 두 개 합 또한 넘파이 재활용 규칙으로 (은닉층 + 변수, 은닉층).\n",
        "        # 출력값은 (배치, 은닉층) 이 됩니다.\n",
        "        self.H_out = tanh(self.H_int) # 은닉층 값에 활성화 함수를 거칩니다.\n",
        "\n",
        "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value'] + params_dict['B_v']['value'])\n",
        "        # W_v는 (은닉층, 출력층), B_f는 (1, 출력층) 합은 재활용 규칙으로 (은닉층, 출력층)\n",
        "        # 출력값은 (배치, 출력층)이 됩니다.\n",
        "\n",
        "        return self.X_out, self.H_out # 출력층 값, 은닉층 값 돌려줍니다.\n",
        "\n",
        "\n",
        "    def backward(self, X_out_grad: ndarray, H_out_grad: ndarray,\n",
        "                 params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
        "\n",
        "        assert_same_shape(X_out_grad, self.X_out)\n",
        "        assert_same_shape(H_out_grad, self.H_out)\n",
        "        \n",
        "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis = 0)\n",
        "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
        "\n",
        "        dh = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
        "        dh += H_out_grad\n",
        "\n",
        "        dH_int = dh * dtanh(self.H_int)\n",
        "\n",
        "        params_dict['B_f']['deriv'] += dH_int.sum(axis = 0)\n",
        "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, dH_int)\n",
        "\n",
        "        dz = np.dot(dH_int, params_dict['W_f']['value'].T)\n",
        "\n",
        "        X_in_grad = dz[:, :self.X_in.shape[1]]\n",
        "        H_in_grad = dz[:, self.X_in.shape[1]:]\n",
        "\n",
        "        assert_same_shape(X_out_grad, self.X_out)\n",
        "        assert_same_shape(H_out_grad, self.H_out)\n",
        "\n",
        "        return X_in_grad, H_in_grad"
      ],
      "metadata": {
        "id": "ivgyQuHOblDS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 역전파는 수리적으로 다소 어렵고 모델 구조 이해와 큰 연관이 없기 때문에 순전파 위주로 학습하겠습니다.\n",
        "\n",
        "여기서 주목할 건 학습할 때 입력값만 쓰는 것이 아니라 이전 은닉층 값도 사용한다는 것입니다.\n",
        "\n",
        "이전 은닉층 값은 이전 입력값과 연관이 있기 때문에 순서가 있는 데이터에서 상당히 유용합니다.\n",
        "\n",
        "나머지 부분은 코드 내 주석을 잘 읽어보시고, 이해가 안되시면 댓글 써주시면 아는 선에서 답변 드리겠습니다."
      ],
      "metadata": {
        "id": "t3ImKCDF9kOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(object):\n",
        "\n",
        "    def __init__(self, layers: List[RNNLayer], \n",
        "                 sequence_length, vocab_size, loss: Loss):\n",
        "        \n",
        "        self.layers = layers # 층을 리스트 단위로 입력\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.loss = loss\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # 층마다 sequence_length 값 지정\n",
        "            setattr(layer, 'sequence_length', sequence_length)\n",
        "\n",
        "\n",
        "    def forward(self, x_batch: ndarray):\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # x_batch : (배치, 순서값, 특성값)\n",
        "            x_batch = layer.forward(x_batch)\n",
        "\n",
        "        return x_batch\n",
        "\n",
        "    def backward(self, loss_grad: ndarray):\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            loss_grad = layer.backward(loss_grad)\n",
        "\n",
        "        return loss_grad\n",
        "\n",
        "    def single_step(self, x_batch, y_batch):\n",
        "\n",
        "        # 순방향 계산\n",
        "        x_batch_out = self.forward(x_batch)\n",
        "        \n",
        "        # 손실 및 손실의 기울기 계산\n",
        "        loss = self.loss.forward(x_batch_out, y_batch)\n",
        "        loss_grad = self.loss.backward()\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer._clear_gradients()\n",
        "\n",
        "        # 역방향 계산\n",
        "        self.backward(loss_grad)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "PdEITIeNdHph"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 클래스는 입력과 목푯값으로 신경망을 실제로 학습해 손실을 계산합니다.\n",
        "\n",
        "층들을 리스트로 입력받으며 single_step 함수를 사용해 실제 연산을 진행합니다."
      ],
      "metadata": {
        "id": "2iBGEAi8J_3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자연어 관련 실습"
      ],
      "metadata": {
        "id": "4vSOnrDaMojz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNTrainer:\n",
        "\n",
        "    def __init__(self, text_file: str, model: RNNModel,\n",
        "                 optim: RNNOptimizer, batch_size = 32):\n",
        "        \n",
        "        self.data = open(text_file, 'r').read()\n",
        "        self.model = model\n",
        "        self.chars = list(set(self.data))\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.char_to_idx = {ch:i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i:ch for i, ch in enumerate(self.chars)}\n",
        "        self.sequence_length = self.model.sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.optim = optim\n",
        "        setattr(self.optim, 'model', self.model)\n",
        "\n",
        "    def _generate_inputs_targets(self, start_pos):\n",
        "        \n",
        "        inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
        "        targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
        "        \n",
        "        for i in range(self.batch_size):\n",
        "            \n",
        "            inputs_indices[i, :] = np.array([self.char_to_idx[ch] \n",
        "                            for ch in self.data[start_pos + i: start_pos + self.sequence_length  + i]])\n",
        "            targets_indices[i, :] = np.array([self.char_to_idx[ch] \n",
        "                         for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]])\n",
        "\n",
        "        return inputs_indices, targets_indices\n",
        "\n",
        "\n",
        "    def _generate_one_hot_array(self, indices: ndarray):\n",
        "        '''\n",
        "        param indices: 모양이 (batch_size, sequence_length)인 넘파이 배열\n",
        "        return batch - 모양이 (batch_size, sequence_length, vocab_size)인 넘파이 배열\n",
        "        ''' \n",
        "        batch = []\n",
        "        for seq in indices:\n",
        "            \n",
        "            one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size))\n",
        "            \n",
        "            for i in range(self.sequence_length):\n",
        "                one_hot_sequence[i, seq[i]] = 1.0\n",
        "\n",
        "            batch.append(one_hot_sequence) \n",
        "\n",
        "        return np.stack(batch)\n",
        "\n",
        "\n",
        "    def sample_output(self, input_char, sample_length):\n",
        "        '''\n",
        "        현재 학습된 모델로 한 글자씩 출력을 생성한다.\n",
        "        param input_char: int - 연속열을 시작하는 글자의 인덱스에 해당하는 정수\n",
        "        param sample_length: int - 생성할 연속열의 길이\n",
        "        return txt: string - 길이가 sample_length이며 모델을 통해 생성한 문자열\n",
        "        '''\n",
        "        indices = []\n",
        "        \n",
        "        sample_model = deepcopy(self.model)\n",
        "        \n",
        "        for i in range(sample_length):\n",
        "            input_char_batch = np.zeros((1, 1, self.vocab_size))\n",
        "            \n",
        "            input_char_batch[0, 0, input_char] = 1.0\n",
        "            \n",
        "            x_batch_out = sample_model.forward(input_char_batch)\n",
        "            \n",
        "            x_softmax = batch_softmax(x_batch_out)\n",
        "            \n",
        "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
        "            \n",
        "            indices.append(input_char)\n",
        "            \n",
        "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
        "        return txt\n",
        "\n",
        "    def train(self, num_iterations, sample_every = 100):\n",
        "        '''\n",
        "        \"글자 생성기\"를 학습\n",
        "        각 반복마다 신경망에 크기가 1인 배치가 입력된다\n",
        "        num_iterations회 반복한다. 매 반복마다 현재 학습된 모델로 생성한 텍스트가 출력된다.\n",
        "        '''\n",
        "        plot_iter = np.zeros((0))\n",
        "        plot_loss = np.zeros((0))\n",
        "        \n",
        "        num_iter = 0\n",
        "        start_pos = 0\n",
        "        \n",
        "        moving_average = deque(maxlen=100)\n",
        "        while num_iter < num_iterations:\n",
        "            \n",
        "            if start_pos + self.sequence_length + self.batch_size + 1 > len(self.data):\n",
        "                start_pos = 0\n",
        "            \n",
        "            ## 모델 수정\n",
        "            inputs_indices, targets_indices = self._generate_inputs_targets(start_pos)\n",
        "\n",
        "            inputs_batch, targets_batch = \\\n",
        "                self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices)\n",
        "            \n",
        "            loss = self.model.single_step(inputs_batch, targets_batch)\n",
        "            self.optim.step()\n",
        "            \n",
        "            moving_average.append(loss)\n",
        "            ma_loss = np.mean(moving_average)\n",
        "            \n",
        "            start_pos += self.batch_size\n",
        "            \n",
        "            plot_iter = np.append(plot_iter, [num_iter])\n",
        "            plot_loss = np.append(plot_loss, [ma_loss])\n",
        "            \n",
        "            if num_iter % 100 == 0:\n",
        "                plt.plot(plot_iter, plot_loss)\n",
        "                display.clear_output(wait=True)\n",
        "                plt.show()\n",
        "                \n",
        "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], \n",
        "                                                 200)\n",
        "                print(sample_text)\n",
        "\n",
        "            num_iter += 1"
      ],
      "metadata": {
        "id": "lwffTQZmKAHM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 파일과 모델을 전달받아 이어지는 글자를 예측하는 트레이너 입니다.\n",
        "\n",
        "자연어 관련 실습을 하기 위해 가져온 트레이너이기 때문에 자세한 설명은 생략하겠습니다."
      ],
      "metadata": {
        "id": "98obgMWfMah0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [RNNLayer(hidden_size=256, output_size=62)]\n",
        "mod = RNNModel(layers=layers,\n",
        "               vocab_size=62, sequence_length=10,\n",
        "               loss=SoftmaxCrossEntropy())\n",
        "optim = SGD(lr=0.001, gradient_clipping=True)\n",
        "trainer = RNNTrainer('input.txt', mod, optim)\n",
        "trainer.train(1000, sample_every=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "vd4bP62hMtqF",
        "outputId": "0a7e51f3-4f8d-4b84-fdb9-840b8ee7855a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "haglaw ahd Fh wo:'ZiNJyFB yPsiEIiw :O woreyVVklEer y chimulmwofeoyTid?Phee\n",
            "IQSFZUinl;QhJvkP ikQw;KtUe\n",
            ";khoahort fhaonZgb!ugor.;;wlZA oa;eyDbiT\n",
            " NlhbmoeLe Pneeh'by,ne ajburPtUs zY xar ary\n",
            "jIsuor.wZwTDH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "간단한 실습을 해봤습니다. 사실 자세한 내용은 잘 이해가 안되는데 직접 구현한 코드로도 실제 실습이 된다는 점을 중요하게 생각하겠습니다."
      ],
      "metadata": {
        "id": "yOYaPcr0N30W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 느낀점"
      ],
      "metadata": {
        "id": "67Mr65FlOEHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드는 엄청 길지 않은데 진짜 이해하는데 한참걸렸던 것 같습니다.\n",
        "\n",
        "RNN, 순환 신경망. 재사용한다는 것 까진 알겠는데 뭐가 어떻게 재사용이 되서 시계열, 자연어 처리에 쓰이는데? 하는 의문을 가졌는데요.\n",
        "\n",
        "100% 해결했다라고 자신있기 말하진 못하겠지만 어느정도 느낌은 받았던 것 같습니다. RNN 구조에 대해 직관적으로 머리 속에 들어온 것 같아요.\n",
        "\n",
        "특히 RNN 층 관련 내용 학습할 때, 긴 시간 씨름하다 이해가 됬을때 정말 기뻤습니다. 또 자동미분부분도 재밌게 했던 것 같아요."
      ],
      "metadata": {
        "id": "nR7Dwvf8OFXH"
      }
    }
  ]
}