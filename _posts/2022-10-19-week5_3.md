---
toc: true
layout: post
description: 부스트캠프 23일차 공부 정리.
image : images/221019.PNG
categories: [BoostCamp, Recommender system, Deep Learning, CTR, DeepFM, DIN, MAB, LinUCB]
title: "[BoostCamp]Day23 Recommender system3"
search_exclude: True
---
# 23일차 공부 정리

## Memorization & Generalization

머신러닝 모델은 기본적으로 Memorization와 Generalization 두 문제를 해결하는 것이 목적입니다.

Memorization(암기)은 함께 빈번히 등장하는 특성 관계를 과거 데이터로부터 학습하는 것이고, Generalization(일반화)은 발생한 적 없는 아이템/특성 조합을 기존 관계로부터 발견한 것 입니다.

추천 시스템 모델에도 마찬가지로 이 문제들을 해결해야 하는데 Memorization은 자주 함께 등장한 아이템을 잘 기억하는 방식으로 Logistic Regression(LR)과 같은 선형 모델이 이를 잘 수행합니다.

일반적으로 Memorization은 오프라인 테스트에서, Generalization은 온라인 테스트에서 중요한 요소입니다.

Generalization은 이전에 잘 등장하지 않은 아이템 조합을 다른 아이템 관계를 잘 분석하였을 때 임베딩을 잘 하여 예측하는 방식으로 FM, DNN과 같은 임베딩 기반 모델이 이를 잘 수행합니다.

좋은 추천시스템 모델을 만들기 위해선 두 문제를 동시에 해결해야 한다는 것에 초점을 맞춰 다음 모델을 살펴보겠습니다.

## DeepFM

![DeepFM](https://user-images.githubusercontent.com/79916736/196593186-729350b6-59a0-453a-a9f6-84802b1d5ba8.png)

DeepFM은 Memorization와 Generalization을 동시에 고려하는 것을 목표로 한 모델입니다.

먼저 필드로 구분된 피처들을 동일한 차원으로 임베딩 합니다. 왼쪽부분은 임베딩 한 벡터를 $V_i$로 취급하는 FM 모델로 Memorization 부분을 담당합니다. 오른쪽 부분은 임베딩한 벡터를 모두 ANN(딥러닝) 연산을 수행합니다.

최종 출력값으로 $Y_{FM}, Y_{DNN}$ 두 값이 나오는데 두 값의 합을 sigmoid에 넣어 최종 y 값을 출력합니다.

본 모델은 CTR(주어진 아이템을 클릭할 확률)을 예측하는 문제로 y는 0과 1로 주어지며 성능이 우수합니다.

왼쪽 모델 부분을 조금 수정한 Wide & Deep 모델도 존재하나 feature engineering이 필요하다는 단점이 있습니다.

## Deep Interest Network

Deep Interest Network(이하 DIN)은 사용자가 기존에 소비한 아이템의 리스트를 사용해 예측 대상 아이템과 이미 소비한 아이템 사이의 관련성을 학습하는 방식입니다.

![DIN](https://user-images.githubusercontent.com/79916736/196597474-0cde1d8e-f245-4e61-92d6-fc585be0d438.png)

관심있게 볼 부분은 역시 User Behaviors 입니다. 후보군과 이전에 소비했던 아이템을 입력값으로 한 Activation Unit을 통과합니다.

이때 Activation Unit은 트렌스포머 구조에 일부를 본따온 것으로 두 아이템 간 상호작용을 구합니다.

Activation Unit을 통과한 뒤 시간을 기준으로 가중합을 구한 뒤 다른 특성들과 합쳐줍니다.

## Behavior Sequence Transformer

Behavior Sequence Transformer(이하 BST)는 User Behaviors을 트렌스포머를 이용해 분석한 모델 입니다.

![BST](https://user-images.githubusercontent.com/79916736/196598152-c15e0768-2e4c-4298-bb3f-1d1cefece6c1.png)

트렌스포머의 인코더 부분을 채용했습니다. User Behaviors 부분의 시퀀스를 고려해 입력값으로 넣었습니다.

트렌스포머의 핵심 기능인 어텐션을 통해 상호 관계성을 모델이 잘 학습하는 것이 장점입니다.

기존 트렌스포머와 차이점은 dropout을 사용했으며 활성화함수로 leakyReLU을 선택했고 인코더 레이어를 여러개 쌓지 않고 1개만 쌓았습니다. User Behaviors의 시퀀셜한 정보가 언어보단 훨씬 덜 중요하기 때문입니다.

또 Positional Encoding은 사인/코사인을 이용한 값을 넣지 않고 아이템 간 시간 차이를 사용하였습니다. 긴 기간을 사이에 두고 구매한 아이템인지 짧은 기간을 두고 구매한 아이템인지에 대한 정보가 중요하기 때문입니다.

앞서 설명한 여러가지 장점 덕분에 BST은 CTR Prediction Task에서 SOTA의 성능을 거두고 있습니다.

## Multi-Armed Bandit

성능이 각기다른 K개의 슬롯머신을 N번 선택할 수 있다면 어느 전략이 가장 좋을 것 같습니까?

가치관에 따라 다르겠지만 크게는 슬롯머신을 exploration(탐색) 하는 것과 탐색한 결과를 바탕으로 가장 좋은 슬롯머신을 선택하는 exploitation(활용) 부분으로 나눌 수 있고 탐색에 힘을 얼마나 주는지는 사용자의 선택입니다.

exploration(탐색)을 적게 할 경우 가장 좋은 슬롯머신을 착각할 확률이 늘어나 손해를 볼 가능성이 생기고 exploration(탐색)을 많이 할 경우 가장 좋은 슬롯머신을 선택할 확률이 늘어나지만 실제로 이득을 얻는 exploitation(활용) 횟수가 줄어든다는 단점이 있습니다.

exploration와 exploitation의 trade-off 관계를 최소화 하기 위한 Epsilon-Greedy Algorithm을 소개합니다.

### Epsilon-Greedy Algorithm

Epsilon-Greedy Algorithm은 exploration(탐색)이 부족할 수도 있다는 가정 하에 나온 알고리즘 입니다.

![Epsilon-Greedy Algorithm](https://user-images.githubusercontent.com/79916736/196599948-fe30657b-cff8-4a41-8ba1-6a185be08d7f.png)

a는 슬롯머신을 나타내는 변수, 입실론은 하이퍼파라미터 입니다. 입실론 만큼의 확률로 exploitation(활용)만 하는 것이 아니라 exploration(탐색)도 수행하는 알고리즘 입니다. 심플하면서도 강력하죠.

### Upper Confidence Bound

![Upper Confidence Bound](https://user-images.githubusercontent.com/79916736/196600137-3ee4a121-e522-401f-bff3-4aa580faed15.png)

이 알고리즘은 적절한 표본을 얻기 전까지 $Q_t(a)$ 결과를 신뢰하지 않겠다는 생각으로 나왔습니다.

$N_t(a)$는 a 슬롯머신의 t 시점 사용 개수로 이 값이 커질수록 전체 값이 작아지는데요.

모든 슬롯머신을 일정 횟수 exploration(탐색) 하고 싶어하는 의지가 담겨있습니다.

### Thompson Sampling

베타 분포를 이용한 방법입니다. 베타 분포가 일정 표본이 있을 때 확률의 기대 분포를 나타내는 분포인 점을 이용합니다.

각 슬롯머신 별 표본으로 베타분포 $\alpha, \beta$을 채웁니다. 그 베타분포를 따르는 랜덤 값을 추출한 뒤 가장 높은 값을 추출한 슬롯머신을 누르는 방식입니다.

슬롯머신을 누르면 또 슬롯머신 별 표본이 쌓일 것이고 가장 좋은 성능을 내는 슬롯머신으로 추측되는 것을 누를 확률이 높아지면서 지속적으로 exploration(탐색)도 진행하는 유용한 방식 입니다.

### MAB를 이용한 추천시스템 모델

유저에게 추천하는 아이템 후보 리스트를 슬롯머신이라고 생각한다면 추천시스템 문제은 앞서 설명한 MAB 문제를 푸는 것과 비슷하게 생각할 수 있습니다.

exploration(탐색)은 지속적으로 변하는 유저의 취향 탐색으로, exploitation(활용)은 유저의 취향에 맞는 아이템 추천으로 생각할 수 있겠네요.

다만 슬롯머신과 다르게 아이템 개수가 상당히 많으므로 클러스터링을 통해 비슷한 유저끼리 그룹화를 한 뒤 해당 그룹 내에서 인기도 기반이나 CF 등 다양한 방법으로 아이템 군을 추린 뒤 MAB 방법을 사용합니다.

또 다른 사용방법으로 유사 아이템 추천이 있습니다. 주어진 아이템과 유사한 아이템을 추천할 때 MF나 Content-Based 기반 모델을 사용해 유사 아이템 후보군을 추린 뒤 MAB 방법을 사용하는 것 입니다.

### LinUCB

LinUCB은 유저의 context 정보에 따라 동일한 action이더라도 다른 reward를 가진다는 것을 가정한 모델입니다.

어떻게 생각하면 추천시스템에서 개인화 추천이 진행되지 않을 수가 없겠죠.

![LinUCB](https://user-images.githubusercontent.com/79916736/196603478-28a51004-0c35-4e88-b6cf-9d5e4f86a0ac.png)

$x^T_{t,a}\theta_a$에서 x는 유저의 context 정보, 세타는 학습 파라미터를 의미합니다. 뒷 항은 계산은 다소 복잡하지만 a가 커질수록 작아지는 항 입니다.

![LinUCB2](https://user-images.githubusercontent.com/79916736/196604064-461ca899-f906-40f7-be90-c436289e4eb6.png)

LinUCB의 간단한 예시 입니다. 각 아이템이 유저의 어떤 특성을 담고 있는지를 세타가 나타내도록 학습합니다.

이 방법은 Naver AiRS 추천 시스템에도 이용됩니다. 구체적으로 설명하면 인기도 기반 필터링으로 exploration(탐색) 대상을 축소한 뒤 LinUCB 방식을 사용해 유저의 취향을 탐색하고 또 활용한다고 합니다.

## 느낀점

계속 비슷하게 느끼는 것인데 추천 시스템에 다양한 방식으 많은 것 같습니다.

지속적으로 어떤 차이가 있는지 고민할 때 스스로가 성장을 할 수 있을 것 같아요.

이번 주 계속 컨디션이 영 별로인데 몸관리도 잘 했으면 좋겠습니다. 화이팅!

** 위 수식과 그림은 부스트캠프 AI Tech 교육 자료를 참고하였습니다.