---
toc: true
layout: post
description: 부스트캠프 21일차 공부 정리.
image : images/221017.png
categories: [BoostCamp, Recommender system, Deep Learning, NGCF, LightGCN, GRU4Rec, FM, FFM]
title: "[BoostCamp]Day21 Recommender system2"
search_exclude: false
---
# 21일차 공부 정리

## Neural Graph Collaborative Filtering

Neural Graph Collaborative Filtering(이하 NGCF) 모델은 이전까지 공부한 모델들의 유저나 아이템 임베딩 레이어에서 유저-아이템 상호작용을 학습하지 못했다는 아이디어에서 시작합니다.

![NGCF](https://user-images.githubusercontent.com/79916736/196130439-dab1d845-c3a7-4211-a180-b28a314164f3.png)

아이템 벡터와 유저 간 연결이 있는 것을 그래프 형식으로 나타낸 그림 입니다.

물론 임베딩 과정 또한 역전파를 통해서 유저와 아이템 간 상호작용이 학습되지만 직접적인 학습은 아닙니다.

NGCF의 핵심 아이디어는 임베딩 레이어에서 유저와 아이템 간 상호작용 정보를 사용하는 것 입니다.

![NGCF2](https://user-images.githubusercontent.com/79916736/196131241-9c39903c-237f-4103-8419-00662228ec0f.png)

![NGCF3](https://user-images.githubusercontent.com/79916736/196131313-73e90884-bfec-4205-adf4-3745313fc7d4.png)

임베딩 과정을 간단히 설명하겠습니다. 우선 아이템, 유저를 간단하게 임베딩 합니다. ($e_i, e_u$)

다음으로 유저 관점으로 봤을 때 그래프로써 1차적으로 연결되어 있는 아이템(쉽게 얘기해서 윗 그림에서 유저 - 아이템 - 유저 식으로 연결되어 있는 내용 의미)의 모든 값의 메세지 벡터($m_{u\leftarrow i}$)를 윗 수식을 사용해 구합니다. 이 때 가중치 행렬 $W_1, W_2$는 공유하며 크기를 정규화 하기 위해 앞에 연결된 유저와 아이템($N_u, N_i$) 수 만큼 나눠줍니다. 이후 LeakyRelu 활성화함수를 사용해 비선형성을 나타내주며 첫번쨰 임베딩을 만듭니다.

![NGCF4](https://user-images.githubusercontent.com/79916736/196132909-6ec7c730-a791-42a4-b134-cd40a662da89.png)

이 과정을 $l$번 반복하면 1차적으로 연결되있는 값 뿐만 아니라 $l$차적으로 연결되있는 정보까지 사용하게 됩니다. 물론 앞에 곱하는 가중치 행렬은 반복할 때 마다 다른 행렬을 사용합니다.

![NGCF5](https://user-images.githubusercontent.com/79916736/196133296-133ea3c8-3f05-4f62-9a93-eb8eb1cce6a3.png)

모든 과정이 끝나면 각 과정마다 임베딩 벡터가 하나씩 나오게 되는데요. 이를 concatenate 하여 최종 임베딩 벡터로 사용합니다.

실제로 모델을 사용하면 반복하는 횟수 L을 너무 많이 쌓으면 오버피팅이 발생하기 쉬워서 3~4번 사용할 때 가장 좋은 성능을 보인다고 합니다.

![NGCF6](https://user-images.githubusercontent.com/79916736/196134532-f2f4ade4-d3a2-4c76-bcb4-e6a952aba5d5.png)

모델의 전체 과정은 다음과 같습니다. 마지막에 유저 임베딩과 아이템 임베딩을 내적해 Y값을 배출합니다.

(concatenate 하는 이유도 같은 반복을 수행한 임베딩 벡터끼리에 내적을 수행한다 생각하면 좋습니다.)

## LightGCN

LightGCN은 앞서 사용한 NGCN의 핵심적인 부분만을 사용해 모델을 가볍게 하고자 했습니다.

![LightGCN](https://user-images.githubusercontent.com/79916736/196135568-590df19a-8a12-48c6-a6c4-bea4d842b504.png)

앞서 사용한 NGCN와 달리 가중치 행렬을 빼버렸습니다. 이전 단계에 연결되있는 아이템 혹은 유저 임베딩 벡터를 단순히 가중합 하는 것만으로도 충분하다고 생각한 것 같습니다. 학습할 파라미터는 오직 첫번째 임베딩 레이어 뿐이니 모델이 가벼워집니다.

![LightGCN2](https://user-images.githubusercontent.com/79916736/196136102-f3c2f270-3a32-4f84-9471-02b9f57ddaa9.png)

최종 임베딩 벡터를 구할 때도 concatenate을 사용하지 않고 여러 단계를 거친 임베딩 벡터에 더 작은 값을 주는 가중합으로 임베딩 벡터를 계산합니다. 가중되는 정도($a_k$)는 하이퍼 파라미터로써 미리 정할수도 있고 학습하는 파라미터로 둘 수도 있습니다. 성능은 비슷하다고 합니다.

파라미터를 덜 쓴 가벼운 모델이기 때문에 NGCN 보다 일반화 성능이 뛰어납니다.

## GRU4Rec

![GRU4Rec](https://user-images.githubusercontent.com/79916736/196138200-e009f549-5757-429a-b468-b5302fe0d247.png)

GRU4Rec은 고객의 선호는 고정되지 않고 계속 바뀐다는 아이디어를 가지고 '지금' 고객이 좋아하는 것은 무엇인가에 초점을 맞춰 시퀀스 정보를 적극적으로 사용한 모델입니다.

시퀀스 정보를 잘 활용하기 위해 RNN모델의 변형인 GRU모델을 핵심층에 사용하습니다.

데이터는 온라인 상에서 쿠키 형태로 저장되는 정보를 사용합니다. 이렇게 되면 데이터 별로 시퀀스 길이가 많이 차이날 수도 있게 되는데 길이가 짧은 데이터는 단독으로 사용되지 않게 유도리 있게 병합하여 사용합니다.

또 Negative Sampling을 할 때 상호작용이 없는 데이터를 모두 사용하지 않고 subset하며 아이템의 인기가 높은데도 상호작용이 없었다면 사용자가 관심이 없는 아이템이라고 가정하기 때문에 인기가 높은 아이템이 더 잘 Sampling 될 수 있게 진행합니다.

## Context-aware Recommender System

Context-aware Recommender System, 컨텍스트 기반 추천 시스템은 지금까지 학습한 CF(협업 필터링) 모델에서 사용하지 않았던 유저나 아이템의 여러 특성들을 고려한 추천 시스템 입니다.

유저-아이템 상호작용 정보만으로 추천을 하는 것 보다 유저와 아이템의 좋은 특성이 있다면 사용하는 것이 모델이 더 고도화 될 것은 당연합니다.

![Context](https://user-images.githubusercontent.com/79916736/196140409-ad9eaa8b-8b3a-4782-91e0-7432407a3e17.png)

이 정보를 사용하기 위해 다음과 같이 어느 입력값에도 대응가능한 원핫인코딩 형식을 사용합니다. 추천 시스템에서 사용하는 데이터는 대부분 범주형 변수인 점도 고려됬습니다.

### Factorization Machine

Factorization Machine(이하 FM)은 SVM 모델과 MF 모델을 결합한 모델입니다.

![FM1](https://user-images.githubusercontent.com/79916736/196141277-0f9ae253-8c69-4adf-a4ea-87515006c46a.png)

유저와 아이템 그리고 기타 특징까지 모두 V라는 임베딩 벡터로 표현하는 것이 첫번째입니다.

이 정보를 사용해 다음과 같은 수식을 적용하여 Y를 예측합니다. 이 때 $<V_i,V_j>$은 벡터 내적 입니다.

![FM](https://user-images.githubusercontent.com/79916736/196143042-e46b26b4-38f3-44a2-8a36-93f845cea687.png)

다음 예시를 확인하면 이해가 다소 편할 수 있습니다. 결국 원핫인코딩은 대부분 0을 만들 기 때문에 위 수식에서 대부분 값은 0이 된다고 생각하면 생각보다 단순한 문제를 푼다고 생각될 수 있어요.

### Field-aware Factorization Machine

Field-aware Factorization Machine(이하 FFM)은 앞서 본 FM모델을 조금 변형한 모델입니다.

![FFM](https://user-images.githubusercontent.com/79916736/196143565-e0f1fbd3-3d07-4bc7-9567-8989109a1c5e.png)

앞선 FM 모델과 달라진 수식이 $<V_{i,f_j},V_{j,f_i}>$ 부분 입니다. 필드마다 다른 임베딩 벡터를 사용한다는 것이 특징입니다.

여기서 필드는 모델을 설계할 때 정의되는 정보로 같은 의미를 같는 변수들의 집합으로 설정합니다. 앞선 FM 그림에서 User, Movie, Other Movies rated, Time, Last Movie rated 등이 모두 필드 입니다.

FM과 비교하여 임베딩 벡터가 필드마다 다르기 때문에 훨씬 파라미터가 많아지는 복잡한 모델이 됩니다.

필드별로 구분하는 것이 유용하지 않을 때는 파라미터만 늘린 꼴이 되어 오버피팅에 유려가 있습니다.

## 느낀점

추천시스템 이론을 추가로 학습했는데 SOTA가 있는 CNN, NLP와  주어진 상황에 따라 다르게 적용할 수 있는 모델이 많아 부담되지만 매력 있는 분야인 것 같습니다.

다음주에 대회가 진행되는데 이에 맞춰 학습을 진행해 팀원들과 좋은 성적을 거둘 수 있도록 노력하겠습니다.

다시 돌아온 주 5일 월요일인데 의욕이 떨어지지만 힘내보겠습니다. 화이팅!!

** 위 수식과 그림은 부스트캠프 AI Tech 교육 자료를 참고하였습니다.