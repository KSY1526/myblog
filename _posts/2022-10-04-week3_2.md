---
toc: true
layout: post
description: 부스트캠프 12일차 공부 정리.
image : images/221004.PNG
categories: [BoostCamp, Pytorch, DeepLearning, CNN]
title: "[BoostCamp]Day12 CNN"
---
# 12일차 공부 정리
## CNN 파라미터

![cnn1](https://user-images.githubusercontent.com/79916736/193746267-a3aeb349-162a-4273-af05-4aa0cb315047.png)

CNN 모델을 사용한다면 사용되는 파라미터 수는 사용하는 (커널 가로) X (커널 세로) X (입력 채널) X (출력 채널) 입니다.

즉, 모든 노드간 연결이 되어있는 일반적인 full connected layer보다는 커널을 사용하는 CNN이 파라미터수가 작습니다.

파라미터수가 늘어나면 대체로 일반화성능(테스트 데이터에 잘 들어맞는지)이 떨어지기 때문에 파라미터 수를 줄이는 것이 관건입니다.

파라미터 수를 줄이는 것에 초점을 두고 과거 CNN 모델에 발전을 관찰하겠습니다.

## CNN 모델 발전과정

### AlexNet

앞선 포스팅에서 말한대로 딥러닝 모델이 처음으로 전통적인 모델을 이긴 AlexNet입니다.

ReLU를 사용하였으며 Data augmatation, dropout 등 지금 많이 사용하는 기법들을 채택했습니다.

다만 11X11 커널을 사용해 8 layers를 사용했음에도 파라미터 수가 정말 많아졌습니다.

### VGGNet

3X3 커널을 사용했다는 점이 매우 중요합니다. 커널 차원을 줄이게되면 파라미터 수가 많이 줄어드는데요. 

![CNN2](https://user-images.githubusercontent.com/79916736/193748708-36a9ace9-9049-4576-99ae-9993ef0e1dff.png)

3X3 커널을 2번 통과하게 되면 커버되는 영역(Receptive field)은 5X5와 같게 됩니다.

그러면서 사용하는 파라미터 수는 윗 그림을 참고하면 상당히 감소함을 알 수 있습니다.

커널 크기를 축소해 파라미터 수를 줄였기 때문에 층을 늘릴 여유가 생겨 층을 늘려줍니다.(19-layers)

이후 대부분의 CNN 모델은 5X5보다 커널크기가 커지지 않게 됩니다.

### GoogLeNet

1X1 Conv를 파라미터 수를 줄이기 위해 적극적으로 사용한 모델입니다.

![cnn3](https://user-images.githubusercontent.com/79916736/193749928-c3c5982e-5237-422c-87e5-3b86465dd8e5.png)

그림 우측 모델은 중간에 1X1 Conv 층을 집어 넣었습니다. 1X1 층은 출력 채널의 크기를 줄여주는 역할을 합니다.

CNN 모델 파라미터는 (커널 가로) X (커널 세로) X (입력 채널) X (출력 채널) 인데 입력 채널 크기를 줄여 파라미터 수를 큰 폭으로 감소할 수 있게 됩니다.

![CNN4](https://user-images.githubusercontent.com/79916736/193750271-7d8d40a2-0cca-4ced-b37b-6bf228ed16c3.png)

실제로 1X1 Conv을 사용한 GoogLeNet이 파라미터 수를 큰 폭으로 줄인 모습을 확인할 수 있습니다.

### ResNet

깊게 쌓은 neural networks은 기울기 소실 문제 때문에 일반화 성능이 보통 좋지 못합니다.

이 문제를 해결하고자 ResNet은 한번의 작은 모델 작업이 돌아간 후에 입력값을 다시 더해줍니다.

좀더 쉽게 말하면 f(x) <= x + f(x)로 이해하면 편할 것 같습니다.

이렇게 되면 미분을 했을 때 더해진 x가 1이 되어 기울기 소실 문제가 해결됩니다.

ResNet이 층을 깊게 쌓아도 일반화 성능이 떨어지지 않는 다는 점을 잘 보여주었습니다.

### DenseNet

DenseNet은 ResNet 부분에서 입력값을 출력값에 단순히 더해주는 것이 아니라 concatenation 해주는 방식입니다.

이런식으로 concatenation을 하게되면 사이즈가 커지기 때문에 Pooling 처리를 적절히 진행해주었습니다.

## Semantic Segmentation

<추후 업데이트 예정>

## 느낀점

강의에서 방향성을 정하고 CNN 모델의 발전과정을 살펴보니 이해가 정말 잘 됬던 것 같습니다.

혼자 조금씩 어설프게 보았던 부분을 깔끔하게 딱딱 설명해주셔서 공부하는데 정말 효율적이였습니다.

피어세션(조별학습) 시간에 동료들이 CNN을 이용한 구체적인 Semantic Segmentation, Detection 경험을 얘기해주어서 이해에 도움이 많이 되었습니다.

** 위 수식과 그림은 부스트캠프 AI Tech 교육 자료를 참고하였습니다.