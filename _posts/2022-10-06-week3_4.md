---
toc: true
layout: post
description: 부스트캠프 14일차 공부 정리.
image : images/221006.PNG
categories: [BoostCamp, Pytorch, DeepLearning, Generative Models, GAN]
title: "[BoostCamp]Day14 Generative Models"
---
# 14일차 공부 정리

## Autoregressive Models

![Autoregressive1](https://user-images.githubusercontent.com/79916736/194295764-ccde1c5a-0c8a-46ee-bf0b-257dc4499241.png)

X가 2개의 값만 가질 때 1부터 n까지 모두 독립인 항이라면 파라미터 수는 n개입니다.

![Autoregressive2](https://user-images.githubusercontent.com/79916736/194295930-e90343ae-6249-410c-86a8-d7e507925c70.png)

그렇다면 각 값이 바로 직전 값에 연관이 있다고 가정해보죠. 이 때의 파라미터는 (2n-1)입니다.

우리가 어느 생성 모델을 만들 때 각 인자가 모두 독립이라고 가정하면 모델이 심플하나 의미있는 생성물이라고 하기 힘듭니다. 그렇다고 모든 변수가 전부 연관되어 있다면 파라미터가 정말 많은 모델이 되어 학습하기가 힘들어집니다.

이런 이유로 각 인자는 하나 혹은 적은 수의 종속을 가지게 되고 이를 Autoregressive Models 이라고 합니다.

## Generative Models Intro

우리가 하고 싶은건 강아지 그림을 생성하는 모델을 만드는 것 입니다. 그 것이 존재한다는 것은 알지만 정확히 강아지 그림을 생성하는 모델을 찾기란 불가능에 가깝습니다.

그렇기 때문에 이와 유사한 모델을 찾기 위한 노력을 해야하고, 두 모델이 유사하다를 판단할 수 있는 지표가 존재해야합니다.

이 지표로 통계학적으로 두 분포의 근사적인 거리를 표현하는 여러가지 방법 중 KL-divergence 지표를 사용하면 Variational Autoencoder(VAE)이고, Jensen-Shannon divergence 지표를 사용한다고 해석되는 것이 Generative Adversarial Network(GAN) 입니다.

## Variational Autoencoder(VAE)

![VAE1](https://user-images.githubusercontent.com/79916736/194321117-feb41462-08d3-4de9-9fb7-87e449f93171.png)

Variational Autoencoder 모델의 원리를 알아봅시다. KL-divergence 지표를 사용한다고 하지만 강아지 그림을 생성하는 완벽한 모델(분포)을 구하려고 시도를 하기 때문에 이 지표를 이용한다는 것은 웃긴 얘기죠.

대신 실제 강아지 사진을 완벽한 분포에서 나왔다고 가정한 채로 우리의 모델을 학습할 수 있겠습니다. 위 사진에서 D를 실제 강아지 사진으로 생각합니다. 이후 MLE 개념을 사용해 모델을 학습합니다. 모델을 학습한다는 것은 강아지 사진을 만들 분포를 더 정교하게 만든다는 얘기 입니다.

![VAE2](https://user-images.githubusercontent.com/79916736/194321817-4a77a3fe-1365-4545-bfdb-c37cc6d2dbfd.png)

복잡한 계산은 생략하고 위 그림에서 볼 때 ELBO값을 키우는 것이 목표입니다. 우변 식을 보면 Reconstruction Term을 키우고 Prior Fitting Term을 낮추는 식으로 한다는 소리죠.

우선 인코더 디코더를 간단히 정의하면 인코더는 입력값을 바탕으로 잠재변수를 만드는 모델, 디코더는 잠재변수를 바탕으로 입력값과 유사한 출력을 하길 원하는 모델 입니다.

Reconstruction Term은 오토인코더 로스를 최소화 시키는 것 입니다. 오토인코더는 통과 전 값과 인코더와 디코더를 모두 통과 시킨 뒤 나오는 값간의 차이를 얘기합니다.

Prior Fitting Term은 인코더 부분에서 나오는 Z(잠재 변수)의 분포를 완벽한 분포로부터 나오는 Z의 분포와 비슷하게 하고 싶다는 말 입니다.

이 중 Reconstruction Term만을 사용한 것을 GAN이라고 부르며 GAN의 이론은 [링크](https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/tensorflow/gan/2022/02/15/handssu6.html)을 참고해주세요.

## Diffusion Models

Diffusion Models은 2020년 기준 핫한 모델입니다. 주목을 받은 이유가 당연히 성능이 뛰어나서 인 것 같습니다.

![Diffusion Models](https://user-images.githubusercontent.com/79916736/194324799-e2238c74-f682-478d-b6ef-e1d7f4824e38.png)

노이즈만 존재하는 값에서 의미있는 이미지를 생성하는 원리로 학습을 할때는 원래 이미지에서 노이즈를 계속 주입하고, 이를 리버스한 방식으로 이미지를 생성합니다. 이 때 여러 단계에 걸처서 노이즈를 서서히 넣게 됩니다.

수식은 많이 복잡해서 생략합니다. 다만 성능이 뛰어난 생성 모델이기 때문에 한번 간단히 집고 넘어갑니다.

## 느낀점

오늘 배운 부분은 나름 통계학과이지만 수식을 배우다가 참 지쳤던 것 같습니다.

나름 설명이라고 글을 작성했는데 오개념이 많이 포함되지 않았나 걱정됩니다.

그래도 지금 많은 지식을 주입하는 단계이기 때문에 100% 이해하지 못했더라도 다음 스텝을 열심히 밟겠습니다.

이렇게 공부한게 잊어먹는 것도 많겠지만 남는게 분명 많을 거라 믿습니다.

level2 이후 팀원을 구하기 위해 글이 올라오는 걸 보니 긴장이 되네요. 좋은 팀원 잘 구했으면 좋겠습니다.

** 위 수식과 그림은 부스트캠프 AI Tech 교육 자료를 참고하였습니다.