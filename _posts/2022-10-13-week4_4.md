---
toc: true
layout: post
description: 부스트캠프 19일차 공부 정리.
image : images/221013.png
categories: [BoostCamp, Collaborative Filtering, Recommender system, Item2Vec, ANN, ANNOY, Deep Learning, YouTube, Autoencoder, CDAE]
title: "[BoostCamp]Day19 Recommender system1"
search_exclude: false
---
# 19일차 공부 정리

## Item2Vec

단어 간 유사성을 고려한 임베딩을 하기 위해서 Word2Vec 학습 방식을 사용합니다. [Word2Vec 설명](https://ksy1526.github.io/myblog/ssuda/book/deep%20learning/natural%20language/bert/tokenizer/2022/03/18/SSUDA22_4.html).

추천 시스템 모델에서도 유저, 아이템 임베딩을 위해 Word2Vec 방식 중 Skip-Gram with Negative Sampling(SGNS)을 응용한 Item2Vec 이 존재합니다.

Item2Vec의 내용을 요약하면 같이 산 아이템 리스트를 word 임베딩 하듯이 아이템 변수를 임베딩 합니다.

구체적인 예시를 들면 유저 I가 A,B,C 상품을 동시에 소비했다고 가정하면 A와 B => 1, B와 C => 1, A와 C => 1, A와 D => 0 이런식으로 동시에 소비한 데이터는 1, 그렇지 않은 데이터는 0 라벨을 부여한 뒤 이 정보를 기반으로 아이템 벡터 간 임베딩을 수행합니다.

여러 분야에서 응용되는 Item2Vec 으론
- 아프리카 TV의 Live2Vec {유저의 시청 이력}, {라이브 방송}
- Spotify의 Song2Vec {유저의 플레이리스트}, {노래}
- 이커머스 시장의 Prod2Vec {유저의 쇼핑 세션}, {상품}

등이 있습니다.

## ANNOY

이전에 배운 MF 모델이나 Item2Vec 방식 모두 아이템과 유저 정보를 임베딩 한 것 입니다. 그렇다면 실제 추천 모델을 서빙할 때는 타겟 유저 벡터와 유사한 임베딩 벡터를 가진 아이템을 찾아야 하는데 Brute Force 하게 가장 유사한 임베딩 벡터를 찾는 것은 시간 코스트가 많이 들어 추천 시스템 모델에 부적절합니다.

가장 유사한 아이템 임베딩 벡터를 찾는 것을 포기하고 꽤 많이 유사한 아이템 임베딩 벡터를 훨씬 빠른 속도로 찾는 Brute Force 이외에 다양한 방법들을 사용하는 것이 현명할 것 입니다.

앞서 말한 것이 Approximate Nearest Neighbor(이하 ANN) 방법의 필요성입니다. 그 중 한 기법인 ANNOY에 대해 알아봅시다.

ANNOY은 Spotify에서 만든 기법으로 주어진 벡터들을 여러 개의 subset으로 나눠 트리 형태로 탐색하는 방법입니다.

랜덤하게 2개 벡터를 골라 hyperplane로 Vector Space를 나누는 방식을 subspace가 미리 지정한 K개 이하가 될 때 까지 반복합니다. 이 때 subspace을 트리 계층 구조로 기록합니다.

이후 타겟 유저 벡터가 속해 있는 subspace 안에 있는 아이템 벡터들만을 활용해 유사도를 구하게 됩니다. 이런 방식으로 하면 탐색 시간이 현저히 줄어듭니다.

![ANNOY](https://user-images.githubusercontent.com/79916736/195502649-34b7b40b-04af-4c61-8b40-878772265626.png)

다만 ANNOY 방식은 타겟 유저 벡터와 가장 근접한 아이템 벡터가 다른(옆) subspace 안에 존재하면 그 아이템 벡터를 찾지 못한다는 단점이 존재합니다.

이를 개선하기 위해 priority queue을 사용해 가까운 다른 subspace까지 탐색하는 방법, 이 binary tree 자체를 여러번 하는 방법이 존재하는데요. 시간을 더 투자해서 조금 더 정확도 높은 아이템을 찾기 위한 노력입니다.

number_of_trees(binary tree의 수), search_k(탐색하는 subspace의 수)를 하이퍼파라미터로 사용하여 시간과 정확도 사이 trade-off를 사용자가 조절할 수 있습니다.

이 방법 이외에도 Hierarchical Navigable Small World Graphs (HNSW), Inverted File Index (IVF), Product Quantization - Compression 방법이 존재한다는 소개를 받아서 간단하게는 알아봤는데 여유가 있을 때 추가 학습하여 블로그에 정리하겠습니다.

## Recommender System with MLP

Neural Collaborative Filtering 은 MF 모델의 한계를 신경망 기반의 구조를 사용해 극복하자는 [논문](https://dl.acm.org/doi/pdf/10.1145/3038912.3052569?casa_token=vTVZLCKNe5wAAAAA:9am227a6HNfs7T9D0f4jLqj4lVaa3Ht5iejdyokMol6THszHfo6A4Wfz9l4TMgRtFjDtWVUxLypV-aA)입니다.

MF 모델은 R을 $P^TQ$로 표현하는데 이 방식은 선형 구조를 벗어날 수 없습니다.

선형 구조를 왜 벗어나지 못하는지 간략하게 예시를 들면 

![MF](https://user-images.githubusercontent.com/79916736/195505952-ebd416ff-ca64-445f-abc7-273e8cf7e42d.png)

다음 그림에서 U4가 나오기 이전 상황에서 자카드 유사도 기준으로 U1은 U3보다 U2와 더 밀접하기 때문에 P1,P2,P3 임베딩 벡터는 그림과 같은 모양으로 구성될 것 입니다. (2차원 임베딩 가정, U1은 U3보다 U2와 유사하고 U3은 U1보다 U2와 유사함)

이 때 U1과 매우 유사한 U4가 등장했다고 가정합니다. U4는 U1과 가까우기 때문에 파란색 선이 P4의 후보가 됩니다. 다만 두 후보 벡터 모두 P3보다 P2가 가깝게 되는데요. 이 때 자카드 유사도 기준 U4는 U2보단 U3에 가깝습니다. 

즉 선형 구조 모델에서는 이런 모순적인 문제가 발생하기 쉽고 이를 해결하기 위해서는 차원을 높여야 하는데 고차원 임베딩은 오버피팅을 유발하기 쉽습니다. 즉 일부 모순을 극복하고자 함부로 차원을 늘리게 될 수 없겠죠.

근본적인 원인을 해결하려면 모델이 비선형 구조를 가지는 것 입니다. 활성화 함수를 장착한 딥러닝 모델이 이 문제를 해결할 수 있다는 아이디어로 MF 모델에 대한으로 제시된 것 입니다.

![MLP](https://user-images.githubusercontent.com/79916736/195507003-9904b109-38e8-48b5-a7ee-217f7341aa98.png)

모델 구조는 일반 딥러닝을 잘 이해하고 있다면 간단합니다. 유저와 아이템을 임베딩 층을 거치게 한 뒤 일반 MLP 층을 거치고 마지막에 소프트맥스 함수를 사용해 타겟을 맞추며 가중치를 업데이트 하는 방식입니다.

이 때 Y는 user와 item 사이의 관련도 입니다.

![MLP+](https://user-images.githubusercontent.com/79916736/195507391-91ae706b-7441-4a8f-b4a6-1a434c7aa337.png)

앞서 구한 MLP 모델과 MF(GMF) 모델을 결합한 앙상블 모델입니다. 논문에서는 이 모델이 두 모델의 장점만을 살려 효과가 좋게 나온다고 합니다. 두 모델은 서로 다른 임베딩 벡터를 사용한다는 것도 기억하면 좋을 것 같아요.

## YouTube Recommendation

앞서 학습한 내용과 달리 추천 시스템 모델을 실제 서빙에 도입한 유튜브 알고리즘에 대해 알아보겠습니다.

유튜브 추천 모델 서빙은 다음과 같은 문제를 극복해야 합니다.
- Scale : 거대한 유저와 아이템(동영상), 제한된 컴퓨터 파워
- Freshness : 새로 업로드 된 동영상을 적절하게 추천해야함
- Noise : 높은 Sparsity, 다양한 외부 요인으로 유저 행동 예측이 어려움

이 문제를 해결하고자 모델 과정을 Candidate Generation 와 Ranking 2가지로 나눴습니다. 

Candidate Generation은 TOP N 출력물을 내는데 N은 수백정도의 수로 후보군을 추리는 과정이며 Ranking은 추린 N개의 레이팅을 구한 과정 입니다. 연산을 가볍게 하기 위한 작업 입니다.

자세히 살펴보기 전에 YouTube Recommendation 시스템은 성과가 좋은 추천 모델 서빙 과정을 논문으로 살펴본다는 점에서 의의가 큽니다. 시간이 허락하면 전문을 한번 읽어보도록 노력하겠습니다.

### Candidate Generation

![YouTube](https://user-images.githubusercontent.com/79916736/195508961-da13dad9-8d30-4910-91df-9247c20dd867.png)

Candidate Generation(이하 CG) 구조의 큰 흐름 입니다. 

그림을 보시면 유저 벡터 u를 만들기 까지 많은 과정이 있는 것을 확인할 수 있는데요. 입력값으로 다음과 같은 유저의 정보를 넣습니다. 
- 시청한 동영상(임베딩)
- 검색 단어(임베딩) 
- 개인 정보(성별, 나이 등 임베딩)
- Example Age(시청 로그가 학습 시점으로 부터 경과한 정도, 과거 데이터 위주로 편향되지 않게)

이 정보들을 모두 concatenate 하여 입력값으로 넣으면 MLP 층을 거쳐 유저 정보가 담긴 임베딩 벡터로 출력됩니다. 출력된 유저 벡터를 동영상 임베딩 벡터 내적한 뒤 softmax 한 결과가 동영상을 볼 확률과 맞도록 학습합니다.

이렇게 모델의 모든 파라미터를 학습한 이후 유저 벡터와 모든 동영상 벡터를 내적을 계산한 뒤 다양한 ANN 방법을 사용해 상위 N개 비디오를 추정합니다.

### Ranking

![Ranking](https://user-images.githubusercontent.com/79916736/195513285-17c92127-77a4-4aca-bf6d-2997d6b7378a.png)

CG 단계에서 생성한 동영상 N개를 input으로 하여 최종 추천할 동영상의 순위를 매기는 과정입니다. N의 개수가 크지 않기 때문에 보다 복잡한 연산도 수행 가능합니다.

보다 복잡한 이용자 정보로 다음과 같은 정보를 추가로 입력값에 넣습니다.

- 이용자가 사용하는 언어
- 주로 시청한 동영상의 언어
- 특정 채널에서 얼마나 많은 영상을 봤는지
- 특정 토픽의 동영상을 본 지 얼마나 지났는지
- 영상의 과거 시청 여부

어떤 정보를 넣는지 선택하는 것은 도메인 전문가의 분야이니 가볍게 생각하고 넘어가면 좋겠습니다.

또 Loss function을 단순 binary(봤다 1, 아니다 0)가 아닌 비디오 시청 시간을 가중치로 하는 cross-entropy을 사용함으로써 낚시성 컨텐츠를 서빙하지 않도록 합니다.

## CDAE

CDAE은 Denoising Autoencoder를 CF에 적용해 top-N 추천에 활용한 [논문](https://alicezheng.org/papers/wsdm16-cdae.pdf) 입니다.

![CDAE](https://user-images.githubusercontent.com/79916736/195515122-e8ef5d59-0a94-4973-a19a-830c219a006c.png)

기존 오토인코더를 이용한 추천시스템(AutoRec)을 조금 응용했는데요. 인풋 값에 노이즈를 주고 User 정보가 담긴 노드를 추가했습니다. 또 문제 단순화를 위해 유저-아이템 상호 작용 정보를 이진(0 또는 1) 정보로 바꿔서 학습 데이터로 사용했습니다.

논문에서는 기존 모델보다 성능이 뛰어나다고 주장합니다.

## 느낀점

이번주 목표한 추천시스템 6강까지 새로 알게 된 내용이 참 많았는데 시간 갈아서 잘 정리해 힘들긴 하지만 매우 뿌듯합니다.

이전에 롯데멤버스 공모전을 통해 추천시스템 모델을 구축하였는데요. 이론을 차근차근 공부해보니 맨땅에 헤딩을 제대로 했던 것 같습니다. 그래도 소중한 경험이라 생각합니다. 이해가 잘 되는 역할을 한 것 같아요.

이해가 된 것 같다가도 다시 처다보면 이게 뭐지 싶은데 실제로 데이터를 이용해 모델 구현을 해보면 내가 배웠던 게 이런거구나 감이 잡힐 것 같아요. 물론 고생 많이 할 제가 보입니다.

추천시스템 모델 기초를 배운 한 주인 것 같은데 저에게 맞는 분야를 잘 정한 것 같습니다. 문제정의가 정해저 있다기 보다 상황에 따라 유연하게 적용하는 것이 제가 추구하던 생각과 일치하는 것 같아요.

YouTube Recommendation 등 읽어보고 싶은 논문이 늘어나는데 부스트캠프는 시간 여유를 허락하지 않는 것 같습니다. ㅋㅋㅋ.

** 위 수식과 그림은 부스트캠프 AI Tech 교육 자료를 참고하였습니다.