---
toc: true
layout: post
description: 부스트캠프 13일차 공부 정리.
image : images/221005.PNG
categories: [BoostCamp, Pytorch, DeepLearning, RNN, LSTM, GRU, Transformer]
title: "[BoostCamp]Day13 RNN, LSTM, Transformer"
---
# 13일차 공부 정리

## RNN

RNN은 길이가 얼마인지 정확히 모르거나 유동적인 순서가 있는 데이터를 다룰 때 매우 유용한 방식입니다.

Hidden state을 이용해 이전정보를 계속 요약해 가기 때문에 상당히 효과적입니다.

하지만 입력값의 길이가 길어질수록 오래 전 입력된 값의 기억이 사라져갑니다.(이전에 자세히 다뤘습니다.)

## LSTM 이론

RNN이 가진 오래된 정보를 소실한다는 단점을 극복하기 위해 LSTM을 사용합니다.

![lstm1](https://user-images.githubusercontent.com/79916736/193789678-5e4ef31d-1f54-41f4-831c-02388f90cc52.png)

우선 Hidden state가 hidden state와 cell state 두 개로 분리되어 있습니다.

hidden state은 실제 Output으로 전달되는 값이고 cell state은 잠재적으로 다음 과정에 전달되는 역할입니다.

Gate로 먼저 Forget Gate를 살펴보면 이전 hidden 값과 현재 input 값을 concatenation 하여 가중치 행렬과 곱한 후 시그모이드 활성함수를 거칩니다. 시그모이드 함수를 거쳤기 때문에 이 값과 이전 cell 값 간 행렬곱(실제로는 벡터내적곱) 연산을 하기 때문에 이전 cell 값의 일부 정보를 지워주게 됩니다.

다음으로 Input Gata를 살펴보면 이전 hidden 값과 현재 input 값을 concatenation 한 값을 형태가 다른 두 번에 연산을 진행하게 됩니다. 이 때 첫번째 시그모이드 활성함수를 거치는 것은 마찬가지로 신호등 역할을 하게 되고 두번째 tanh 함수를 거치는 것은 새로운 cell 데이터를 생성해준다고 생각하면 됩니다.

정리하면 cell 값은 Forget Gate에서 나오는 값과 행렬곱을 진행해 일부 정보를 지워주고, Input Gata에서 나오는 값을 새로운 정보로써 더해주게 됩니다.

마지막으로 Output Gate는 마찬가지로 이전 hidden 값과 현재 input 값을 concatenation 한 뒤 시그모이드 활성함수를 거친 값을 사용하고, Forget Gate와 Input Gate를 통과하고 tanh 활성화 함수를 거친 cell 값과 행렬곱 연산을 한 것을 다음 hidden 값으로 전달하게 됩니다. 이 때 출력된 hidden 값은 output 값으로 활용되기도 합니다.

## LSTM 간단한 실습

~~~python
# LSTM 선언
nn.LSTM(
    input_size=self.xdim, # 입력되는 X 길이.(한 값 당) 
    hidden_size=self.hdim, # 히든 층 길이.
    num_layers=self.n_layer, # LSTM 층 개수. (디폴트 = 1)
    batch_first=True # 보통 True 사용.
)

# 초기 값 설정
h0 = torch.zeros(
            # (층 개수, 배치 사이즈, 히든 층 길이)
            self.n_layer, # 층 개수
            x.size(0), # 배치 사이즈
            self.hdim # 히든 층 길이            
).to(device)

c0 = torch.zeros( # ho와 마찬가지임.
            self.n_layer, x.size(0), self.hdim
).to(device)

# LSTM 사용
## x : [배치 수, 입력되는 x 개수(단어 개수), x 길이(단어 길이, xdim)]
## ho, co : [층 개수(보통 1), 배치 수, 히든 층 길이]
## rnn_out : [배치 수, 입력된 x 개수(뒷 1개만 필요), 히든 층 길이]
rnn_out,(hn,cn) = self.lstm(x, (h0,c0)) 

# 마지막 부분 linear 사용
## Linear 선언, (히든 층 개수, 출력 층 개수)
self.linear = nn.Linear(self.hdim,self.ydim)

## linear로 full-conneted layer를 달아줌.
out = self.linear(
        rnn_out[:, -1:] # 마지막 히든 층에 있는 값이 중요
        # 또는 hn.view(-1, self.hdim)도 가능
).view([-1,self.ydim]) 
~~~

## GRU

![gru](https://user-images.githubusercontent.com/79916736/193816177-c31c437f-2b2e-4dfe-aea2-a611d042e267.png)

앞서 살펴본 LSTM과 비슷한 논리로 동작하는 또 다른 모델 GRU입니다.

LSTM과에 차이점은 cell state가 따로 없이 hidden state은 존재하지 않고 Gate도 보다 단순하기 때문에 파라미터 수가 상대적으로 적게 되어 데이터에 따라 조금 다르긴 하지만 일반화 성능이 괜찮은 경우가 꽤 있다고 합니다.

RNN 대비 LSTM, GRU 두 모델 모두 많이 개선되었지만 여전히 문제점을 완전히 해결 못하였고 트렌스포머 모델이 나오며 최근에 자주 쓰이는 모델은 아니게 되었습니다.

## Transformer

LSTM, GRU 모델 역시 오래된 정보를 소실한다는 단점을 완벽히 극복하지 못하였습니다. 

2017년 문장 전체구조를 기계가 이해하기 좋은형식인 Transformer라는 모델이 나오게 됩니다.

이전에 한번 열심히 학습한 경험이 있어 더 자세한 내용은 링크를 첨부합니다.

(https://ksy1526.github.io/myblog/ssuda/book/deep%20learning/natural%20language/2022/03/04/SSUDA22_1.html)

### Transformer Q, K, V

우선 입력 데이터 형태가 여러가지가 될 수 있지만 임베딩 된 단어들이 입력된다고 가정하겠습니다.

크기가 어떻게 임베딩 되는지는 자유지만 모든 단어는 동일한 크기의 임베딩이 되어있다고 생각합니다.

입력 데이터 예시 입니다.

~~~python
# I was car, 임베딩 크기 4일때
[
    [0.4, 0.3, 0.5, 0.2], # I
    [0.2, 0.7, 0.6, 0.6], # was
    [0.5, 0.9, 0.1, 0.2]  # car
]
~~~

각 단어마다 Q 벡터, K 벡터, V 벡터를 생성하게 됩니다. 이 때 벡터의 길이는 일반적으로 Q,K,V 모두 같지만 V는 달라도 상관이 없습니다.

이후 Q 행렬(단어마다 벡터, 가로로 쌓으면 행렬이 되겠죠)와 K 행렬을*(전치후) 행렬곱 합니다.

이 과정을 직관적으로 설명하면 Q는 단어 자체, K는 Q 단어와 얼마나 연관성이 있는지 알려주는 행렬입니다. 즉 Q와 K의 행렬곱을 하면 단어와 단어사이의 관련성을 나타내는 score 행렬이 나오게 됩니다. 이 때 score 행렬의 사이즈는 (단어 수) X (단어 수)로 행 기준으로 관찰했을 때 해당 단어가 문장 내 다른 단어와 얼마나 상호작용하는지를 나타냅니다.

그 다음 score 행렬을 K 벡터의 길이에 루트값으로 나눠준 뒤 행 단위로 softmax를 취해줘서 해당 단어가 문장 내 다른 단어와 얼마나 상호작용하는지를 확률로써 나타냅니다.

마지막으로 계산 된 score 행렬을 V와 행렬곱을 합니다. 직관적으로 설명하면 한 단어가 문장 내에 모든 단어들로 표현하는데, score 값이 높은 단어의 V 벡터 값을 더 많이 취급해준다는 의미입니다. 이렇게 되면 V는 설명되는 단어벡터로 생각할 수 있겠죠.

위 과정을 하나의 그림으로 정리했습니다.

![Transformer1](https://user-images.githubusercontent.com/79916736/193984823-b096a93b-3c2d-43b8-b2cd-3056e4e0abe5.png)


### 멀티 헤드 어텐션

앞서 설명한 연산을 여러번 수행해서 Z 행렬을 여러 개 구해서 세로로 concatenation 시켜준 것이 멀티 헤드 어텐션입니다.

멀티 헤드 어텐션을 수행하면 벡터 길이 값이 세로로 커지게 되는데 가중치 행렬곱을 수행해서 원래 크기로 돌아가게 만듭니다. Transformer는 이런 과정을 여러번 반복하기 때문에 원래 크기를 유지하는 것이 중요합니다.

이론은 이렇게 이해했는데 실제 구현 방식은 조금 다릅니다. 밑 부분에 다시 설명하겠습니다.

![Transformer2](https://user-images.githubusercontent.com/79916736/193985869-f4e893cf-9f3d-4383-a89e-abf5440e4836.png)

### 코드 구현(차원을 집중해서)

~~~python
class MultiHeadAttention(nn.Module):
    def __init__(
        self,
        d_feat=128, # Q,K,V 벡터 길이 
        n_head=5, # 멀티 헤드가 몇개인지
        actv=F.relu, USE_BIAS=True, dropout_p=0.1, device=None
        ):

        super(MultiHeadAttention,self).__init__()

        if (d_feat%n_head) != 0:
            raise ValueError("d_feat(%d) should be divisible by b_head(%d)"%(d_feat,n_head)) 
        self.d_feat = d_feat # Q,K,V 벡터 길이
        self.n_head = n_head # 멀티 헤드가 몇개인지
        self.d_head = self.d_feat // self.n_head
        self.actv = actv
        self.USE_BIAS = USE_BIAS
        self.dropout_p = dropout_p

        self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)
        self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)
        self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)
        self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)

        self.dropout = nn.Dropout(p=self.dropout_p)
    
    def forward(self,Q,K,V,mask=None):
        """
        :param Q: [배치 수, 단어 개수, 임베딩 길이]
        :param K: [배치 수, 단어 개수, 임베딩 길이]
        :param V: [배치 수, 단어 개수, 임베딩 길이] 
        n_K and n_V must be the same 
        """
        n_batch = Q.shape[0]
        Q_feat = self.lin_Q(Q) 
        K_feat = self.lin_K(K) 
        V_feat = self.lin_V(V)
        
        """
        Multi-head : 임베딩 길이를 헤드 수로 나눠줌. 이 부분이 차이가 있음.
        Q_split: [배치 수, 멀티 헤드 수, 단어 수, 임베딩 길이 / 멀티 헤드 수]
        K_split: [배치 수, 멀티 헤드 수, 단어 수, 임베딩 길이 / 멀티 헤드 수]
        V_split: [배치 수, 멀티 헤드 수, 단어 수, 임베딩 길이 / 멀티 헤드 수]
        permute : transpose와 비슷, 2차원 이상에서도 동작.
        """

        # 멀티 헤드 수가 2번째로 가도록 permute 함수로 위치 조정함.
        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)
        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)
        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)

        d_K = K.size()[-1] # 나눠주기 위해
        scores = Q_split.matmul(K_split.transpose(-2,-1)) / np.sqrt(d_K)

        if mask is not None:
            scores = scores.masked_fill(mask==0,-1e9)
        attention = torch.softmax(scores,dim=-1)
        x_raw = torch.matmul(self.dropout(attention),V_split)

        # attention: [배치 수, 멀티 헤드 수, Q 단어 수, K 단어 수]
        # x_raw: [배치 수, 멀티 헤드 수, Q 단어 수, 임베딩 길이 / 멀티 헤드 수]

        # concatenation을 위한 작업
        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()
        # x_rsh1: [배치 수, Q 단어 수, 멀티 해드 수, 임베딩 길이 / 멀티 헤드 수]
        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)
        # x_rsh2: [배치 수, Q 단어 수, 임베딩 길이]
        # 멀티 해드 수, 임베딩 길이 / 멀티 헤드 수를 쭉 펴줌

        # Linear
        x = self.lin_O(x_rsh2)
        # x: [배치 수, Q 단어 수, 임베딩 길이], 한번 더 통과해줌
        return x
~~~


## 느낀점

시퀀스 데이터를 처리하는 RNN, LSTM, Transformer 모델을 학습했습니다.

이름도 들어봤고 이론도 어느정도 아는 상태에서 수강해서 다소 수월했으나 역시 방대한 양이네요.

부족했던 부분도 많이 보충하고 이 부분에 대한 직관이 많이 생긴 것에 만족합니다.

추천시스템 분야지만 Transformer은 굉장히 중요하기 때문에 잘 인지해보도록 노력하겠습니다.

부스트캠프 초반이 그나마 덜 힘들다고 하는데.. 참 두려우면서 기대가 됩니다.

** 위 수식과 그림은 부스트캠프 AI Tech 교육 자료를 참고하였습니다.