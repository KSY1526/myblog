---
toc: true
layout: post
description: 부스트캠프 4일차 공부 정리.
image : images/rnn.PNG
categories: [BoostCamp, Python, RNN]
title: "[BoostCamp]Day4 RNN"
math : true
search_exclude: false
---
# 4일차 공부 정리
## RNN

소리, 문자열, 주가 등 순서가 의미 있는 시퀀스 데이터는 독립동일분포(iid) 가정을 쉽게 위반한다는 특징이 있습니다.

그렇기 때문에 시퀀스 데이터에서 앞으로 발생할 데이터의 확률분포를 다루기 위해선 이전 정보를 활용한 조건부 확률을 이용할 수 있습니다.

$P(X_1, ..., X_t) = P(X_t\vert X_1, ...,X_{t-1})P(X_1,...,X_{t-1})=\prod_sP(X_s\vert X_{s-1}, ...,X_1)$

이런 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요합니다. 이때 사용되는 것이 잠재변수 H 입니다.

![잠재변수](https://user-images.githubusercontent.com/79916736/191647756-3f17fe3e-01b2-4951-9397-c90f28ebe5d7.png)

![잠재변수2](https://user-images.githubusercontent.com/79916736/191647975-02be78d9-a6c9-4b50-8851-f5f0ce038b50.png)

$X_t$를 구하기 위해선 이전시점($X_{t-1}$)과 잠재변수($X_{t-2}, ...,X_1$ 변수를 이용한 $H_t$)을 사용해서 구하게 됩니다.

![RNN1](https://user-images.githubusercontent.com/79916736/191656439-4246a2ab-45a1-4486-b3a1-ce4da401e6f8.png)

$O_t = H_tW^{(2)} + b^{(2)}$

$H_t =\sigma (X_tW_x^{(1)}+H_{t-1}W_H^{(1)}+b^{(1)})$

윗 개념을 이용한 기본적인 RNN 네트워크 그림과 수식입니다.

잠재변수 $H_t$는 $X_t$와 이전 잠재변수($H_{t-1}$)를 사용해 만들어지며 잠재변수는 이전 시점의 X들의 정보를 가지고 있습니다.

즉 순서가 있는 시퀀스형 데이터를 처리하기 적합한 방식입니다.

마지막으로 잠재변수를 이용하여 목표로 하는 아웃풋 $O_t$를 출력합니다. 이때 가중치 행렬은 시점에 따라 크기가 바뀌는 가변적인 형태가 아닌점을 기억해야합니다.


![잠재변수역전파](https://user-images.githubusercontent.com/79916736/191659536-b96400b0-2445-42a1-b07b-e1f2a0f53d24.png)

다만 기본적인 RNN 모형은 시퀀스 길이가 길어진다면 역전파를 통해 기울기를 구할 때 기울기 소실문제가 두들어지게 나타납니다.

윗 그림 네모칸을 보면 시퀀스 길이가 길어지면 저 곱해지는 항 개수가 비례해서 늘어나는데 1보다 작은 값인 경우 기울기 소실문제가 일어납니다.

기울기 소실문제가 일어나게 되면 가중치 업데이트가 잘 일어나지 않아 모델의 학습이 되지 않는 문제가 발생합니다.

![truncated_BPTT](https://user-images.githubusercontent.com/79916736/191660010-411cd3a5-849f-4b26-9fce-b8cd8f7b5fc5.png)

$H_t$에 있는 기울기는 $H_{t-1}$에서 오는 기울기를 끊고 $O_t$에서 오는 기울기만을 사용하는 방식입니다.

시퀀스 길이가 길어지는 경우 이와 같은 방식으로 역전파를 중간에 끊어주면 기울기 문제를 어느정도 해결할 수 있습니다.

하지만 이런 문제를 완전히 해결할 순 없고 오늘날에는 기본적인 RNN보단 변형된 LSTM, GRU를 많이 사용합니다.

## 느낀점

강의도 듣고 심화문제도 풀어봤습니다. (공유는 불가합니다.)

너무 질 좋고 분명 알고 있던 내용이라 생각했는데 제 허점을 찾아서 딱딱 알려주는 느낌이였습니다.

특히 경사하강법, RNN 등 유익했습니다.