<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>[BoostCamp]Day13 RNN, LSTM, Transformer | 나의 빅데이터 공부 기록</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="[BoostCamp]Day13 RNN, LSTM, Transformer" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="부스트캠프 13일차 공부 정리." />
<meta property="og:description" content="부스트캠프 13일차 공부 정리." />
<link rel="canonical" href="https://ksy1526.github.io/myblog/boostcamp/pytorch/deeplearning/rnn/lstm/gru/transformer/2022/10/05/week3_3.html" />
<meta property="og:url" content="https://ksy1526.github.io/myblog/boostcamp/pytorch/deeplearning/rnn/lstm/gru/transformer/2022/10/05/week3_3.html" />
<meta property="og:site_name" content="나의 빅데이터 공부 기록" />
<meta property="og:image" content="https://ksy1526.github.io/myblog/images/221005.PNG" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-10-05T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2022-10-05T00:00:00-05:00","url":"https://ksy1526.github.io/myblog/boostcamp/pytorch/deeplearning/rnn/lstm/gru/transformer/2022/10/05/week3_3.html","@type":"BlogPosting","image":"https://ksy1526.github.io/myblog/images/221005.PNG","headline":"[BoostCamp]Day13 RNN, LSTM, Transformer","dateModified":"2022-10-05T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ksy1526.github.io/myblog/boostcamp/pytorch/deeplearning/rnn/lstm/gru/transformer/2022/10/05/week3_3.html"},"description":"부스트캠프 13일차 공부 정리.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/myblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ksy1526.github.io/myblog/feed.xml" title="나의 빅데이터 공부 기록" /><link rel="shortcut icon" type="image/x-icon" href="/myblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/myblog/">나의 빅데이터 공부 기록</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/myblog/about/">About Me</a><a class="page-link" href="/myblog/search/">Search</a><a class="page-link" href="/myblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[BoostCamp]Day13 RNN, LSTM, Transformer</h1><p class="page-description">부스트캠프 13일차 공부 정리.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-10-05T00:00:00-05:00" itemprop="datePublished">
        Oct 5, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/myblog/categories/#BoostCamp">BoostCamp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/myblog/categories/#Pytorch">Pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/myblog/categories/#DeepLearning">DeepLearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/myblog/categories/#RNN">RNN</a>
        &nbsp;
      
        <a class="category-tags-link" href="/myblog/categories/#LSTM">LSTM</a>
        &nbsp;
      
        <a class="category-tags-link" href="/myblog/categories/#GRU">GRU</a>
        &nbsp;
      
        <a class="category-tags-link" href="/myblog/categories/#Transformer">Transformer</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#13일차-공부-정리">13일차 공부 정리</a>
<ul>
<li class="toc-entry toc-h2"><a href="#rnn">RNN</a></li>
<li class="toc-entry toc-h2"><a href="#lstm-이론">LSTM 이론</a></li>
<li class="toc-entry toc-h2"><a href="#lstm-간단한-실습">LSTM 간단한 실습</a></li>
<li class="toc-entry toc-h2"><a href="#gru">GRU</a></li>
<li class="toc-entry toc-h2"><a href="#transformer">Transformer</a>
<ul>
<li class="toc-entry toc-h3"><a href="#transformer-q-k-v">Transformer Q, K, V</a></li>
<li class="toc-entry toc-h3"><a href="#멀티-헤드-어텐션">멀티 헤드 어텐션</a></li>
<li class="toc-entry toc-h3"><a href="#코드-구현차원을-집중해서">코드 구현(차원을 집중해서)</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#느낀점">느낀점</a></li>
</ul>
</li>
</ul><h1 id="13일차-공부-정리">
<a class="anchor" href="#13%EC%9D%BC%EC%B0%A8-%EA%B3%B5%EB%B6%80-%EC%A0%95%EB%A6%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>13일차 공부 정리</h1>

<h2 id="rnn">
<a class="anchor" href="#rnn" aria-hidden="true"><span class="octicon octicon-link"></span></a>RNN</h2>

<p>RNN은 길이가 얼마인지 정확히 모르거나 유동적인 순서가 있는 데이터를 다룰 때 매우 유용한 방식입니다.</p>

<p>Hidden state을 이용해 이전정보를 계속 요약해 가기 때문에 상당히 효과적입니다.</p>

<p>하지만 입력값의 길이가 길어질수록 오래 전 입력된 값의 기억이 사라져갑니다.(이전에 자세히 다뤘습니다.)</p>

<h2 id="lstm-이론">
<a class="anchor" href="#lstm-%EC%9D%B4%EB%A1%A0" aria-hidden="true"><span class="octicon octicon-link"></span></a>LSTM 이론</h2>

<p>RNN이 가진 오래된 정보를 소실한다는 단점을 극복하기 위해 LSTM을 사용합니다.</p>

<p><img src="https://user-images.githubusercontent.com/79916736/193789678-5e4ef31d-1f54-41f4-831c-02388f90cc52.png" alt="lstm1"></p>

<p>우선 Hidden state가 hidden state와 cell state 두 개로 분리되어 있습니다.</p>

<p>hidden state은 실제 Output으로 전달되는 값이고 cell state은 잠재적으로 다음 과정에 전달되는 역할입니다.</p>

<p>Gate로 먼저 Forget Gate를 살펴보면 이전 hidden 값과 현재 input 값을 concatenation 하여 가중치 행렬과 곱한 후 시그모이드 활성함수를 거칩니다. 시그모이드 함수를 거쳤기 때문에 이 값과 이전 cell 값 간 행렬곱(실제로는 벡터내적곱) 연산을 하기 때문에 이전 cell 값의 일부 정보를 지워주게 됩니다.</p>

<p>다음으로 Input Gata를 살펴보면 이전 hidden 값과 현재 input 값을 concatenation 한 값을 형태가 다른 두 번에 연산을 진행하게 됩니다. 이 때 첫번째 시그모이드 활성함수를 거치는 것은 마찬가지로 신호등 역할을 하게 되고 두번째 tanh 함수를 거치는 것은 새로운 cell 데이터를 생성해준다고 생각하면 됩니다.</p>

<p>정리하면 cell 값은 Forget Gate에서 나오는 값과 행렬곱을 진행해 일부 정보를 지워주고, Input Gata에서 나오는 값을 새로운 정보로써 더해주게 됩니다.</p>

<p>마지막으로 Output Gate는 마찬가지로 이전 hidden 값과 현재 input 값을 concatenation 한 뒤 시그모이드 활성함수를 거친 값을 사용하고, Forget Gate와 Input Gate를 통과하고 tanh 활성화 함수를 거친 cell 값과 행렬곱 연산을 한 것을 다음 hidden 값으로 전달하게 됩니다. 이 때 출력된 hidden 값은 output 값으로 활용되기도 합니다.</p>

<h2 id="lstm-간단한-실습">
<a class="anchor" href="#lstm-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%8B%A4%EC%8A%B5" aria-hidden="true"><span class="octicon octicon-link"></span></a>LSTM 간단한 실습</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># LSTM 선언
</span><span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">xdim</span><span class="p">,</span> <span class="c1"># 입력되는 X 길이.(한 값 당) 
</span>    <span class="n">hidden_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">hdim</span><span class="p">,</span> <span class="c1"># 히든 층 길이.
</span>    <span class="n">num_layers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">n_layer</span><span class="p">,</span> <span class="c1"># LSTM 층 개수. (디폴트 = 1)
</span>    <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span> <span class="c1"># 보통 True 사용.
</span><span class="p">)</span>

<span class="c1"># 초기 값 설정
</span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="c1"># (층 개수, 배치 사이즈, 히든 층 길이)
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">n_layer</span><span class="p">,</span> <span class="c1"># 층 개수
</span>            <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># 배치 사이즈
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">hdim</span> <span class="c1"># 히든 층 길이            
</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span> <span class="c1"># ho와 마찬가지임.
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">n_layer</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">hdim</span>
<span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># LSTM 사용
## x : [배치 수, 입력되는 x 개수(단어 개수), x 길이(단어 길이, xdim)]
## ho, co : [층 개수(보통 1), 배치 수, 히든 층 길이]
## rnn_out : [배치 수, 입력된 x 개수(뒷 1개만 필요), 히든 층 길이]
</span><span class="n">rnn_out</span><span class="p">,(</span><span class="n">hn</span><span class="p">,</span><span class="n">cn</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span><span class="n">c0</span><span class="p">))</span> 

<span class="c1"># 마지막 부분 linear 사용
## Linear 선언, (히든 층 개수, 출력 층 개수)
</span><span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hdim</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">ydim</span><span class="p">)</span>

<span class="c1">## linear로 full-conneted layer를 달아줌.
</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span>
        <span class="n">rnn_out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># 마지막 히든 층에 있는 값이 중요
</span>        <span class="c1"># 또는 hn.view(-1, self.hdim)도 가능
</span><span class="p">).</span><span class="n">view</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">ydim</span><span class="p">])</span> 
</code></pre></div></div>

<h2 id="gru">
<a class="anchor" href="#gru" aria-hidden="true"><span class="octicon octicon-link"></span></a>GRU</h2>

<p><img src="https://user-images.githubusercontent.com/79916736/193816177-c31c437f-2b2e-4dfe-aea2-a611d042e267.png" alt="gru"></p>

<p>앞서 살펴본 LSTM과 비슷한 논리로 동작하는 또 다른 모델 GRU입니다.</p>

<p>LSTM과에 차이점은 cell state가 따로 없이 hidden state은 존재하지 않고 Gate도 보다 단순하기 때문에 파라미터 수가 상대적으로 적게 되어 데이터에 따라 조금 다르긴 하지만 일반화 성능이 괜찮은 경우가 꽤 있다고 합니다.</p>

<p>RNN 대비 LSTM, GRU 두 모델 모두 많이 개선되었지만 여전히 문제점을 완전히 해결 못하였고 트렌스포머 모델이 나오며 최근에 자주 쓰이는 모델은 아니게 되었습니다.</p>

<h2 id="transformer">
<a class="anchor" href="#transformer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer</h2>

<p>LSTM, GRU 모델 역시 오래된 정보를 소실한다는 단점을 완벽히 극복하지 못하였습니다.</p>

<p>2017년 문장 전체구조를 기계가 이해하기 좋은형식인 Transformer라는 모델이 나오게 됩니다.</p>

<p>이전에 한번 열심히 학습한 경험이 있어 더 자세한 내용은 <a href="https://ksy1526.github.io/myblog/ssuda/book/deep%20learning/natural%20language/2022/03/04/SSUDA22_1.html">링크</a>를 첨부합니다.</p>

<h3 id="transformer-q-k-v">
<a class="anchor" href="#transformer-q-k-v" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer Q, K, V</h3>

<p>우선 입력 데이터 형태가 여러가지가 될 수 있지만 임베딩 된 단어들이 입력된다고 가정하겠습니다.</p>

<p>크기가 어떻게 임베딩 되는지는 자유지만 모든 단어는 동일한 크기의 임베딩이 되어있다고 생각합니다.</p>

<p>입력 데이터 예시 입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># I was car, 임베딩 크기 4일때
</span><span class="p">[</span>
    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="c1"># I
</span>    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="c1"># was
</span>    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>  <span class="c1"># car
</span><span class="p">]</span>
</code></pre></div></div>

<p>각 단어마다 Q 벡터, K 벡터, V 벡터를 생성하게 됩니다. 이 때 벡터의 길이는 일반적으로 Q,K,V 모두 같지만 V는 달라도 상관이 없습니다.</p>

<p>이후 Q 행렬(단어마다 벡터, 가로로 쌓으면 행렬이 되겠죠)와 K 행렬을*(전치후) 행렬곱 합니다.</p>

<p>이 과정을 직관적으로 설명하면 Q는 단어 자체, K는 Q 단어와 얼마나 연관성이 있는지 알려주는 행렬입니다. 즉 Q와 K의 행렬곱을 하면 단어와 단어사이의 관련성을 나타내는 score 행렬이 나오게 됩니다. 이 때 score 행렬의 사이즈는 (단어 수) X (단어 수)로 행 기준으로 관찰했을 때 해당 단어가 문장 내 다른 단어와 얼마나 상호작용하는지를 나타냅니다.</p>

<p>그 다음 score 행렬을 K 벡터의 길이에 루트값으로 나눠준 뒤 행 단위로 softmax를 취해줘서 해당 단어가 문장 내 다른 단어와 얼마나 상호작용하는지를 확률로써 나타냅니다.</p>

<p>마지막으로 계산 된 score 행렬을 V와 행렬곱을 합니다. 직관적으로 설명하면 한 단어가 문장 내에 모든 단어들로 표현하는데, score 값이 높은 단어의 V 벡터 값을 더 많이 취급해준다는 의미입니다. 이렇게 되면 V는 설명되는 단어벡터로 생각할 수 있겠죠.</p>

<p>위 과정을 하나의 그림으로 정리했습니다.</p>

<p><img src="https://user-images.githubusercontent.com/79916736/193984823-b096a93b-3c2d-43b8-b2cd-3056e4e0abe5.png" alt="Transformer1"></p>

<h3 id="멀티-헤드-어텐션">
<a class="anchor" href="#%EB%A9%80%ED%8B%B0-%ED%97%A4%EB%93%9C-%EC%96%B4%ED%85%90%EC%85%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>멀티 헤드 어텐션</h3>

<p>앞서 설명한 연산을 여러번 수행해서 Z 행렬을 여러 개 구해서 세로로 concatenation 시켜준 것이 멀티 헤드 어텐션입니다.</p>

<p>멀티 헤드 어텐션을 수행하면 벡터 길이 값이 세로로 커지게 되는데 가중치 행렬곱을 수행해서 원래 크기로 돌아가게 만듭니다. Transformer는 이런 과정을 여러번 반복하기 때문에 원래 크기를 유지하는 것이 중요합니다.</p>

<p>이론은 이렇게 이해했는데 실제 구현 방식은 조금 다릅니다. 밑 부분에 다시 설명하겠습니다.</p>

<p><img src="https://user-images.githubusercontent.com/79916736/193985869-f4e893cf-9f3d-4383-a89e-abf5440e4836.png" alt="Transformer2"></p>

<h3 id="코드-구현차원을-집중해서">
<a class="anchor" href="#%EC%BD%94%EB%93%9C-%EA%B5%AC%ED%98%84%EC%B0%A8%EC%9B%90%EC%9D%84-%EC%A7%91%EC%A4%91%ED%95%B4%EC%84%9C" aria-hidden="true"><span class="octicon octicon-link"></span></a>코드 구현(차원을 집중해서)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">d_feat</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="c1"># Q,K,V 벡터 길이 
</span>        <span class="n">n_head</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="c1"># 멀티 헤드가 몇개인지
</span>        <span class="n">actv</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">USE_BIAS</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span>
        <span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">d_feat</span><span class="o">%</span><span class="n">n_head</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"d_feat(%d) should be divisible by b_head(%d)"</span><span class="o">%</span><span class="p">(</span><span class="n">d_feat</span><span class="p">,</span><span class="n">n_head</span><span class="p">))</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span> <span class="o">=</span> <span class="n">d_feat</span> <span class="c1"># Q,K,V 벡터 길이
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span> <span class="c1"># 멀티 헤드가 몇개인지
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">actv</span> <span class="o">=</span> <span class="n">actv</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">USE_BIAS</span> <span class="o">=</span> <span class="n">USE_BIAS</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="n">dropout_p</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">lin_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">USE_BIAS</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lin_K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">USE_BIAS</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lin_V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">USE_BIAS</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lin_O</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">USE_BIAS</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout_p</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">Q</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">V</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""
        :param Q: [배치 수, 단어 개수, 임베딩 길이]
        :param K: [배치 수, 단어 개수, 임베딩 길이]
        :param V: [배치 수, 단어 개수, 임베딩 길이] 
        n_K and n_V must be the same 
        """</span>
        <span class="n">n_batch</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Q_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span> 
        <span class="n">K_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin_K</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> 
        <span class="n">V_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin_V</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
        
        <span class="s">"""
        Multi-head : 임베딩 길이를 헤드 수로 나눠줌. 이 부분이 차이가 있음.
        Q_split: [배치 수, 멀티 헤드 수, 단어 수, 임베딩 길이 / 멀티 헤드 수]
        K_split: [배치 수, 멀티 헤드 수, 단어 수, 임베딩 길이 / 멀티 헤드 수]
        V_split: [배치 수, 멀티 헤드 수, 단어 수, 임베딩 길이 / 멀티 헤드 수]
        permute : transpose와 비슷, 2차원 이상에서도 동작.
        """</span>

        <span class="c1"># 멀티 헤드 수가 2번째로 가도록 permute 함수로 위치 조정함.
</span>        <span class="n">Q_split</span> <span class="o">=</span> <span class="n">Q_feat</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">K_split</span> <span class="o">=</span> <span class="n">K_feat</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">V_split</span> <span class="o">=</span> <span class="n">V_feat</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_head</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="n">d_K</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 나눠주기 위해
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">Q_split</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">K_split</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_K</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x_raw</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">),</span><span class="n">V_split</span><span class="p">)</span>

        <span class="c1"># attention: [배치 수, 멀티 헤드 수, Q 단어 수, K 단어 수]
</span>        <span class="c1"># x_raw: [배치 수, 멀티 헤드 수, Q 단어 수, 임베딩 길이 / 멀티 헤드 수]
</span>
        <span class="c1"># concatenation을 위한 작업
</span>        <span class="n">x_rsh1</span> <span class="o">=</span> <span class="n">x_raw</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># x_rsh1: [배치 수, Q 단어 수, 멀티 해드 수, 임베딩 길이 / 멀티 헤드 수]
</span>        <span class="n">x_rsh2</span> <span class="o">=</span> <span class="n">x_rsh1</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_batch</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">d_feat</span><span class="p">)</span>
        <span class="c1"># x_rsh2: [배치 수, Q 단어 수, 임베딩 길이]
</span>        <span class="c1"># 멀티 해드 수, 임베딩 길이 / 멀티 헤드 수를 쭉 펴줌
</span>
        <span class="c1"># Linear
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin_O</span><span class="p">(</span><span class="n">x_rsh2</span><span class="p">)</span>
        <span class="c1"># x: [배치 수, Q 단어 수, 임베딩 길이], 한번 더 통과해줌
</span>        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h2 id="느낀점">
<a class="anchor" href="#%EB%8A%90%EB%82%80%EC%A0%90" aria-hidden="true"><span class="octicon octicon-link"></span></a>느낀점</h2>

<p>시퀀스 데이터를 처리하는 RNN, LSTM, Transformer 모델을 학습했습니다.</p>

<p>이름도 들어봤고 이론도 어느정도 아는 상태에서 수강해서 다소 수월했으나 역시 방대한 양이네요.</p>

<p>부족했던 부분도 많이 보충하고 이 부분에 대한 직관이 많이 생긴 것에 만족합니다.</p>

<p>추천시스템 분야지만 Transformer은 굉장히 중요하기 때문에 잘 인지해보도록 노력하겠습니다.</p>

<p>부스트캠프 초반이 그나마 덜 힘들다고 하는데.. 참 두려우면서 기대가 됩니다.</p>

<p>** 위 수식과 그림은 부스트캠프 AI Tech 교육 자료를 참고하였습니다.</p>

  </div><script src="https://utteranc.es/client.js"
        repo="KSY1526/myblog"
        issue-term="pathname"
        theme="github-dark"
        crossorigin="anonymous"
        async>
</script>
  <a class="u-url" href="/myblog/boostcamp/pytorch/deeplearning/rnn/lstm/gru/transformer/2022/10/05/week3_3.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/myblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/myblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/myblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>다양한 머신러닝/딥러닝 코드들이 기록되어 있습니다.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/KSY1526" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/myblog/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
