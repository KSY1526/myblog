{
  
    
        "post0": {
            "title": "[BoostCamp]Day2, 3 파이썬",
            "content": "2, 3일차 공부 정리 . 파이썬 리스트 . . 리스트 내 어떤 데이터 타입이든 들어갈 수 있습니다. 그림과 같이 리스트의 원소로 리스트가 들어갈 수도 있습니다. . 이게 가능한 이유는 리스트 변수에는 리스트 주소값이 저장되기 때문입니다. 그러므로 형식이 자유로워 집니다. . a = [5,4,3,2,1] b = [1,2,3,4,5] b = a # 같은 리스트 값을 복제하는 방식으로 b에 할당하고 싶을때 b = a[:] . . 다음 코드를 실행하게 되면(b = a) 컴퓨터 내부적으로 윗 그림과 같은 일이 벌어집니다. 두 변수 모두 서로 같은 리스트를 가리키게 되죠. . 변수는 리스트를 입력받을 때 리스트의 주소 값을 할당하는 방식으로 입력받기 때문입니다. . 같은 리스트를 할당하지 않고, 리스트를 복제하여 할당하고 싶을 때는 밑 코드(b = a[:])을 실행해야 합니다. . import copy kor_score = [49, 79, 20, 100, 80] math_score = [43, 59, 85, 30, 90] eng_score = [49, 79, 48, 60, 100] midterm_score = [kor_score, math_score, eng_score] # 아래 식은 복제가 되지 않음. (일차원 리스트와 다릅니다.) # midterm_copy = midterm_score[:] # copy 패키지 내 deepcopy를 사용해야함. midterm_copy = copy.deepcopy(midterm_score) . 이차원 리스트는 일차원 리스트와 또 다르게 복제하려면 위와 같이 다른 방법을 사용해야 합니다. . 파이썬 함수 . Call by Reference . 파이썬 함수 호출 방식은 ‘Call by Reference’ 입니다. 함수에 인자를 넘길 때 메모리 주소를 넘기고, 함수 내에 인자 값 변경 시 호출자의 값도 변경됩니다. . 이 사실을 잘 모르고 있다면 함수를 사용한 코드를 디버깅 할 때 혼란스러울 수 있습니다. . 개인적으로 예전에 코딩테스트 시험를 볼 때 함수를 거친 것 뿐인데 값이 도대체 왜 바뀌는 지에 대한 의문이 풀립니다. . def spam(eggs): # 인자로 넘어온 주소값을 eggs에 넣음. eggs.append(1) # 기존 객체의 주소 값에 [1] 추가. eggs = [2,3] # 새로운 객체 생성. ham = [0] spam(ham) print(ham) # 결과 : [0, 1] # ham 값이 spam 함수를 거치니 달라졌음. . ‘함수에 인자를 넘길 때 메모리 주소를 넘기고, 함수 내에 인자 값 변경 시 호출자의 값도 변경됩니다.’ 를 잘 보여주는 예시 코드 입니다. . 스와핑 . def swap_value(x, y): temp = x x = y y = temp def swap_reference(list_ex, offset_x, offset_y): temp = list_ex[offset_x] list_ex[offset_x] = list_ex[offset_y] list_ex[offset_y] = temp ex = [1,2,3,4,5] swap_value(ex[0], ex[1]) print(ex) # [1,2,3,4,5], 리스트 값이 안 바뀐다. swap_reference(ex, 0,1) print(ex) # [2,1,3,4,5], 리스트 값이 바뀐다. . 윗 함수는 변수 스와핑이 일어나지 않고 아랫 함수는 변수 스와핑이 일어납니다. 리스트 내 변수 스와핑을 할 때 유의해야 합니다. . 또 참고할 점은 파이썬에서 연산 속도를 위해 자주쓰는 -5~256 까지의 숫자의 주소 값은 미리 할당했다는 것을 기억하면 좋겠습니다. . 지역변수, 전역변수 . 다음으로 함수 내 변수를 새로 선언하면 지역변수로써 그 함수 내에서만 사용이 가능합니다. 함수 바깥에 있는 변수는 전역변수로써 함수에서도 차용해서 사용이 가능하구요. . 다만 함수 내에 전역변수와 이름이 같은 변수를 선언하면 새로운 지역 변수가 생깁니다. . 만약 함수 내에서 전역변수로 사용하고 싶다면 global 키워드를 이용하면 좋겠습니다. . def f(): global s s = &#39;I love Hanam!&#39; print(s) # &#39;I love Hanam!&#39; s = &#39;I love Seoul!&#39; print(s) # &#39;I love Seoul!&#39; f() print(s) # &#39;I love Hanam!&#39; . 가변인자 . 개수가 정해지지 않는 변수를 함수의 파라미터로 사용할 때 사용합니다. . Asterisk(*) 기호를 사용하고 가변인자는 맨 마지막 파라미터 위치에서 한 개만 사용 가능합니다. 그리고 입력된 값은 튜플로 사용됩니다. . 또 Asterisk(*) 기호를 2개 사용하는 키워드 가변인자도 존재합니다. 입력 값은 dict 타입을 사용해야합니다. . def asterisk_test(a, b, *args): return a+b+sum(args) def kwargs_test_3(one,two, *args, **kwargs): print(one+two+sum(args)) print(kwargs) print(asterisk_test(1, 2, 3, 4, 5)) kwargs_test_3(3,4,5,6,7,8,9, first=3, second=4, third=5) . 일등함수 . 파이썬의 함수는 일등함수 입니다. 일등함수란 변수나 데이터 구조에 할당이 가능한 객체이고, 파라미터로 전달이 가능하며 리턴 값으로도 사용할 수 있어야 합니다. . 개인적으로 이전에 R을 배웠었는데 R과 작동 원리가 똑같은 것 같습니다. . 일등함수 성질을 사용하면 다음과 같은 응용이 가능합니다. . def print_msg(msg): def printer(): print(msg) return printer another = print_msg(&quot;Hello, Python&quot;) another() # another 함수는 &#39;Hello, Python&#39;을 프린트 해주는 함수. # 위와 같이 함수를 찍어내는 함수를 만들 수 있음. 이를 클로저 라고 함. . 파이썬 내 알아두면 좋을 것들 . docstring . docstring 이란? 함수에 대한 상세스펙을 함수명 아래에 작성하는 것 입니다. . vscode 내 docstring 을 설치하고 명령창에 docstring을 쓰면 쉽게 사용 가능합니다. . . 코딩 컨벤션 . 협업을 진행할 때 쉽게 이해하도록 개인의 코딩 습관을 자제하고, 정해진 코딩 규칙을 지키는 것 입니다. . 일반적인 코딩 규칙으로 . 들어쓰기 4칸 | 한 줄은 최대 79자(화면 크기가 작은 사람 배려) | 불필요한 공백은 피함 | 코드 마지막엔 항상 한 줄 추가 | . 등이 있습니다. . 참고로 파이썬 내 black 모듈을 사용하면 pep8 like 라는 수준을 준수하는 코드로 자동 변경 되니 협업 하는 경우 한번 돌리고 사용해도 좋을 것 같네요. . 포멧팅 . 다른 포멧팅 방법도 많이 있고, 다른 사람의 코드를 해석하기 위해서 알긴 해야겠지만 f-포멧팅 방법이 가장 편한 것 같아 익숙해 지기 위해 기록합니다. . 사용방법은 문자열 앞에 f를 붙이면 되며 {}로 변수 이름을 감싸면 됩니다. . name = &#39;Seong Yeon&#39; age = 25 print(f&#39;Hello, {name}. You are {age}.&#39;) # Hello, Seong Yeon. You are 25. . 모듈 . 남의 코드를 빌려올 때 참 많이 쓰지만 모듈 불러오는 과정이 매일 햇갈려서 정리합니다. . import 모듈 as 별명 | from 모듈 import 특정함수(또는 *) | from 폴더 import 모듈 | . collections . C++의 STL 처럼 파이썬 내 최적화 된 자료구조를 제공하는 모듈로 collections이 있습니다. . 주로 쓰게 되는 것은 deque, Counter, defalutdict를 하나 하나 적어보려 합니다. . deque (연결리스트 기반 구현, 기존 리스트 보다 효율적) . from collections import deque 로 모듈 임포트. | lst = deque() 를 사용하면 선언됨. | lst.append(1), lst.popleft() 으로 큐 처럼 사용 가능. | lst.appendleft(1), lst.pop() 으로 양쪽 방향 모두 사용 가능. | . Counter (데이터가 몇개인지 새어 딕셔너리 형태로 표기) . from collections import Counter c = Counter(&#39;hello&#39;) print(c) # Counter({&#39;l&#39;: 2, &#39;h&#39;: 1, &#39;e&#39;: 1, &#39;o&#39;: 1}) . defalutdict (기본적으로 dict 타입이며 정의되지 않은 생성 시 미리 정한 디폴드 값 적용.) . from collections import defaultdict d = defaultdict(lambda: 0) # Default 값을 0으로 설정합 print(d[&quot;first&quot;]) # 0 출력 . 제너레이터 . 제너레이터를 알기 전 iterator를 간단히 알아보겠습니다. . iterator란 데이터를 순서대로 출력하기 위해 튜플이나 리스트 등 에서 사용하며 내부적으로 iter와 next 함수를 사용합니다. . cities = [&quot;Seoul&quot;, &quot;Busan&quot;, &quot;Jeju&quot;] iter_obj = iter(cities) print(next(iter_obj)) # Seoul print(next(iter_obj)) # Busan print(next(iter_obj)) # Jeju next(iter_obj) # 오류 . 이런 iterable 객체를 특수한 형태로 사용하는 것이 제너레이터(Generator) 입니다. [] 대신 ()을 사용하여 표현합니다. . gen_ex = (n*n for n in range(500)) # 제너레이터 print(type(gen_ex)) # &lt;class &#39;generator&#39;&gt; 출력 print(list(gen_ex)) # 원래 리스트 형태로 출력 . 제너레이터를 사용하게 되면 메모리 용량을 큰 폭으로 아낄 수 있어서 큰 데이터를 다룰 때 주로 사용됩니다. . 또 중간 과정에서 loop가 중단 될 수도 있기 때문에 list 타입 데이터를 반환하는 함수는 제너레이터로 만드는 것이 좋습니다. . 다음은 함수 내 yield을 사용한 제너레이터 예시 입니다. . def square_numbers(nums): for i in nums: yield i * i my_nums = square_numbers(([1,2,3,4,5])) print(my_nums) # &lt;generator object square_numbers at 0x0000018F6AFB7350&gt; for num in my_nums: print(num) # 1, 4, 9, 16, 25 . 느낀점 . 적어놓은 것 이외에도 많이 배웠습니다만.. 정말 방대한 분량이기 때문에 최소한 이것만은 알고가자 하는 것만 적었습니다. . 가끔씩 내용은 얼핏 생각 나는데 개념이나 정확한 코드가 생각나면 스스로 많이 찾아올 것 같습니다. . 들으면서 느끼는 것이 파이썬 개념이 많이 모자랐구나라는 생각과 제 부족한 부분을 잘 채워주는 정말 좋은 강의인 것 같아요. . 쓰느라 참 함들었습니다. 꼭 꾸준히 썼으면 좋겠습니다! .",
            "url": "https://ksy1526.github.io/myblog/boostcamp/python/list/function/generator/2022/09/21/week1_2.html",
            "relUrl": "/boostcamp/python/list/function/generator/2022/09/21/week1_2.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[BoostCamp]Day1 기초 수학",
            "content": "1일차 공부 정리 . 무어-펜로즈(Moore-Penrose) 역행렬 . 우선 역행렬부터 간단히 정의하면 어떤 행렬에 대해 행렬곱을 진행할 때(방향 상관 없이) 항등행렬이 나오는 행렬을 그 행렬의 역행렬이라고 합니다. . 행과 열 길이가 같고 행렬식이 0이 아닌 경우 역행렬은 항상존재합니다만, 앞으로 다룰 행렬이 대부분 행과 열 길이가 다른 행렬입니다. . 이 경우 행렬 곱 순서에 따라 다른 행렬이 나오기 때문에 역행렬을 정의할 수 없습니다. . 다만 역행렬에 기능을 유사하게 수행하기 위해 무어-펜로즈 역행렬을 다음과 같이 정의합니다. . 행이 열보다 길이가 긴 경우 $A^+ = (A^TA)^{-1}A^T$, 열이 행보다 길이가 긴 경우 $A^+ = A^T(A^TA)^{-1}$ 입니다. . 행이 열보다 길이가 긴 경우 $A^+A = I_{m}$, 열이 행보다 길이가 긴 경우 $AA^+ = I_{n}$ 을 만족합니다. . 행과 열 중 길이가 작은 값 기준으로 항등행렬이 생성된다고 생각하면 기억이 오래갈 것 같네요. . 몬테카를로 샘플링 . 기계학습을 진행하게 되면 확률분포를 명시적으로 모르는 경우가 대부분입니다. . 확률분포를 모를 때 데이터를 이용해서 기대값을 계산하는 방법으로 몬테카를로 샘플링 방법을 사용합니다. . . 위 수식에서 x는 데이터를, f는 타겟으로 하는 함수를 의미합니다. 데이터들을 추출해 함수에 대입한 뒤 값을 평균내는 식으로 진행하게 됩니다. . . 예시를 통해 이해하면 쉬운데요. 윗 함수의 [-1,1] 범위 내 적분값을 구하는 것을 몬테카를로 샘플링을 이용해보겠습니다. . import numpy as np # 범위 내 함수 적분 값 구하는 몬테카를로 샘플링 함수(시뮬레이션) def mc_int(fun, low, high, sample_size = 100, repeat = 10): int_len = np.abs(high - low) # x축 길이(범위) stat = [] # 이 행위 repeat만큼 반복 for _ in range(repeat): # 범위 내 값 sample_size 만큼 뽑기. x = np.random.uniform(low = low, high = high, size = sample_size) # y 값 추출 fun_x = fun(x) # x축길이(가로) + 평균 구하기(세로) int_val = int_len * np.mean(fun_x) stat.append(int_val) # 평균 뿐만 아니라 반복 수행했기 때문에 표준편차도 구할 수 있음. # 표준편차로 이 값의 오차범위를 알 수 있음. return np.mean(stat), np.std(stat) def f_x(x): return np.exp(-x**2) print(mc_int(f_x, low = -1, high = 1, sample_size=10000, repeat=100)) # (1.4941721073750045, 0.0043400930289970545) # 다른방법으로 계산한 값 : 1.49364 . 참고링크 . 최대가능도 추정법 . 우선 가능도 함수부터 정의하겠습니다. 가능도 함수란 확률밀도함수와 수식은 같으나 중점으로 보는 변수가 x가 아닌 구하고자 하는 모수 $ theta$입니다. . 최대가능도 추정법은 확률밀도함수를 기반으로 한 가능도 함수를 통해서 가장 가능성이 높은 모수를 추정하는 방법입니다. . 다만 데이터 집합 x가 독립적으로 추출되었을 경우 가능도 함수가 개별 가능도 함수의 곱으로 표현되어있기 때문에 로그가능도를 사용합니다. . 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로 가능도를 계산하는 것이 힘듭니다. . 또 경사하강법으로 가능도를 최적화한다면 미분연산을 사용하는데 로그 가능도함수에 경우 시간복잡도가 $O(N^2)$에서 $O(N)$으로 줄여주는 역할을 합니다. . 쿨백-라이블러 발산 . 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도합니다. . 두 확률분포의 거리는 여러가지 측정 방법이 있는데, 쿨백-라이블러 발산을 소개합니다. . 쿨백-라이블러 발산을 최소화하면 분류문제에서 최대가능도 추정법을 이용하는 것과 마찬가지인데요. . 정답레이블을 P, 모델 예측을 Q라 두면 쿨백-라이블러 발산의 정의는 . $KL(P parallel Q) = int_XP( bold x)log( frac{P( bold x)}{Q( bold x)})d bold x$ 입니다. . 또 $KL(P parallel Q) = -E_{ bold x sim P( bold x)}[log Q( bold x)] + E_{ bold x sim P( bold x)}[log P( bold x)]$ . 로 분해할 수 있으며 왼쪽식은 크로스 엔트로피, 오른쪽 식은 엔트로피로 해석할 수 있습니다. . 이렇게 분해되는 이유는 통계에서 기대값 정의 $ int_xf(x)P(x)dx = E_{ bold x sim P( bold x)}[f(x)]$ 를 응용한 것으로 이해하면 됩니다. . 느낀점 . 첫 시작이라 많이 설레기도 하고 기대되기도 하고 긴장되기도 했습니다. . 운영진분들이 좋은 학습여건을 위해 많은 노력을 하고 있다는 것이 느껴집니다. . 5개월이 짧은 시간이 아닌 만큼 오버페이스 하지 않고 꾸준히 잘 해보겠습니다. .",
            "url": "https://ksy1526.github.io/myblog/boostcamp/math/moore-penrose/monte%20carlo/mle/kl%20divergence/markdown/2022/09/19/week1_1.html",
            "relUrl": "/boostcamp/math/moore-penrose/monte%20carlo/mle/kl%20divergence/markdown/2022/09/19/week1_1.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[BoostCamp]Gradient Descent",
            "content": "경사하강법 (Gradient Descent) . 백터 변수의 미분 . 볼드체 소문자는 벡터, 볼드체 대문자는 행렬을 의미합니다. . $ partial_{x_i} f( bold x) = displaystyle lim_{h to 0} frac{f( bold x+h bold e_i)-f( bold x)}{h}$ . i번째 변수만을 편미분한 수식입니다. . $ bold{e}_i$ 벡터는 i번째 값이 1, 나머지 값이 0인 벡터로 i번째 변수만을 편미분 하기 위해 사용됩니다. . . $ nabla f = ( partial_{x_1}f, partial_{x_2}f, partial_{x_3}f, …)$ . 벡터 내 각 값마다 편미분 한 값을 다시 벡터 형태로 나타냅니다. . 이 벡터를 ‘그레디언트 벡터’라고 부르며 경사하강법에 직접 사용됩니다. . . 경사하강법의 기초 원리 . 한 점의 미분값은 함수가 증가할때는 양수, 감소할때는 음수로 나타납니다. . 이 성질을 이용해 특정 위치에서 함수 내 극솟값을 찾기 위해선 그 위치의 미분 값에서 빼는 연산을 하면 됩니다. . 극솟값은 x의 미분값이 양수이면 x가 더 작은 값을 가질 것이고 음수이면 x가 더 큰 값을 갖기 때문입니다. 또 극솟값의 미분 값은 0이구요. . 벡터로 확장하면 특정 위치의 벡터에서 앞서 구한 그레이던트 벡터를 빼는 연산을 하면 됩니다. . . 선형회귀에서의 경사하강법 . $ nabla _ beta {( bold y - bold X beta)}^2 =( partial_ { beta _1} {( bold y - bold X beta)}^2, partial _ { beta _2} {( bold y - bold X beta)}^2, …, partial _ { beta _d} {( bold y - bold X beta)}^2)$ . 앞 벡터 중 k번째 값을 뽑으면, . $ partial_{ beta_k} ( bold y - bold X beta)^2 = partial_{ beta_k} ( displaystyle frac 1 n displaystyle sum_{i=1}^{n}(y_i- displaystyle sum_{j=1}^d X_{ij} beta_j)^2)$ . $={ partial_{ beta_k}} ( frac 1 n displaystyle sum_{i=1}^{n}(y_i^2-2y_i displaystyle sum_{j=1}^d X_{ij} beta_j + ( displaystyle sum_{j=1}^d X_{ij} beta_j)^2))$ . $= displaystyle frac 1 n displaystyle sum_{i=1}^{n}(-2y_iX_{ik} beta_k +2X_{ik}( beta_1X_{i1}+ beta_2X_{i2}+ … + beta_dX_{id}))$ . $=- frac 2 n( beta_k displaystyle sum_{i=1}^{n}(y_iX_{ik}) - displaystyle sum_{i=1}^{n}X_{ik}( bold X_{i.} bold beta))$ . $= displaystyle - frac 2 n( bold X_{.k}^T bold y - bold X_{.k}^TX_{i.} bold beta)$ . 데이터 개수가 총 n개, 종류가 총 d개 입니다. 포인트는 베타k로 편미분 되는 사실을 잘 기억하고 시그마 내에 미분 연산을 집어넣는 것 입니다. . 선형회귀에 경우 볼록함수로 극솟값이 하나만 존재하고 전역 최솟값인 것이 보장됩니다. . 다만 앞으로 다룰 목적함수들은 볼록함수가 아닐 가능성이 높습니다. 즉 극솟값이더라도 최솟값임을 보장할 수 없습니다. . . 확률적 경사하강법(SGD) . 미분 가능하고 볼록한 함수에서는 경사하강법이 최솟값을 찾는 것이 보장됩니다. . 하지만 거의 모든 딥러닝 손실함수는 비선형함수 입니다. 극솟값이 전역 최솟값을 보장하지 못합니다. . 즉, 최솟값을 구해야 하나 극솟값에 잘못 수렴할 수 있습니다. . 또 정말 많은 데이터와 파라미터를 다루는 경우 컴퓨터의 메모리 문제도 존재합니다. . 따라서 모든 데이터를 활용해서 파라미터를 업데이트 하는 대신 하나 혹은 일부 데이터를 사용하는 확률적 경사하강법을 주로 사용합니다. . 확률적 경사하강법(SGD)는 모든 데이터를 사용하지 않기 떄문에 목적식이 약간 달라집니다. . 그러므로 극솟값에서의 미분값이 경사하강법과 다르게 0에 가까울 뿐 0이 아닐 가능성이 높습니다. . 즉, 극솟값에서 탈출할 수 있습니다. 극솟값에 머무르지 않는고 전역 최솟값으로 나아갈 수 있다는 얘기죠. . 또 연산자원을 효율적으로 사용할 수 있습니다. 큰 데이터를 다루는 대도 문제가 없습니다. . 다만 너무 작은 개수의 데이터를 사용하면 경사하강법 목적식을 많이 왜곡하므로 이를 지양해야겠습니다. . . 간단한 후기 . 경사하강법의 개념을 모르진 않았는데 제가 부족한 것을 많이 배운 것 같습니다. . 더 많은 내용이 있으나 다 기록하기에 힘이 붙여 저한테 중요한 포인트만 적어봤습니다. . 앞 기수에서 배운 내용을 블로그에 기록하는 것을 보고 정식시작 전 저도 연습 겸 했습니다. . 마크다운 언어를 직접적으로 사용한 것은 처음인데 생각보다 어렵네요. 특히 수식이 많이 까다로웠습니다. . 그래도 익숙해질 겸 미리 경험한 것이 좋았습니다. .",
            "url": "https://ksy1526.github.io/myblog/boostcamp/gradient%20descent/math/markdown/2022/09/14/Gradient_Descent.html",
            "relUrl": "/boostcamp/gradient%20descent/math/markdown/2022/09/14/Gradient_Descent.html",
            "date": " • Sep 14, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[SSUDA] Pytorch로 LSTM, Prophet 사용하기 with 태양광 발전량 예측 경진대회",
            "content": "LSTM . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.optim as optim from sklearn.preprocessing import MinMaxScaler import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/sun/&#39; energy = pd.read_csv(path + &#39;energy.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) energy.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 25632 entries, 0 to 25631 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 time 25632 non-null object 1 dangjin_floating 25608 non-null float64 2 dangjin_warehouse 25584 non-null float64 3 dangjin 25632 non-null int64 4 ulsan 25632 non-null int64 dtypes: float64(2), int64(2), object(1) memory usage: 1001.4+ KB . energy.fillna(energy.mean(),inplace = True) energy = energy.set_index(&#39;time&#39;) energy.head() . dangjin_floating dangjin_warehouse dangjin ulsan . time . 2018-03-01 1:00:00 0.0 | 0.0 | 0 | 0 | . 2018-03-01 2:00:00 0.0 | 0.0 | 0 | 0 | . 2018-03-01 3:00:00 0.0 | 0.0 | 0 | 0 | . 2018-03-01 4:00:00 0.0 | 0.0 | 0 | 0 | . 2018-03-01 5:00:00 0.0 | 0.0 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 결측치를 채우고 시간을 인덱스로 바꿨습니다. . ulsan = energy[&#39;ulsan&#39;].values.astype(float) dangjin_floating = energy[&#39;dangjin_floating&#39;].values.astype(float) dangjin_warehouse = energy[&#39;dangjin_warehouse&#39;].values.astype(float) dangjin = energy[&#39;dangjin&#39;].values.astype(float) . 데이터를 각각 뽑아냅니다. . dangjin_floating . learning_rate = 0.0001 sequence_length = 12 # 24 일때가 가장 좋았음. but 코렙 램이 터지는 관계로 12 사용 epochs = 2000 . def make_batch(input_data, sl): train_x = [] train_y = [] L = len(input_data) for i in range(L-sl): # sl기간 만큼 있는 데이터에서 다음 시점 맞추기. train_seq = input_data[i:i+sl] train_label = input_data[i+sl:(i+sl+1)] # 리스트 값을 train_x에 어팬드함. train_x.append(train_seq) train_y.append(train_label) return train_x, train_y . class simple_lstm(nn.Module): def __init__(self): super().__init__() self.input_vector = 1 # 입력 벡터 길이 self.sequence_length = 12 # 데이터 묶음 길이(24개 데이터 사용) self.output_vector = 100 # 은닉층 사이즈 self.num_layers = 4 # 층 개수 self.lstm = nn.LSTM(input_size = self.input_vector, hidden_size = self.output_vector, num_layers = self.num_layers, batch_first = True) self.linear = nn.Sequential( nn.Linear(self.output_vector, 50), nn.Linear(50, 30), nn.Linear(30, 10), nn.Linear(10, 1) ) def forward(self, x): output, _ = self.lstm(x) return self.linear(output[:, -1, :]) device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . train_x, train_y = make_batch(dangjin_floating.reshape(-1,1), sequence_length) # 텐서에 데이터 실기 tensor_x = torch.Tensor(train_x) tensor_y = torch.Tensor(train_y) dangjin_floatings = simple_lstm() # 모델을 디바이스에 실음 (디바이스에는 모델과 데이터를 실어야함) dangjin_floatings = dangjin_floatings.to(device) # 아담 옵티마이저 사용 (학습하려는 모델 파라미터, 학습률) optimizer = torch.optim.Adam(dangjin_floatings.parameters(), lr = learning_rate) criterion = nn.MSELoss() for i in range(epochs): # 모델 학습 모드 dangjin_floatings.train() tensor_x = tensor_x.to(device) tensor_y = tensor_y.to(device) output = dangjin_floatings(tensor_x) loss = criterion(output, tensor_y.view(-1,1)) # 옵티마이저 초기화 (배치마다 해줘야함) optimizer.zero_grad() # 로스함수를 사용해 역전파 loss.backward() # 옵티마이저를 이용해 가중치 업데이트 optimizer.step() # 100번째 배치마다 로스 값 출력 if i % 100 == 0: print(&#39;Epoch {}, Loss {:.5f}&#39;.format(i, loss.item())) . Epoch 0, Loss 51745.66797 Epoch 100, Loss 51092.85156 Epoch 200, Loss 47514.81641 Epoch 300, Loss 41005.27734 Epoch 400, Loss 37112.99609 Epoch 500, Loss 36839.90625 Epoch 600, Loss 36838.98828 Epoch 700, Loss 36828.25000 Epoch 800, Loss 15298.88477 Epoch 900, Loss 4058.15771 Epoch 1000, Loss 2399.74683 Epoch 1100, Loss 2103.90039 Epoch 1200, Loss 2016.97937 Epoch 1300, Loss 1976.83508 Epoch 1400, Loss 1952.15393 Epoch 1500, Loss 1933.21155 Epoch 1600, Loss 1917.11658 Epoch 1700, Loss 1901.57031 Epoch 1800, Loss 1888.42883 Epoch 1900, Loss 1876.48096 . x_input = np.array(energy.dangjin_floating[-12:]) x_input = x_input.reshape((1,12,1)) dangjin_floating_pred = [] for i in range(672): x_input = torch.Tensor(x_input) x_input = x_input.to(device) # 모델에 넣고 output값 확인 위해서는 cpu로 돌린 뒤 넘파이로 변환. predict = dangjin_floatings(x_input).cpu().detach().numpy() # 예측값 배출 new_input = predict.reshape((1,1,1)) # 예측값을 실제값인 것 처럼 재사용하여 다시 모델에 넣음(나이브한 방식) x_input = np.concatenate((x_input[:,-11:].cpu(), new_input), axis = 1) # 예측값은 dangjin_floating_pred 리스트에 계속 저장해둠 dangjin_floating_pred.append(predict[0][0]) . dangjin_warehouse . # train_x = [길이 12 어레이, 길이 12 어레이, ...] # train_y = [길이 1 어레이, 길이 1 어레이, ..] train_x, train_y = make_batch(dangjin_warehouse.reshape(-1,1), sequence_length) # 데이터 텐서로 실기(리스트도 실기 가능) tensor_x = torch.Tensor(train_x) tensor_y = torch.Tensor(train_y) # 모델 불러와서 디바이스에 실기 (GPU 사용 위해선 모델과 데이터를 실어야함) dangjin_warehouses = simple_lstm() dangjin_warehouses = dangjin_warehouses.to(device) # 옵티마이저 아담(많이 사용됨) 사용 optimizer = torch.optim.Adam(dangjin_warehouses.parameters(), lr = learning_rate) # 손실함수 MSE 사용 criterion = nn.MSELoss() for i in range(epochs): # 학습 모드(가중치 업데이트 됨)로 변환 dangjin_warehouses.train() # 데이터(텐서 형식) 디바이스에 실기 tensor_x = tensor_x.to(device) tensor_y = tensor_y.to(device) # 모델에 x 데이터 넣기 output = dangjin_warehouses(tensor_x) # 로스 값 구하기 loss = criterion(output, tensor_y.view(-1,1)) # 옵티마이저 초기화(매 배치마다 초기화 해야함) optimizer.zero_grad() # 손실함수 이용해 역전파 loss.backward() # 옵티마이저 사용해 가중치 업데이트 optimizer.step() # 100번째 배치마다 로스 계산 if i % 100 == 0: # loss.item 함수 사용하면 로스 값 제출해줌 print(&#39;Epoch {}, Loss {:.5f}&#39;.format(i, loss.item())) # 테스트 데이터 계산 위해 맨 뒤 12개 데이터 사용 x_input = np.array(energy.dangjin_warehouse[-12:]) x_input = x_input.reshape((1,12,1)) dangjin_warehouse_pred = [] . Epoch 0, Loss 29900.15234 Epoch 100, Loss 29317.77930 Epoch 200, Loss 26276.70508 Epoch 300, Loss 22257.02539 Epoch 400, Loss 21406.61328 Epoch 500, Loss 21391.32422 Epoch 600, Loss 8447.42188 Epoch 700, Loss 2214.76782 Epoch 800, Loss 1341.09875 Epoch 900, Loss 1182.24084 Epoch 1000, Loss 1131.52197 Epoch 1100, Loss 1107.95605 Epoch 1200, Loss 1092.98474 Epoch 1300, Loss 1083.00134 Epoch 1400, Loss 1074.64624 Epoch 1500, Loss 1068.63171 Epoch 1600, Loss 1064.08704 Epoch 1700, Loss 1058.59131 Epoch 1800, Loss 1054.51086 Epoch 1900, Loss 1050.88635 . for i in range(672): x_input = torch.Tensor(x_input) x_input = x_input.to(device) predict = dangjin_warehouses(x_input).cpu().detach().numpy() new_input = predict.reshape((1,1,1)) x_input = np.concatenate((x_input[:,-23:].cpu(), new_input), axis = 1) dangjin_warehouse_pred.append(predict[0][0]) . dangjin . ulsan . 나머지 2개 변수 예측은 추후에 lstm 코드 복습 시 사용. (앞 두 변수 방식과 동일) . &#44208;&#44284; &#51228;&#52636; . sample_submission.iloc[:24*28, 1] = dangjin_floating_pred sample_submission.iloc[:24*28, 2] = dangjin_warehouse_pred # sample_submission.iloc[:24*28, 3] = dangjin_pred # sample_submission.iloc[:24*28, 4] = ulsan_pred # sample_submission . Prophet . &#54596;&#50836;&#54620; &#54056;&#53412;&#51648; &#49444;&#52824;&#54616;&#44592; . from datetime import datetime from neuralprophet import NeuralProphet energy = pd.read_csv(path + &#39;energy.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting neuralprophet Downloading neuralprophet-0.3.2-py3-none-any.whl (74 kB) |████████████████████████████████| 74 kB 3.7 MB/s Requirement already satisfied: LunarCalendar&gt;=0.0.9 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (0.0.9) Requirement already satisfied: pandas&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (1.3.5) Requirement already satisfied: tqdm&gt;=4.50.2 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (4.64.0) Requirement already satisfied: torch&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (1.12.0+cu113) Collecting torch-lr-finder&gt;=0.2.1 Downloading torch_lr_finder-0.2.1-py3-none-any.whl (11 kB) Requirement already satisfied: matplotlib&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (3.2.2) Requirement already satisfied: python-dateutil&gt;=2.8.0 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (2.8.2) Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (1.21.6) Requirement already satisfied: convertdate&gt;=2.1.2 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (2.4.0) Requirement already satisfied: holidays&gt;=0.11.3.1 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (0.14.2) Requirement already satisfied: ipywidgets&gt;=7.5.1 in /usr/local/lib/python3.7/dist-packages (from neuralprophet) (7.7.1) Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Requirement already satisfied: pymeeus&lt;=1,&gt;=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate&gt;=2.1.2-&gt;neuralprophet) (0.5.11) Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays&gt;=0.11.3.1-&gt;neuralprophet) (0.2.1) Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays&gt;=0.11.3.1-&gt;neuralprophet) (2.2.4) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.2.0) Requirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.5.0) Requirement already satisfied: traitlets&gt;=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.1.1) Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;neuralprophet) (4.10.1) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;neuralprophet) (1.1.1) Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;neuralprophet) (3.6.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.3.5) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.1.1) Requirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (57.4.0) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (1.0.18) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.7.5) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (4.4.2) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (4.8.0) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (2.6.1) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.8.1) Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from LunarCalendar&gt;=0.0.9-&gt;neuralprophet) (2022.1) Requirement already satisfied: ephem&gt;=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar&gt;=0.0.9-&gt;neuralprophet) (4.1.3) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.0.0-&gt;neuralprophet) (0.11.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.0.0-&gt;neuralprophet) (1.4.4) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.0.0-&gt;neuralprophet) (3.0.9) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&gt;=2.0.0-&gt;neuralprophet) (4.1.1) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (1.15.0) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.2.5) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder&gt;=0.2.1-&gt;neuralprophet) (21.3) Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.3.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (1.8.0) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (2.11.3) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.6.1) Requirement already satisfied: jupyter-core&gt;=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (4.11.1) Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.4.0) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.13.3) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (23.2.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.7.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (2.0.1) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.0.1) Requirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.4) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (1.5.0) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.8.4) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.6.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.7.1) Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (2.16.1) Requirement already satisfied: jsonschema&gt;=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (4.3.3) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.18.1) Requirement already satisfied: importlib-resources&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (5.9.0) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (21.4.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (4.12.0) Requirement already satisfied: zipp&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources&gt;=1.4.0-&gt;jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (3.8.1) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;neuralprophet) (0.5.1) Installing collected packages: torch-lr-finder, dataclasses, neuralprophet Successfully installed dataclasses-0.6 neuralprophet-0.3.2 torch-lr-finder-0.2.1 . def convert_time(x): # 2018-03-01 1:00:00 값 변환하기 Ymd, HMS = x.split(&#39; &#39;) H, M, S = HMS.split(&#39;:&#39;) H = str(int(H) - 1) # 다시 시간 합치기 HMS = &#39;:&#39;.join([H, M, S]) return &#39; &#39;.join([Ymd, HMS]) energy[&#39;time&#39;] = energy[&#39;time&#39;].apply(lambda x:convert_time(x)) . &#47784;&#45944; &#51201;&#50857;&#54616;&#44592; . column = &#39;dangjin_floating&#39; df = pd.DataFrame() df[&#39;ds&#39;] = energy[&#39;time&#39;] df[&#39;y&#39;] = energy[column] . model = NeuralProphet() # 훈련 loss = model.fit(df, freq = &#39;H&#39;) # 미래 예측용 데이터 프레임 만들기 df_pred = model.make_future_dataframe(df, periods = 18000) # 미레 예측 하기 predict = model.predict(df_pred) . INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.996% of the data. INFO:NP.df_utils:Major frequency H corresponds to 99.996% of the data. INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H INFO:NP.df_utils:Defined frequency is equal to major frequency - H INFO - (NP.forecaster.__handle_missing_data) - dropped 24 NAN row in &#39;y&#39; INFO:NP.forecaster:dropped 24 NAN row in &#39;y&#39; INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training. INFO:NP.config:Setting normalization to global as only one dataframe provided for training. INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64 INFO:NP.config:Auto-set batch_size to 64 INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 81 INFO:NP.config:Auto-set epochs to 81 INFO - (NP.utils_torch.lr_range_test) - lr-range-test results: steep: 6.76E-02, min: 1.19E+00 INFO:NP.utils_torch:lr-range-test results: steep: 6.76E-02, min: 1.19E+00 INFO - (NP.utils_torch.lr_range_test) - lr-range-test results: steep: 6.76E-02, min: 1.19E+00 INFO:NP.utils_torch:lr-range-test results: steep: 6.76E-02, min: 1.19E+00 INFO - (NP.forecaster._init_train_loader) - lr-range-test selected learning rate: 7.71E-02 INFO:NP.forecaster:lr-range-test selected learning rate: 7.71E-02 Epoch[81/81]: 100%|██████████| 81/81 [01:16&lt;00:00, 1.06it/s, SmoothL1Loss=0.0161, MAE=67.5, RMSE=101, RegLoss=0] INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.996% of the data. INFO:NP.df_utils:Major frequency H corresponds to 99.996% of the data. INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H INFO:NP.df_utils:Defined frequency is equal to major frequency - H INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.994% of the data. INFO:NP.df_utils:Major frequency H corresponds to 99.994% of the data. INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H INFO:NP.df_utils:Defined frequency is equal to major frequency - H INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.994% of the data. INFO:NP.df_utils:Major frequency H corresponds to 99.994% of the data. INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H INFO:NP.df_utils:Defined frequency is equal to major frequency - H . predict_1 = predict.copy() predict_1 = predict_1.query(&#39;ds &gt;= &quot;2021-02-01 00:00:00&quot;&#39;) predict_1 = predict_1.query(&#39;ds &lt; &quot;2021-03-01 00:00:00&quot;&#39;) # 2021-06-09 ~ 2021-07-09 predict_2 = predict.copy() predict_2 = predict_2.query(&#39;ds &gt;= &quot;2021-06-09 00:00:00&quot;&#39;) predict_2 = predict_2.query(&#39;ds &lt; &quot;2021-07-09 00:00:00&quot;&#39;) # 제출 파일 업데이트 sample_submission[column] = list(predict_1[&#39;yhat1&#39;]) + list(predict_2[&#39;yhat1&#39;]) sample_submission.head() . time dangjin_floating dangjin_warehouse dangjin ulsan . 0 2021-02-01 01:00:00 | -29.331793 | 0 | 0 | 0 | . 1 2021-02-01 02:00:00 | -28.439514 | 0 | 0 | 0 | . 2 2021-02-01 03:00:00 | -29.095627 | 0 | 0 | 0 | . 3 2021-02-01 04:00:00 | -29.376926 | 0 | 0 | 0 | . 4 2021-02-01 05:00:00 | -27.988543 | 0 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#52280;&#44256; &#51088;&#47308; . 데이콘 대회 . https://dacon.io/competitions/official/235720/overview/description . LSTM . https://dacon.io/competitions/official/235720/codeshare/2609 . Prophet . https://dacon.io/competitions/official/235720/codeshare/2492 .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/ssuda/pytorch/deep%20learning/lstm/2022/07/31/SSUDA_sun.html",
            "relUrl": "/dacon/jupyter/ssuda/pytorch/deep%20learning/lstm/2022/07/31/SSUDA_sun.html",
            "date": " • Jul 31, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[SSUDA] 비지도 학습",
            "content": "다양한 비지도 학습 방식을 이번기회에 공부하려고 합니다. . 비지도 학습은 크게 이상치 탐지와 군집화 두 섹션으로 나눌 수 있는데요. . 이상치 탐지 중 IsolationForest를 중점적으로 공부하고, 군집화는 유명한 K-Means 이외에 거리기반 DBSCAN와 퍼지 군집을 살펴보겠습니다. . . 다양한 분야에 군집화 모델들 입니다. . IsolationForest . . IsolationForest는 여러개의 변수가 있을 때 이상치를 찾아내는 기법입니다. . 여러 개의 의사결정나무를 종합한 앙상블 기반의 이상탐지 기법인데요. . 의사결정트리를 지속적으로 분기시키며 데이터 관측치가 어느정도 고립되었는지에 따라 이상치를 판별하는 방식입니다. . . 직관적으로 비정상 데이터라면 의사결정트리의 루트와 가까운 곳에서 고립이 될 것이라 생각합니다. 그림에서 빨간색 부분이죠. . 알고리즘은 랜덤포레스트와 유사하게 매번 샘플링된 데이터를 이용하고 트리 분기에 사용할 변수들을 랜덤하게 선택합니다. . 선택한 변수들의 최소/최대 값 사이에 정의된 유니폼 분포에서 샘플링하여 분기하게 됩니다. . 이런식으로 트리를 여러번 분기하였을 때 데이터 별 루트 노드로부터의 평균 거리가 짧은 값을 이상치로 판별합니다. . IQR 이나 Z-score 대비 고차원에 데이터 형식일 때 성능도 괜찮고 컴퓨팅 효율도 좋습니다. . 다만 이 기법을 적용하기 위해선 비정상 데이터가 다른 데이터와 다른 형태라는 가정이 필요합니다. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.ensemble import IsolationForest . IsolationForest 모델은 사이킷런 내부에 존재해서 쉽게 사용할 수 있습니다. . state_value = np.random.RandomState(20) # 학습 데이터 생성하기 X = 0.3 * state_value.randn(100, 2) X_train = np.r_[X + 2, X - 2] # 두 배열을 합쳐줌 X_train = pd.DataFrame(X_train, columns = [&#39;x1&#39;, &#39;x2&#39;]) # 정규값과 비슷한 데이터 추가해주기 X = 0.3 * state_value.randn(20, 2) X_test = np.r_[X + 2, X - 2] X_test = pd.DataFrame(X_test, columns = [&#39;x1&#39;, &#39;x2&#39;]) # 정규적이지 않은 데이터 추가해주기(인위적인 이상값 구현을 위함) X_outliers = state_value.uniform(low=-4, high=4, size=(20, 2)) X_outliers = pd.DataFrame(X_outliers, columns = [&#39;x1&#39;, &#39;x2&#39;]) # 생성한 데이터 시각화 하기 plt.rcParams[&#39;figure.figsize&#39;] = [10, 10] p1 = plt.scatter(X_train.x1, X_train.x2, c=&#39;white&#39;, s=20*4, edgecolor=&#39;k&#39;, label=&#39;training observations&#39;) # p2 = plt.scatter(X_test.x1, X_test.x2, c=&#39;green&#39;, s=20*4, edgecolor=&#39;k&#39;, label=&#39;new regular obs.&#39;) p3 = plt.scatter(X_outliers.x1, X_outliers.x2, c=&#39;red&#39;, s=20*4, edgecolor=&#39;k&#39;, label=&#39;new abnormal obs.&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7ff28774e6d0&gt; . 정상 데이터는 -2나 2 근방에 생성했고 비정상 데이터는 아에 랜덤으로 생성했습니다. . # max_samples : 정수일 경우 지정한 개수만큼, 0~1사이 실수이면 그 퍼센트만큼 데이터 샘플링. # contamination : 전체 데이터 중 사용자가 생각하는 이상치 비율. 임계치로 활용됨. clf = IsolationForest(max_samples=100, contamination = 0.1, random_state=42) clf.fit(X_train) y_pred_train = clf.predict(X_train) y_pred_test = clf.predict(X_test) y_pred_outliers = clf.predict(X_outliers) # 모델링이 잘 됬는지 시각화하기 X_outliers = X_outliers.assign(y = y_pred_outliers) p1 = plt.scatter(X_train.x1, X_train.x2, c=&#39;white&#39;, s=20*4, edgecolor=&#39;k&#39;, label=&quot;training observations&quot;) p2 = plt.scatter(X_outliers.loc[X_outliers.y == -1, [&#39;x1&#39;]], X_outliers.loc[X_outliers.y == -1, [&#39;x2&#39;]], c=&#39;red&#39;, s=20*4, edgecolor=&#39;k&#39;, label=&quot;detected outliers&quot;) p3 = plt.scatter(X_outliers.loc[X_outliers.y == 1, [&#39;x1&#39;]], X_outliers.loc[X_outliers.y == 1, [&#39;x2&#39;]], c=&#39;green&#39;, s=20*4, edgecolor=&#39;k&#39;, label=&quot;detected regular obs&quot;) plt.legend() . /usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names &#34;X does not have valid feature names, but&#34; . &lt;matplotlib.legend.Legend at 0x7ff2876a5490&gt; . IsolationForest 모델 관련 핵심 파라미터 max_samples와 contamination을 설명했습니다. . 모델의 출력값으로 정상 데이터는 1, 비정상 데이터는 -1을 출력합니다. . 빨간색은 디텍팅한 비정상 데이터, 초록색은 디텍팅에 실패한 비정상 데이터입니다. . 모델 가정과 들어 맞는 가상 데이터를 사용했지만 모델이 꽤 똘똘하게 돌아가는게 관찰됩니다. . DBSCAN . DBSCAN은 Density-Based Spatial Clustering of Applications with Noise의 약자 입니다. . 밀도 기반으로 클러스터링을 하며 노이즈를 적용한다는 뜻 입니다. . K-Means 모델은 이상치가 있어도 이상치 값을 이해할 수 없으며 군집의 중심 위치를 왜곡시키는 문제가 있습니다. . 반면 DBSCAN은 특정 요소가 클러스터에 속하는 경우 해당 클러스터 내 다른 많은 요소와 가까운 위치에 있어야한다는 전제가 있습니다. . 계산을 위해 직경(R), 최소 요소(M) 두 값을 사용합니다. . 쉽게 값을 설명하면 직경(R)은 군집의 크기를 나타내며 최소 요소(M)는 군집을 이루기 위한 최소 데이터 개수를 나타냅니다. . . 알고리즘 진행 과정을 간략하게 설명하면 우선 데이터 별로 R의 크기(원 넓이)를 체크해 주변 데이터가 몇 개 있는지 탐색합니다. . 다음으로 R 크기 내 M개 이상의 데이터가 존재하면 해당 데이터를 군집의 중심값 취급합니다. . 마지막으로 군집을 의미하는 원의 내부에 포함되지 못한 데이터는 이상값으로 취급되어 어느 군집에도 속하지 못한 값이 됩니다. . . K-Means와 차이점을 나타 낸 그림입니다. 저런 형식의 데이터에 경우 K-Means 보다 DBSCAN이 훨씬 우수합니다. . 또한 군집의 개수, K 값을 미리 지정할 필요가 없다는 것도 큰 장점입니다. . from sklearn.cluster import DBSCAN from sklearn.datasets import load_iris import pandas as pd iris_data = load_iris() irisDF = pd.DataFrame(data=iris_data.data, columns=[&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;]) dbscan = DBSCAN(eps=0.6, min_samples=8, metric=&#39;euclidean&#39;) dbscan_labels = dbscan.fit_predict(iris_data.data) irisDF[&#39;dbscan_cluster&#39;] = dbscan_labels irisDF[&#39;target&#39;] = iris_data.target iris_result = irisDF.groupby([&#39;target&#39;])[&#39;dbscan_cluster&#39;].value_counts() print(iris_result) . target dbscan_cluster 0 0 49 -1 1 1 1 46 -1 4 2 1 42 -1 8 Name: dbscan_cluster, dtype: int64 . iris 데이터를 사용해 DBSCAN 모델을 적합시켰습니다. . eps(R)과 min_samples(M)에 따라 다르겠지만 3 종류의 꽃을 두 종류로 구분하는 걸 보니 적절한 파라미터를 찾는 것 또한 골칫거리 입니다. . 또 직관적으로 생각했을 때 차원이 높아진다면 거리의 측정이 애매해져서 이 모델을 적용하기 어려울 것 같습니다. . 그리고 모든 데이터간 거리를 구해야 하기 때문에 데이터의 개수가 커진다면 시간복잡도가 상당히 높을 것으로 추측됩니다. . 적절한 파라미터 2개를 찾아야하는데 시간복잡도 마저 높아진다면 골칫거리일 것 입니다. . 이런 문제를 해결하고자 입력 파라미터를 제거한 DBCLASD 모델도 존재합니다. . from sklearn.decomposition import PCA pca = PCA(n_components=2) pca_transformed = pca.fit_transform(iris_data.data) irisDF[&#39;pca_x&#39;] = pca_transformed[:,0] irisDF[&#39;pca_y&#39;] = pca_transformed[:,1] #print(irisDF) maker0_ind = irisDF[irisDF[&#39;dbscan_cluster&#39;]==0].index maker1_ind = irisDF[irisDF[&#39;dbscan_cluster&#39;]==1].index maker2_ind = irisDF[irisDF[&#39;dbscan_cluster&#39;]==-1].index plt.scatter(x=irisDF.loc[maker0_ind,&#39;pca_x&#39;], y=irisDF.loc[maker0_ind, &#39;pca_y&#39;], marker=&#39;o&#39;) plt.scatter(x=irisDF.loc[maker1_ind,&#39;pca_x&#39;], y=irisDF.loc[maker1_ind, &#39;pca_y&#39;], marker=&#39;s&#39;) plt.scatter(x=irisDF.loc[maker2_ind,&#39;pca_x&#39;], y=irisDF.loc[maker2_ind, &#39;pca_y&#39;], marker=&#39;^&#39;) plt.xlabel(&#39;PCA 1&#39;) plt.ylabel(&#39;PCA 2&#39;) plt.title(&#39;3 Cluster Visualization by 2 PCA Components&#39;) plt.show() . iris 데이터를 2차원으로 차원 축소 한 뒤 시각화 하였습니다. . 세모 모양은 모델이 이상치로 분류 한 데이터 입니다. . &#54140;&#51648; &#44400;&#51665; . . fuzzy는 &#39;모호함, 애매함&#39; 의 뜻을 가지는데 각 데이터가 한 개 이상의 군집에 속할 수 있도록 군집을 형성하는 것 입니다. . 데이터의 분포를 보았을 때 특정 군집으로 분류하기 애매한 데이터가 많이 있다면 퍼지 군집을 선택하는 방법도 있습니다. . !pip install fuzzy-c-means import numpy as np from fcmeans import FCM from matplotlib import pyplot as plt . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting fuzzy-c-means Downloading fuzzy_c_means-1.6.3-py3-none-any.whl (9.1 kB) Requirement already satisfied: pydantic&lt;2.0.0,&gt;=1.8.2 in /usr/local/lib/python3.7/dist-packages (from fuzzy-c-means) (1.9.1) Collecting typer&lt;0.4.0,&gt;=0.3.2 Downloading typer-0.3.2-py3-none-any.whl (21 kB) Requirement already satisfied: numpy&lt;2.0.0,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from fuzzy-c-means) (1.21.6) Requirement already satisfied: tabulate&lt;0.9.0,&gt;=0.8.9 in /usr/local/lib/python3.7/dist-packages (from fuzzy-c-means) (0.8.10) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&lt;2.0.0,&gt;=1.8.2-&gt;fuzzy-c-means) (4.1.1) Requirement already satisfied: click&lt;7.2.0,&gt;=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer&lt;0.4.0,&gt;=0.3.2-&gt;fuzzy-c-means) (7.1.2) Installing collected packages: typer, fuzzy-c-means Attempting uninstall: typer Found existing installation: typer 0.4.2 Uninstalling typer-0.4.2: Successfully uninstalled typer-0.4.2 Successfully installed fuzzy-c-means-1.6.3 typer-0.3.2 . 퍼지 C-Means 모델을 사용하기 위해선 패키지 설치가 필요합니다. . n_samples = 5000 X = np.concatenate(( np.random.normal((-2, -2), size=(n_samples, 2)), np.random.normal((2, 2), size=(n_samples, 2)) )) # 클러스터 개수를 지정해야 합니다. fcm = FCM(n_clusters=2) fcm.fit(X) # outputs fcm_centers = fcm.centers fcm_labels = fcm.predict(X) # plot result f, axes = plt.subplots(1, 2, figsize=(11,5)) axes[0].scatter(X[:,0], X[:,1], alpha=.1) axes[1].scatter(X[:,0], X[:,1], c=fcm_labels, alpha=.1) axes[1].scatter(fcm_centers[:,0], fcm_centers[:,1], marker=&quot;+&quot;, s=500, c=&#39;w&#39;) plt.show() . 시각화 부분만 확인하면 군집의 수를 정해야하는 등 K-means와 유사합니다. . fcm.u . array([[0.018585 , 0.981415 ], [0.12806578, 0.87193422], [0.02850495, 0.97149505], ..., [0.94492172, 0.05507828], [0.99808667, 0.00191333], [0.97513129, 0.02486871]]) . 하지만 각 데이터 마다 어느 군집에 들어가 있는지 확률을 제공합니다. . 데이터마다 어느 군집에 속할 지 애매하거나 실제로 여러 군집에 부분적으로 속해있을 수 있는 경우 사용하기 좋습니다. . &#52280;&#44256;&#51088;&#47308; . Isolation Forest . https://hongl.tistory.com/150 . https://partrita.github.io/posts/isolation-forest/ . DBSCAN . https://needjarvis.tistory.com/720 . https://nicola-ml.tistory.com/10 . 퍼지군집 . https://syj9700.tistory.com/41 . https://towardsdatascience.com/fuzzy-c-means-clustering-is-it-better-than-k-means-clustering-448a0aba1ee7 .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/unsupervised%20learning/2022/07/27/unsupervised_learning.html",
            "relUrl": "/ssuda/jupyter/unsupervised%20learning/2022/07/27/unsupervised_learning.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[DACON] 쇼핑물 매출액 예측 경진대회",
            "content": "&#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . 개인 구글 드라이브에 파일을 업로드 한뒤 코렙 계정과 연동합니다. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/shop/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id Store Date Temperature Fuel_Price Promotion1 Promotion2 Promotion3 Promotion4 Promotion5 Unemployment IsHoliday Weekly_Sales . 0 1 | 1 | 05/02/2010 | 42.31 | 2.572 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1643690.90 | . 1 2 | 1 | 12/02/2010 | 38.51 | 2.548 | NaN | NaN | NaN | NaN | NaN | 8.106 | True | 1641957.44 | . 2 3 | 1 | 19/02/2010 | 39.93 | 2.514 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1611968.17 | . 3 4 | 1 | 26/02/2010 | 46.63 | 2.561 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1409727.59 | . 4 5 | 1 | 05/03/2010 | 46.50 | 2.625 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1554806.68 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 데이터 분석에 필요한 필수 패키지를 설치하고 데이터를 불러옵니다. . print(train.shape) print(test.shape) . (6255, 13) (180, 12) . 칼럼 개수는 총 12개, 트레인 데이터는 6255개, 테스트 데이터는 180개 입니다. 트레인 데이터가 어느정도 있습니다. . &#48152;&#51025; &#48320;&#49688; &#48516;&#49437; . plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train[&#39;Weekly_Sales&#39;].values)) plt.show() . 데이터 분석시 가장 먼저 반응변수 형태를 분석해야합니다. 매출액 데이터이기 때문에 튀는 값이 다소 관찰됩니다. . sns.distplot(train[&#39;Weekly_Sales&#39;], fit=stats.norm) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc627bdd6d0&gt; . 반응변수의 히스토그램과 만약 정규분포일 때 히스토그램을 비교했습니다. . 정규분포와 비교했을 때 오른쪽 꼬리가 다소 길어보입니다. . sns.distplot(np.log1p(train[&#39;Weekly_Sales&#39;]), fit=stats.norm) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc627bbe9d0&gt; . 로그변환시 이전보다 확실히 정규분포에 가까워졌습니다. . . &#48320;&#49688; &#51204;&#52376;&#47532; . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6255 entries, 0 to 6254 Data columns (total 13 columns): # Column Non-Null Count Dtype -- -- 0 id 6255 non-null int64 1 Store 6255 non-null int64 2 Date 6255 non-null object 3 Temperature 6255 non-null float64 4 Fuel_Price 6255 non-null float64 5 Promotion1 2102 non-null float64 6 Promotion2 1592 non-null float64 7 Promotion3 1885 non-null float64 8 Promotion4 1819 non-null float64 9 Promotion5 2115 non-null float64 10 Unemployment 6255 non-null float64 11 IsHoliday 6255 non-null bool 12 Weekly_Sales 6255 non-null float64 dtypes: bool(1), float64(9), int64(2), object(1) memory usage: 592.6+ KB . 데이터를 전반적으로 살펴보고 결측치 여부 또한 알아봅니다. Promotion 변수 5개가 결측값이 상당하네요. . import datetime train[&#39;Date&#39;] = pd.to_datetime(train[&#39;Date&#39;]) test[&#39;Date&#39;] = pd.to_datetime(test[&#39;Date&#39;]) train[&#39;Year&#39;] = train[&#39;Date&#39;].dt.year train[&#39;Month&#39;] = train[&#39;Date&#39;].dt.month test[&#39;Year&#39;] = test[&#39;Date&#39;].dt.year test[&#39;Month&#39;] = test[&#39;Date&#39;].dt.month train.head() . id Store Date Temperature Fuel_Price Promotion1 Promotion2 Promotion3 Promotion4 Promotion5 Unemployment IsHoliday Weekly_Sales Year Month . 0 1 | 1 | 2010-05-02 | 42.31 | 2.572 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1643690.90 | 2010 | 5 | . 1 2 | 1 | 2010-12-02 | 38.51 | 2.548 | NaN | NaN | NaN | NaN | NaN | 8.106 | True | 1641957.44 | 2010 | 12 | . 2 3 | 1 | 2010-02-19 | 39.93 | 2.514 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1611968.17 | 2010 | 2 | . 3 4 | 1 | 2010-02-26 | 46.63 | 2.561 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1409727.59 | 2010 | 2 | . 4 5 | 1 | 2010-05-03 | 46.50 | 2.625 | NaN | NaN | NaN | NaN | NaN | 8.106 | False | 1554806.68 | 2010 | 5 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 파이썬 내 datetime를 임포트하고 Date 값을 datetime으로 변환하면 손쉽게 날짜 데이터를 다룰수 있습니다. . 여기서 날짜 데이터의 연과 월 값을 간편하게 뽑아냈습니다. . print(sum(train[&#39;Promotion1&#39;] &lt; 0)) print(sum(train[&#39;Promotion2&#39;] &lt; 0)) print(sum(train[&#39;Promotion3&#39;] &lt; 0)) print(sum(train[&#39;Promotion4&#39;] &lt; 0)) print(sum(train[&#39;Promotion5&#39;] &lt; 0)) . 0 18 4 0 0 . Promotion 값이 0 미만인게 혹시 있는지 확인 했습니다. 일부 관측되는 것으로 보입니다. . train[&#39;Promotion1&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc62ad283d0&gt; . train[&#39;Promotion2&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc627af3bd0&gt; . train[&#39;Promotion3&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc627e43d50&gt; . train[&#39;Promotion4&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc627e52550&gt; . train[&#39;Promotion5&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc6289226d0&gt; . Promotion 변수의 히스토그램 5개를 확인했습니다. 대부분 작은 값을 가지고 있으나 일부 값이 극단적으로 만 단위를 넘어갑니다. . 정말 가끔씩 0 이하의 값도 존재하고 만이 넘어가는 값도 있고 결측값도 상당히 많아 어떤 변수인지 추측하기 어렵습니다. . tem = train.dropna() promotion = [&#39;Promotion1&#39;, &#39;Promotion2&#39;, &#39;Promotion3&#39;, &#39;Promotion4&#39;, &#39;Promotion5&#39;] plt.figure(figsize=(15,10)) ax = sns.heatmap(tem[promotion + [&#39;Weekly_Sales&#39;]].corr(), annot=True) plt.show() . 프로모션 변수를 더 관찰하기 위해 결측값이 있는 데이터는 제외하고 상관관계를 살펴보았습니다. . 프로모션1과 프로모션4가 연관이 상당히 있으며 프로모션2를 제외하곤 타겟값에 긍정적인 영향을 일부 끼친다고 볼 수 있습니다. . 즉 유의미한 변수이긴 하다라고 생각해야겠네요. . print(tem[&#39;Weekly_Sales&#39;].mean()) print(train[&#39;Weekly_Sales&#39;].mean()) . 1243139.141739766 1047619.073811351 . 그럼 프로모션 변수가 결측값인 것은 의미가 있을까가 궁금했는데요. . 프로모션 변수가 결측값이 아닌 데이터의 타겟값 평균이 전체 데이터의 타겟값 평균보다 꽤 큰 것을 알 수 있습니다. . train[train[&#39;Year&#39;] == 2010].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 2160 entries, 0 to 6163 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 id 2160 non-null int64 1 Store 2160 non-null int64 2 Date 2160 non-null datetime64[ns] 3 Temperature 2160 non-null float64 4 Fuel_Price 2160 non-null float64 5 Promotion1 0 non-null float64 6 Promotion2 0 non-null float64 7 Promotion3 0 non-null float64 8 Promotion4 0 non-null float64 9 Promotion5 0 non-null float64 10 Unemployment 2160 non-null float64 11 IsHoliday 2160 non-null bool 12 Weekly_Sales 2160 non-null float64 13 Year 2160 non-null int64 14 Month 2160 non-null int64 dtypes: bool(1), datetime64[ns](1), float64(9), int64(4) memory usage: 255.2 KB . 2010년 데이터입니다. 프로모션 변수는 모두 결측값입니다. . train[train[&#39;Year&#39;] == 2011].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 2340 entries, 48 to 6215 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 id 2340 non-null int64 1 Store 2340 non-null int64 2 Date 2340 non-null datetime64[ns] 3 Temperature 2340 non-null float64 4 Fuel_Price 2340 non-null float64 5 Promotion1 354 non-null float64 6 Promotion2 293 non-null float64 7 Promotion3 342 non-null float64 8 Promotion4 302 non-null float64 9 Promotion5 360 non-null float64 10 Unemployment 2340 non-null float64 11 IsHoliday 2340 non-null bool 12 Weekly_Sales 2340 non-null float64 13 Year 2340 non-null int64 14 Month 2340 non-null int64 dtypes: bool(1), datetime64[ns](1), float64(9), int64(4) memory usage: 276.5 KB . 2011년 데이터입니다. 프로모션 변수는 대부분 결측값이네요. . train[train[&#39;Year&#39;] == 2012].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1755 entries, 100 to 6254 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 id 1755 non-null int64 1 Store 1755 non-null int64 2 Date 1755 non-null datetime64[ns] 3 Temperature 1755 non-null float64 4 Fuel_Price 1755 non-null float64 5 Promotion1 1748 non-null float64 6 Promotion2 1299 non-null float64 7 Promotion3 1543 non-null float64 8 Promotion4 1517 non-null float64 9 Promotion5 1755 non-null float64 10 Unemployment 1755 non-null float64 11 IsHoliday 1755 non-null bool 12 Weekly_Sales 1755 non-null float64 13 Year 1755 non-null int64 14 Month 1755 non-null int64 dtypes: bool(1), datetime64[ns](1), float64(9), int64(4) memory usage: 207.4 KB . 2012년 데이터입니다. 프로모션 변수는 대부분 결측값이 아닙니다. . 2011년 특정 시점부터 프로모션 변수를 체크하기 시작됬다고 생각할 수 있습니다. . train.fillna(0, inplace = True) test.fillna(0, inplace = True) train[&#39;Promotion1&#39;][train[&#39;Promotion1&#39;] &lt; 0] = 0 train[&#39;Promotion2&#39;][train[&#39;Promotion2&#39;] &lt; 0] = 0 train[&#39;Promotion3&#39;][train[&#39;Promotion3&#39;] &lt; 0] = 0 train[&#39;Promotion4&#39;][train[&#39;Promotion4&#39;] &lt; 0] = 0 train[&#39;Promotion5&#39;][train[&#39;Promotion5&#39;] &lt; 0] = 0 test[&#39;Promotion1&#39;][test[&#39;Promotion1&#39;] &lt; 0] = 0 test[&#39;Promotion2&#39;][test[&#39;Promotion2&#39;] &lt; 0] = 0 test[&#39;Promotion3&#39;][test[&#39;Promotion3&#39;] &lt; 0] = 0 test[&#39;Promotion4&#39;][test[&#39;Promotion4&#39;] &lt; 0] = 0 test[&#39;Promotion5&#39;][test[&#39;Promotion5&#39;] &lt; 0] = 0 train[&#39;Promotion1&#39;] = np.log1p(train[&#39;Promotion1&#39;]) train[&#39;Promotion2&#39;] = np.log1p(train[&#39;Promotion2&#39;]) train[&#39;Promotion3&#39;] = np.log1p(train[&#39;Promotion3&#39;]) train[&#39;Promotion4&#39;] = np.log1p(train[&#39;Promotion4&#39;]) train[&#39;Promotion5&#39;] = np.log1p(train[&#39;Promotion5&#39;]) test[&#39;Promotion1&#39;] = np.log1p(test[&#39;Promotion1&#39;]) test[&#39;Promotion2&#39;] = np.log1p(test[&#39;Promotion2&#39;]) test[&#39;Promotion3&#39;] = np.log1p(test[&#39;Promotion3&#39;]) test[&#39;Promotion4&#39;] = np.log1p(test[&#39;Promotion4&#39;]) test[&#39;Promotion5&#39;] = np.log1p(test[&#39;Promotion5&#39;]) . 프로모션 변수가 결측값이 아닐때 타겟값이 커진다는 사실에 기반해 결측값인 경우 0으로 채워넣었습니다. . 또 프로모션 변수는 값이 극단적으로 큰 경우가 나오기 때문에 로그변환을 했습니다. . 이때 0보다 작은 값은 로그변환시 좋지 않으므로 모두 0으로 바꾼 뒤 로그변환을 했습니다. . &#48320;&#49688; &#44288;&#52272; . def discrete_plot(variable): plt.figure(figsize=(12,8)) sns.violinplot(x= train[variable], y= train[&#39;Weekly_Sales&#39;]) plt.tight_layout(rect=[0, 0.03, 1, 0.95]) plt.show() discrete_plot(&#39;Year&#39;) . 연도 변수별 매출액 그래프 입니다. 매해 비슷한 모습을 보입니다. 다만 2010, 2011년 매출액 피크지점이 다소 높아보이네요. . discrete_plot(&#39;Month&#39;) . 달별 매출액 그래프 입니다. 평균값이 달마다 조금씩 차이나 보입니다. 12월달이 역시 매출이 잘 나오네요. . discrete_plot(&#39;IsHoliday&#39;) . 공휴일 여부에 따른 매출액 분포입니다. 크진 않으나 약간의 차이가 보이네요. . discrete_plot(&#39;Store&#39;) . 매장 별 매출액 차이입니다. 사실 매장별로 매출액의 분포가 큰 폭으로 다른 것은 상식적으로 당연합니다. . 분석시 유의사항으로 매장들이 현재 숫자형 값으로 입력되어있는데 순서형 자료로 분석되서는 절때 안됩니다. . 매장 종류가 45가지나 되므로 트리모델을 사용하던, 회귀모형을 사용하던 1~45 값 그대로 넣는 것은 위험합니다. . 모델에 넣기 전 원-핫 인코딩을 수행하겠습니다. . continuou = [&#39;Temperature&#39;, &#39;Fuel_Price&#39;, &#39;Unemployment&#39;, &#39;Promotion1&#39;, &#39;Promotion2&#39;, &#39;Promotion3&#39;, &#39;Promotion4&#39;, &#39;Promotion5&#39;] plt.figure(figsize=(15,10)) ax = sns.heatmap(tem[continuou + [&#39;Weekly_Sales&#39;]].corr(), annot=True) plt.show() . 연속형 변수간 상관관계와 매출액과에 상관관계를 알아보고자 합니다. . 온도와 연료가격은 상관관계로 확인했을때는 별로 중요한 변수는 아닌걸로 판단됩니다. . 다만 상관관계가 타겟변수를 예측하는데 절대적인 수치는 아닌 점을 고려하여 정말 필요가 없는 변수라면 모델에서 알아서 걸러지겠지라는 마인드로 그대로 사용합니다. . &#47784;&#45944;&#47553; . train = pd.get_dummies(train, columns = [&#39;Store&#39;]) test = pd.get_dummies(test, columns = [&#39;Store&#39;]) print(train.shape) print(test.shape) train.head() . (6255, 59) (180, 58) . id Date Temperature Fuel_Price Promotion1 Promotion2 Promotion3 Promotion4 Promotion5 Unemployment ... Store_36 Store_37 Store_38 Store_39 Store_40 Store_41 Store_42 Store_43 Store_44 Store_45 . 0 1 | 2010-05-02 | 42.31 | 2.572 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.106 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2 | 2010-12-02 | 38.51 | 2.548 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.106 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 3 | 2010-02-19 | 39.93 | 2.514 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.106 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 2010-02-26 | 46.63 | 2.561 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.106 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 5 | 2010-05-03 | 46.50 | 2.625 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.106 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 59 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 앞서 언급한대로 매장별로 원핫인코딩을 수행합니다. . train_label = np.log1p(train[&#39;Weekly_Sales&#39;]) train.drop([&#39;Weekly_Sales&#39;, &#39;id&#39;, &#39;Date&#39;], axis = 1, inplace = True) test.drop([&#39;id&#39;, &#39;Date&#39;], axis = 1, inplace = True) . 타겟변수를 로그변환하고 필요없는 변수를 제거합니다. . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;Weekly_Sales&#39;] = np.expm1(rf.predict(test)) sample_submission.to_csv(&#39;Sales1.csv&#39;,index=False) . 간단한 렌덤포레스트 모델을 사용했습니다. 성능은 Public 기준 58454 정도 나오네요. . 더 다양한 모델을 사용하거나 하이퍼파라미터 튜닝을 한다면 점수가 다소 오를 수 있습니다. . 또 시도해볼만한 것은 시계열적 요소를 고려하면 좋을 것 같네요. . 읽어주셔서 감사합니다. .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/eda/regression/2022/07/23/dacon_shop.html",
            "relUrl": "/dacon/jupyter/eda/regression/2022/07/23/dacon_shop.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "[SSUDA] 영화 데이터를 활용한 추천시스템 실습",
            "content": "&#52628;&#52380;&#49884;&#49828;&#53596; . 단순하게 떠올릴 수 있는 영화추천시스템은 3가지가 있습니다. . 1) 보편적으로 많은 사람이 좋아하는 영화를 추천합니다. 가장 단순하고 강력한 방법입니다만 개인별 추천시스템과는 거리가 있습니다. . 2) 특정 항목 내 비슷한 내용이 있는 영화를 추천합니다. 영화의 장르, 감독, 설명, 배우 등을 고려합니다. 어떤 사람이 특정 영화를 좋아한다면 비슷한 성격의 영화도 좋아할 것이다라는 논리 입니다. . 3) 관심사가 비슷한 사용자를 매칭시키고 매칭된 사용자를 참고해 영화를 추천합니다. . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/movie/&#39; df1=pd.read_csv(path + &#39;tmdb_5000_credits.csv&#39;) df2=pd.read_csv(path + &#39;tmdb_5000_movies.csv&#39;) df1.columns = [&#39;id&#39;,&#39;tittle&#39;,&#39;cast&#39;,&#39;crew&#39;] df2= df2.merge(df1,on=&#39;id&#39;) df2.head() . budget genres homepage id keywords original_language original_title overview popularity production_companies ... runtime spoken_languages status tagline title vote_average vote_count tittle cast crew . 0 237000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.avatarmovie.com/ | 19995 | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | en | Avatar | In the 22nd century, a paraplegic Marine is di... | 150.437577 | [{&quot;name&quot;: &quot;Ingenious Film Partners&quot;, &quot;id&quot;: 289... | ... | 162.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}, {&quot;iso... | Released | Enter the World of Pandora. | Avatar | 7.2 | 11800 | Avatar | [{&quot;cast_id&quot;: 242, &quot;character&quot;: &quot;Jake Sully&quot;, &quot;... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | . 1 300000000 | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | http://disney.go.com/disneypictures/pirates/ | 285 | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | en | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | 139.082615 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}, {&quot;... | ... | 169.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | At the end of the world, the adventure begins. | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | Pirates of the Caribbean: At World&#39;s End | [{&quot;cast_id&quot;: 4, &quot;character&quot;: &quot;Captain Jack Spa... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | . 2 245000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.sonypictures.com/movies/spectre/ | 206647 | [{&quot;id&quot;: 470, &quot;name&quot;: &quot;spy&quot;}, {&quot;id&quot;: 818, &quot;name... | en | Spectre | A cryptic message from Bond’s past sends him o... | 107.376788 | [{&quot;name&quot;: &quot;Columbia Pictures&quot;, &quot;id&quot;: 5}, {&quot;nam... | ... | 148.0 | [{&quot;iso_639_1&quot;: &quot;fr&quot;, &quot;name&quot;: &quot;Fran u00e7ais&quot;},... | Released | A Plan No One Escapes | Spectre | 6.3 | 4466 | Spectre | [{&quot;cast_id&quot;: 1, &quot;character&quot;: &quot;James Bond&quot;, &quot;cr... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | . 3 250000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 80, &quot;nam... | http://www.thedarkknightrises.com/ | 49026 | [{&quot;id&quot;: 849, &quot;name&quot;: &quot;dc comics&quot;}, {&quot;id&quot;: 853,... | en | The Dark Knight Rises | Following the death of District Attorney Harve... | 112.312950 | [{&quot;name&quot;: &quot;Legendary Pictures&quot;, &quot;id&quot;: 923}, {&quot;... | ... | 165.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | The Legend Ends | The Dark Knight Rises | 7.6 | 9106 | The Dark Knight Rises | [{&quot;cast_id&quot;: 2, &quot;character&quot;: &quot;Bruce Wayne / Ba... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | . 4 260000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://movies.disney.com/john-carter | 49529 | [{&quot;id&quot;: 818, &quot;name&quot;: &quot;based on novel&quot;}, {&quot;id&quot;:... | en | John Carter | John Carter is a war-weary, former military ca... | 43.926995 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}] | ... | 132.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | Lost in our world, found in another. | John Carter | 6.1 | 2124 | John Carter | [{&quot;cast_id&quot;: 5, &quot;character&quot;: &quot;John Carter&quot;, &quot;c... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | . 5 rows × 23 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 데이터를 불러옵니다. . 1) &#48372;&#54200;&#51201;&#51004;&#47196; &#52572;&#44256;&#51032; &#50689;&#54868; &#52628;&#52380;&#54616;&#44592; . 기술이 고도화됨에 따라 개인별 맞춤 추천 시스템을 많이 주목하지만 보편적으로 최고인 영화를 과소평가해선 안됩니다. . 두 가지 모두 고려해야 더 좋은 효과를 보일 수 있습니다. . 최고의 영화를 판별하는 기준만 정한다면 쉽게 구할 수 있습니다. . 단순히 평점을 기준으로 한다면 소규모 평가가 이루워진 영화가 고평가 될 수 있습니다. . 그래서 다음과 같은 기준을 사용합니다. . . v는 평가한 인원의 수, m은 차트에 기록되기 위한 최소 평가 인원수 . R은 영화의 평점, C는 모든 영화 평점의 평균 입니다. . 직관적으로 식을 해석해보면 평가한 인원이 많다면(V가 크다면) 영화의 평점 영향력을 크게하겠다 입니다. . C= df2[&#39;vote_average&#39;].mean() C . 6.092171559442016 . 모든 영화의 평점의 평균은 10점만점에 6점정도 되네요. . m= df2[&#39;vote_count&#39;].quantile(0.9) m . 1838.4000000000015 . q_movies = df2.copy().loc[df2[&#39;vote_count&#39;] &gt;= m] q_movies.shape . (481, 23) . 보편적으로 최고의 영화다라고 판별하기 위해서는 최소한 영화 평가가 어느정도 있는 영화를 대상으로 판별해야합니다. . 평가 개수가 상위 10%인 영화만(481개) 선별합니다. . def weighted_rating(x, m=m, C=C): v = x[&#39;vote_count&#39;] R = x[&#39;vote_average&#39;] return (v/(v+m) * R) + (m/(m+v) * C) q_movies[&#39;score&#39;] = q_movies.apply(weighted_rating, axis=1) . 앞서 설명한 가중치를 이용해 영화를 평가하는 기준 값을 apply 함수를 사용해 구합니다. . q_movies = q_movies.sort_values(&#39;score&#39;, ascending=False) q_movies[[&#39;title&#39;, &#39;vote_count&#39;, &#39;vote_average&#39;, &#39;score&#39;]].head(10) . title vote_count vote_average score . 1881 The Shawshank Redemption | 8205 | 8.5 | 8.059258 | . 662 Fight Club | 9413 | 8.3 | 7.939256 | . 65 The Dark Knight | 12002 | 8.2 | 7.920020 | . 3232 Pulp Fiction | 8428 | 8.3 | 7.904645 | . 96 Inception | 13752 | 8.1 | 7.863239 | . 3337 The Godfather | 5893 | 8.4 | 7.851236 | . 95 Interstellar | 10867 | 8.1 | 7.809479 | . 809 Forrest Gump | 7927 | 8.2 | 7.803188 | . 329 The Lord of the Rings: The Return of the King | 8064 | 8.1 | 7.727243 | . 1990 The Empire Strikes Back | 5879 | 8.2 | 7.697884 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; q_movies = q_movies.sort_values(&#39;vote_average&#39;, ascending=False) q_movies[[&#39;title&#39;, &#39;vote_count&#39;, &#39;vote_average&#39;, &#39;score&#39;]].head(10) . title vote_count vote_average score . 1881 The Shawshank Redemption | 8205 | 8.5 | 8.059258 | . 3337 The Godfather | 5893 | 8.4 | 7.851236 | . 1818 Schindler&#39;s List | 4329 | 8.3 | 7.641883 | . 2731 The Godfather: Part II | 3338 | 8.3 | 7.515889 | . 662 Fight Club | 9413 | 8.3 | 7.939256 | . 3865 Whiplash | 4254 | 8.3 | 7.633781 | . 2294 Spirited Away | 3840 | 8.3 | 7.585209 | . 3232 Pulp Fiction | 8428 | 8.3 | 7.904645 | . 809 Forrest Gump | 7927 | 8.2 | 7.803188 | . 3057 American History X | 3016 | 8.2 | 7.401749 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 윗 코드는 새로운 지표 기준 상위 10개 영화, 밑 코드는 단순한 평점 기준 상위 10개 영화입니다. 1등을 제외하고 지표가 다릅니다. . 이러한 보편적으로 최고의 영화를 추천하는 방식은 특정 사용자의 흥미나 취향에 민감하지 않다는 단점이 있습니다. . 2) &#45236;&#50857; &#44592;&#48152; &#50689;&#54868; &#52628;&#52380;&#54616;&#44592; . df2[&#39;overview&#39;].head(5) . 0 In the 22nd century, a paraplegic Marine is di... 1 Captain Barbossa, long believed to be dead, ha... 2 A cryptic message from Bond’s past sends him o... 3 Following the death of District Attorney Harve... 4 John Carter is a war-weary, former military ca... Name: overview, dtype: object . 유사한 overview을 가진 영화를 추천하는 모델을 만들어보겠습니다. 그러기 위해선 overview 내 언어를 숫자로 변환해야하는데요. . 영어 기준으로 단어 단위로 나오는 횟수를 단순히 원핫인코딩 방식으로 기록하는 CountVectorizer 함수를 고려할 수 있습니다. . CountVectorizer 은 입력받은 모든 단어를 단어사전에 기록하고 원핫인코딩 방식으로 각 문장을 숫자로 반환해줍니다. . 다만 관계대명사 등 자주 나오는 단어이나 영화 내용을 파악하는데 불필요한 단어가 많이 기록된다는 단점이 있는데요. . 이를 보안한 것이 TfidfVectorizer 입니다. . from sklearn.feature_extraction.text import TfidfVectorizer tfidf = TfidfVectorizer(stop_words=&#39;english&#39;) df2[&#39;overview&#39;] = df2[&#39;overview&#39;].fillna(&#39;&#39;) tfidf_matrix = tfidf.fit_transform(df2[&#39;overview&#39;]) print(tfidf_matrix[1].toarray()) print(tfidf_matrix.shape) . [[0. 0. 0. ... 0. 0. 0.]] (4803, 20978) . TfidfVectorizer 의 토대가 되는 기호 정리부터 하겠습니다. . TF(Term Frequency) : 특정 단어가 하나의 데이터 안에서 등장하는 횟수. . DF(Document Frequency) : 특정 단어가 여러 데이터에 자주 등장하는지를 알려주는 지표. . IDF(Inverse Document Frequency) : DF에 역수. . TF-IDF : TF와 IDF를 곱한 값. 즉 TF가 높고, DF가 낮을수록 값이 커지는 것을 이용하는 것입니다. . 다른 문장에 많이 나오지 않는 단어는 고유한 그 영화만에 특성을 가진 단어라고 볼 수 있고 TF-IDF은 이를 반영한 지표입니다. . 약 4800개 영화에 21000개 정도 단어종류가 확인됬군요. . from sklearn.metrics.pairwise import linear_kernel cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) print(cosine_sim.shape) . (4803, 4803) . . 단어집합의 유사도 검정을 할 때 코사인 유사도 값을 많이 사용합니다. . cosine_sim 은 각각의 영화별 코사인 유사도 값을 기록한 행렬값 입니다. . indices = pd.Series(df2.index, index=df2[&#39;title&#39;]).drop_duplicates() def get_recommendations(title, cosine_sim=cosine_sim): # 입력받은 영화 제목의 인덱스 추출 idx = indices[title] # 입력받은 영화와 다른 모든 영화의 코사인 유사도를 행렬에서 추출함. # enumerate로 인덱스를 붙히고 리스트로 변환함. sim_scores = list(enumerate(cosine_sim[idx])) # 코사인 유사도가 높은 순서대로 정렬함. sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # 코사인 유사도가 높은 상위 10개 영화를 추출. 이때 인덱스 값이 같이 추출됨. sim_scores = sim_scores[1:11] # 코사인 유사도가 높은 상위 10개 영화의 인덱스만 추출 movie_indices = [i[0] for i in sim_scores] # 인덱스를 이용해 영화 제목 추출. return df2[&#39;title&#39;].iloc[movie_indices] . get_recommendations(&#39;The Dark Knight Rises&#39;) . 65 The Dark Knight 299 Batman Forever 428 Batman Returns 1359 Batman 3854 Batman: The Dark Knight Returns, Part 2 119 Batman Begins 2507 Slow Burn 9 Batman v Superman: Dawn of Justice 1181 JFK 210 Batman &amp; Robin Name: title, dtype: object . get_recommendations(&#39;The Avengers&#39;) . 7 Avengers: Age of Ultron 3144 Plastic 1715 Timecop 4124 This Thing of Ours 3311 Thank You for Smoking 3033 The Corruptor 588 Wall Street: Money Never Sleeps 2136 Team America: World Police 1468 The Fountain 1286 Snowpiercer Name: title, dtype: object . 영화 제목 입력받으면 그와 비슷한(코사인 유사도기반) overview를 가진 상위 10개 영화를 추출하는 함수를 제작했습니다. . 3) &#44288;&#49900;&#49324;&#44032; &#48708;&#49847;&#54620; &#49324;&#50857;&#51088;&#47196; &#50689;&#54868; &#52628;&#52380;&#54616;&#44592; . !pip install scikit-surprise . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.1.0) Requirement already satisfied: numpy&gt;=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.21.6) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0) Requirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.7.3) . surprise는 다양한 추천시스템 함수를 가진 패키지 입니다. . from surprise import Reader, Dataset, SVD from surprise.model_selection import cross_validate reader = Reader() ratings = pd.read_csv(path +&#39;ratings_small.csv&#39;) ratings.head() . userId movieId rating timestamp . 0 1 | 31 | 2.5 | 1260759144 | . 1 1 | 1029 | 3.0 | 1260759179 | . 2 1 | 1061 | 3.0 | 1260759182 | . 3 1 | 1129 | 2.0 | 1260759185 | . 4 1 | 1172 | 4.0 | 1260759205 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; surprise 패키지 내 SVD 함수를 이용하기 위해선 데이터 프레임이 반드시 (사용자ID, 영화ID, 평점) 순으로 있어야합니다. . data = Dataset.load_from_df(ratings[[&#39;userId&#39;, &#39;movieId&#39;, &#39;rating&#39;]], reader) svd = SVD() cross_validate(svd, data, measures=[&#39;RMSE&#39;], cv=5) . {&#39;fit_time&#39;: (8.233148574829102, 5.2691779136657715, 5.370789527893066, 5.306999921798706, 5.467830181121826), &#39;test_mae&#39;: array([0.68962625, 0.69583257, 0.68883765, 0.69072012, 0.69067091]), &#39;test_rmse&#39;: array([0.89545545, 0.89963625, 0.8925816 , 0.89423993, 0.90127922]), &#39;test_time&#39;: (0.15677785873413086, 0.14635586738586426, 0.15284276008605957, 0.15076279640197754, 0.14911937713623047)} . RMSE 값이 크진 않은 것 같습니다. . svd.predict(1, 1172, 3) . Prediction(uid=1, iid=1172, r_ui=3, est=3.423968547182108, details={&#39;was_impossible&#39;: False}) . ratings[(ratings[&#39;userId&#39;] == 1) &amp; (ratings[&#39;movieId&#39;] == 1172)] . userId movieId rating timestamp . 4 1 | 1172 | 4.0 | 1260759205 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 예측값과 실제값의 일부 사례를 관찰해보았습니다. . &#52636;&#52376; . 데이터 . https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata . https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset . 코드 . https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system . 참고자료 . https://wiserloner.tistory.com/917 . https://seing.tistory.com/67 .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/deep%20learning/matrix/svd/recommended%20system/kaggle/2022/07/14/movie.html",
            "relUrl": "/ssuda/jupyter/deep%20learning/matrix/svd/recommended%20system/kaggle/2022/07/14/movie.html",
            "date": " • Jul 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "[SSUDA] 트랜스포머 실습하기 with 데이콘 코드 유사성 대회",
            "content": ". &#44396;&#44544; &#46300;&#46972;&#51060;&#48652; &#50672;&#46041; &#48143; &#54056;&#53412;&#51648; &#49444;&#52824; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . 구글 드라이브와 연동해 파일을 불러옵니다. . import pandas as pd import numpy as np from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics.pairwise import cosine_similarity import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/coding/&#39; train = pd.read_csv(path + &#39;sample_train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . code1 code2 similar . 0 flag = &quot;go&quot; ncnt = 0 nwhile flag == &quot;go&quot;: n ... | # Python 3+ n#--... | 1 | . 1 b, c = map(int, input().split()) n nprint(b * c) | import numpy as np n nn = int(input()) na = np... | 0 | . 2 import numpy as np nimport sys nread = sys.std... | N, M = map(int, input().split()) nif M%2 != 0:... | 0 | . 3 b, c = map(int, input().split()) n nprint(b * c) | n,m=map(int,input().split()) nh=list(map(int,i... | 0 | . 4 s=input() nt=input() nans=0 nfor i in range(le... | import math na,b,h,m=map(int,input().split()) ... | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 필수 패키지를 불러오고 데이터를 불러옵니다. . !pip install transformers knockknock from knockknock import discord_sender . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting transformers Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB) |████████████████████████████████| 4.2 MB 5.3 MB/s Collecting knockknock Downloading knockknock-0.1.8.1-py3-none-any.whl (28 kB) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6) Collecting tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB) |████████████████████████████████| 6.6 MB 48.3 MB/s Collecting pyyaml&gt;=5.1 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 47.0 MB/s Collecting huggingface-hub&lt;1.0,&gt;=0.1.0 Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB) |████████████████████████████████| 86 kB 6.1 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.1.0-&gt;transformers) (4.2.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;transformers) (3.0.9) Collecting keyring Downloading keyring-23.5.1-py3-none-any.whl (33 kB) Collecting yagmail&gt;=0.11.214 Downloading yagmail-0.15.277-py2.py3-none-any.whl (17 kB) Collecting twilio Downloading twilio-7.9.1-py2.py3-none-any.whl (1.4 MB) |████████████████████████████████| 1.4 MB 45.6 MB/s Collecting matrix-client Downloading matrix_client-0.4.0-py2.py3-none-any.whl (43 kB) |████████████████████████████████| 43 kB 2.4 MB/s Collecting python-telegram-bot Downloading python_telegram_bot-13.12-py3-none-any.whl (511 kB) |████████████████████████████████| 511 kB 70.2 MB/s Collecting premailer Downloading premailer-3.10.0-py2.py3-none-any.whl (19 kB) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers) (3.8.0) Collecting jeepney&gt;=0.4.2 Downloading jeepney-0.8.0-py3-none-any.whl (48 kB) |████████████████████████████████| 48 kB 6.2 MB/s Collecting SecretStorage&gt;=3.2 Downloading SecretStorage-3.3.2-py3-none-any.whl (15 kB) Collecting cryptography&gt;=2.0 Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB) |████████████████████████████████| 4.0 MB 45.9 MB/s Requirement already satisfied: cffi&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography&gt;=2.0-&gt;SecretStorage&gt;=3.2-&gt;keyring-&gt;knockknock) (1.15.0) Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=2.0-&gt;SecretStorage&gt;=3.2-&gt;keyring-&gt;knockknock) (2.21) Requirement already satisfied: urllib3~=1.21 in /usr/local/lib/python3.7/dist-packages (from matrix-client-&gt;knockknock) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2022.5.18.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from premailer-&gt;yagmail&gt;=0.11.214-&gt;knockknock) (4.2.6) Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from premailer-&gt;yagmail&gt;=0.11.214-&gt;knockknock) (4.2.4) Collecting cssutils Downloading cssutils-2.4.0-py3-none-any.whl (404 kB) |████████████████████████████████| 404 kB 68.7 MB/s Collecting cssselect Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB) Collecting cachetools Downloading cachetools-4.2.2-py3-none-any.whl (11 kB) Collecting tornado&gt;=6.1 Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB) |████████████████████████████████| 428 kB 23.9 MB/s Requirement already satisfied: pytz&gt;=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot-&gt;knockknock) (2022.1) Collecting APScheduler==3.6.3 Downloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB) |████████████████████████████████| 58 kB 7.3 MB/s Requirement already satisfied: tzlocal&gt;=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3-&gt;python-telegram-bot-&gt;knockknock) (1.5.1) Requirement already satisfied: six&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3-&gt;python-telegram-bot-&gt;knockknock) (1.15.0) Requirement already satisfied: setuptools&gt;=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3-&gt;python-telegram-bot-&gt;knockknock) (57.4.0) Collecting PyJWT&lt;3.0.0,&gt;=2.0.0 Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB) Installing collected packages: jeepney, cssutils, cssselect, cryptography, cachetools, tornado, SecretStorage, pyyaml, PyJWT, premailer, APScheduler, yagmail, twilio, tokenizers, python-telegram-bot, matrix-client, keyring, huggingface-hub, transformers, knockknock Attempting uninstall: cachetools Found existing installation: cachetools 4.2.4 Uninstalling cachetools-4.2.4: Successfully uninstalled cachetools-4.2.4 Attempting uninstall: tornado Found existing installation: tornado 5.1.1 Uninstalling tornado-5.1.1: Successfully uninstalled tornado-5.1.1 Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-colab 1.0.0 requires tornado~=5.1.0; python_version &gt;= &#34;3.0&#34;, but you have tornado 6.1 which is incompatible. Successfully installed APScheduler-3.6.3 PyJWT-2.4.0 SecretStorage-3.3.2 cachetools-4.2.2 cryptography-37.0.2 cssselect-1.1.0 cssutils-2.4.0 huggingface-hub-0.7.0 jeepney-0.8.0 keyring-23.5.1 knockknock-0.1.8.1 matrix-client-0.4.0 premailer-3.10.0 python-telegram-bot-13.12 pyyaml-6.0 tokenizers-0.12.1 tornado-6.1 transformers-4.19.2 twilio-7.9.1 yagmail-0.15.277 . 디스코드 서버와 연동하는 패키지를 다운로드 합니다. 사용법은 밑에서 설명합니다. . print(train.shape) print(test.shape) . (17970, 3) (179700, 3) . BOW &#48169;&#48277; . tem = CountVectorizer() tem.fit(train[&#39;code1&#39;]) tem.vocabulary_ . {&#39;flag&#39;: 4281, &#39;go&#39;: 4603, &#39;cnt&#39;: 2991, &#39;while&#39;: 10034, &#39;int&#39;: 5201, &#39;input&#39;: 5137, &#39;if&#39;: 4958, &#39;stop&#39;: 9031, &#39;else&#39;: 3928, &#39;print&#39;: 7773, &#39;case&#39;: 2833, &#39;str&#39;: 9035, &#39;map&#39;: 6152, &#39;split&#39;: 8907, &#39;import&#39;: 5023, &#39;numpy&#39;: 7117, &#39;as&#39;: 2180, &#39;np&#39;: 7004, &#39;sys&#39;: 9222, &#39;read&#39;: 8108, &#39;stdin&#39;: 9007, &#39;buffer&#39;: 2639, &#39;readline&#39;: 8129, &#39;from&#39;: 4363, &#39;numba&#39;: 7090, &#39;njit&#39;: 6906, &#39;def&#39;: 3485, &#39;getinputs&#39;: 4546, &#39;cs&#39;: 3257, &#39;array&#39;: 2158, &#39;int32&#39;: 5203, &#39;26&#39;: 725, &#39;reshape&#39;: 8212, &#39;return&#39;: 8247, &#39;i8&#39;: 4903, &#39;i4&#39;: 4898, &#39;cache&#39;: 2753, &#39;true&#39;: 9668, &#39;_compute_score1&#39;: 1572, &#39;out&#39;: 7356, &#39;score&#39;: 8599, &#39;last&#39;: 5725, &#39;zeros&#39;: 10428, &#39;for&#39;: 4327, &#39;in&#39;: 5026, &#39;range&#39;: 8075, &#39;len&#39;: 5793, &#39;sum&#39;: 9115, &#39;step1&#39;: 9011, &#39;max_score&#39;: 6252, &#39;10000000&#39;: 225, &#39;best_i&#39;: 2428, &#39;append&#39;: 2103, &#39;pop&#39;: 7622, &#39;output&#39;: 7362, &#39;join&#39;: 5476, &#39;astype&#39;: 2199, &#39;tolist&#39;: 9583, &#39;ans&#39;: 2035, &#39;min&#39;: 6387, &#39;abs&#39;: 1835, &#39;rev&#39;: 8254, &#39;false&#39;: 4161, &#39;list&#39;: 5884, &#39;mini&#39;: 6435, &#39;10&#39;: 219, &#39;count&#39;: 3194, &#39;se&#39;: 8622, &#39;set&#39;: 8700, &#39;tot&#39;: 9595, &#39;add&#39;: 1875, &#39;yes&#39;: 10329, &#39;no&#39;: 6941, &#39;setrecursionlimit&#39;: 8719, &#39;week&#39;: 10019, &#39;sun&#39;: 9180, &#39;mon&#39;: 6544, &#39;tue&#39;: 9686, &#39;wed&#39;: 10018, &#39;thu&#39;: 9464, &#39;fri&#39;: 4357, &#39;sat&#39;: 8572, &#39;main&#39;: 6121, &#39;decode&#39;: 3478, &#39;__name__&#39;: 1532, &#39;__main__&#39;: 1529, &#39;10000&#39;: 222, &#39;break&#39;: 2604, &#39;and&#39;: 2025, &#39;exit&#39;: 4053, &#39;power&#39;: 7673, &#39;mod&#39;: 6498, &#39;bi&#39;: 2449, &#39;format&#39;: 4331, &#39;2進数&#39;: 846, &#39;res&#39;: 8203, &#39;reverse&#39;: 8258, &#39;temp1&#39;: 9363, &#39;temp2&#39;: 9364, &#39;temp&#39;: 9361, &#39;x_&#39;: 10138, &#39;a1&#39;: 1640, &#39;a2&#39;: 1643, &#39;a3&#39;: 1648, &#39;an&#39;: 2020, &#39;at&#39;: 2203, &#39;となるt&#39;: 10732, &#39;sのペアの数は&#39;: 9233, &#39;かつ&#39;: 10484, &#39;sのペアの数と等しく&#39;: 9232, &#39;sのペアの数と等しい&#39;: 9231, &#39;ms&#39;: 6577, &#39;ps&#39;: 7830, &#39;msc&#39;: 6579, &#39;psc&#39;: 7833, &#39;mi&#39;: 6368, &#39;pi&#39;: 7542, &#39;get&#39;: 4475, &#39;ii&#39;: 4973, &#39;li&#39;: 5827, &#39;d2&#39;: 3378, &#39;dict&#39;: 3572, &#39;dp0&#39;: 3737, &#39;dp1&#39;: 3738, &#39;dp2&#39;: 3739, &#39;dp3&#39;: 3740, &#39;max&#39;: 6202, &#39;math&#39;: 6185, &#39;string&#39;: 9059, &#39;itertools&#39;: 5377, &#39;fractions&#39;: 4351, &#39;heapq&#39;: 4778, &#39;collections&#39;: 3052, &#39;re&#39;: 8105, &#39;bisect&#39;: 2487, &#39;random&#39;: 8064, &#39;time&#39;: 9473, &#39;copy&#39;: 3175, &#39;functools&#39;: 4395, &#39;deque&#39;: 3524, &#39;inf&#39;: 5079, &#39;20&#39;: 635, &#39;998244353&#39;: 1449, &#39;dr&#39;: 3774, &#39;dc&#39;: 3445, &#39;li_&#39;: 5830, &#39;lf&#39;: 5819, &#39;float&#39;: 4303, &#39;ls&#39;: 6024, &#39;dp&#39;: 3736, &#39;reduce&#39;: 8149, &#39;gcd&#39;: 4445, &#39;la&#39;: 5708, &#39;inv&#39;: 5238, &#39;pow&#39;: 7661, &#39;lcm&#39;: 5753, &#39;addmod&#39;: 1886, &#39;answer&#39;: 2075, &#39;sums&#39;: 9176, &#39;try&#39;: 9673, &#39;except&#39;: 4034, &#39;eoferror&#39;: 3976, &#39;a_mod&#39;: 1713, &#39;hn&#39;: 4825, &#39;strip&#39;: 9065, &#39;mapint&#39;: 6158, &#39;accumulate&#39;: 1855, &#39;open&#39;: 7313, &#39;a_acc&#39;: 1665, &#39;initial&#39;: 5107, &#39;min_diff&#39;: 6395, &#39;left&#39;: 5777, &#39;right&#39;: 8278, &#39;となるものがいくつあるか&#39;: 10734, &#39;subdp&#39;: 9086, &#39;target&#39;: 9318, &#39;h1&#39;: 4689, &#39;m1&#39;: 6078, &#39;h2&#39;: 4691, &#39;m2&#39;: 6079, &#39;60&#39;: 1150, &#39;usr&#39;: 9819, &#39;bin&#39;: 2460, &#39;env&#39;: 3971, &#39;python3&#39;: 7876, &#39;chain&#39;: 2869, &#39;solve&#39;: 8852, &#39;current&#39;: 3317, &#39;一度も通ったことがない&#39;: 11203, &#39;ステップ目に通った事を記録&#39;: 11090, &#39;loop_len&#39;: 5978, &#39;ループの長さ&#39;: 11191, &#39;rest&#39;: 8219, &#39;残り長さ&#39;: 12102, &#39;残り長さをループの余剰にする&#39;: 12103, &#39;tokens&#39;: 9581, &#39;line&#39;: 5857, &#39;next&#39;: 6834, &#39;type&#39;: 9718, &#39;counter&#39;: 3219, &#39;decimal&#39;: 3475, &#39;numbers&#39;: 7105, &#39;book&#39;: 2565, &#39;bit全探索&#39;: 2511, &#39;xls&#39;: 10199, &#39;cost&#39;: 3185, &#39;x1&#39;: 10120, &#39;y1&#39;: 10287, &#39;int64&#39;: 5204, &#39;u4&#39;: 9732, &#39;uint32&#39;: 9759, &#39;argv&#39;: 2149, &#39;online_judge&#39;: 7299, &#39;pycc&#39;: 7865, &#39;cc&#39;: 2843, &#39;types&#39;: 9720, &#39;my_module&#39;: 6626, &#39;export&#39;: 4061, &#39;factorization&#39;: 4145, &#39;n_max&#39;: 6703, &#39;sqrt&#39;: 8926, &#39;sort&#39;: 8866, &#39;p_max&#39;: 7437, &#39;primes_num&#39;: 7770, &#39;shape&#39;: 8734, &#39;a_start&#39;: 1732, &#39;check&#39;: 2895, &#39;stack&#39;: 8961, &#39;empty&#39;: 3932, &#39;p_stack&#39;: 7449, &#39;compile&#39;: 3123, &#39;in_file&#39;: 5031, &#39;fromstring&#39;: 4367, &#39;sep&#39;: 8690, &#39;pairwise&#39;: 7464, &#39;coprime&#39;: 3174, &#39;setwise&#39;: 8725, &#39;not&#39;: 6974, &#39;dev&#39;: 3536, &#39;product&#39;: 7822, &#39;repeat&#39;: 8191, &#39;lr&#39;: 6015, &#39;csum&#39;: 3267, &#39;encoding&#39;: 3937, &#39;utf&#39;: 9828, &#39;bisect_left&#39;: 2488, &#39;これで二部探索の大小検索が行える&#39;: 10580, &#39;最小公倍数などはこっち&#39;: 11966, &#39;10進数で考慮できる&#39;: 308, &#39;再帰回数上限はでdefault1000&#39;: 11465, &#39;abssort&#39;: 1839, &#39;sorted&#39;: 8873, &#39;key&#39;: 5575, &#39;lambda&#39;: 5719, &#39;tmps&#39;: 9554, &#39;a_abs&#39;: 1664, &#39;tmp_1&#39;: 9504, &#39;deepcopy&#39;: 3483, &#39;tmp_2&#39;: 9507, &#39;tmp_2_&#39;: 9508, &#39;tmp_1_&#39;: 9505, &#39;を一つ消す&#39;: 10962, &#39;remove&#39;: 8176, &#39;を一つたす&#39;: 10961, &#39;がなかった時&#39;: 10508, &#39;を消して&#39;: 10994, &#39;を追加&#39;: 11011, &#39;tmp_1_m&#39;: 9506, &#39;tmp_2_m&#39;: 9509, &#39;pass&#39;: 7492, &#39;elif&#39;: 3924, &#39;正負に関係なくsort&#39;: 12092, &#39;a_p&#39;: 1718, &#39;plusを入れる&#39;: 7584, &#39;a_n&#39;: 1714, &#39;を入れる&#39;: 10973, &#39;ok&#39;: 7267, &#39;正の数が存在している&#39;: 12084, &#39;選択肢がない時&#39;: 12440, &#39;負の数が偶数個&#39;: 12367, &#39;奇数個選ぶ&#39;: 11713, &#39;position&#39;: 7646, &#39;pairs&#39;: 7463, &#39;cnt_p&#39;: 3011, &#39;cnt_n&#39;: 3010, &#39;move&#39;: 6566, &#39;enumerate&#39;: 3969, &#39;_update_score&#39;: 1625, &#39;_random_update&#39;: 1610, &#39;randint&#39;: 8063, &#39;new_score&#39;: 6819, &#39;_random_swap&#39;: 1609, &#39;delta&#39;: 3504, &#39;d1&#39;: 3375, &#39;or&#39;: 7325, &#39;step2&#39;: 9012, &#39;48&#39;: 1011, &#39;rand&#39;: 8058, &#39;13&#39;: 376, &#39;prime_numbers&#39;: 7765, &#39;n以下の素数列挙&#39;: 7202, &#39;eratosthenes&#39;: 3991, &#39;prime_list&#39;: 7761, &#39;prime_factorization&#39;: 7754, &#39;factors&#39;: 4152, &#39;tmp_n&#39;: 9529, &#39;ceil&#39;: 2856, &#39;count_divide&#39;: 3205, &#39;divmod&#39;: 3691, &#39;a_b_max&#39;: 1675, &#39;a_b_min&#39;: 1676, &#39;max_factors&#39;: 6224, &#39;min_factors&#39;: 6398, &#39;a_s&#39;: 1726, &#39;a_r&#39;: 1722, &#39;b_s&#39;: 2332, &#39;b_r&#39;: 2330, &#39;unsafe&#39;: 9783, &#39;safe&#39;: 8551, &#39;tmp&#39;: 9500, &#39;raw_input&#39;: 8087, &#39;linked&#39;: 5874, &#39;defaultdict&#39;: 3487, &#39;i2group&#39;: 4896, &#39;gid&#39;: 4583, &#39;get_root&#39;: 4514, &#39;get_groups&#39;: 4491, &#39;ra&#39;: 8040, &#39;rb&#39;: 8090, &#39;continue&#39;: 3162, &#39;n_connected&#39;: 6685, &#39;n_group&#39;: 6694, &#39;values&#39;: 9876, &#39;clear&#39;: 2951, &#39;zip&#39;: 10438, &#39;iter&#39;: 5371, &#39;class&#39;: 2948, &#39;facts&#39;: 4154, &#39;max_num&#39;: 6242, &#39;__init__&#39;: 1525, &#39;self&#39;: 8682, &#39;fact&#39;: 4128, &#39;power_func&#39;: 7674, &#39;comb&#39;: 3077, &#39;log&#39;: 5954, &#39;r26&#39;: 7999, &#39;r25&#39;: 7998, &#39;25&#39;: 717, &#39;total&#39;: 9601, &#39;s_n&#39;: 8531, &#39;end&#39;: 3938, &#39;is&#39;: 5282, &#39;dice&#39;: 3558, &#39;ns&#39;: 7018, &#39;ew&#39;: 4020, &#39;question&#39;: 7973, &#39;top&#39;: 9587, &#39;front&#39;: 4368, &#39;settop&#39;: 8724, &#39;sides&#39;: 8770, &#39;index&#39;: 5060, &#39;tail&#39;: 9291, &#39;insert&#39;: 5182, &#39;dnum&#39;: 3711, &#39;readlines&#39;: 8130, &#39;lower&#39;: 6004, &#39;rstrip&#39;: 8427, &#39;word&#39;: 10080, &#39;abcdefghijklmnopqrstuvwxyz&#39;: 1823, &#39;insertionsort&#39;: 5187, &#39;lst&#39;: 6035, &#39;pajew&#39;: 7465, &#39;1000000&#39;: 224, &#39;find&#39;: 4229, &#39;unite&#39;: 9778, &#39;grou&#39;: 4646, &#39;arr&#39;: 2155, &#39;gro&#39;: 4644, &#39;gro_no&#39;: 4645, &#39;popleft&#39;: 7635, &#39;distance&#39;: 3649, &#39;returns&#39;: 8252, &#39;minkowski&#39;: 6440, &#39;of&#39;: 7253, &#39;vactor&#39;: 9862, &#39;chebyshev&#39;: 2894, &#39;6f&#39;: 1202, &#39;000000&#39;: 3, &#39;449490&#39;: 991, &#39;154435&#39;: 437, &#39;run&#39;: 8437, &#39;dim&#39;: 3606, &#39;flake8&#39;: 4295, &#39;noqa&#39;: 6967, &#39;building_a&#39;: 2649, &#39;11&#39;: 309, &#39;building_b&#39;: 2650, &#39;building_c&#39;: 2651, &#39;building_d&#39;: 2652, &#39;stdout&#39;: 9009, &#39;write&#39;: 10104, &#39;p_list&#39;: 7432, &#39;c_list&#39;: 2738, &#39;score1&#39;: 8600, &#39;score2&#39;: 8601, &#39;div&#39;: 3667, &#39;getn&#39;: 4554, &#39;getnm&#39;: 4555, &#39;getlist&#39;: 4552, &#39;getarray&#39;: 4534, &#39;intn&#39;: 5227, &#39;rand_n&#39;: 8061, &#39;ran1&#39;: 8056, &#39;ran2&#39;: 8057, &#39;rand_list&#39;: 8060, &#39;rantime&#39;: 8079, &#39;rand_ints_nodup&#39;: 8059, &#39;rand_query&#39;: 8062, &#39;r_query&#39;: 8036, &#39;n_q&#39;: 6709, &#39;combinations&#39;: 3089, &#39;permutations&#39;: 7529, &#39;operator&#39;: 7318, &#39;mul&#39;: 6589, &#39;bisect_right&#39;: 2492, &#39;1000000000&#39;: 227, &#39;code&#39;: 3028, &#39;limit回までコストカットできる&#39;: 5852, &#39;knapsack_6&#39;: 5603, &#39;upper&#39;: 9796, &#39;limit&#39;: 5851, &#39;weight&#39;: 10021, &#39;value&#39;: 9869, &#39;ボーナスでコスト１にするのを使ったか&#39;: 11148, &#39;コストカットできる時&#39;: 11056, &#39;できない時&#39;: 10685, &#39;1000&#39;: 221, &#39;back&#39;: 2344, &#39;namedtuple&#39;: 6725, &#39;uf&#39;: 9753, &#39;rank&#39;: 8078, &#39;size&#39;: 8801, &#39;root&#39;: 8359, &#39;same&#39;: 8565, &#39;friends&#39;: 4359, &#39;block&#39;: 2537, &#39;153_b&#39;: 433, &#39;a0&#39;: 1631, &#39;102&#39;: 264, &#39;log2&#39;: 5956, &#39;logn&#39;: 5966, &#39;db&#39;: 3438, &#39;dbs&#39;: 3441, &#39;now&#39;: 6981, &#39;dll&#39;: 3698, &#39;command&#39;: 3108, &#39;appendleft&#39;: 2107, &#39;delete&#39;: 3497, &#39;deletefirst&#39;: 3499, &#39;coding&#39;: 3033, &#39;sr&#39;: 8934, &#39;ir&#39;: 5276, &#39;左からgreedyに&#39;: 11744, &#39;monsters&#39;: 6549, &#39;bomb&#39;: 2556, &#39;attack&#39;: 2219, &#39;cook&#39;: 3169, &#39;your&#39;: 10360, &#39;dish&#39;: 3632, &#39;here&#39;: 4790, &#39;400&#39;: 956, &#39;599&#39;: 1115, &#39;600&#39;: 1151, &#39;799&#39;: 1279, &#39;800&#39;: 1312, &#39;999&#39;: 1450, &#39;1199&#39;: 338, &#39;1200&#39;: 345, &#39;1399&#39;: 389, &#39;1400&#39;: 391, &#39;1599&#39;: 444, &#39;1600&#39;: 446, &#39;1799&#39;: 480, &#39;1800&#39;: 484, &#39;1999&#39;: 520, &#39;merge&#39;: 6348, &#39;mid&#39;: 6376, &#39;global&#39;: 4594, &#39;n1&#39;: 6664, &#39;n2&#39;: 6669, &#39;mergesort&#39;: 6352, &#39;e_red_scarf&#39;: 3852, &#39;mask&#39;: 6166, &#39;1e9&#39;: 539, &#39;coefs&#39;: 3036, &#39;14&#39;: 390, &#39;22&#39;: 690, &#39;33&#39;: 882, &#39;46&#39;: 999, &#39;15&#39;: 421, &#39;hon&#39;: 4834, &#39;pon&#39;: 7617, &#39;bon&#39;: 2563, &#39;num&#39;: 7039, &#39;ca&#39;: 2752, &#39;val&#39;: 9863, &#39;items&#39;: 5370, &#39;sa&#39;: 8547, &#39;ng&#39;: 6878, &#39;方針&#39;: 11892, &#39;各文字列の出現回数を数え&#39;: 11628, &#39;出現回数が最大なる文字列を昇順に出力する&#39;: 11485, &#39;リスト&#39;: 11168, &#39;は辞書型のサブクラスであり&#39;: 10900, &#39;キーに要素&#39;: 11033, &#39;値に出現回数という形式&#39;: 11381, &#39;most_common&#39;: 6554, &#39;要素&#39;: 12305, &#39;出現回数&#39;: 11481, &#39;というタプルを出現回数順に並べたリスト&#39;: 10713, &#39;max_count&#39;: 6215, &#39;最大の出現回数&#39;: 11945, &#39;出現回数が最も多い単語を集計する&#39;: 11484, &#39;昇順にソートして出力&#39;: 11903, &#39;resolve&#39;: 8216, &#39;300000&#39;: 854, &#39;200000&#39;: 638, &#39;100000&#39;: 223, &#39;bubble_sort_aoj&#39;: 2629, &#39;nums&#39;: 7120, &#39;バブルソート&#39;: 11123, &#39;隣接項の比較&#39;: 12482, &#39;fibo&#39;: 4199, &#39;result&#39;: 8225, &#39;groupby&#39;: 4659, &#39;100&#39;: 220, &#39;101&#39;: 258, &#39;解説と&#39;: 12331, &#39;13355391&#39;: 380, &#39;を参考に実装予定&#39;: 10981, &#39;lonlieness&#39;: 5971, &#39;ab&#39;: 1757, &#39;bad_a&#39;: 2347, &#39;bad_b&#39;: 2348, &#39;_gcd&#39;: 1582, &#39;setdefault&#39;: 8717, &#39;仲の悪いグループも登録しておく&#39;: 11334, &#39;pair&#39;: 7459, &#39;keys&#39;: 5584, &#39;仲の悪いグループは隣り合っているので飛び石で計算&#39;: 11333, &#39;gourp1&#39;: 4617, &#39;から1匹以上選ぶパターン&#39;: 10488, &#39;group2&#39;: 4650, &#39;どちらからも選ばないパターン計算する&#39;: 10758, &#39;group1&#39;: 4649, &#39;は仲が悪いので同時に選ばれることはない&#39;: 10891, &#39;group_num&#39;: 4657, &#39;badgroup_num&#39;: 2350, &#39;全員と仲が悪いイワシのパターンを足し&#39;: 11446, &#39;すべてのイワシを選ばないパターンを除外&#39;: 10608, &#39;mn&#39;: 6490, &#39;diff&#39;: 3589, &#39;500&#39;: 1050, &#39;isupper&#39;: 5358, &#39;sum1&#39;: 9116, &#39;sum2&#39;: 9117, &#39;del&#39;: 3496, &#39;shellsort&#39;: 8744, &#39;262913&#39;: 727, &#39;65921&#39;: 1178, &#39;16577&#39;: 456, &#39;4193&#39;: 974, &#39;1073&#39;: 286, &#39;281&#39;: 740, &#39;77&#39;: 1266, &#39;23&#39;: 700, &#39;n以下が確定していて&#39;: 7198, &#39;0以外の数をk個使ったとき&#39;: 211, &#39;n以下が確定していないときの0以外の数の個数&#39;: 7199, &#39;0を使うことで0以外の数が増えないパターン&#39;: 204, &#39;0以外の数を使うことで0以外の数が増えるパターン&#39;: 212, &#39;今回でn以下が確定するパターン&#39;: 11301, &#39;確定する前までに0以外の数を何個使っているか&#39;: 12177, &#39;今回でn以下が確定することはない&#39;: 11300, &#39;すでにk個以上の0以外の数を使っているとき&#39;: 10602, &#39;ちょうどk個使っている時&#39;: 10670, &#39;0を使うしかない&#39;: 205, &#39;n以下を確定させるためaは使えない&#39;: 7203, &#39;にぶたん&#39;: 10800, &#39;n人のメンバーそれぞれが完食にかかる時間のうち最大値をxに以下にできるか&#39;: 7195, &#39;12&#39;: 343, &#39;need_training&#39;: 6782, &#39;cond&#39;: 3138, &#39;rotate&#39;: 8374, &#39;deck&#39;: 3477, &#39;nnnn&#39;: 6938, &#39;query&#39;: 7971, &#39;num_of_sug&#39;: 7072, &#39;sug&#39;: 9104, &#39;tuple&#39;: 9688, &#39;liar&#39;: 5834, &#39;honest&#39;: 4836, &#39;sug_tmp&#39;: 9106, &#39;sug_&#39;: 9105, &#39;hi&#39;: 4796, &#39;hihi&#39;: 4805, &#39;hihihi&#39;: 4806, &#39;hihihihi&#39;: 4807, &#39;hihihihihi&#39;: 4808, &#39;statistics&#39;: 8999, &#39;amed&#39;: 2015, &#39;median&#39;: 6328, &#39;bmed&#39;: 2547, &#39;sgn&#39;: 8731, &#39;pfugou&#39;: 7536, &#39;選んでないのが2個以下&#39;: 12437, &#39;よってlen&#39;: 10939, &#39;前述で処理済み&#39;: 11546, &#39;なのでここでやることはない&#39;: 10778, &#39;b_num&#39;: 2328, &#39;b_best&#39;: 2309, &#39;a_num&#39;: 1717, &#39;maxmize&#39;: 6282, &#39;none&#39;: 6964, &#39;all_max&#39;: 1977, &#39;st&#39;: 8953, &#39;scores&#39;: 8612, &#39;num_elem&#39;: 7058, &#39;all_sum&#39;: 1980, &#39;max_&#39;: 6205, &#39;max_r&#39;: 6249, &#39;temp_r&#39;: 9379, &#39;temp_max&#39;: 9377, &#39;chr&#39;: 2929, &#39;ord&#39;: 7327, &#39;head&#39;: 4767, &#39;s_temp&#39;: 8544, &#39;dig_0_index&#39;: 3594, &#39;dig_1_index&#39;: 3595, &#39;dig_2_index&#39;: 3596, &#39;_input&#39;: 1588, &#39;wa&#39;: 9980, &#39;ac&#39;: 1841, &#39;str_l&#39;: 9049, &#39;int_l&#39;: 5210, &#39;pp&#39;: 7680, &#39;seikai&#39;: 8665, &#39;matigai&#39;: 6186, &#39;correct&#39;: 3181, &#39;mistake&#39;: 6473, &#39;io&#39;: 5259, &#39;stringio&#39;: 9063, &#39;kuku&#39;: 5627, &#39;s_set&#39;: 8541, &#39;ansl&#39;: 2068, &#39;mat_sum&#39;: 6177, &#39;xrange&#39;: 10224, &#39;color&#39;: 3056, &#39;dfs&#39;: 3540, &#39;mydict&#39;: 6640, &#39;answer_list&#39;: 2077, &#39;wd&#39;: 10011, &#39;ck&#39;: 2946, &#39;pe&#39;: 7513, &#39;penalty&#39;: 7521, &#39;cp&#39;: 3235, &#39;r_map&#39;: 8032, &#39;r_list&#39;: 8031, &#39;最大公約数&#39;: 11957, &#39;最小公倍数&#39;: 11965, &#39;gcd_num&#39;: 4452, &#39;lcm_num&#39;: 5762, &#39;die&#39;: 3583, &#39;pips&#39;: 7550, &#39;move_die&#39;: 6569, &#39;direction&#39;: 3617, &#39;get_upside&#39;: 4530, &#39;init_die&#39;: 5093, &#39;pip&#39;: 7549, &#39;roll_die&#39;: 8333, &#39;directions&#39;: 3618, &#39;maxs&#39;: 6287, &#39;mins&#39;: 6447, &#39;offset&#39;: 7257, &#39;1000000007&#39;: 236, &#39;matrix&#39;: 6189, &#39;cv&#39;: 3342, &#39;fv&#39;: 4399, &#39;simu&#39;: 8784, &#39;pro&#39;: 7809, &#39;end_time&#39;: 3942, &#39;bool&#39;: 2569, &#39;name&#39;: 6724, &#39;inds&#39;: 5078, &#39;ds&#39;: 3779, &#39;xy&#39;: 10236, &#39;この２行でメモリアクセス省略しないとtleになる&#39;: 10573, &#39;nds&#39;: 6770, &#39;c1&#39;: 2717, &#39;c2&#39;: 2718, &#39;164&#39;: 452, &#39;1415926535898&#39;: 403, &#39;08&#39;: 104, &#39;point&#39;: 7603, &#39;sharp&#39;: 8735, &#39;enumerate_divisors&#39;: 3970, &#39;all_divisors&#39;: 1972, &#39;divisor&#39;: 3685, &#39;calculate_reminder&#39;: 2779, &#39;reminder&#39;: 8174, &#39;sorted_lst&#39;: 8880, &#39;qs&#39;: 7951, &#39;rs&#39;: 8414, &#39;sect&#39;: 8640, &#39;lstrip&#39;: 6040, &#39;水たまりを結合&#39;: 12110, &#39;sum_l&#39;: 9136, &#39;room&#39;: 8349, &#39;_s&#39;: 1611, &#39;bitsum&#39;: 2507, &#39;_bit&#39;: 1567, &#39;bitadd&#39;: 2501, &#39;al&#39;: 1940, &#39;al_to_idx&#39;: 1942, &#39;init&#39;: 5091, &#39;n_&#39;: 6678, &#39;bit&#39;: 2494, &#39;idx&#39;: 4946, &#39;_query&#39;: 1607, &#39;decrement&#39;: 3482, &#39;old&#39;: 7277, &#39;increment&#39;: 5052, &#39;_ans&#39;: 1564, &#39;money&#39;: 6546, &#39;inputs&#39;: 5175, &#39;ss&#39;: 8944, &#39;deg&#39;: 3490, &#39;30&#39;: 850, &#39;180&#39;: 483, &#39;360&#39;: 908, &#39;radians&#39;: 8049, &#39;cos&#39;: 3182, &#39;sin&#39;: 8785, &#39;110000&#39;: 312, &#39;kk&#39;: 5595, &#39;lu&#39;: 6049, &#39;1e18&#39;: 537, &#39;2019&#39;: 660, &#39;相対速度&#39;: 12168, &#39;距離&#39;: 12384, &#39;eval&#39;: 4011, &#39;train&#39;: 9638, &#39;回数&#39;: 11669, &#39;nlogn&#39;: 6919, &#39;時間&#39;: 11919, &#39;nlog&#39;: 6918, &#39;maxk&#39;: 6280, &#39;kaisuu&#39;: 5551, &#39;get_theta&#39;: 4523, &#39;m_angle&#39;: 6092, &#39;h_angle&#39;: 4696, &#39;calculate_vector_distance&#39;: 2781, &#39;theta&#39;: 9451, &#39;dictionary&#39;: 3579, &#39;input_num&#39;: 5160, &#39;lim&#39;: 5849, &#39;200004&#39;: 645, &#39;bin_sum&#39;: 2465, &#39;bin_sum2&#39;: 2466, &#39;pop_num&#39;: 7627, &#39;200005&#39;: 646, &#39;整数&#39;: 11865, &#39;整数複数個&#39;: 11871, &#39;改行区切り&#39;: 11846, &#39;スペース区切り&#39;: 11092, &#39;の行列&#39;: 10869, &#39;abc&#39;: 1764, &#39;table&#39;: 9285, &#39;have&#39;: 4753, &#39;check_p&#39;: 2902, &#39;ws&#39;: 10108, &#39;ct&#39;: 3268, &#39;nt&#39;: 7028, &#39;can_eat&#39;: 2791, &#39;f_time&#39;: 4112, &#39;training&#39;: 9640, &#39;high&#39;: 4798, &#39;low&#39;: 6003, &#39;casefold&#39;: 2834, &#39;end_of_text&#39;: 3940, &#39;syo&#39;: 9218, &#39;amari&#39;: 2011, &#39;dic&#39;: 3553, &#39;kaisu&#39;: 5550, &#39;long&#39;: 5969, &#39;ae&#39;: 1903, &#39;bs_meguru&#39;: 2615, &#39;isok&#39;: 5345, &#39;to&#39;: 9563, &#39;59&#39;: 1112, &#39;shell&#39;: 8742, &#39;leng&#39;: 5803, &#39;lists&#39;: 5910, &#39;eg&#39;: 3893, &#39;xs&#39;: 10225, &#39;seen&#39;: 8652, &#39;init_cmb&#39;: 5092, &#39;nmax&#39;: 6925, &#39;出力の制限&#39;: 11478, &#39;g1&#39;: 4410, &#39;元テーブル&#39;: 11397, &#39;g2&#39;: 4411, &#39;逆元テーブル&#39;: 12404, &#39;inverse&#39;: 5247, &#39;逆元テーブル計算用テーブル&#39;: 12405, &#39;cmb&#39;: 2972, &#39;modn&#39;: 6527, &#39;ci&#39;: 2932, &#39;である個数が&#39;: 10683, &#39;となるような数列の数は&#39;: 10735, &#39;ncm&#39;: 6752, &#39;hm&#39;: 4820, &#39;1cn&#39;: 531, &#39;で足算する&#39;: 10707, &#39;wk&#39;: 10059, &#39;hw&#39;: 4864, &#39;cnt_h&#39;: 3008, &#39;cnt_w&#39;: 3015, &#39;max_h&#39;: 6226, &#39;max_w&#39;: 6262, &#39;h_list&#39;: 4704, &#39;w_list&#39;: 9972, &#39;find_primes&#39;: 4240, &#39;rn&#39;: 8324, &#39;prev&#39;: 7720, &#39;pos&#39;: 7639, &#39;alp&#39;: 1989, &#39;atoi&#39;: 2218, &#39;insort_left&#39;: 5191, &#39;tle&#39;: 9491, &#39;dtype&#39;: 3789, &#39;listをsortする&#39;: 5915, &#39;a0cen&#39;: 1636, &#39;b0cen&#39;: 2284, &#39;nn&#39;: 6932, &#39;a0cen1&#39;: 1637, &#39;b0cen1&#39;: 2285, &#39;ei&#39;: 3901, &#39;mx&#39;: 6610, &#39;su&#39;: 9077, &#39;graph&#39;: 4627, &#39;numofedges&#39;: 7114, &#39;visited&#39;: 9908, &#39;edges&#39;: 3883, &#39;col&#39;: 3041, &#39;adj&#39;: 1889, &#39;maxcolor&#39;: 6270, &#39;ans_1&#39;: 2040, &#39;ans_2&#39;: 2041, &#39;amax&#39;: 2014, &#39;n0&#39;: 6661, &#39;1以上となる最小の2のべき乗数&#39;: 598, &#39;afre&#39;: 1907, &#39;パワーの頻度&#39;: 11130, &#39;は切り捨てなので&#39;: 10892, &#39;rintで四捨五入してから&#39;: 8298, &#39;rint&#39;: 8297, &#39;fft&#39;: 4191, &#39;irfft&#39;: 5278, &#39;rfft&#39;: 8267, &#39;scum&#39;: 8618, &#39;cumsum&#39;: 3290, &#39;累積和&#39;: 12229, &#39;bd&#39;: 2393, &#39;上からm個を取り出したい&#39;: 11223, &#39;searchsorted&#39;: 8629, &#39;価値iを生み出せる組みがm個以上ある&#39;: 11367, &#39;価値iが生み出せる選び方の余分なものを引きたい&#39;: 11366, &#39;ret&#39;: 8235, &#39;numberofcards&#39;: 7103, &#39;far&#39;: 4162, &#39;kyu&#39;: 5643, &#39;dist&#39;: 3638, &#39;vec&#39;: 9890, &#39;morau&#39;: 6551, &#39;factinv&#39;: 4136, &#39;solver&#39;: 8863, &#39;bombs&#39;: 2559, &#39;maxx&#39;: 6299, &#39;maxy&#39;: 6300, &#39;gcd1&#39;: 4446, &#39;cmath&#39;: 2971, &#39;inp&#39;: 5125, &#39;nm&#39;: 6923, &#39;heapify&#39;: 4773, &#39;heappop&#39;: 4775, &#39;heappush&#39;: 4776, &#39;17&#39;: 464, &#39;day&#39;: 3432, &#39;data&#39;: 3419, &#39;全探索なら&#39;: 11448, &#39;4000&#39;: 957, &#39;bit全探索でok&#39;: 2512, &#39;一文字ずつlistへ格納&#39;: 11206, &#39;most&#39;: 6553, &#39;縦の全loop&#39;: 12261, &#39;aa&#39;: 1740, &#39;where&#39;: 10032, &#39;b1&#39;: 2286, &#39;b2&#39;: 2288, &#39;b3&#39;: 2289, &#39;alphabet&#39;: 1996, &#39;loop&#39;: 5972, &#39;i1&#39;: 4893, &#39;i2&#39;: 4895, &#39;96&#39;: 1430, &#39;1100&#39;: 311, &#39;get_dist&#39;: 4485, &#39;du&#39;: 3791, &#39;dv&#39;: 3799, &#39;fullmatch&#39;: 4390, &#39;exame&#39;: 4030, &#39;suma&#39;: 9165, &#39;連想配列&#39;: 12413, &#39;先頭からの番号&#39;: 11405, &#39;余分な量&#39;: 11352, &#39;que&#39;: 7965, &#39;k番目以降は一番左のやつ消していく&#39;: 5677, &#39;cur&#39;: 3295, &#39;examf&#39;: 4031, &#39;si&#39;: 8765, &#39;fact_inv&#39;: 4131, &#39;getdivisor&#39;: 4539, &#39;sum_leaf&#39;: 9137, &#39;before_top&#39;: 2410, &#39;29&#39;: 754, &#39;iim&#39;: 4980, &#39;p25&#39;: 7397, &#39;p26&#39;: 7398, &#39;p26inv&#39;: 7399, &#39;576923081&#39;: 1106, &#39;elem&#39;: 3919, &#39;dq&#39;: 3770, &#39;order&#39;: 7330, &#39;deletelast&#39;: 3501, &#39;koch&#39;: 5609, &#39;start&#39;: 8972, &#39;途中の頂点をa&#39;: 12408, &#39;cとする&#39;: 3366, &#39;rr&#39;: 8410, &#39;segmenttree&#39;: 8662, &#39;非再帰&#39;: 12491, &#39;segment&#39;: 8661, &#39;tree&#39;: 9646, &#39;func&#39;: 4392, &#39;配列の長さ&#39;: 12448, &#39;minだとrmqになる&#39;: 6463, &#39;木の高さhとすると&#39;: 12002, &#39;1までのノード数&#39;: 587, &#39;h段目のノードにアクセスするために使う&#39;: 4871, &#39;ノード&#39;: 11119, &#39;parent&#39;: 7475, &#39;child&#39;: 2914, &#39;1とk&#39;: 567, &#39;bit_length&#39;: 2498, &#39;あたいの初期化&#39;: 10470, &#39;build&#39;: 2641, &#39;setの後に一斉更新&#39;: 8727, &#39;reversed&#39;: 8262, &#39;update&#39;: 9791, &#39;aに更新する&#39;: 2267, &#39;更新ぶんをrootまで更新&#39;: 11927, &#39;のfuncを求める&#39;: 10818, &#39;queries&#39;: 7970, &#39;a2n&#39;: 1646, &#39;createinp&#39;: 3250, &#39;seg&#39;: 8658, &#39;terms&#39;: 9396, &#39;51&#39;: 1073, &#39;goukei&#39;: 4616, &#39;route&#39;: 8386, &#39;obs&#39;: 7238, &#39;length&#39;: 5804, &#39;alpha2num&#39;: 1995, &#39;alpha&#39;: 1994, &#39;item&#39;: 5366, &#39;num2alpha&#39;: 7044, &#39;64&#39;: 1169, &#39;90&#39;: 1397, &#39;ap&#39;: 2098, &#39;bust&#39;: 2664, &#39;win&#39;: 10047, &#39;graph_input&#39;: 4630, &#39;friend&#39;: 4358, &#39;group&#39;: 4648, &#39;で頂点&#39;: 10709, &#39;がどの&#39;: 10507, &#39;に属するかを記録していく&#39;: 10809, &#39;後に&#39;: 11782, &#39;に対して&#39;: 10807, ...} . CountVectorizer 함수로 입력되는 단어를 숫자와 매칭시킵니다. . https://datascienceschool.net/03%20machine%20learning/03.01.03%20Scikit-Learn%EC%9D%98%20%EB%AC%B8%EC%84%9C%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%8A%A5.html . tem.transform([train[&#39;code1&#39;][0]]).toarray() . array([[0, 0, 0, ..., 0, 0, 0]]) . tem.transform([train[&#39;code1&#39;][0]]).toarray().shape . (1, 12552) . code1에 있는 단어 중 단어 집합에 해당하는 단어가 있는 경우에만 1을 출력하는 백터로 변환합니다. . cosine_similarity(tem.transform([train[&#39;code1&#39;][0]]), tem.transform([train[&#39;code2&#39;][0]])) . array([[0.32871913]]) . train[&#39;similar&#39;][0] . 1 . cosine_similarity 함수는 위에서 원-핫 인코딩 형태로 변환 된 단어들을 보고 유사성 여부를 판단합니다. . 0.32로 유사성이 일부 있는 것으로 보이는데 실제 두 코드는 유사한 코드 입니다. . https://wikidocs.net/24603 . class BaselineModel(): def __init__(self, threshold = 0.5): super(BaselineModel, self).__init__() self.threshold = threshold self.vectorizer = CountVectorizer() def fit(self, code1, code2): self.vectorizer.fit(code1) self.vectorizer.fit(code2) print(&#39;Done.&#39;) def predict_proba(self, code1, code2): code1_vecs = self.vectorizer.transform(code1) code2_vecs = self.vectorizer.transform(code2) preds = [] for code1_vec, code2_vec in zip(code1_vecs, code2_vecs): preds.append(cosine_similarity(code1_vec, code2_vec)) preds = np.reshape(preds, len(preds)) print(&#39;Done.&#39;) return preds @discord_sender(webhook_url=&quot;https://discordapp.com/api/webhooks/9810o3fUYfVz2jWg7if&quot;) def predict(self, code1, code2): preds = self.predict_proba(code1, code2) preds = np.where(preds &gt; self.threshold, 1, 0) return preds . 트레인 데이터를 이용해 단어 집합을 만들고 테스트 데이터를 원-핫 인코딩 방식으로 변환합니다. . 다음으로 cosine_similarity 함수를 사용해 원-핫 인코딩 벡터의 유사성을 검정해 임개값보다 크면 1, 작으면 0을 출력합니다. . 딥러닝이라기 보다 단순한 컴퓨터 노가다에 가깝죠. 베이스라인으로 사용하기에 좋은 모델인 것 같습니다. . 참고로 함수 위에 @discord_sender(webhook_url = &#39;디스코드 웹 서버&#39;)를 입력하면 함수 시작하는 시점, 끝나는 시점이 디스코드 알림으로 옵니다. . 학습하는데 오래걸리는 딥러닝 모델에 경우, 핸드폰으로 알림을 받을 수 있으니 알아두면 정말 좋은 기능이 될 것 같아요. . model = BaselineModel(threshold = 0.4) model.fit(train[&#39;code1&#39;], train[&#39;code2&#39;]) . Done. . preds = model.predict(test[&#39;code1&#39;], test[&#39;code2&#39;]) . Done. . (train[&#39;similar&#39;]).mean() . 0.5011129660545354 . (preds).mean() . 0.5092877017250974 . threshold = 0.5일때 약 0.3, threshold = 0.3일때 약 0.7, threshold = 0.4일때 약 0.5를 가지는 것을 확인했습니다. . 실제 트레인 데이터의 유사성이 1일 확률이 0.5에 가까움으로 임계값은 0.4로 잡겠습니다. . (대회는 ACCURACY 기준) . predtrain = model.predict(train[&#39;code1&#39;], train[&#39;code2&#39;]) (predtrain == train[&#39;similar&#39;]).mean() . Done. . 0.7125765164162493 . 임계값 0.4를 사용해 트레인 데이터를 예측에 사용하면 약 0.71에 정확성을 보입니다. . 사실 거창한 딥러닝을 사용하지 않아도 쓰는 단어가 얼추 비슷하면 유사성 판단은 어느정도는 하는 것을 알 수 있습니다. . sample_submission[&#39;similar&#39;] = preds sample_submission.to_csv(&#39;dacon_codes.csv&#39;, index = False) # 결과 : 0.688 . huggingface &#53812; &#49324;&#50857; . !pip install transformers !pip install transformers datasets . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2) Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3) Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.1.0-&gt;transformers) (4.2.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;transformers) (3.0.9) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers) (3.8.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2022.5.18.1) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2) Collecting datasets Downloading datasets-2.2.2-py3-none-any.whl (346 kB) |████████████████████████████████| 346 kB 4.3 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3) Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1) Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.1.0-&gt;transformers) (4.2.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;transformers) (3.0.9) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5) Collecting xxhash Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB) |████████████████████████████████| 212 kB 10.1 MB/s Requirement already satisfied: pyarrow&gt;=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1) Collecting fsspec[http]&gt;=2021.05.0 Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB) |████████████████████████████████| 140 kB 10.9 MB/s Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 12.0 MB/s Collecting dill&lt;0.3.5 Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB) |████████████████████████████████| 86 kB 6.1 MB/s Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2) Collecting responses&lt;0.19 Downloading responses-0.18.0-py3-none-any.whl (38 kB) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2022.5.18.1) Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 28.0 MB/s Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (21.4.0) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 28.3 MB/s Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB) |████████████████████████████████| 94 kB 1.8 MB/s Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB) |████████████████████████████████| 144 kB 29.7 MB/s Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (2.0.12) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers) (3.8.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;datasets) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;datasets) (2022.1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;datasets) (1.15.0) Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, responses, datasets Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 Attempting uninstall: dill Found existing installation: dill 0.3.5.1 Uninstalling dill-0.3.5.1: Successfully uninstalled dill-0.3.5.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-colab 1.0.0 requires tornado~=5.1.0; python_version &gt;= &#34;3.0&#34;, but you have tornado 6.1 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2 . from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding from datasets import load_dataset, load_metric import torch #device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot; model = &quot;klue/bert-base&quot; # &#39;microsoft/graphcodebert-base&#39; MAX_LEN = 256 dataset = load_dataset(&#39;csv&#39;, data_files = path+&#39;sample_train.csv&#39;)[&#39;train&#39;] tokenizer = AutoTokenizer.from_pretrained(model) . Using custom data configuration default-e6c40baceae51225 Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-e6c40baceae51225/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519) https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjiwfrzjb storing https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843 creating metadata file for /root/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843 https://huggingface.co/klue/bert-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3hhezuli storing https://huggingface.co/klue/bert-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621 creating metadata file for /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621 loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621 Model config BertConfig { &#34;_name_or_path&#34;: &#34;klue/bert-base&#34;, &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;classifier_dropout&#34;: null, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 512, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.19.2&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 32000 } https://huggingface.co/klue/bert-base/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf76t5zqa storing https://huggingface.co/klue/bert-base/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456 creating metadata file for /root/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456 https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5jkm4tpg storing https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7 creating metadata file for /root/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7 https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp34fn1p8b storing https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26 creating metadata file for /root/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26 loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456 loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7 loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26 loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843 loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621 Model config BertConfig { &#34;_name_or_path&#34;: &#34;klue/bert-base&#34;, &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;classifier_dropout&#34;: null, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 512, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.19.2&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 32000 } . load_dataset 함수 : csv 파일을 가공하기 쉬운 데이터 셋으로 만들어줍니다. . AutoTokenizer내 from_pretrained 함수에서 프리트레인 모델 이름만 입력하면 자동으로 토크나이징이 됩니다. . def example_fn(examples): outputs = tokenizer(examples[&#39;code1&#39;], examples[&#39;code2&#39;], padding = True, max_length = MAX_LEN, truncation = True) if &#39;similar&#39; in examples: outputs[&#39;labels&#39;] = examples[&#39;similar&#39;] return outputs dataset = dataset.map(example_fn, remove_columns = [&#39;code1&#39;, &#39;code2&#39;, &#39;similar&#39;]) dataset = dataset.train_test_split(0.1) dataset . DatasetDict({ train: Dataset({ features: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;], num_rows: 16173 }) test: Dataset({ features: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;], num_rows: 1797 }) }) . dataset 내 map 함수 : 원소별로 입력된 함수를 적용합니다. . dataset 내 train_test_split 함수 : 트레인-테스트 데이터 셋으로 분할한 딕셔너리를 만듭니다. . _collator = DataCollatorWithPadding(tokenizer = tokenizer) # 아래 사진으로 해당함수 설명 _metric = load_metric(&#39;glue&#39;, &#39;sst2&#39;) # 측정함수도 huggingface 내 존재 # https://huggingface.co/docs/datasets/v1.0.1/loading_metrics.html 참고문서 def metric_fn(p): # 측정함수 preds, labels = p output = _metric.compute(references = labels, predictions = np.argmax(preds, axis = -1)) return output model = BertForSequenceClassification.from_pretrained(model) args = TrainingArguments( &#39;runs/&#39;, per_device_train_batch_size = 32, num_train_epochs = 3, do_train = True, do_eval = True, save_strategy = &#39;epoch&#39;, logging_strategy = &#39;epoch&#39;, evaluation_strategy = &#39;epoch&#39;, ) . loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621 Model config BertConfig { &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;classifier_dropout&#34;: null, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 512, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.19.2&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 32000 } https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpb8lhvjfk storing https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6 creating metadata file for /root/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6 loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6 Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. PyTorch: setting up devices The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-). . . trainer = Trainer( model = model, args = args, data_collator = _collator, train_dataset = dataset[&#39;train&#39;], eval_dataset = dataset[&#39;test&#39;], tokenizer = tokenizer, compute_metrics = metric_fn ) @discord_sender(webhook_url=&quot;https://discordapp.com/api/webhooks/98101yo3fUYfVz2jWg7if&quot;) def tem(): trainer.train() tem() . ***** Running training ***** Num examples = 16173 Num Epochs = 3 Instantaneous batch size per device = 32 Total train batch size (w. parallel, distributed &amp; accumulation) = 32 Gradient Accumulation steps = 1 Total optimization steps = 1518 . . [1518/1518 39:04, Epoch 3/3] Epoch Training Loss Validation Loss Accuracy . 1 | 0.304100 | 0.188209 | 0.920423 | . 2 | 0.117700 | 0.130098 | 0.960490 | . 3 | 0.036200 | 0.093612 | 0.978297 | . &lt;/div&gt; &lt;/div&gt; ***** Running Evaluation ***** Num examples = 1797 Batch size = 8 Saving model checkpoint to runs/checkpoint-506 Configuration saved in runs/checkpoint-506/config.json Model weights saved in runs/checkpoint-506/pytorch_model.bin tokenizer config file saved in runs/checkpoint-506/tokenizer_config.json Special tokens file saved in runs/checkpoint-506/special_tokens_map.json ***** Running Evaluation ***** Num examples = 1797 Batch size = 8 Saving model checkpoint to runs/checkpoint-1012 Configuration saved in runs/checkpoint-1012/config.json Model weights saved in runs/checkpoint-1012/pytorch_model.bin tokenizer config file saved in runs/checkpoint-1012/tokenizer_config.json Special tokens file saved in runs/checkpoint-1012/special_tokens_map.json ***** Running Evaluation ***** Num examples = 1797 Batch size = 8 Saving model checkpoint to runs/checkpoint-1518 Configuration saved in runs/checkpoint-1518/config.json Model weights saved in runs/checkpoint-1518/pytorch_model.bin tokenizer config file saved in runs/checkpoint-1518/tokenizer_config.json Special tokens file saved in runs/checkpoint-1518/special_tokens_map.json Training completed. Do not forget to share your model on huggingface.co/models =) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; test_dataset = load_dataset(&#39;csv&#39;, data_files = path+&#39;test.csv&#39;)[&#39;train&#39;] test_dataset = test_dataset.map(example_fn, remove_columns = [&#39;code1&#39;, &#39;code2&#39;]) predictions = trainer.predict(test_dataset) sample_submission[&#39;similar&#39;] = np.argmax(predictions.predictions, axis = -1) sample_submission.to_csv(&#39;dacon_codes2.csv&#39;, index = False) # 결과 : 0.787 . Using custom data configuration default-6692cc772abf77e3 Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-6692cc772abf77e3/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519) The following columns in the test set don&#39;t have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: pair_id. If pair_id are not expected by `BertForSequenceClassification.forward`, you can safely ignore this message. ***** Running Prediction ***** Num examples = 179700 Batch size = 8 . . [22463/22463 49:25] predictions.predictions . array([[ 4.3174033, -3.926483 ], [-4.4421177, 4.0519753], [-4.2312655, 3.7239847], ..., [ 2.8522801, -2.7274592], [-4.4676304, 4.039054 ], [ 2.7921119, -2.535519 ]], dtype=float32) . sample_submission[&#39;similar&#39;].mean() . 0.6730996104618809 . &lt;/div&gt; .",
            "url": "https://ksy1526.github.io/myblog/ssuda/transformer/deep%20learning/natural%20language/dacon/2022/06/01/dacon_coding.html",
            "relUrl": "/ssuda/transformer/deep%20learning/natural%20language/dacon/2022/06/01/dacon_coding.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "[SSUDA] 빅데이터 분석 과목 코드 정리",
            "content": ". &#44396;&#44544; &#50672;&#46041;(&#47196;&#52972;&#54872;&#44221;&#50640;&#49436;&#45716; &#48520;&#54596;&#50836;&#54632;) . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . path = &#39;/content/drive/MyDrive/bigdatas/&#39; . &#45336;&#54028;&#51060; . import numpy as np ar1= np.arange(15).reshape(3,5) # arange, reshape ar1 . array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) . A = np.array([[1,2], [3,4]]) B = np.array([[2,3], [4,5]]) A@B # 넘파이 내적연산 . array([[10, 13], [22, 29]]) . np.random.randn(3,2) . array([[ 0.41582846, 0.45521608], [ 0.9416985 , -0.06815956], [ 0.48875541, 0.53657774]]) . np.random.randint(1, 100, size=6).reshape(3,2) . array([[43, 60], [16, 85], [72, 13]]) . B = np.arange(12).reshape (3,4) B . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . B.sum(axis=0) # 열 기준 연산 . array([12, 15, 18, 21]) . B.mean(axis=1) # 행 기준 연산 . array([1.5, 5.5, 9.5]) . x = np.arange(30).reshape((5,6))[:,::2] # 열 기준 0,2,4 컬럼 뽑기 x . array([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16], [18, 20, 22], [24, 26, 28]]) . &#54032;&#45796;&#49828; . import pandas as pd # 데이터프레임 딕셔너리로 만들기 dic= { &#39;gender&#39; : [ 1, 2, 1,2], &#39;bloodtype&#39;: [&quot;A&quot;, &quot;B&quot;, &quot;O&quot;, &quot;AB&quot;]} df1= pd.DataFrame(dic) df1 . gender bloodtype . 0 1 | A | . 1 2 | B | . 2 1 | O | . 3 2 | AB | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; dates = pd.date_range(&quot;20210309&quot;, periods=6) # 데이터프레임 넘파이로 만들기 df = pd.DataFrame(np.random.randn(6, 4), index= dates, columns=list(&quot;ABCD&quot;)) df . A B C D . 2021-03-09 1.408594 | -0.360413 | -0.658324 | -1.524480 | . 2021-03-10 -0.199638 | 0.808160 | 0.973753 | -0.213467 | . 2021-03-11 -0.312141 | -0.199871 | 0.492419 | -0.654314 | . 2021-03-12 1.626976 | -0.194653 | -1.064668 | -1.527266 | . 2021-03-13 -0.248378 | -0.575600 | -2.008171 | -1.580801 | . 2021-03-14 0.114982 | -0.451940 | 1.172818 | 0.967527 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.tail() # 밑에서부터 5개 출력 . A B C D . 2021-03-10 -0.199638 | 0.808160 | 0.973753 | -0.213467 | . 2021-03-11 -0.312141 | -0.199871 | 0.492419 | -0.654314 | . 2021-03-12 1.626976 | -0.194653 | -1.064668 | -1.527266 | . 2021-03-13 -0.248378 | -0.575600 | -2.008171 | -1.580801 | . 2021-03-14 0.114982 | -0.451940 | 1.172818 | 0.967527 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.describe() . A B C D . count 6.000000 | 6.000000 | 6.000000 | 6.000000 | . mean 0.398399 | -0.162386 | -0.182029 | -0.755467 | . std 0.882163 | 0.497636 | 1.262302 | 1.013996 | . min -0.312141 | -0.575600 | -2.008171 | -1.580801 | . 25% -0.236193 | -0.429058 | -0.963082 | -1.526570 | . 50% -0.042328 | -0.280142 | -0.082952 | -1.089397 | . 75% 1.085191 | -0.195957 | 0.853420 | -0.323679 | . max 1.626976 | 0.808160 | 1.172818 | 0.967527 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.sort_values(by=&quot;B&quot;) # B 기준으로 정렬 . A B C D . 2021-03-13 -0.248378 | -0.575600 | -2.008171 | -1.580801 | . 2021-03-14 0.114982 | -0.451940 | 1.172818 | 0.967527 | . 2021-03-09 1.408594 | -0.360413 | -0.658324 | -1.524480 | . 2021-03-11 -0.312141 | -0.199871 | 0.492419 | -0.654314 | . 2021-03-12 1.626976 | -0.194653 | -1.064668 | -1.527266 | . 2021-03-10 -0.199638 | 0.808160 | 0.973753 | -0.213467 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df=pd.read_csv(path + &quot;studentlist.csv&quot;, encoding=&quot;cp949&quot;) df . name sex age grade absence bloodtype height weight dept . 0 김길동 | 남자 | 23 | 3 | 유 | O | 165.3 | 68.2 | 2 | . 1 이미린 | 여자 | 22 | 2 | 무 | AB | 170.1 | 53.0 | 3 | . 2 홍길동 | 남자 | 24 | 4 | 무 | B | 175.0 | 80.1 | 1 | . 3 김철수 | 남자 | 23 | 3 | 무 | AB | 182.1 | 85.7 | 2 | . 4 손세수 | 여자 | 20 | 1 | 유 | A | 168.0 | 49.5 | 1 | . 5 박미희 | 여자 | 21 | 2 | 무 | O | 162.0 | 52.0 | 3 | . 6 강수친 | 여자 | 22 | 1 | 무 | O | 155.2 | 45.3 | 2 | . 7 이희수 | 여자 | 23 | 1 | 무 | A | 176.9 | 55.0 | 1 | . 8 이철린 | 남자 | 23 | 3 | 무 | B | 178.5 | 64.2 | 1 | . 9 방희철 | 남자 | 22 | 2 | 무 | B | 176.1 | 61.3 | 3 | . 10 박수호 | 남자 | 24 | 4 | 유 | O | 167.1 | 62.0 | 3 | . 11 임동민 | 남자 | 22 | 2 | 무 | AB | 180.0 | 75.8 | 3 | . 12 김민수 | 남자 | 21 | 1 | 무 | A | 162.2 | 55.3 | 1 | . 13 이희진 | 여자 | 23 | 3 | 무 | O | 176.1 | 53.1 | 2 | . 14 김미진 | 여자 | 22 | 2 | 무 | B | 158.2 | 45.2 | 3 | . 15 김동수 | 남자 | 24 | 4 | 유 | B | 168.6 | 70.2 | 1 | . 16 여수근 | 남자 | 21 | 1 | 무 | A | 169.2 | 62.2 | 2 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df[0:3] . name sex age grade absence bloodtype height weight dept . 0 김길동 | 남자 | 23 | 3 | 유 | O | 165.3 | 68.2 | 2 | . 1 이미린 | 여자 | 22 | 2 | 무 | AB | 170.1 | 53.0 | 3 | . 2 홍길동 | 남자 | 24 | 4 | 무 | B | 175.0 | 80.1 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df[[&quot;age&quot;, &quot;grade&quot;]] . age grade . 0 23 | 3 | . 1 22 | 2 | . 2 24 | 4 | . 3 23 | 3 | . 4 20 | 1 | . 5 21 | 2 | . 6 22 | 1 | . 7 23 | 1 | . 8 23 | 3 | . 9 22 | 2 | . 10 24 | 4 | . 11 22 | 2 | . 12 21 | 1 | . 13 23 | 3 | . 14 22 | 2 | . 15 24 | 4 | . 16 21 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.iloc[1:5, 0:3] # 행, 열 . name sex age . 1 이미린 | 여자 | 22 | . 2 홍길동 | 남자 | 24 | . 3 김철수 | 남자 | 23 | . 4 손세수 | 여자 | 20 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.loc[df[&quot;bloodtype&quot;].isin( [&quot;B&quot;, &quot;A&quot;] ), [&quot;name&quot;, &quot;age&quot;]] . name age . 2 홍길동 | 24 | . 4 손세수 | 20 | . 7 이희수 | 23 | . 8 이철린 | 23 | . 9 방희철 | 22 | . 12 김민수 | 21 | . 14 김미진 | 22 | . 15 김동수 | 24 | . 16 여수근 | 21 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2 = df[1:3] # 2개 row df3 = df[5:8] df4 = pd.concat([df2, df3], axis=0) # 행 기준으로 합체 df4 . name sex age grade absence bloodtype height weight dept . 1 이미린 | 여자 | 22 | 2 | 무 | AB | 170.1 | 53.0 | 3 | . 2 홍길동 | 남자 | 24 | 4 | 무 | B | 175.0 | 80.1 | 1 | . 5 박미희 | 여자 | 21 | 2 | 무 | O | 162.0 | 52.0 | 3 | . 6 강수친 | 여자 | 22 | 1 | 무 | O | 155.2 | 45.3 | 2 | . 7 이희수 | 여자 | 23 | 1 | 무 | A | 176.9 | 55.0 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; dff= pd.read_csv(path+&quot;footsize.csv&quot;, encoding=&quot;cp949&quot;) df3= pd.merge(df, dff, left_on=&#39;name&#39;, right_on=&#39;realname&#39;) df3 . name sex age grade absence bloodtype height weight dept realname footsize . 0 김길동 | 남자 | 23 | 3 | 유 | O | 165.3 | 68.2 | 2 | 김길동 | 275 | . 1 이미린 | 여자 | 22 | 2 | 무 | AB | 170.1 | 53.0 | 3 | 이미린 | 245 | . 2 김철수 | 남자 | 23 | 3 | 무 | AB | 182.1 | 85.7 | 2 | 김철수 | 280 | . 3 손세수 | 여자 | 20 | 1 | 유 | A | 168.0 | 49.5 | 1 | 손세수 | 240 | . 4 박미희 | 여자 | 21 | 2 | 무 | O | 162.0 | 52.0 | 3 | 박미희 | 240 | . 5 강수친 | 여자 | 22 | 1 | 무 | O | 155.2 | 45.3 | 2 | 강수친 | 245 | . 6 이희수 | 여자 | 23 | 1 | 무 | A | 176.9 | 55.0 | 1 | 이희수 | 245 | . 7 이철린 | 남자 | 23 | 3 | 무 | B | 178.5 | 64.2 | 1 | 이철린 | 260 | . 8 방희철 | 남자 | 22 | 2 | 무 | B | 176.1 | 61.3 | 3 | 방희철 | 275 | . 9 박수호 | 남자 | 24 | 4 | 유 | O | 167.1 | 62.0 | 3 | 박수호 | 280 | . 10 임동민 | 남자 | 22 | 2 | 무 | AB | 180.0 | 75.8 | 3 | 임동민 | 280 | . 11 김민수 | 남자 | 21 | 1 | 무 | A | 162.2 | 55.3 | 1 | 김민수 | 270 | . 12 이희진 | 여자 | 23 | 3 | 무 | O | 176.1 | 53.1 | 2 | 이희진 | 245 | . 13 김미진 | 여자 | 22 | 2 | 무 | B | 158.2 | 45.2 | 3 | 김미진 | 235 | . 14 김동수 | 남자 | 24 | 4 | 유 | B | 168.6 | 70.2 | 1 | 김동수 | 265 | . 15 여수근 | 남자 | 21 | 1 | 무 | A | 169.2 | 62.2 | 2 | 여수근 | 265 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.groupby([&quot;sex&quot;,&quot;bloodtype&quot;]).mean() . age grade height weight dept . sex bloodtype . 남자 A 21.00 | 1.00 | 165.700000 | 58.750000 | 1.500000 | . AB 22.50 | 2.50 | 181.050000 | 80.750000 | 2.500000 | . B 23.25 | 3.25 | 174.550000 | 68.950000 | 1.500000 | . O 23.50 | 3.50 | 166.200000 | 65.100000 | 2.500000 | . 여자 A 21.50 | 1.00 | 172.450000 | 52.250000 | 1.000000 | . AB 22.00 | 2.00 | 170.100000 | 53.000000 | 3.000000 | . B 22.00 | 2.00 | 158.200000 | 45.200000 | 3.000000 | . O 22.00 | 2.00 | 164.433333 | 50.133333 | 2.333333 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#47607;&#54540;&#47196;&#47549; . import matplotlib.pyplot as plt plt.plot( df[[&quot;height&quot;, &quot;weight&quot;]] ) . [&lt;matplotlib.lines.Line2D at 0x7fe80e23cc50&gt;, &lt;matplotlib.lines.Line2D at 0x7fe80e1f5e50&gt;] . plt.plot( df[&quot;height&quot;], df[&quot;weight&quot;] , &#39;gs&#39;) # 색깔/기호(o, ^, s, -, --) . [&lt;matplotlib.lines.Line2D at 0x7fe80e6ceb10&gt;] . names = [&#39;group_a&#39;, &#39;group_b&#39;, &#39;group_c&#39;] # X values = [1, 10, 100] # Y plt.figure(figsize=(7, 3)) #전체 가로세로 크기 plt.subplot(231) # 2 x 3 구성의 1번 plt.bar(names, values) plt.subplot(232) plt.scatter(names, values) plt.subplot(236) plt.plot(names, values) plt.suptitle(&#39;Categorical Plotting&#39;) . Text(0.5, 0.98, &#39;Categorical Plotting&#39;) . plt.scatter ( &quot;height&quot;, &quot;weight&quot;, c=&quot;grade&quot; ,data=df) # c : 유형별 컬러 다르게 . &lt;matplotlib.collections.PathCollection at 0x7fe80db70fd0&gt; . &#44592;&#49696;&#53685;&#44228; . import seaborn as sns import pandas as pd titanic= sns.load_dataset(&quot;titanic&quot;) # 클래스 별로 점 색깔 다르게 하기. sns.scatterplot(x = &#39;fare&#39;, y = &#39;age&#39;, data = titanic, hue=&#39;class&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe7fec33310&gt; . titanic.to_csv(&quot;titanic.csv&quot;, index=False) titanic.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sex 891 non-null object 3 age 714 non-null float64 4 sibsp 891 non-null int64 5 parch 891 non-null int64 6 fare 891 non-null float64 7 embarked 889 non-null object 8 class 891 non-null category 9 who 891 non-null object 10 adult_male 891 non-null bool 11 deck 203 non-null category 12 embark_town 889 non-null object 13 alive 891 non-null object 14 alone 891 non-null bool dtypes: bool(2), category(2), float64(2), int64(4), object(5) memory usage: 80.7+ KB . print(titanic.dtypes) print() titanic.dtypes [ titanic.dtypes==np.int64 ] . survived int64 pclass int64 sex object age float64 sibsp int64 parch int64 fare float64 embarked object class category who object adult_male bool deck category embark_town object alive object alone bool dtype: object . survived int64 pclass int64 sibsp int64 parch int64 dtype: object . titanic[&#39;survived&#39;].astype(float) . 0 0.0 1 1.0 2 1.0 3 1.0 4 0.0 ... 886 0.0 887 1.0 888 0.0 889 1.0 890 0.0 Name: survived, Length: 891, dtype: float64 . titanic.sex.value_counts(normalize=True) . male 0.647587 female 0.352413 Name: sex, dtype: float64 . titanic.survived.value_counts().plot(kind=&#39;pie&#39;) # &#39;bar&#39; . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe7feb47ed0&gt; . sns.countplot(x=&#39;survived&#39;, data=titanic) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe7feaa7950&gt; . print(titanic[&#39;fare&#39;].quantile(0.5), titanic[&#39;fare&#39;].quantile(0.75)) plt.boxplot(titanic.fare, vert=False) . 14.4542 31.0 . {&#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7fe7fea3fb50&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7fe7fea46b90&gt;, &lt;matplotlib.lines.Line2D at 0x7fe7fea4f110&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7fe7fea4fbd0&gt;], &#39;means&#39;: [], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7fe7fea4f690&gt;], &#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7fe7fea46110&gt;, &lt;matplotlib.lines.Line2D at 0x7fe7fea46650&gt;]} . numeric_columns = [&#39;age&#39;, &#39;fare&#39;] for i in range(1,3): c = numeric_columns[i-1] plt.subplot(2,1,i) plt.title(c) plt.boxplot(titanic[c].dropna(), vert=False) # vert : 세로로 하기 plt.show() . sns.histplot(x=&#39;fare&#39;, data=titanic, bins=[0,100,200,300,400,500,600]) # 또는 bins= 갯수 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe7fe95e0d0&gt; . titanic.dropna(subset=[&#39;age&#39;]) . survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | Third | man | True | NaN | Southampton | no | False | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | First | woman | False | C | Cherbourg | yes | False | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | Third | woman | False | NaN | Southampton | yes | True | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | First | woman | False | C | Southampton | yes | False | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | Third | man | True | NaN | Southampton | no | True | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 885 0 | 3 | female | 39.0 | 0 | 5 | 29.1250 | Q | Third | woman | False | NaN | Queenstown | no | False | . 886 0 | 2 | male | 27.0 | 0 | 0 | 13.0000 | S | Second | man | True | NaN | Southampton | no | True | . 887 1 | 1 | female | 19.0 | 0 | 0 | 30.0000 | S | First | woman | False | B | Southampton | yes | True | . 889 1 | 1 | male | 26.0 | 0 | 0 | 30.0000 | C | First | man | True | C | Cherbourg | yes | True | . 890 0 | 3 | male | 32.0 | 0 | 0 | 7.7500 | Q | Third | man | True | NaN | Queenstown | no | True | . 714 rows × 15 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; titanic.age.fillna( titanic.age.median()) # 중앙값 대체 . 0 22.0 1 38.0 2 26.0 3 35.0 4 35.0 ... 886 27.0 887 19.0 888 28.0 889 26.0 890 32.0 Name: age, Length: 891, dtype: float64 . pd.Categorical( titanic.who, categories=[&quot;man&quot;, &quot;woman&quot;]) . [&#39;man&#39;, &#39;woman&#39;, &#39;woman&#39;, &#39;woman&#39;, &#39;man&#39;, ..., &#39;man&#39;, &#39;woman&#39;, &#39;woman&#39;, &#39;man&#39;, &#39;man&#39;] Length: 891 Categories (2, object): [&#39;man&#39;, &#39;woman&#39;] . &#53685;&#44228;&#44160;&#51221; . import scipy.stats as st import numpy as np sample1=titanic.sample (n=30) sample2=titanic.sample (frac=0.2) # 소규모 t검정 자유도 n-1 print (&quot;sample1:&quot;, np.mean(sample1.fare), st.t.interval(alpha=0.95, df=len(sample1)-1, loc=np.mean(sample1.fare), scale=st.sem(sample1.fare))) # 대규모 정규분포 검정. print (&quot;sample2:&quot;, np.mean(sample2.fare), st.norm.interval(alpha=0.95, loc=np.mean(sample2.fare), scale=st.sem(sample2.fare))) print (&quot;population:&quot;, np.mean(titanic.fare)) . sample1: 24.72999666666667 (7.08208569698947, 42.37790763634386) sample2: 25.84440393258429 (20.87124582777264, 30.817562037395938) population: 32.2042079685746 . st.ttest_1samp(sample1.fare, 20) . Ttest_1sampResult(statistic=0.5481628622491003, pvalue=0.5877752933697968) . from scipy.stats import chisquare num_class=sample2[&quot;class&quot;].value_counts() print (num_class) print (chisquare (num_class)) . Third 102 First 40 Second 36 Name: class, dtype: int64 Power_divergenceResult(statistic=46.15730337078652, pvalue=9.485689908542853e-11) .",
            "url": "https://ksy1526.github.io/myblog/ssuda/2022/04/25/bigdata1.html",
            "relUrl": "/ssuda/2022/04/25/bigdata1.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "[SSUDA] 자연어 임베딩 방법",
            "content": ". &#55148;&#49548;&#54665;&#47148;&#44284; &#48128;&#51665;&#54665;&#47148; . . 우리가 일상적으로 쓰는 단어들을 컴퓨터가 이해하는 것은 힘듭니다. 숫자로 변환하여 입력하여야 하죠. . 단어를 숫자로 변환하는 가장 간단한 아이디어는 원-핫 인코딩 입니다. 단어장을 정의하고 단어장 내 단어 개수 만큼 백터 차원을 키운 뒤 해당하는 단어가 나올 때 그 값만 1을 넣어주는 방식입니다. . 하지만 이런 방식은 단어집합이 커진다면 하나의 값만 1이고 나머지 값은 0인 고차원 벡터가 됩니다. 공간적으로 낭비가 심합니다. . 또 단어간의 유사성을 나타낼 어느 요소도 없습니다. 만들기만 쉽지 실제 활용도는 매우 떨이집니다. . 대안으로 나온 것이 밀집행렬입니다. 밀집행렬은 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞추고 값도 실수로 채워집니다. . 밀집행렬은 단어집합이 커진다고 해서 벡터 차원이 늘어나지 않기 때문에 연산에 보다 효율적입니다. . 또 각 차원을 주성분의 관점에서 살펴볼 수 있습니다. 차원 값이 유사한 단어끼리는 유사도가 높다고 할 수 있습니다. . 물론 저절로 유사도가 높은 값을 바로 찾는 것은 아닙니다. 학습이 필요합니다. 지금부터 살펴볼 것은 이 학습방법 입니다. . &#50892;&#46300;&#53804;&#48177;&#53552;(Word2Vec) . 앞서 말한 내용을 정리하면, 밀집 행렬은 저차원에 단어의 의미를 여러차원에 분산하여 표현한다는 말 입니다. 이렇게 되면 단어 벡터 간 유의미한 유사도를 계산할 수 있고, 이를 학습해야합니다. . 워드투백터(Word2Vec) 방식은 이 학습 방식 중 하나 입니다. 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다는 것을 먼저 가정합니다. . Word2Vec은 주변에 있는 단어로 중간 단어를 예측하거나 중간에 있는 단어로 주변 단어를 예측하는 방식을 통해 단어 간 연관성을 학습합니다. . 쉽게 설명하면 단어들이 같은 문장에 자주 등장할수록 단어 간 유사도가 있다라고 판단하여 학습하는 것 입니다. . 주변에 있는 단어로 중간 단어를 예측하는 것을 CBOW, 중간 단어로 주변 단어들을 예측하는 것을 Skip-Gram이라고 하는데 주로 Skip-Gram을 많이 사용합니다. . 다만 원리를 이해하는데 CBOW 비교적 수월해서 CBOW 과정을 더 자세히 살펴보겠습니다. . . &#39;나는 이른 아침에 사과를 먹었다&#39; 라는 입력 문장이 있다고 생각해봅시다. 여기서 주변 단어들로 중심단어를 맞추는 모델을 만들겁니다. . 어디까지를 주변 단어로 정할 것인가를 윈도우라고 합니다. 윈도우가 2이고 중심단어가 &#39;아침에&#39;라면 나는/이른/사과를/먹었다가 모두 주변단어 입니다. . 주변 단어들은 원-핫 인코딩 형식으로 정리가 되어 있을 것입니다. 여기에 가중치 행렬 (단어 집합 크기 * 단어 임베딩 크기)를 곱합니다. 여기서 단어 임베딩 크기는 사용자가 임의로 정합니다. . 이 가중치 행렬을 행 단위로 보면 각 단어의 의미가 내포되어 있습니다. 왜냐하면 원-핫 인코딩 된 벡터는 가중치 행렬에서 해당하는 행 벡터만 추출하기 때문입니다. . 원-핫 인코딩 벡터와 가중치 행렬 연산을 하게 되면 주변 단어 개수 만큼(윈도우 수 만큼) 값이 나오게 되는데 값들을 모두 평균냅니다. 여기서 주의할 점은 가중치 행렬은 모두 동일한 행렬을 사용합니다. . 그렇게 되면 결과물이 단어 임베딩 크기를 같는 벡터 (1 단어 임베딩 크기)가 나오게 됩니다. 여기에 또 다른 가중치 행렬 (단어 임베딩 크기 단어 집합 크기)를 곱하면 초기 원-핫 인코딩 입력 벡터인 1 * N 꼴로 나오게 됩니다. . 최종 결과물에 소프트 맥스 함수를 취했을때 원하는 단어 &#39;아침에&#39; 에 해당되는 원-핫 인코딩 벡터가 나오도록 모델을 학습시키는 것 입니다. . 이런식으로 학습시키면 문장 내 단어가 붙어있는 비중이 클 수록 유사도가 높아지는 방식으로 모델이 학습됩니다. . 여기서 유사도는 처음 나온 가중치 행렬 (단어 집합 크기 * 단어 임베딩 크기)의 행 값이 서로 유사해 지는 것을 의미합니다. 모델을 잘 학습시킨 뒤 이 가중치 행렬의 행 벡터를 각 단어의 M차원의 임베딩 벡터, 밀집 행렬로써 취급합니다. . Skip-Gram 방법은 설명한 CBOW 방법과 반대로 진행하면 됩니다. 중심 단어로 주변 단어를 추론하는 방법으로 진행합니다. . GloVe . 앞서 공부한 Word2Vec는 여러가지 장점이 있지만, 사용자가 지정한 윈도우 크기에 성능이 의존된다는 치명적 단점이 있습니다. . 이를 극복하기 위해 문장 전체의 정보를 이용하여 단어 임베딩을 한 모델이 Glove 입니다. . . 위 표는 문장 내 단어들이 동시에 등장할 확률을 정리한 값입니다. 열 값을 보면 ice와 steam이 있어 두 단어를 비교합니다. . &#39;단단한&#39;의미를 갖는 solid는 아무래도 ice와 같이 등장할 확률이 높고, gas는 steam과 같이 등장할 확률이 높습니다. . ice, steam 과 연관성이 조금씩 있는 water이나, 연관성이 전혀 없는 fashion의 비는 모두 1과 가까운 값을 같게 되는군요. . 임베딩한 두 단어벡터의 내적은 전체 말뭉치의 등장 확률이 되도록 하는 것이 Glove 모델입니다. 나름 합리적인 모델입니다. . &#53664;&#53360;&#54868; . 자연어 처리에서 또 중요한 것중 하나가 토큰화 입니다. 토큰화란 문장을 토큰 시퀀스로 나누는 과정인데요. . 가장 간단히 생각할 수 있는 토큰화 방법 두 가지로 단어 단위 토큰화와 문자 단위 토큰화가 있습니다. . 단어 단위로 토큰화를 할때 단어를 구분하는 기준을 공백으로 한다면 갔었어, 갔었는데요 등 표현이 살짝 바뀌어도 다른 단어로 취급합니다. . 학습된 토크나이저(은전한닢 등)을 사용한다면 이 문제를 조금 해결할 수 있겠지만, 어휘 집합 크기가 커지는 문제를 근본적으로 막기 힘듭니다. . 문자 단위로 토큰화를 진행한다면 한글 기준 단어집합이 약 1만 2천개로 적절한 수준이고, 미등록 된 토큰이 나올 가능성도 없습니다. . 하지만 각 문자 토큰은 의미 있는 단위가 되기 어렵습니다. 단순히 &#39;어&#39; 라는 단어를 보고 어떤 의미인지 유추하기 힘들겠죠. . 또 토큰 시퀀스 길이가 너무 길어집니다. 문자 당 하나의 토큰이니 문장 내 문자 개수가 조금만 많아져도 모델로 학습하기 힘듭니다. . BPE . BPE(Byte Pair Encoding), 바이트 페어 인코딩은 원래 정보 압축 알고리즘으로 쓰였으나 최근 자연어 처리 모델에 많이 쓰이는 토큰화 기법입니다. . 말뭉치의 모든 문장을 공백으로 나눠줍니다. 이를 프리토크나이즈 라고 합니다. . 공백 기준으로 나눠진 단어로 초기 어휘 집합을 만듭니다. 여기까지는 단어 단위 토큰화와 같죠. 또 토큰의 빈도를 새어 정리합니다. . . 그 후 초기 어휘 집합에 있는 토큰들을 문자별로 모두 쪼갭니다. 그 후 쪼개진 문자들만을 가지고 다시 어휘 집합을 만듭니다. . 예시 단어 기준으로 현재 어휘 집합은 (b, g, h, n, p, s, u) 입니다. . . 그 후 토큰을 2개씩 묶은 것의 빈도를 구합니다. 예를 들어 (h, u)는 hug로 10번, hugs로 5번이기 때문에 빈도는 총 15번이 됩니다. . 가장 많이 등장한 토큰 쌍은 (u, g) 인데 이 토큰 쌍을 어휘 집합에 넣습니다. . 그렇게 되면 어휘 집합은 (b, g, h, n, p, s, u, ug)가 됩니다. . . 어휘 집합에 ug 가 추가된 뒤 다시 토큰 쌍의 빈도를 모두 구합니다. 변동이 조금 있죠. . (u, n) 토큰쌍이 가장 많은 빈도를 가지기 때문에 이 토큰 쌍을 어휘 집합 내 넣습니다. . 그렇게 되면 어휘 집합은 (b, g, h, n, p, s, u, ug, un)가 됩니다. . . 마찬가지로 어휘 집합이 업데이트 된 뒤 토큰 쌍의 빈도를 구합니다. (h, ug) 토큰 쌍의 빈도가 가장 크네요. . 이 토큰 쌍을 어휘 집합 내 넣게 됩니다. 이런 식으로 토큰 쌍의 빈도를 계속 구해 어휘 집합을 업데이트 합니다. . 어휘 집합 업데이트는 사용자가 지정하는 어휘 집합 크기가 될 때 까지 계속 반복됩니다. . 또 중요한 것은 병합 우선순위를 기록하는 것입니다. 먼저 병합된 토큰을 높은 우선순위로 취급하는데, 새로운 단어를 토큰화 하는데 필요합니다. . . 앞서 살펴본 단어 중심, 문자 중심 토큰화와 다르게 BPE 방법은 단어 사전 크기를 억제할 수 있고, 정보도 효율적으로 압축할 수 있습니다. . 또 분석 대상 언어에 대한 사전 지식이 거의 필요없습니다. 여기서 사용한 지식은 띄어쓰기가 보편적인 단어 구분 기준이 된다는 점 밖에 없습니다. . 가장 중요한 장점으로 OOV(Out-Of-Vocabulary) 문제를 완화할 수 있습니다. 단어 사전에 없는 신조어나 오타도 단어를 문자별로 쪼갠뒤 토큰화 하므로 최대한 단어 내 찾을 수 있는 의미를 건저냅니다. . &#50892;&#46300;&#54588;&#49828; . 앞서 살펴본 BPE를 조금 변형한 것이 워드피스 입니다. BPE는 GPT 모델에 쓰이고, 워드피스는 BERT 모델에 사용됩니다. . BPE는 매번 단어 사전에 넣는 토큰 쌍의 기준을 빈도로 잡았습니다. 직관적으로 당연한 얘기 입니다. . 반면 워드피스는 빈도가 아닌 우도를 기준으로 단어 사전에 넣는 토큰 쌍을 결정합니다. . 우도를 자세히 설명하면 단순히 같이 많이 나오는 것 보다 두 토큰이 쓰이는 빈도 대비 같이 나오는 비율이 높은 것을 기준으로 잡겠다는 겁니다. . BPE와 또 다른 차이점은 병합 우선순위를 만들지 않습니다. 만약 서브워드 후보가 여러게 포함되어 있을 경우 가장 긴 서브워드를 선택합니다. . BERT &#47784;&#45944; &#44396;&#51312; . BERT는 2018년 구글에서 발표한 임베딩 모델 입니다. Bidirectional Encoder Representations from Transformers 의 약자 입니다. . 앞서 다룬 Word2Vec 등과 가장 큰 차이점은 문맥을 고려했다는 것 입니다. . 겉으로 보기에 같은 단어라도 동음이이어일수도 있고, 문맥 내 다소 다른 뜻으로 적용될 수도 있습니다. . 이런점까지 고려한것이 문맥을 고려한 자연어 임베딩 BERT입니다. 어떻게 문맥을 고려했는지 살펴보겠습니다. . . BERT는 이전에 살펴본 트랜스포머 모델 중 인코더 부분만 사용합니다. 이중 멀티 헤드 어텐션이 문맥을 파악하는데 큰 역할을 합니다. . 멀티 헤드 어텐션을 진행하면 문장의 각 단어를 문장의 다른 모든 단어와 연결해 관계 및 문맥을 고려해 의미를 학습하게 됩니다. . 이전 글에 설명한 부분이기 때문에 멀티 헤드 어텐션 관련해서 더 자세히 설명하지는 않겠습니다. . BERT도 여러가지 구조가 있는데 기본 구조는 인코더 레이어 12개, 어텐션 해드 12개, 은닉 유닛 768개를 사용합니다. . . 공부하다가 이 부분에서 의문이 많이 들었는데요. 트랜스포머는 이미 임베딩 된 입력 벡터를 입력 값으로 사용했습니다. . 하지만 지금 공부하는 것은 임배딩을 하기 위함이라 다소 모순이 있는데, 입력하는 곳에서는 어떤 일이 벌어지고 있는 것일까요? . 우선 입력하는 문장을 워드피스 방식을 이용해 토큰화를 합니다. 토큰으로 쪼갠 뒤 단어 사전 내 숫자와 매핑 시킨것을 토큰 임베딩이라고 합니다. . 여기서 매핑된 숫자는 (1 * 단어 임베딩 크기) 벡터와 또 다시 매핑되는데요. 이 매핑된 벡터는 단어 사전 개수 만큼 존재하겠죠. . 이 벡터를 지속적으로 학습시키는 것이 기본 아이디어입니다. 그래서 여기까지 진행한 것만 보면 Word2Vec와 다를 것이 없습니다. . 세그먼트 임베딩은 문장이 2개인 경우 두 문장을 0과 1로 구분해주는 과정이고, 위치 임베딩은 트랜스포머에서 배운 위치 임베딩과 동일합니다. . 모델 맨 앞에 이 과정을 모두 합한 임베딩 레이어가 존재합니다. 이렇게 입력값을 임베딩 레이어에 통과시킨 뒤 트랜스포머 인코더를 계속 통과시키게 되고, 인코더 내 멀티 헤드 어텐션 부분에서 문맥까지 파악하는 것입니다. . 다시 정리하면 임베딩 레이어에서 토큰 그 자체만 보고 학습이 진행되고, 또 트랜스포머 인코더 부분에서 문맥을 학습하는 것 입니다. . 그렇다면 이 모델을 어떤 방식으로 학습시킬까요? 바로 알아봅시다. . BERT &#49324;&#51204;&#54617;&#49845; . . 첫번째 BERT 사전학습 방식인 MLM (masked language modeling) 입니다. 입력 토큰 중 일부를 마스킹 한 후 원래 단어를 맞추는 방식이죠. . 마스킹 된 토큰 행을 기준으로 출력 값의 행 벡터 (1 * 단어 임베딩 크기)를 피드 포워드 층에 입력하고 소프트 맥스 함수를 씌웁니다. . 그렇게 되면 마스크 된 단어가 어느 단어를 가르키는지에 대한 확률이 나오는데 이 부분을 지속적으로 학습하는 것 입니다. . 이런식으로 학습하게 되면 모델 내 파라미터들이 모두 업데이트가 되겠죠. . 다만 이 BERT 모델로 임베딩 된 것을 추후 사용할 때 마스킹 된 토큰을 입력값으로 사용하지 않는 경우 모델 간 불일치가 벌어집니다. . 이 문제를 극복하기 위해 전체 데이터 중 12%는 마스킹, 1.5%는 무작위로 다른 단어 대체, 1.5%는 그대로 단어를 사용합니다. . 예시 그림을 보게되면 MASK 토큰 행의 출력값을 가지고 무슨 단어가 올 지 예측해야하고, he 대신 king을 입력값으로 사용한 행의 출력값을 가지고 문장 내 원래 쓰인 he 단어를 예측해야합니다. 또 play 단어는 실제로도 play가 맞기 때문에 play 토큰 행의 출력값을 가지고 다시 play 단어를 예측해야 합니다. . . 두번째 BERT 사전학습 방식은 NSP (next sentence prediction) 입니다. . 두 문장을 입력하고 두번째 문장이 첫번째 문장의 다음 문장이 맞는지 이진분류 하는 방식입니다. . 여기서 맨 앞 CLS 토큰은 문장의 시작임을 알려주는 토큰으로 사실 이전부터 지속적으로 사용했던 토큰입니다. . 이 CLS 토큰 행을 기준으로 한 출력 값의 행 벡터는 모든 토큰의 집계 표현을 보유하므로 문장 전체에 대한 표현을 담고 있다고 할 수 있습니다. . 그렇기 때문에 CLS 토큰 기준 출력 값의 행 벡터를 사용해 피드포워드 층과 소프트맥스 함수를 통과시켜 이진 분류를 수행합니다. . MLM 방식과 마찬가지로 이 이진 분류가 잘 되게 모델을 계속 학습합니다. . 사실 두 사전학습 방식을 설명하기 위해 따로 이미지를 분류했지만, 동시에 이루어지는 프로세스라는 것 또한 기억합시다. . &#45712;&#45184;&#51216; . 자연어 처리 분야를 공부하기 위해 임베딩 부분 학습이 필수입니다. 제 나름대로 학습순서를 정해 공부했는데요. . 개인적으로 정말 궁금햇던 부분인데 학습하는데 어렵기도 했고, 질문사항이 생길때 해결하기가 힘들었습니다. . 하지만 명쾌하게 이해하고 나니 정말 기분이 좋습니다. 저도 부족하지만 혹시 이해 안되시면 질문 남겨주세요. . 다음에는 사전학습 된 BERT 모델을 가지고 파인튜닝하여 다양한 문제를 해결하는 모델을 구현해보겠습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/deep%20learning/natural%20language/bert/tokenizer/2022/03/18/SSUDA22_4.html",
            "relUrl": "/ssuda/book/deep%20learning/natural%20language/bert/tokenizer/2022/03/18/SSUDA22_4.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "[DACON] 손동작 분류 경진대회 with Pytorch",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . 데이터를 구글 드라이브에 올려놓고, 코랩과 연동합니다. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/hand_classification/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id sensor_1 sensor_2 sensor_3 sensor_4 sensor_5 sensor_6 sensor_7 sensor_8 sensor_9 ... sensor_24 sensor_25 sensor_26 sensor_27 sensor_28 sensor_29 sensor_30 sensor_31 sensor_32 target . 0 1 | -6.149463 | -0.929714 | 9.058368 | -7.017854 | -2.958471 | 0.179233 | -0.956591 | -0.972401 | 5.956213 | ... | -7.026436 | -6.006282 | -6.005836 | 7.043084 | 21.884650 | -3.064152 | -5.247552 | -6.026107 | -11.990822 | 1 | . 1 2 | -2.238836 | -1.003511 | 5.098079 | -10.880357 | -0.804562 | -2.992123 | 26.972724 | -8.900861 | -5.968298 | ... | -1.996714 | -7.933806 | -3.136773 | 8.774211 | 10.944759 | 9.858186 | -0.969241 | -3.935553 | -15.892421 | 1 | . 2 3 | 19.087934 | -2.092514 | 0.946750 | -21.831788 | 9.119235 | 17.853587 | -21.069954 | -15.933212 | -9.016039 | ... | -6.889685 | 54.052330 | -6.109238 | 12.154595 | 6.095989 | -40.195088 | -3.958124 | -8.079537 | -5.160090 | 0 | . 3 4 | -2.211629 | -1.930904 | 21.888406 | -3.067560 | -0.240634 | 2.985056 | -29.073369 | 0.200774 | -1.043742 | ... | -2.126170 | -1.035526 | 2.178769 | 10.032723 | -1.010897 | -3.912848 | -2.980338 | -12.983597 | -3.001077 | 1 | . 4 5 | 3.953852 | 2.964892 | -36.044802 | 0.899838 | 26.930210 | 11.004409 | -21.962423 | -11.950189 | -20.933785 | ... | -2.051761 | 10.917567 | 1.905335 | -13.004707 | 17.169552 | 2.105194 | 3.967986 | 11.861657 | -27.088846 | 2 | . 5 rows × 34 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 데이터를 판다스로 읽고 잘 읽혔는지 확인합니다. path는 데이터 파일이 저장되어있는 경로를 의미합니다. . &#45936;&#51060;&#53552; &#49332;&#54196;&#48372;&#44592; . print(train.shape) print(test.shape) . (2335, 34) (9343, 33) . 데이터의 칼럼수는 타겟 열 제외하고 33개이고 트레인 데이터 2335개. 테스트 데이터 9343개 입니다. . 데이터 개수가 2335개면 파라미터가 많은 모델을 사용하기 조금 적은 데이터이고, 테스트 데이터가 훨씬 많은 것도 특징입니다. . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2335 entries, 0 to 2334 Data columns (total 34 columns): # Column Non-Null Count Dtype -- -- 0 id 2335 non-null int64 1 sensor_1 2335 non-null float64 2 sensor_2 2335 non-null float64 3 sensor_3 2335 non-null float64 4 sensor_4 2335 non-null float64 5 sensor_5 2335 non-null float64 6 sensor_6 2335 non-null float64 7 sensor_7 2335 non-null float64 8 sensor_8 2335 non-null float64 9 sensor_9 2335 non-null float64 10 sensor_10 2335 non-null float64 11 sensor_11 2335 non-null float64 12 sensor_12 2335 non-null float64 13 sensor_13 2335 non-null float64 14 sensor_14 2335 non-null float64 15 sensor_15 2335 non-null float64 16 sensor_16 2335 non-null float64 17 sensor_17 2335 non-null float64 18 sensor_18 2335 non-null float64 19 sensor_19 2335 non-null float64 20 sensor_20 2335 non-null float64 21 sensor_21 2335 non-null float64 22 sensor_22 2335 non-null float64 23 sensor_23 2335 non-null float64 24 sensor_24 2335 non-null float64 25 sensor_25 2335 non-null float64 26 sensor_26 2335 non-null float64 27 sensor_27 2335 non-null float64 28 sensor_28 2335 non-null float64 29 sensor_29 2335 non-null float64 30 sensor_30 2335 non-null float64 31 sensor_31 2335 non-null float64 32 sensor_32 2335 non-null float64 33 target 2335 non-null int64 dtypes: float64(32), int64(2) memory usage: 620.4 KB . 데이터 칼럼명과 결측치를 확인하기 위한 함수입니다. 결측치는 없는 것으로 확인됩니다. . train[&#39;target&#39;].value_counts() . 3 599 2 593 1 574 0 569 Name: target, dtype: int64 . 우리가 맞춰야 하는 타겟 값의 분포입니다. 상당히 골고루 분포되어있는 걸 알 수 있습니다. . 불균형하게 분포되어있다면 별도의 조정이 필요합니다만 그럴 필요는 없어보입니다. . plt.figure(figsize=[12,8]) plt.text(s=&quot;Target variables&quot;,x=0,y=1.3, va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;,fontsize=25) plt.pie(train[&#39;target&#39;].value_counts(),autopct=&#39;%1.1f%%&#39;, pctdistance=1.1) plt.legend([&#39;3&#39;, &#39;2&#39;, &#39;1&#39;, &#39;0&#39;], loc = &quot;upper right&quot;,title=&quot;Programming Languages&quot;, prop={&#39;size&#39;: 15}) plt.show() . 방금 살펴본 내용을 파이 그래프를 통해 시각화 하였습니다. . train.describe().T . count mean std min 25% 50% 75% max . id 2335.0 | 1168.000000 | 674.200761 | 1.000000 | 584.500000 | 1168.000000 | 1751.500000 | 2335.000000 | . sensor_1 2335.0 | -1.122174 | 11.486353 | -94.746969 | -4.036597 | -0.951398 | 2.895540 | 68.876142 | . sensor_2 2335.0 | -1.024673 | 7.399859 | -63.942094 | -4.031957 | -1.015582 | 2.140456 | 39.913391 | . sensor_3 2335.0 | -0.672769 | 26.519159 | -122.195138 | -14.878500 | -0.961088 | 13.974075 | 127.124171 | . sensor_4 2335.0 | -0.147724 | 15.551500 | -111.870691 | -7.116633 | -0.890469 | 6.110973 | 102.015561 | . sensor_5 2335.0 | -0.327494 | 11.461970 | -94.147972 | -3.968687 | -0.871690 | 2.970387 | 89.059852 | . sensor_6 2335.0 | -0.423462 | 7.314322 | -70.916786 | -3.957699 | -0.804810 | 3.006144 | 34.923040 | . sensor_7 2335.0 | 0.676275 | 26.869479 | -105.956553 | -13.937806 | 0.058910 | 13.934438 | 120.046277 | . sensor_8 2335.0 | -0.936019 | 15.598104 | -102.965354 | -8.053214 | -1.095551 | 4.955494 | 125.160611 | . sensor_9 2335.0 | -0.797432 | 12.015022 | -81.268085 | -4.031148 | -0.944613 | 2.235557 | 74.101715 | . sensor_10 2335.0 | -0.704585 | 7.384626 | -47.937561 | -3.983620 | -0.932964 | 2.883284 | 47.030119 | . sensor_11 2335.0 | -1.099322 | 26.262009 | -115.943693 | -15.165419 | -1.116522 | 13.022905 | 127.110419 | . sensor_12 2335.0 | -0.843473 | 15.498328 | -102.916207 | -8.082508 | -1.054003 | 6.021600 | 99.932331 | . sensor_13 2335.0 | -0.491915 | 11.894939 | -115.053373 | -3.893967 | -0.908079 | 2.992981 | 107.910041 | . sensor_14 2335.0 | -0.851473 | 7.401702 | -59.689434 | -3.982224 | -0.937905 | 2.854699 | 40.026878 | . sensor_15 2335.0 | -0.344029 | 25.815937 | -107.985386 | -14.953749 | -0.858820 | 12.965905 | 126.981907 | . sensor_16 2335.0 | -1.128676 | 15.513633 | -126.950747 | -8.096568 | -1.004242 | 5.508252 | 120.974880 | . sensor_17 2335.0 | -0.959658 | 11.654236 | -95.956853 | -4.038010 | -0.947597 | 2.895085 | 85.952050 | . sensor_18 2335.0 | -0.639778 | 7.586333 | -83.854213 | -3.996916 | -0.967231 | 2.876743 | 39.993408 | . sensor_19 2335.0 | -0.559455 | 26.885734 | -108.964270 | -15.179515 | -0.964579 | 13.978336 | 117.934200 | . sensor_20 2335.0 | -0.658692 | 15.936823 | -108.094304 | -7.851749 | -1.013369 | 5.917309 | 121.026042 | . sensor_21 2335.0 | -0.611461 | 11.942224 | -103.876936 | -4.002134 | -0.942706 | 2.948692 | 102.882569 | . sensor_22 2335.0 | -0.741168 | 7.548507 | -59.993001 | -3.973502 | -0.968065 | 2.920789 | 40.917741 | . sensor_23 2335.0 | 0.027448 | 26.671928 | -93.171275 | -14.102903 | -1.104314 | 12.137937 | 121.959404 | . sensor_24 2335.0 | -0.356441 | 16.531906 | -127.797649 | -7.980628 | -0.926120 | 6.002985 | 127.161055 | . sensor_25 2335.0 | -0.927744 | 12.021560 | -99.115177 | -4.004750 | -0.907301 | 2.863184 | 58.113657 | . sensor_26 2335.0 | -0.589060 | 7.440983 | -86.193378 | -4.001112 | -0.897015 | 2.951682 | 59.105536 | . sensor_27 2335.0 | -0.081374 | 25.923355 | -105.751637 | -14.096840 | -0.954791 | 13.903783 | 123.179253 | . sensor_28 2335.0 | -0.370812 | 15.541803 | -105.890010 | -8.004561 | -0.989293 | 5.922250 | 111.137925 | . sensor_29 2335.0 | -0.726941 | 11.636507 | -74.977182 | -3.981055 | -0.889780 | 2.972719 | 54.098746 | . sensor_30 2335.0 | -0.809534 | 7.469744 | -74.006065 | -3.988965 | -0.928504 | 2.519426 | 35.896503 | . sensor_31 2335.0 | -0.495062 | 25.291238 | -121.097086 | -13.998874 | -0.955684 | 13.926128 | 125.974107 | . sensor_32 2335.0 | -0.743585 | 16.300385 | -123.876153 | -7.873898 | -1.019547 | 5.121679 | 104.959621 | . target 2335.0 | 1.523340 | 1.118221 | 0.000000 | 1.000000 | 2.000000 | 3.000000 | 3.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 데이터의 분포를 보다 자세히 살펴보기 위해 describe 함수를 사용했습니다. 칼럼수가 다소 많기 때문에 보기 편하기 위해 T 함수를 사용했습니다. . 변수들의 이름이 모두 sensor_ 형태로 구성되어 있습니다. 또 평균값이 모두 0 근처에 있군요. . 변수들마다 조금씩 차이는 있지만 대체로 최솟값은 -130 밑으로는 안떨어지고 최댓값도 +130 위로는 올라가지 않습니다. . 변수들의 분포가 상당히 유사해 보이는데요. 이미지의 하나의 픽셀값 같이 하나의 데이터를 32분할한 데이터로 추측됩니다. . 이런 데이터는 변수간 일관성이 있기 때문에 딥러닝 기반 모델이 효율적일 것으로 판단됩니다. . &#45936;&#51060;&#53552; &#49828;&#52992;&#51068;&#47553; . train_x = train.drop([&#39;id&#39;, &#39;target&#39;], axis = 1) test_x = test.drop([&#39;id&#39;], axis = 1) mins = train_x.min() maxs = train_x.max() mins[:5] . sensor_1 -94.746969 sensor_2 -63.942094 sensor_3 -122.195138 sensor_4 -111.870691 sensor_5 -94.147972 dtype: float64 . 데이터 내 칼럼별로 최솟값, 최댓값을 추출했습니다. 데이터들을 스케일링 하기 위한 목적입니다. . train_x = (train_x - mins) / (maxs - mins) test_x = (test_x - mins) / (maxs - mins) train_x.describe().T[[&#39;min&#39;, &#39;max&#39;]] . min max . sensor_1 0.0 | 1.0 | . sensor_2 0.0 | 1.0 | . sensor_3 0.0 | 1.0 | . sensor_4 0.0 | 1.0 | . sensor_5 0.0 | 1.0 | . sensor_6 0.0 | 1.0 | . sensor_7 0.0 | 1.0 | . sensor_8 0.0 | 1.0 | . sensor_9 0.0 | 1.0 | . sensor_10 0.0 | 1.0 | . sensor_11 0.0 | 1.0 | . sensor_12 0.0 | 1.0 | . sensor_13 0.0 | 1.0 | . sensor_14 0.0 | 1.0 | . sensor_15 0.0 | 1.0 | . sensor_16 0.0 | 1.0 | . sensor_17 0.0 | 1.0 | . sensor_18 0.0 | 1.0 | . sensor_19 0.0 | 1.0 | . sensor_20 0.0 | 1.0 | . sensor_21 0.0 | 1.0 | . sensor_22 0.0 | 1.0 | . sensor_23 0.0 | 1.0 | . sensor_24 0.0 | 1.0 | . sensor_25 0.0 | 1.0 | . sensor_26 0.0 | 1.0 | . sensor_27 0.0 | 1.0 | . sensor_28 0.0 | 1.0 | . sensor_29 0.0 | 1.0 | . sensor_30 0.0 | 1.0 | . sensor_31 0.0 | 1.0 | . sensor_32 0.0 | 1.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; (데이터 - 최솟값) / (최댓값 - 최솟값) 연산을 거치게 되면 데이터 값들이 모두 0~1 사이로 가지게 됩니다. . 딥러닝에서 입력값을 표준화 시키는 것이 상당히 중요합니다. . &#45936;&#51060;&#53552; &#47196;&#45908; &#47564;&#46308;&#44592; . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import random random.seed(42) torch.manual_seed(42) . &lt;torch._C.Generator at 0x7ff6406a2030&gt; . 딥러닝에 필요한 파이토치 관련 패키지들을 설치합니다. . train_x = torch.from_numpy(train_x.to_numpy()).float() train_y = torch.tensor(train[&#39;target&#39;].to_numpy(), dtype = torch.int64) test_x = torch.from_numpy(test_x.to_numpy()).float() train_x . tensor([[0.5415, 0.6067, 0.5264, ..., 0.6256, 0.4657, 0.4889], [0.5654, 0.6060, 0.5106, ..., 0.6646, 0.4742, 0.4719], [0.6957, 0.5955, 0.4939, ..., 0.6374, 0.4574, 0.5188], ..., [0.5240, 0.6733, 0.4574, ..., 0.5721, 0.4617, 0.5373], [0.5547, 0.6447, 0.4422, ..., 0.6371, 0.5505, 0.4885], [0.5678, 0.5481, 0.3061, ..., 0.6840, 0.4012, 0.4843]]) . 데이터가 판다스의 데이터 프레임 형식으로 저장되어 있습니다. 데이터 프레임을 넘파이로, 다시 넘파이를 텐서 형태로 바꾼 모습입니다. . 파이토치 모델을 이용하기 위해서 데이터는 텐서형태로 변환해주어야 합니다. . train_dataset = TensorDataset(train_x, train_y) print(train_dataset.__len__()) print(train_dataset.__getitem__(1)) . 2335 (tensor([0.5654, 0.6060, 0.5106, 0.4722, 0.5095, 0.6418, 0.5882, 0.4123, 0.4846, 0.4620, 0.4892, 0.4825, 0.5209, 0.6385, 0.3529, 0.5038, 0.5435, 0.6448, 0.5070, 0.4935, 0.5120, 0.5833, 0.5024, 0.4934, 0.5799, 0.5716, 0.5003, 0.5383, 0.6573, 0.6646, 0.4742, 0.4719]), tensor(1)) . 입력한 데이터를 파이토치에서 지원하는 TensorDataset 함수를 사용해 데이터 셋 형태로 만들었습니다. . 데이터가 데이터 셋 내 잘 입력 됬는지 len, getitem 함수를 통해 확인했네요. . 바로 뒤에 나오는 데이터 로더를 사용하기 위해선 데이터 셋 형태로 데이터를 만들어야 합니다. . train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = True) for batch_idx, samples in enumerate(train_dataloader): if batch_idx &gt; 0: break print(samples[0].shape) print(samples[1]) . torch.Size([16, 32]) tensor([1, 3, 1, 1, 1, 2, 1, 0, 3, 0, 3, 0, 3, 2, 2, 0]) . 데이터 셋을 파이토치 내 DataLoader 함수에 넣어 데이터 로더를 만들었습니다. . 데이터 로더 형식을 이용하면 배치단위로 데이터를 모델에 넣을 수 있고 shuffle 인자를 사용해 데이터를 쉽게 섞을 수도 있습니다. . &#47784;&#45944; &#51201;&#54633;&#54616;&#44592; . class Models(nn.Module): def __init__(self): super().__init__() self.linear_relu_stack = nn.Sequential( nn.Linear(32, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2), nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2), nn.Linear(128, 4), ) def forward(self, x): x = self.linear_relu_stack(x) return x model = Models() print(model) . Models( (linear_relu_stack): Sequential( (0): Linear(in_features=32, out_features=64, bias=True) (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Dropout(p=0.2, inplace=False) (4): Linear(in_features=64, out_features=128, bias=True) (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (6): ReLU() (7): Dropout(p=0.2, inplace=False) (8): Linear(in_features=128, out_features=4, bias=True) ) ) . 간단한 딥러닝 모델을 직접 제작했습니다. 파이토치 내 nn.Module 클래스를 상속받았습니다. . 32개에 입력 데이터를 받아 64개, 128개로 은닉 층 내 노드 개수를 늘리다가 예측 값이 4개이기 때문에 최종 출력 노드는 4개로 구성했습니다. . BatchNorm1d와 Dropout을 이용해 배치 정규화와 드롭아웃 기능을 사용했으며 활성화 함수로는 Relu를 사용했습니다. . 데이터가 적기 때문에 파라미터수가 많으면 안될 것 같아서 층을 적게 쌓았습니다. . criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.005) for epoch in range(20): running_loss = 0.0 accuracy = 0 for i, data in enumerate(train_dataloader, 0): inputs, labels = data optimizer.zero_grad() # 매개변수를 0으로 만듭니다. 매 학습시 초기화해줘야합니다. outputs = model(inputs) # 입력값을 넣어 순전파를 진행시킵니다. loss = criterion(outputs, labels) # 모델 출력값와 실제값을 손실함수에 대입합니다. loss.backward() # 손실함수에서 역전파 수행합니다. optimizer.step() # 옵티마이저를 사용해 매개변수를 최적화합니다. running_loss += loss.item() _, predictions = torch.max(outputs, 1) for label, prediction in zip(labels, predictions): if label == prediction: accuracy = accuracy + 1 print(f&#39;{epoch + 1} 에포크 loss: {running_loss / i:.3f}&#39;) print(f&#39;{epoch + 1} 에포크 정확도: {accuracy / (i * 16):.3f}&#39;) . 1 에포크 loss: 1.222 1 에포크 정확도: 0.453 2 에포크 loss: 0.986 2 에포크 정확도: 0.605 3 에포크 loss: 0.891 3 에포크 정확도: 0.631 4 에포크 loss: 0.857 4 에포크 정확도: 0.657 5 에포크 loss: 0.807 5 에포크 정확도: 0.686 6 에포크 loss: 0.790 6 에포크 정확도: 0.685 7 에포크 loss: 0.730 7 에포크 정확도: 0.719 8 에포크 loss: 0.721 8 에포크 정확도: 0.719 9 에포크 loss: 0.743 9 에포크 정확도: 0.715 10 에포크 loss: 0.695 10 에포크 정확도: 0.736 11 에포크 loss: 0.699 11 에포크 정확도: 0.727 12 에포크 loss: 0.692 12 에포크 정확도: 0.732 13 에포크 loss: 0.683 13 에포크 정확도: 0.734 14 에포크 loss: 0.672 14 에포크 정확도: 0.736 15 에포크 loss: 0.655 15 에포크 정확도: 0.745 16 에포크 loss: 0.640 16 에포크 정확도: 0.761 17 에포크 loss: 0.631 17 에포크 정확도: 0.761 18 에포크 loss: 0.647 18 에포크 정확도: 0.755 19 에포크 loss: 0.660 19 에포크 정확도: 0.750 20 에포크 loss: 0.600 20 에포크 정확도: 0.776 . 20 에포크를 진행했으며 에포크마다 학습이 잘 되어가는지를 평가했습니다. 옵티마이저로 무난한 Adam을 사용했습니다. . 손실함수는 CrossEntropyLoss을 사용했는데, 타겟값을 원-핫 인코딩 하지 않아도 알아서 적용시켜주며 소프트맥스함수까지 알아서 적용해줘서 손실값을 구해주는 편리한 함수 입니다. . &#47784;&#45944; &#54217;&#44032; . model.eval() # 모델을 평가모드로 바꿉니다. dropout이 일어나지 않습니다. with torch.no_grad(): # 이 안의 코드는 가중치 업데이트가 일어나지 않습니다. outputs = model(test_x) _, pred = torch.max(outputs, 1) pred . tensor([0, 0, 1, ..., 2, 0, 3]) . 앞서 구한 모델에 테스트 데이터를 넣어서 결과를 출력합니다. torch.max 함수가 매우 편리합니다. . sample_submission[&#39;target&#39;] = pred.numpy() sample_submission[&#39;target&#39;].value_counts() . 1 3056 0 2239 2 2034 3 2014 Name: target, dtype: int64 . 테스트 데이터의 타겟 예측 값 분포 입니다. 다소 불균형이 있지만 그래도 모델이 어느정도 기능을 하는 것 같습니다. . sample_submission.to_csv(&#39;dacon_hands_4.csv&#39;,index=False) . 최종 결과를 csv 형태로 저장합니다. . &#45712;&#45184;&#51216; . 역시 간간히 진행하지 않으면 실력이 금방 녹쓰는 것 같습니다. 간단한 딥러닝 코드를 쓰는 것도 쉽지 않네요. . 모델을 더 최적화 시킬수 있을 것 같습니다. 층 개수 조정, 히든층 노드 수 변경 등 여러가지를 시도할 수 있겠네요. . 데이터가 적어서 딥러닝 모델이 제 성능을 발휘할까 의심이 있었는데 꽤 괜찮은 모습을 보인 것 같습니다. . 딥러닝 코드는 여기서 많이 참고했습니다. 감사합니다. . (http://www.gisdeveloper.co.kr/?p=8443) .",
            "url": "https://ksy1526.github.io/myblog/jupyter/book/deep%20learning/pytorch/dacon/classifier/2022/03/07/dacon_hands.html",
            "relUrl": "/jupyter/book/deep%20learning/pytorch/dacon/classifier/2022/03/07/dacon_hands.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "[SSUDA] 트랜스포머 모델 구조 살펴보기",
            "content": ". &#53944;&#47004;&#49828;&#54252;&#47672;&#44032; &#45208;&#50724;&#44172; &#46108; &#48176;&#44221; . 2017년 &#39;Attention Is All You Need&#39; 논문 이전 자연어 처리 분야는 RNN 기반 딥러닝을 사용했습니다. . RNN은 오래된 데이터를 잘 잊고, 이를 극복하려 나온 LSTM 조차도 장기 의존성 문제가 있습니다. . 이는 자연어 처리 분야에서 문장의 길이가 길어지면 앞의 과거 정보가 마지막 시점까지 전달되지 못하는 현상을 유발합니다. . 언어는 과거 정보가 중요한 경우가 상당히 많은데 장기 의존성 문제가 있다면 성능이 떨어지겠죠. . 이런 단점을 극복하고자 트랜스포머가 등장했습니다. . &#49472;&#54532; &#50612;&#53584;&#49496; . &#39;A dog are the food because it was hungry&#39; . 위 문장에서 it는 dog와 food 중 dog를 의미한다는 것을 사람은 쉽게 알 수 있습니다. . 컴퓨터 모델에서 it이 food가 아닌 dog라는 것을 이해시키기 위해선 어떻게 해야할까요. . . 문장 내용은 조금 틀리긴 하지만 위 그림을 참고해봅시다. . 문장 내 모든 단어가 it 하고 상호작용을 하고 있습니다. 실제 문장이라는게 모든 단어가 연관되어 있으니깐요. . 그 중 어느 단어가 it 단어와 상호작용이 강한지 혹은 약한지를 수치로 나타냅니다. . 이 수치가 정확하다면 모델이 문장 구조를 이해하는데 큰 도움이 될 것입니다. 이것이 셀프 어텐션의 기초 이론입니다. . 셀프 어텐선을 구체적으로 살펴보려고 하기 때문에 예시 문장을 보다 간단하게 I am good 으로 바꾸겠습니다. . 예시 문장을 입력받게 되면 컴퓨터는 단어를 숫자로 인식해야 하기 때문에 변환을 해야 합니다. . 사용하는 모든 단어를 모아서 원핫 인코딩을 할 순 있겠지만 행렬 사이즈가 너무 커지고 희소행렬이 되는 등 비효율적입니다. . 그래서 주로 단어 임베딩을 많이 하는데 이 부분은 다음 시간에 자세히 다루겠습니다. . 간단히 임베딩이란 각 단어마다 고유 벡터값이 있는 것을 말합니다. 벡터 길이는 정하기 나름이겠죠. . . 셀프 어텐션의 구조를 간단히 표현한 그림입니다. 왼쪽 부분 n은 단어 개수, c는 임베딩 벡터 길이 입니다. . 3개의 같은 크기(임베딩 벡터 길이 X 임의에 차원 D) 가중치 행렬이 나옵니다. . 단어 임베딩 행렬과 3개의 가중치 행렬을 행렬곱 해서 3개의 새로운 행렬(단어 길이(3) X 임의에 차원 D)을 만듭니다. . 행렬은 각각 Q(쿼리), K(키), V(밸류) 행렬로 지칭합니다. 3개의 행렬 모두 행 기준으로 단어와 연관된 값이 순차적으로 저장될 것입니다. . 예시를 통해 설명하면 단어 임베딩 행렬에서 1행 I, 2행 am, 3행 good 단어가 임베딩되있는데 출력된 3개의 행렬 또한 행 기준으로 1행 I, 2행 am, 3행 good 단어와 관련된 정보가 기록되어 있을 것 입니다. . . 구한 쿼리(Q) 행렬과 키(K) 행렬의 전치 행렬을 행렬곱합니다. 행렬 크기를 먼저 살펴보면 (단어수 n 임의에 차원 D) (임의에 차원 D 단어수 n) = (단어수 n 단어수 n)이 나옵니다. . 여기서 쿼리(Q) 행렬은 현재 시점의 단어를 의미하고 키(K) 행렬은 attention을 구하고자 하는 대상 단어를 의미합니다. . 예시문장으로 설명하면 Q 1행, 즉 I 단어를 사용한다고 합시다. 이 I단어를 기준으로 Q 1행이 K 1행과 내적한다면 I와 관계, K 2행과 내적한다면 am 단어와 관계, K 3행과 내적한다면 good 단어와 관계를 나타내는 수치가 나옵니다. . 이런식으로 한 단어를 기준으로 문장 내 모든 단어와의 관계가 얼마나 깊은지를 나타내는 수치를 구하는 행동을 모든 단어에서 수행합니다. . 간단하게 설명하면 Q * T(K) 행렬에 결과로 1행 값은 I단어가 자신 포함 모든 단어와의 관련성 수치를 나타냅니다. . . 앞서 구한 Q * T(K) 행렬내 모든 값을 가중치 행렬에서 사용한 임의에 차원(D)의 제곱근 값으로 나눠줍니다. . 이 행위에 목적은 안정적인 경사값을 얻기 위함입니다. . 다음으로 소프트맥스 함수를 적용시킵니다. 이렇게 되면 임의에 행 백터의 합을 1로 만들 수 있습니다. . 이는 해석이 상당히 용이해지는데요. 예시로 첫 행을 보면 단어 I는 자기자신과 몇 퍼센트, am 단어와 몇 퍼센트, good 단어와 몇 퍼센트 연관있는지 구할 수 있습니다. . 여기서 몇 퍼센트는 전체 단어 내 몇퍼센트인지를 의미하니 직관적인 이해가 높아집니다. . 다음 단계로 지금까지 구한 행렬에 아까 구한 밸류(V) 행렬을 곱해줍니다. . . 쓰던 예시문장과 조금 다른임을 감안하고 참고해주세요! . 지금까지 구한 행렬의 특정 한 행(it 단어)을 관찰해봅시다. it과 관련이 높다면 큰 값(확률)을 갖고 낮다면 작은 값을 가집니다. . 이 값들이 밸류(V) 행렬과 곱하게 되면 관련이 높은 단어는 높은 값과 곱해지고 낮은 단어는 낮은 값만 곱해집니다. . 쉽게 얘기해서 관련이 높은 단어의 밸류(V) 행 값은 많이 적용되고, 관련이 낮은 단어의 밸류(V) 행 값은 적게 적용됩니다. . 최종 결과는 어텐션 행렬(Z)로 크기는 (단어 개수 N * 임의에 차원 D)이 됩니다. 지금까지 셀프 어텐션에 대해 알아봤습니다. . &#47680;&#54000; &#54756;&#46300; &#50612;&#53584;&#49496; . . 어텐션을 헤드 한 개만 사용한 형태에 대해서 알아봤는데 이를 확장하려고 합니다. . 어텐션 결과의 정확도를 높이기 위해 방금 했던 셀프 어텐션을 통해 어텐션 행렬(Z)를 만드는 행위를 여러번 반복합니다. . 당연히 Z의 버전을 여러개 만들면 Q, K, V 또한 새로운 버전을 만들어야 합니다. 이를 멀티 헤드 어텐션이라고 합니다. . 어텐션 행렬 여러개를 구한 뒤 열 방향으로 연결한 후 새로운 가중치 행렬을 행렬곱하면 최종 출력값이 나오게 됩니다. . 이때 어텐션 행렬(Z) 크기는 (단어 개수 N (임의에 차원 D 어텐션 행렬 개수)) 가 됩니다. 여기서 가중치의 크기는 (임의에 차원 어텐션 행렬 개수) 임베딩 백터 길이(c)를 해주기 때문에 최종 출력값의 크기는 (단어 개수 N * 임베딩 백터 길이 c)가 나오게 됩니다. . 이 최종 출력값의 크기를 입력 값의 크기와 일치시킨 이유는 이 작업을 여러번 반복하기 위함입니다. . &#50948;&#52824; &#51064;&#53076;&#46377; . . 트랜스포머는 앞서 말한것 처럼 RNN 류의 기법을 사용하지 않기 때문에 단어의 순서 정보를 사용하지 않을 수 있습니다. . 사실 단어의 순서 정보는 문장의 의미를 명확히 이해하기 위해선 상당히 중요합니다. 이를 위해서 위치 인코딩을 사용합니다. . 위치 인코딩은 단어를 임베딩하여 입력하기 전 위치 인코딩 행렬값을 추가하여 입력 값 자체에 위치 정보를 다는 것을 의미합니다. . Attention Is All You Need 논문 저자는 위치 인코딩을 계산하는 데 사인파 함수를 이용했다고 합니다. . &#51064;&#53076;&#45908; . . 앞서 말했던 멀티 헤드 어텐션을 사용하는 하나의 인코더를 관찰하겠습니다. 먼저 그림에는 생략되었지만 입력값에 위치 인코딩을 더해줍니다. . 다음으로 입력값을 멀티 헤드 어텐션에 넣어서 최종 출력값을 제출합니다. 그 후 add와 Norm 과정을 거치게 됩니다. . add는 다르게 표현하면 잔차 연결(Residual connection) 과정 입니다. 멀티 헤드 어텐션 출력값에 입력값(X)를 더해줍니다. 두 행렬의 크기가 동일하기 때문에 더하기 연산이 가능합니다. . 잔차 연결 과정은 하위 층에서 학습된 정보가 데이터 처리 과정에서 손실되는 것을 방지합니다. . 다음으로 Norm은 다르게 표현하면 층 정규화(Layer Normalization) 과정입니다. 행 기준으로 값들을 정규화 시켜 출력합니다. . 그 다음 피드포워드 네트워크 층을 통과시킵니다. 논문에서는 2개의 전결합층과 Relu 활성화 함수를 사용했습니다. 물론 출력하는 행렬 크기는 일정해야합니다. . 이후 add와 norm 과정을 한번 더 적용시키면 첫번째 인코더 과정이 끝납니다. 인코더 출력 행렬의 크기가 입력 행렬의 크기와 같다는 걸 잘 기억해야합니다. . 이러한 인코딩 과정을 여러번 수행합니다. 앞 인코더의 출력값을 다음 인코더의 입력값으로 계속 사용합니다. 논문에서는 총 6번 수행했다고 하네요. . &#46356;&#53076;&#45908; . . &#39;나는 차를 가지고 있습니다&#39; 문장으로 &#39;I have a car&#39; 문장을 생성하는 변역기를 만든다고 가정합시다. . 첫 인코더에 &#39;나는 차를 가지고 있습니다&#39; 을 넣고 인코더들을 계속해서 거치면서 만들어진 최종 출력값을 디코더의 입력값에 사용하게 됩니다. . 여기서 먼저 알아야할 것은 문장의 시작을 알리는 단어토큰으로 &#39;sos&#39; 이 있고, 문장의 끝을 나타내는 단어토큰으로 &#39;eos&#39; 가 있습니다. . 실제 모델을 사용할때 입력문장 &#39;나는 차를 가지고 있습니다&#39;을 인코더에 넣어 출력값을 만들면 그 출력값과 &#39;sos&#39; 토큰을 디코더에 넣을때 다음 값인 &#39;I&#39; 단어가 나와야합니다. . 다음으로 인코더 출력값과 &#39;sos&#39;토큰, 또 &#39;I&#39;단어를 디코더에 넣으면 &#39;am&#39; 단어가 나옵니다. 이런식으로 다음 단어를 찾는 식으로 진행됩니다. . 디코더에 넣어서 나온 단어들을 또 추가해서 다시 디코더에 넣어 다음 단어를 찾는 행위를 반복하다가 &#39;eos&#39; 단어 토큰이 나오면 중단하고 변역을 마칩니다. 이 부분 또한 중요한 것이 문장 변역시 입력된 문장과 출력된 문장간 길이는 다를 수 있다는 점을 고려했다는 것 입니다. . 주의할 점은 디코더에 단어를 넣을때도 마찬가지로 문장 순서 또한 중요하기 때문에 인코더와 마찬가지로 위치 인코딩 값을 넣어줍니다. . 이러한 디코더 모델을 만들기 위해 어떻게 학습이 진행되는지 살펴봅시다. . . 하나의 디코더는 3개의 서브 층으로 구성됩니다. 인코더와 다르게 첫번째 서브 층에 마스크 멀티 헤드 어텐션 부분이 추가 됬는데요. . 앞서 든 문장 번역 예시로 학습을 진행하면 타겟 문장 &#39;I have a car&#39;을 예측하는데 단어 단위로 보면 미리 다음 단어를 안 상태로 어텐션을 진행하면 안됩니다. 이걸 위해 마스크 멀티 헤드 어텐션 부분이 존재하는데 뒤에 더 자세히 설명하겠습니다. . 타겟 문장은 앞서 말한 것 처럼 한 단어씩 늘려서 입력하여 다음 단어를 추론하는데, 타겟 문장은 마스크 멀티 헤드 어텐션의 입력값으로 넣고, 인코더에서 나온 출력값과 마스크 멀티 헤드 어텐션의 출력값을 동시에 디코더 두번째 서브층인 멀티 헤드 어텐션에 넣습니다. . 이외에 디코더 입력값에 위치 인코더 값을 더해주는것과 add &amp; norm 층과 피드포워드 네트워크 층은 인코더와 동일합니다. 또 인코더와 마찬가지로 여러개의 디코더를 연이어서 붙힙니다. . 디코더의 최종 출력 값은 다음 단어를 예측하는 일이기 때문에 softmax 함수를 이용해 알맞는 단어를 출력해줍니다. . . 마스크 멀티 헤드 어텐션 부분에 대해 좀 더 자세히 알아보겠습니다. 한 단어씩 예측하는 모델이기 때문에 다음 단어는 모르는 것이 당연합니다. . 어텐션 단계에서도 마찬가지 인데요. all 단어만 있었던 디코더 입력 시기에는 밑에 단어는 알지 못합니다. all 단어 중심 어텐션에서 밑에 단어를 고려한 어텐션을 하는 것 자체가 모순이죠. . 일반 멀티 헤드 어텐션과 과정은 동일하나 쿼리(Q) 행렬과 키(K) 전치 행렬 간 행렬 곱한 행렬을 봅시다. 행 기준 단어와 다른 단어간 연관도 정도를 구하는데 이 부분에서 자기 단어보다 우측 단어(열 기준)는 몰랐던 단어이기 때문에 어텐션 값을 구하지 않습니다. . 그림에서 보다싶이 이 과정에서 우 삼각 행렬 값에 마이너스 무한대 값을 가진 행렬을 더해줍니다. 그 다음 과정이 시그모이드 함수에 적용하는 것인데 마이너스 무한대 값은 0으로 변환되겠네요. 즉 뒷 단어 값이 현 단어와의 상관관계가 0%라는 것을 표현합니다. . 이 과정 이외에 나머지 과정은 일반 멀티 헤드 어텐션과 동일합니다. . . 디코더의 두번째 서브층인 멀티 헤드 어텐션 관련 설명입니다. 인코더 내 멀티 헤드 어텐션에서는 입력값(X)에서 쿼리(Q), 키(K), 밸류(V) 행렬을 모두 만들었는데요. 디코더 멀티 헤드 어텐션에서는 조금 다릅니다. . 퀴리(Q) 행렬은 디코더의 첫번째 서브층, 마스크 멀티 헤드 어텐션의 출력값에서 추출되고, 키(K)와 밸류(V) 행렬은 인코더의 최종 출력물 행렬로 부터 추출 됩니다. . 이렇게 연산을 하는 이유는 멀티 헤드 어텐션을 진행하는 과정에서 쿼리(Q) 행렬과 키(K) 전치 행렬의 행렬곱을 구할때 확인할 수 있습니다. 행렬 곱 내 행 부분이 쿼리(Q) 행렬의 행 부분과 일치하고, 행렬곱 내 열 부분은 키(K) 행렬의 열 부분과 일치합니다. 이를 해석하면 타겟 값으로 입력된 문장 내 모든 단어(&#39;나는 차를 가지고 있습니다&#39;)와 인코더에서 출력된 입력 문장의 모든 단어(&#39;I have a car&#39;) 간 어텐션 연산이 진행됩니다. . 그 뒤 밸류(V) 행렬과 행렬 곱 연산을 진행하면 타겟 값으로 입력된 단어별로 인코더에서 출력된 입력 문장의 모든 단어의 연관도에 비례하여 값이 더해져서 출력값을 배출합니다. . 지금 다룬 내용을 아래 그림과 함께 보시면 이해가 잘 되실 것 같아요. . . &#45712;&#45184;&#51216; . &#39;구글 BERT의 정석&#39; 책과 &#39;attention is all you need&#39; 논문을 참고하여 트랜스포머 모델구조를 관찰했습니다. . 그냥 어텐션을 사용했다, 자연어 처리에 기초 모델이다로 알고 있었던 개념을 정확히 알 수 있어 좋은 기회였던 것 같습니다. . 개인적으로 자연어 처리 부분을 주로 다루는 것이 목표인데 다소 지루하지만 꼭 필요했던 과정인 것 같습니다. . 개념이 다소 복잡해 이해하는데 정말 시간을 많이 들였는데, 그래도 모델 구조만큼은 떳떳하게 이해했다고 말 할수 있어서 뿌듯하네요. . 설명이 부족한 부분은 댓글로 질문 써주시면 최대한 답변해보겠습니다. 감사합니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/deep%20learning/natural%20language/2022/03/04/SSUDA22_1.html",
            "relUrl": "/ssuda/book/deep%20learning/natural%20language/2022/03/04/SSUDA22_1.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "[DACON] 사물 이미지 분류 경진대회 With Pytorch",
            "content": ". &#45936;&#51060;&#53552; &#44396;&#44544; &#46300;&#46972;&#51060;&#48652;&#50640; &#51200;&#51109;&#54616;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . 코랩과 구글 드라이브를 연동하는 코드입니다. 구글 드라이브 내 파일을 코랩에서도 접근할 수 있게됩니다. . # !unzip -qq &#39;/content/drive/MyDrive/ObjectDetect/data.zip&#39; . 데이터를 저장하는 다른 더 좋고 빠른 방법이 있을 수도 있는데, 제가 발견한 방법 중 가장 좋은 방법을 소개합니다. . 우선 데이콘 내 데이터를 압축된 형태로 다운받고, 구글 드라이브 내 압축된 파일 그대로 업로드 합니다. . (제 기준 /content/drive/MyDrive/ObjectDetect 공간에 data.zip 파일을 넣었습니다. 여러분 상황에 맞게 주소를 바꾸시면 됩니다.) . 그 후 위 코드를 주석을 해제한 뒤 주소를 data.zip을 저장한 위치로 수정하시고 코드를 실행하시면 약 5분 뒤 압축 해제가 잘 됩니다. . 압축 해제 후 구글 드라이브 업로드, 구글 드라이브 내 압축 해제보다는 이 방법이 괜찮은 것 같아 소개합니다. . 더 좋은 방법 있으면 같이 공유 부탁드려요~ . &#54596;&#50836;&#54620; &#54056;&#53412;&#51648; &#49444;&#52824;&#54616;&#44256; &#44592;&#48376; &#49444;&#51221; &#54616;&#44592; . import os from glob import glob from typing import Tuple, Sequence, Callable import numpy as np import pandas as pd import PIL from PIL import Image from matplotlib.pyplot import imshow import torch import torch.optim as optim from torch import nn, Tensor import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader !pip install torchinfo from torchinfo import summary import torchvision from torchvision import transforms from torchvision.models import resnet50 from tqdm import tqdm, tqdm_notebook . Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.6.3) . 파이토치, 넘파이, 판다스 등 필요한 패키지를 임포트 했습니다. . path = &#39;/content/drive/MyDrive/ObjectDetect/&#39; batch_size = 32 num_epochs = 20 learning_rate = 0.001 labels = {0:&#39;airplane&#39;, 1:&#39;automobile&#39;, 2:&#39;bird&#39;, 3:&#39;cat&#39;, 4:&#39;deer&#39;, 5:&#39;dog&#39;, 6:&#39;frog&#39;, 7:&#39;horse&#39;, 8:&#39;ship&#39;, 9:&#39;truck&#39;} # gpu 사용중일땐 cuda, gpu 사용 불가능할땐 cpu 사용. device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) torch.manual_seed(777) if device ==&#39;cuda&#39;: torch.cuda.manual_seed_all(777) . 데이터의 저장 위치와 배치 사이즈, 에포크 수, 러닝 레이트 값, 장치 등을 지정했습니다. . 각 레이블 번호는 알파벳 순서대로 배정했습니다. . 이 코드를 따라하신다면 path만 파일 저장 위치로 바꾸시면 됩니다. . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . img = Image.open(path + &#39;train/airplane/0001.jpg&#39;) imshow(np.asarray(img)) . &lt;matplotlib.image.AxesImage at 0x7fcd91417310&gt; . PIL와 matplotlib 패키지를 가지고 이미지를 시각화 할 수 있습니다. . 데이터 분석을 하기 전 그래도 이미지 데이터인데 한번 시각적으로 확인했습니다. . train_images = [] train_labels = [] i = 0 for filename in sorted(glob(path + &quot;train/*&quot;)): for img in tqdm(glob(filename + &quot;/*.jpg&quot;)): an_img = PIL.Image.open(img) # 이미지를 읽습니다. img_array = np.array(an_img) # 이미지 데이터를 숫자로 변환합니다. train_images.append(img_array) # 리스트에 데이터 실기. label = i train_labels.append(label) i += 1 # 다음 폴더에는 다음 라벨 train_images = np.array(train_images) print(train_images.shape) . 100%|██████████| 5000/5000 [00:16&lt;00:00, 302.23it/s] 100%|██████████| 5000/5000 [00:18&lt;00:00, 270.39it/s] 100%|██████████| 5000/5000 [00:18&lt;00:00, 266.35it/s] 100%|██████████| 5000/5000 [00:12&lt;00:00, 404.58it/s] 100%|██████████| 5000/5000 [00:13&lt;00:00, 358.46it/s] 100%|██████████| 5000/5000 [00:11&lt;00:00, 418.30it/s] 100%|██████████| 5000/5000 [00:11&lt;00:00, 425.91it/s] 100%|██████████| 5000/5000 [00:12&lt;00:00, 392.43it/s] 100%|██████████| 5000/5000 [00:12&lt;00:00, 397.02it/s] 100%|██████████| 5000/5000 [00:12&lt;00:00, 404.29it/s] . (50000, 32, 32, 3) . 파이토치 내 ImageFolder 함수를 사용하는 것이 편리하지만, 이후 모델 학습시 데이터를 찾을때 계속 구글드라이브를 탐색하는 형태이기 때문에 지나치게 느립니다. . 그래서 트레인 데이터를 폴더 별로 반복문을 사용해 모두 직접 다운로드 하였습니다. . sorted로 폴더 이름을 정렬한 뒤 i를 업데이트 했기 때문에 알파벳 순서대로 라벨 숫자를 입력했습니다. . 입력된 데이터를 관찰하면 개수가 5만개이며 32 * 32 이미지 픽셀이고, 3채널(RGB값) 으로 구성되있습니다! . test_images = [] for filename in tqdm(sorted(glob(path + &quot;test/*.jpg&quot;))): an_img = PIL.Image.open(filename) # 이미지를 읽습니다. img_array = np.array(an_img) # 이미지 데이터를 숫자로 변환합니다. test_images.append(img_array) # 리스트에 데이터 실기. test_images = np.array(test_images) . 100%|██████████| 10000/10000 [00:25&lt;00:00, 385.08it/s] . 테스트 데이터도 마찬가지로 진행했습니다. . &#48520;&#47084;&#50728; &#45936;&#51060;&#53552;&#47196; &#45936;&#51060;&#53552; &#49483; &#47564;&#46308;&#44592; . class CustomDataset(Dataset): def __init__(self, transform, mode = &#39;train&#39;): self.transform = transform if mode == &#39;train&#39;: self.img_list = train_images self.img_labels = train_labels elif mode == &#39;test&#39;: self.img_list = test_images self.img_labels = [0] * 10000 # 형식을 맞추기 위해 def __len__(self): return len(self.img_list) def __getitem__(self, idx): return self.transform(self.img_list[idx]), self.img_labels[idx] . 파이토치 내 Dataset 클래스를 상속해서 Dataset을 정의했습니다. 뒤에 다루는 DataLoader 클래스와 호환이 되기 때문에 반드시 필요합니다. . 추상 클래스이기 때문에 반드시 새로 정의해야 하는 함수는 init, len, getitem 입니다. . init 부분은 클래스 선언시 자동으로 실행되는 함수입니다. 생성자 개념이죠. 이미지 데이터와 라벨값을 저장해줍니다. . 이때 테스트 데이터는 라벨이 없지만, 형식을 맞춰주기 위해 임의로 0라벨을 붙히겠습니다. . len 부분은 데이터 길이만 배출하면 됩니다. getitem 부분은 정해진 인덱스의 데이터를 출력해줘야 합니다. . DataLoader가 len과 getitem 함수를 사용하기 때문에 잘 정의해야합니다. . tensor = transforms.Compose( [transforms.ToTensor()] ) train_set = CustomDataset(tensor, mode = &#39;train&#39;) test_set = CustomDataset(tensor, mode = &#39;test&#39;) . 파이토치 모델은 텐서 단위로 다루기 때문에 텐서로 변환하는 transforms 을 만든 뒤 데이터 셋을 만듭니다. . meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x,_ in train_set] stdRGB = [np.std(x.numpy(), axis=(1,2)) for x,_ in train_set] meanR = np.mean([m[0] for m in meanRGB]) meanG = np.mean([m[1] for m in meanRGB]) meanB = np.mean([m[2] for m in meanRGB]) stdR = np.mean([s[0] for s in stdRGB]) stdG = np.mean([s[1] for s in stdRGB]) stdB = np.mean([s[2] for s in stdRGB]) print(meanR, meanG, meanB) print(stdR, stdG, stdB) . 0.49125046 0.48208782 0.44651186 0.20106333 0.19889629 0.20135581 . 입력된 이미지 데이터를 정규화(표준화) 시켜주겠습니다. 정규화를 위해서 표본 평균과 표준오차를 구하는 모습입니다. . transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize([meanR, meanG, meanB], [stdR, stdG, stdB]) ]) train_set.transform = transform test_set.transform = transform . 파이토치 내 Compose 함수를 한번 더 사용하고, 앞서 구한 표본평균과 표준오차를 이용해 transform을 만들었습니다. . 데이터를 데이터 셋 형태로 정리했기 때문에 &#39;train_set.transform = transform&#39; 이 코드만 사용하면 간편하게 적용이 가능합니다. . train_set.__len__() . 50000 . train_set.__getitem__(40) . (tensor([[[-0.5124, -0.5124, -0.4538, ..., -0.4343, -0.4538, -0.4538], [-0.5124, -0.5124, -0.4538, ..., -0.4538, -0.4538, -0.4733], [-0.5514, -0.5709, -0.5124, ..., -0.4733, -0.4733, -0.4928], ..., [-1.6826, -1.6436, -1.6241, ..., -1.7801, -1.7216, -1.5656], [-1.6631, -1.6436, -1.6241, ..., -1.3510, -1.3705, -1.5071], [-1.7606, -1.7411, -1.7216, ..., -1.7216, -1.7021, -1.5656]], [[ 0.2379, 0.2379, 0.2971, ..., 0.3957, 0.3759, 0.3759], [ 0.2379, 0.2379, 0.2971, ..., 0.3759, 0.3759, 0.3562], [ 0.2576, 0.2379, 0.2971, ..., 0.3562, 0.3562, 0.3365], ..., [-1.1028, -1.1028, -1.0831, ..., -1.3591, -1.3591, -1.2014], [-1.0831, -1.0634, -1.0831, ..., -0.9845, -1.0042, -1.1422], [-1.1817, -1.1619, -1.1422, ..., -1.3591, -1.3394, -1.2014]], [[ 0.4117, 0.4117, 0.4701, ..., 0.3922, 0.3728, 0.3728], [ 0.4117, 0.4117, 0.4701, ..., 0.3728, 0.3728, 0.3533], [ 0.4117, 0.3922, 0.4507, ..., 0.3533, 0.3533, 0.3338], ..., [-1.1464, -1.1269, -1.1074, ..., -1.3411, -1.3216, -1.1658], [-1.1269, -1.1074, -1.1074, ..., -0.9126, -0.9321, -1.0685], [-1.2243, -1.2048, -1.1853, ..., -1.2827, -1.2632, -1.1269]]]), 0) . 실제 값으로 확인해보면 32 * 32가 맞는것으로 보이며, 뒤에 라벨 값으로 0이 달려있는 모습입니다. . len , getitem 함수로 데이터 값을 불러올 수 있는 걸 보니 파이토치 내 Dataset 클래스 형태로 데이터가 잘 입력된 것 같습니다. . &#45936;&#51060;&#53552;&#49483;&#51012; &#45936;&#51060;&#53552; &#47196;&#45908;&#50640; &#49892;&#44592; . train_loader = DataLoader(train_set, batch_size = batch_size, num_workers=2, shuffle=True) test_loader = DataLoader(test_set, batch_size = batch_size, num_workers=2) . 앞서 정의한 데이터 셋을 파이토치 내 데이터 로더에 실습니다. 이렇게 하는 이유는 배치단위로 모델에 적용하기 쉽기 때문입니다. . 여기서 중요한 점은 테스트 데이터는 셔플을 해줘야 합니다. 데이터가 라벨 값으로 정렬되어 있는데, 학습에 큰 문제가 되기 때문입니다. . 셔플을 True로 해주게 되면 또 하나의 장점이 에포크가 바뀔때마다 로더 내 데이터 패치를 다시 섞어줍니다. . for batch_idx, samples in enumerate(train_loader): if batch_idx &gt; 0: break print(samples[0].shape) print(samples[1]) . torch.Size([32, 3, 32, 32]) tensor([2, 8, 2, 5, 7, 1, 9, 0, 8, 0, 3, 1, 7, 2, 9, 6, 3, 6, 2, 9, 2, 1, 5, 5, 9, 8, 2, 8, 1, 6, 5, 1]) . 데이터 로더를 보다 상세히 관찰해보면 배치단위, 채널수, 32 * 32 로 잘 들어가 있는걸 관찰할 수 있으며, 라벨값도 잘 들어가 있습니다. . &#47784;&#45944; &#47564;&#46308;&#44592; . class Model(nn.Module): def __init__(self) -&gt; None: super().__init__() self.resnet = resnet50(pretrained=False) self.classifier = nn.Linear(1000, 10) def forward(self, x): x = self.resnet(x) x = self.classifier(x) return x . 파이토치 내 nn.Module 클래스를 상속하고 resnet50 을 사용하여 모델을 정의했습니다. . 이때 대회 규칙 상 사전학습이 안되므로 pretrained=False 로 잘 지정해줘야합니다. . model = Model().to(device) print(summary(model, input_size = (1,3,32,32))) . =============================================================================================== Layer (type:depth-idx) Output Shape Param # =============================================================================================== Model -- -- ├─ResNet: 1-1 [1, 1000] -- │ └─Conv2d: 2-1 [1, 64, 16, 16] 9,408 │ └─BatchNorm2d: 2-2 [1, 64, 16, 16] 128 │ └─ReLU: 2-3 [1, 64, 16, 16] -- │ └─MaxPool2d: 2-4 [1, 64, 8, 8] -- │ └─Sequential: 2-5 [1, 256, 8, 8] -- │ │ └─Bottleneck: 3-1 [1, 256, 8, 8] 75,008 │ │ └─Bottleneck: 3-2 [1, 256, 8, 8] 70,400 │ │ └─Bottleneck: 3-3 [1, 256, 8, 8] 70,400 │ └─Sequential: 2-6 [1, 512, 4, 4] -- │ │ └─Bottleneck: 3-4 [1, 512, 4, 4] 379,392 │ │ └─Bottleneck: 3-5 [1, 512, 4, 4] 280,064 │ │ └─Bottleneck: 3-6 [1, 512, 4, 4] 280,064 │ │ └─Bottleneck: 3-7 [1, 512, 4, 4] 280,064 │ └─Sequential: 2-7 [1, 1024, 2, 2] -- │ │ └─Bottleneck: 3-8 [1, 1024, 2, 2] 1,512,448 │ │ └─Bottleneck: 3-9 [1, 1024, 2, 2] 1,117,184 │ │ └─Bottleneck: 3-10 [1, 1024, 2, 2] 1,117,184 │ │ └─Bottleneck: 3-11 [1, 1024, 2, 2] 1,117,184 │ │ └─Bottleneck: 3-12 [1, 1024, 2, 2] 1,117,184 │ │ └─Bottleneck: 3-13 [1, 1024, 2, 2] 1,117,184 │ └─Sequential: 2-8 [1, 2048, 1, 1] -- │ │ └─Bottleneck: 3-14 [1, 2048, 1, 1] 6,039,552 │ │ └─Bottleneck: 3-15 [1, 2048, 1, 1] 4,462,592 │ │ └─Bottleneck: 3-16 [1, 2048, 1, 1] 4,462,592 │ └─AdaptiveAvgPool2d: 2-9 [1, 2048, 1, 1] -- │ └─Linear: 2-10 [1, 1000] 2,049,000 ├─Linear: 1-2 [1, 10] 10,010 =============================================================================================== Total params: 25,567,042 Trainable params: 25,567,042 Non-trainable params: 0 Total mult-adds (M): 85.52 =============================================================================================== Input size (MB): 0.01 Forward/backward pass size (MB): 3.64 Params size (MB): 102.27 Estimated Total Size (MB): 105.92 =============================================================================================== . 파이토치에서 중요하게 알아두어야 할 것이 디바이스(GPU를 주로 사용합니다) 내 실어줘야 하는것이 모델과 입력 값, 라벨 값 입니다. . 여기서 to(device) 를 사용하여 모델을 디바이스 내 실어주었습니다. . 앞서 정의한 모델을 직관적으로 관찰하기 위해 torchinfo(summary) 를 사용하였습니다. . 모델 구조가 복잡한 것도 그렇고, 모델 파라미터는 25,567,042개니 역시 &#39;딥&#39;러닝이네요. . &#47784;&#45944; &#54617;&#49845;&#54616;&#44592; . optimizer = optim.Adam(model.parameters(), lr = learning_rate) criterion = nn.CrossEntropyLoss() model.train() for epoch in range(num_epochs): for i, (images, targets) in tqdm(enumerate(train_loader)): optimizer.zero_grad() images = images.to(device) targets = targets.to(device) outputs = model(images) loss = criterion(outputs, targets) loss.backward() optimizer.step() if (i+1) % 200 == 0: max_vals, max_indices = torch.max(outputs, 1) acc = (targets == max_indices).float().mean() print(f&#39;{loss.item():.5f}, {acc.item():.5f}&#39;) . 201it [00:20, 9.48it/s] . 2.03402, 0.25000 . 402it [00:41, 9.71it/s] . 1.68516, 0.40625 . 601it [01:01, 9.59it/s] . 2.04422, 0.28125 . 802it [01:21, 9.71it/s] . 1.48867, 0.46875 . 1001it [01:41, 9.63it/s] . 1.43756, 0.46875 . 1202it [02:02, 9.70it/s] . 1.60528, 0.40625 . 1402it [02:22, 9.63it/s] . 1.23092, 0.62500 . 1563it [02:39, 9.82it/s] 201it [00:20, 9.62it/s] . 1.57936, 0.43750 . 401it [00:40, 9.75it/s] . 1.51674, 0.43750 . 602it [01:01, 9.75it/s] . 1.81981, 0.37500 . 801it [01:21, 9.59it/s] . 1.41190, 0.56250 . 1000it [01:42, 9.32it/s] . 1.63900, 0.40625 . 1201it [02:04, 9.56it/s] . 1.07908, 0.59375 . 1401it [02:26, 9.69it/s] . 1.33048, 0.56250 . 1563it [02:42, 9.62it/s] 201it [00:21, 9.40it/s] . 1.38050, 0.53125 . 401it [00:41, 9.25it/s] . 1.46519, 0.50000 . 601it [01:02, 9.59it/s] . 1.54906, 0.43750 . 801it [01:22, 9.61it/s] . 1.55637, 0.40625 . 1001it [01:43, 9.65it/s] . 1.28245, 0.59375 . 1202it [02:03, 9.77it/s] . 1.17159, 0.59375 . 1401it [02:24, 9.53it/s] . 1.21823, 0.59375 . 1563it [02:40, 9.72it/s] 201it [00:20, 9.41it/s] . 1.38219, 0.53125 . 401it [00:41, 9.45it/s] . 1.07596, 0.68750 . 601it [01:02, 9.57it/s] . 1.51185, 0.50000 . 801it [01:22, 9.51it/s] . 0.88233, 0.68750 . 1002it [01:46, 9.64it/s] . 0.92160, 0.68750 . 1202it [02:07, 9.68it/s] . 0.87461, 0.68750 . 1401it [02:27, 9.55it/s] . 0.92864, 0.62500 . 1563it [02:44, 9.49it/s] 201it [00:21, 9.57it/s] . 1.02033, 0.71875 . 401it [00:42, 9.56it/s] . 0.67408, 0.78125 . 601it [01:03, 6.73it/s] . 1.29992, 0.53125 . 801it [01:24, 9.57it/s] . 1.12536, 0.59375 . 1001it [01:45, 9.60it/s] . 1.28840, 0.56250 . 1201it [02:05, 9.43it/s] . 0.82536, 0.71875 . 1401it [02:27, 9.25it/s] . 0.70235, 0.78125 . 1563it [02:44, 9.53it/s] 201it [00:22, 9.50it/s] . 0.93481, 0.68750 . 401it [00:42, 9.29it/s] . 0.46380, 0.84375 . 601it [01:03, 9.21it/s] . 1.15582, 0.56250 . 801it [01:23, 9.25it/s] . 0.64076, 0.78125 . 1001it [01:44, 9.57it/s] . 1.25706, 0.65625 . 1201it [02:05, 9.49it/s] . 1.23735, 0.65625 . 1401it [02:26, 9.52it/s] . 0.80131, 0.65625 . 1563it [02:42, 9.60it/s] 201it [00:20, 9.37it/s] . 0.81935, 0.68750 . 401it [00:41, 9.49it/s] . 1.07775, 0.65625 . 601it [01:02, 9.49it/s] . 0.79303, 0.75000 . 801it [01:22, 9.32it/s] . 0.60452, 0.78125 . 1001it [01:43, 9.25it/s] . 0.89765, 0.71875 . 1201it [02:04, 9.45it/s] . 0.79112, 0.65625 . 1401it [02:24, 9.49it/s] . 0.47540, 0.81250 . 1563it [02:41, 9.70it/s] 201it [00:20, 9.49it/s] . 0.50384, 0.87500 . 401it [00:43, 8.21it/s] . 0.74633, 0.75000 . 601it [01:09, 9.47it/s] . 0.68515, 0.78125 . 801it [01:30, 9.59it/s] . 0.64061, 0.84375 . 1001it [01:50, 9.50it/s] . 0.85828, 0.59375 . 1201it [02:11, 9.44it/s] . 0.45279, 0.87500 . 1401it [02:31, 9.39it/s] . 0.80346, 0.71875 . 1563it [02:49, 9.25it/s] 201it [00:20, 9.57it/s] . 0.44945, 0.87500 . 401it [00:45, 7.71it/s] . 0.79414, 0.71875 . 601it [01:12, 6.42it/s] . 0.48421, 0.87500 . 801it [01:33, 9.57it/s] . 1.00993, 0.68750 . 1001it [01:54, 9.46it/s] . 0.53396, 0.87500 . 1201it [02:14, 9.52it/s] . 0.63844, 0.71875 . 1401it [02:35, 9.50it/s] . 0.67957, 0.81250 . 1563it [02:52, 9.07it/s] 201it [00:20, 9.38it/s] . 0.25926, 0.93750 . 401it [00:41, 9.47it/s] . 0.61564, 0.71875 . 601it [01:01, 9.54it/s] . 0.74993, 0.84375 . 801it [01:22, 9.48it/s] . 0.98167, 0.71875 . 1000it [01:43, 9.49it/s] . 0.33597, 0.87500 . 1201it [02:03, 9.45it/s] . 0.72271, 0.71875 . 1401it [02:24, 9.53it/s] . 0.45406, 0.90625 . 1563it [02:41, 9.71it/s] 201it [00:20, 9.44it/s] . 0.57367, 0.81250 . 401it [00:41, 9.44it/s] . 0.34390, 0.87500 . 600it [01:01, 9.42it/s] . 0.66048, 0.75000 . 801it [01:22, 9.40it/s] . 0.64899, 0.75000 . 1001it [01:42, 9.57it/s] . 0.51046, 0.84375 . 1201it [02:03, 9.41it/s] . 0.49834, 0.87500 . 1401it [02:23, 9.45it/s] . 0.38570, 0.84375 . 1563it [02:40, 9.75it/s] 201it [00:29, 9.44it/s] . 0.44295, 0.84375 . 400it [00:50, 9.39it/s] . 0.63138, 0.81250 . 601it [01:10, 9.49it/s] . 1.09141, 0.68750 . 800it [01:31, 9.58it/s] . 0.79246, 0.68750 . 1000it [01:51, 9.40it/s] . 0.35505, 0.81250 . 1202it [02:12, 9.74it/s] . 0.53283, 0.84375 . 1401it [02:32, 9.53it/s] . 0.32168, 0.93750 . 1563it [02:49, 9.23it/s] 201it [00:20, 9.53it/s] . 0.25396, 0.87500 . 402it [00:41, 9.67it/s] . 0.36281, 0.84375 . 601it [01:01, 9.56it/s] . 0.16314, 0.90625 . 802it [01:21, 9.74it/s] . 0.53413, 0.81250 . 1002it [01:42, 9.68it/s] . 0.17970, 0.93750 . 1201it [02:02, 9.54it/s] . 0.30832, 0.87500 . 1401it [02:22, 9.51it/s] . 0.43612, 0.84375 . 1563it [02:39, 9.80it/s] 201it [00:20, 9.50it/s] . 0.28544, 0.90625 . 401it [00:40, 9.35it/s] . 0.20452, 0.93750 . 602it [01:01, 9.65it/s] . 0.57266, 0.81250 . 801it [01:21, 9.50it/s] . 0.44896, 0.84375 . 1001it [01:42, 9.59it/s] . 0.07010, 0.96875 . 1201it [02:02, 9.60it/s] . 0.47427, 0.84375 . 1402it [02:23, 9.73it/s] . 0.28636, 0.87500 . 1563it [02:39, 9.80it/s] 201it [00:20, 9.52it/s] . 0.64141, 0.84375 . 400it [00:40, 9.33it/s] . 0.35076, 0.90625 . 601it [01:01, 9.54it/s] . 0.61805, 0.84375 . 801it [01:21, 9.43it/s] . 0.41816, 0.93750 . 1001it [01:42, 9.39it/s] . 0.27354, 0.87500 . 1202it [02:02, 9.61it/s] . 1.46630, 0.84375 . 1401it [02:23, 9.58it/s] . 0.71938, 0.75000 . 1563it [02:39, 9.79it/s] 202it [00:20, 9.64it/s] . 0.16202, 0.93750 . 401it [00:40, 9.47it/s] . 0.10540, 1.00000 . 602it [01:01, 9.64it/s] . 0.16295, 0.93750 . 802it [01:21, 9.69it/s] . 0.14185, 0.96875 . 1001it [01:42, 9.51it/s] . 0.08471, 1.00000 . 1201it [02:02, 9.46it/s] . 0.20972, 0.90625 . 1401it [02:23, 9.38it/s] . 0.31754, 0.87500 . 1563it [02:39, 9.79it/s] 201it [00:20, 9.57it/s] . 0.02871, 1.00000 . 400it [00:40, 9.41it/s] . 0.62937, 0.87500 . 600it [01:01, 9.41it/s] . 0.17031, 0.93750 . 801it [01:21, 9.31it/s] . 0.30475, 0.90625 . 1001it [01:42, 9.49it/s] . 0.18417, 0.93750 . 1201it [02:03, 9.44it/s] . 0.29402, 0.87500 . 1402it [02:24, 9.53it/s] . 0.22230, 0.90625 . 1563it [02:40, 9.72it/s] 201it [00:20, 9.50it/s] . 0.15602, 0.93750 . 401it [00:41, 9.35it/s] . 0.13093, 0.96875 . 601it [01:01, 9.37it/s] . 0.16693, 0.93750 . 801it [01:22, 9.50it/s] . 0.14665, 0.96875 . 1001it [01:43, 9.41it/s] . 0.69271, 0.81250 . 1201it [02:03, 9.47it/s] . 0.25087, 0.90625 . 1400it [02:24, 9.23it/s] . 0.67821, 0.78125 . 1563it [02:40, 9.72it/s] 201it [00:20, 9.45it/s] . 0.38167, 0.87500 . 401it [00:41, 9.37it/s] . 0.27044, 0.87500 . 601it [01:01, 9.52it/s] . 0.11917, 0.96875 . 801it [01:22, 9.47it/s] . 0.07680, 0.96875 . 1001it [01:42, 9.58it/s] . 0.36528, 0.81250 . 1202it [02:03, 9.70it/s] . 0.27552, 0.93750 . 1401it [02:23, 9.49it/s] . 0.16641, 0.93750 . 1563it [02:40, 9.76it/s] 201it [00:20, 9.46it/s] . 0.03546, 1.00000 . 401it [00:41, 9.46it/s] . 0.18016, 0.90625 . 601it [01:01, 9.55it/s] . 0.06724, 0.96875 . 801it [01:21, 9.44it/s] . 0.35314, 0.90625 . 1000it [01:42, 9.45it/s] . 0.90582, 0.81250 . 1201it [02:02, 9.54it/s] . 0.22282, 0.96875 . 1401it [02:23, 9.48it/s] . 0.10338, 0.93750 . 1563it [02:39, 9.79it/s] . 옵티마이저로 Adam을, 손실함수로 크로스엔트로피 함수를 사용합니다. 파이토치 내 전부 내장되어있죠. . 모델을 학습형태로 바꾼 뒤 설정한 에포크 만큼 학습을 진행하겠습니다. 20 에포크 코랩 무료 GPU 기준 1시간정도 걸리네요. . a = [[0.1, 0.2, 0.3], [0.3,0.4,0.2]] a = torch.tensor(a) torch.max(a, 1) . torch.return_types.max(values=tensor([0.3000, 0.4000]), indices=tensor([2, 1])) . 파이토치 내 max 함수를 사용했을때 결과입니다. 결과를 보시면 윗 반복문 내 if문을 저렇게 사용했는지 아실것 같습니다. . &#53944;&#47112;&#51064; &#45936;&#51060;&#53552;&#50640; &#47784;&#45944; &#51201;&#50857;&#54616;&#44592; . sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) model.eval() batch_index = 0 for i, (images, targets) in enumerate(test_loader): images = images.to(device) outputs = model(images) batch_index = i * batch_size max_vals, max_indices = torch.max(outputs, 1) sample_submission.iloc[batch_index:batch_index + batch_size, 1:] = max_indices.long().cpu().numpy()[:,np.newaxis] . 모델을 평가모드로 바꾸고 (가중치 업데이트를 하지 않기 때문입니다.) 테스트 데이터 로더를 모델에 넣습니다. . 테스트 데이터도 마찬가지로 gpu 사용을 위해선 device에 넣어줘야 합니다. . labels = {0:&#39;airplane&#39;, 1:&#39;automobile&#39;, 2:&#39;bird&#39;, 3:&#39;cat&#39;, 4:&#39;deer&#39;, 5:&#39;dog&#39;, 6:&#39;frog&#39;, 7:&#39;horse&#39;, 8:&#39;ship&#39;, 9:&#39;truck&#39;} sample_submission[&#39;target&#39;] = sample_submission[&#39;target&#39;].map(labels) sample_submission.to_csv(&#39;dacon_object_1.csv&#39;, index=False) . 숫자로 나온 값을 문자로 매핑시킨 뒤 csv 파일로 저장합니다. public 값 기준 0.7662정도 나왔는데 과적합이 된 것으로 보여요. .",
            "url": "https://ksy1526.github.io/myblog/jupyter/deep%20learning/pytorch/dacon/classifier/conputer%20vision/2022/02/24/torch4.html",
            "relUrl": "/jupyter/deep%20learning/pytorch/dacon/classifier/conputer%20vision/2022/02/24/torch4.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "[DACON] 자연어 처리 문장 쌍 분류하기2 With Pytorch",
            "content": ". &#54056;&#53412;&#51648; &#49444;&#52824;&#54616;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . 구글 드라이브와 연동합니다. . !pip install mxnet-cu101 !pip install gluonnlp pandas tqdm !pip install sentencepiece==0.1.85 !pip install transformers==2.1.1 !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master import pandas as pd import numpy as np from tqdm import tqdm, tqdm_notebook import torch from torch import nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader import gluonnlp as nlp #kobert from kobert.utils import get_tokenizer from kobert.pytorch_kobert import get_pytorch_kobert_model #transformers from transformers import AdamW # 스케줄러 조정 함수 from transformers.optimization import get_cosine_schedule_with_warmup import warnings warnings.filterwarnings(&quot;ignore&quot;) . Collecting mxnet-cu101 Downloading mxnet_cu101-1.9.0-py3-none-manylinux2014_x86_64.whl (358.1 MB) |████████████████████████████████| 358.1 MB 4.7 kB/s Requirement already satisfied: numpy&lt;2.0.0,&gt;1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (1.21.5) Requirement already satisfied: requests&lt;3,&gt;=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (2.23.0) Collecting graphviz&lt;0.9.0,&gt;=0.8.1 Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet-cu101) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet-cu101) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet-cu101) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet-cu101) (2021.10.8) Installing collected packages: graphviz, mxnet-cu101 Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 Successfully installed graphviz-0.8.4 mxnet-cu101-1.9.0 Collecting gluonnlp Downloading gluonnlp-0.10.0.tar.gz (344 kB) |████████████████████████████████| 344 kB 11.0 MB/s Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.21.5) Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.27) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;gluonnlp) (3.0.7) Building wheels for collected packages: gluonnlp Building wheel for gluonnlp (setup.py) ... done Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595727 sha256=21bfa5933185a4a625340e1bfaf8e75fd58e3e8d80269d47e30a5d2587306f84 Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00 Successfully built gluonnlp Installing collected packages: gluonnlp Successfully installed gluonnlp-0.10.0 Collecting sentencepiece==0.1.85 Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB) |████████████████████████████████| 1.0 MB 11.2 MB/s Installing collected packages: sentencepiece Successfully installed sentencepiece-0.1.85 Collecting transformers==2.1.1 Downloading transformers-2.1.1-py3-none-any.whl (311 kB) |████████████████████████████████| 311 kB 11.2 MB/s Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (0.1.85) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2.23.0) Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2019.12.20) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (4.62.3) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (1.21.5) Collecting boto3 Downloading boto3-1.21.3-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 51.3 MB/s Collecting sacremoses Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 51.4 MB/s Collecting jmespath&lt;1.0.0,&gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting botocore&lt;1.25.0,&gt;=1.24.3 Downloading botocore-1.24.3-py3-none-any.whl (8.5 MB) |████████████████████████████████| 8.5 MB 7.0 MB/s Collecting s3transfer&lt;0.6.0,&gt;=0.5.0 Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 8.5 MB/s Collecting urllib3&lt;1.27,&gt;=1.25.4 Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB) |████████████████████████████████| 138 kB 53.8 MB/s Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore&lt;1.25.0,&gt;=1.24.3-&gt;boto3-&gt;transformers==2.1.1) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.25.0,&gt;=1.24.3-&gt;boto3-&gt;transformers==2.1.1) (1.15.0) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==2.1.1) (3.0.4) Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 49.3 MB/s Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==2.1.1) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==2.1.1) (2.10) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==2.1.1) (1.1.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==2.1.1) (7.1.2) Installing collected packages: urllib3, jmespath, botocore, s3transfer, sacremoses, boto3, transformers Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. Successfully installed boto3-1.21.3 botocore-1.24.3 jmespath-0.10.0 s3transfer-0.5.1 sacremoses-0.0.47 transformers-2.1.1 urllib3-1.25.11 Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-3u3t0irc Running command git clone -q &#39;https://****@github.com/SKTBrain/KoBERT.git&#39; /tmp/pip-req-build-3u3t0irc Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.21.3) Requirement already satisfied: gluonnlp&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0) Collecting mxnet&gt;=1.4.0 Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB) |████████████████████████████████| 47.3 MB 37.5 MB/s Collecting onnxruntime==1.8.0 Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB) |████████████████████████████████| 4.5 MB 39.4 MB/s Requirement already satisfied: sentencepiece&gt;=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.1.85) Requirement already satisfied: torch&gt;=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111) Collecting transformers&gt;=4.8.1 Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB) |████████████████████████████████| 3.5 MB 40.1 MB/s Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (2.0) Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (3.17.3) Requirement already satisfied: numpy&gt;=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (1.21.5) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (21.3) Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (0.29.27) Requirement already satisfied: requests&lt;3,&gt;=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.23.0) Requirement already satisfied: graphviz&lt;0.9.0,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (0.8.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (1.25.11) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.10) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.7.0-&gt;kobert==0.2.3) (3.10.0.2) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.4.2) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.11.0) Collecting huggingface-hub&lt;1.0,&gt;=0.1.0 Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB) |████████████████████████████████| 67 kB 6.6 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (2019.12.20) Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (0.0.47) Collecting pyyaml&gt;=5.1 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 44.1 MB/s Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.62.3) Collecting tokenizers!=0.11.3,&gt;=0.10.1 Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB) |████████████████████████████████| 6.8 MB 49.8 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (3.0.7) Requirement already satisfied: s3transfer&lt;0.6.0,&gt;=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3-&gt;kobert==0.2.3) (0.5.1) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3-&gt;kobert==0.2.3) (0.10.0) Requirement already satisfied: botocore&lt;1.25.0,&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from boto3-&gt;kobert==0.2.3) (1.24.3) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore&lt;1.25.0,&gt;=1.24.3-&gt;boto3-&gt;kobert==0.2.3) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.25.0,&gt;=1.24.3-&gt;boto3-&gt;kobert==0.2.3) (1.15.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.7.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (1.1.0) Building wheels for collected packages: kobert Building wheel for kobert (setup.py) ... done Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15449 sha256=5ca56065402e4d6b5d7a3099cd3639179f1e840149ba1a0fbe8273c500f816f1 Stored in directory: /tmp/pip-ephem-wheel-cache-kat_1l4g/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0 Successfully built kobert Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, onnxruntime, mxnet, kobert Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: transformers Found existing installation: transformers 2.1.1 Uninstalling transformers-2.1.1: Successfully uninstalled transformers-2.1.1 Successfully installed huggingface-hub-0.4.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 tokenizers-0.11.5 transformers-4.16.2 . 필요한 패키지를 설치하고 임포트 합니다. . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . path = &#39;/content/drive/MyDrive/sentence/&#39; train = pd.read_csv(path + &#39;train_data.csv&#39;) test = pd.read_csv(path + &#39;test_data.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . index premise hypothesis label . 0 0 | 씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이... | 씨름의 여자들의 놀이이다. | contradiction | . 1 1 | 삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,... | 자작극을 벌인 이는 3명이다. | contradiction | . 2 2 | 이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다. | 예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다. | entailment | . 3 3 | 광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ... | 원주민들은 종합대책에 만족했다. | neutral | . 4 4 | 진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는... | 이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다. | neutral | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 데이터를 불러옵니다. 데이터에 대한 설명은 이전에 했으니 생략합니다. . label_dict = {&#39;entailment&#39; : 0, &#39;contradiction&#39; : 1, &#39;neutral&#39; : 2} train_content = [] test_content = [] for idx in tqdm(train.index): train_content.append(list([[train.loc[idx, &#39;premise&#39;], train.loc[idx, &#39;hypothesis&#39;]], label_dict[train.loc[idx, &#39;label&#39;]]])) for idx in tqdm(test.index): test_content.append(list([[test.loc[idx, &#39;premise&#39;], test.loc[idx, &#39;hypothesis&#39;]]])) dataset_train = train_content[:20000] dataset_valid = train_content[20000:] dataset_test = test_content dataset_train[:5] . 100%|██████████| 24998/24998 [00:01&lt;00:00, 23116.36it/s] 100%|██████████| 1666/1666 [00:00&lt;00:00, 39118.13it/s] . [[[&#39;씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이 넓고 평평한 백사장이나 마당에서 모여 서로 힘과 슬기를 겨루는 것이다.&#39;, &#39;씨름의 여자들의 놀이이다.&#39;], 1], [[&#39;삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나, 중국 내에서의 여론은 자작극이라는 증거가 충분함에도 불구하고 좋지 않다.&#39;, &#39;자작극을 벌인 이는 3명이다.&#39;], 1], [[&#39;이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.&#39;, &#39;예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.&#39;], 0], [[&#39;광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 적극 나섰다.&#39;, &#39;원주민들은 종합대책에 만족했다.&#39;], 2], [[&#39;진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는 책임 있는 모습을 보여주는 것이 필요하다.&#39;, &#39;이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.&#39;], 2]] . 입력된 데이터를 리스트 형태로 다시 구성합니다. 이때 [[전제, 가설], 라벨] 구성으로 데이터 한 세트를 만듭니다. . 2만개를 트레인 세트, 나머지 약 5천개를 valid 세트로 구성합니다. . &#45936;&#51060;&#53552; &#47196;&#45908; &#44396;&#52629;&#54616;&#44592; . max_len = 70 batch_size = 64 warmup_ratio = 0.1 num_epochs = 5 max_grad_norm = 1 log_interval = 200 learning_rate = 5e-5 if torch.cuda.is_available(): device = torch.device(&#39;cuda:0&#39;) else: device = torch.device(&#39;cpu&#39;) # kobert 패키지 내 BERT 모델과 어휘 집합을 입력받습니다. bertmodel, vocab = get_pytorch_kobert_model(cachedir = &#39;.cache&#39;) # kobert 패키지 내 토크나이저를 입력받습니다. tokenizer = get_tokenizer() # 입력받은 토크나이저를 입력받은 어휘 집합을 이용해 학습합니다. tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False) . /content/.cache/kobert_v1.zip[██████████████████████████████████████████████████] /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████] using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece . 기본적인 하이퍼 파라미터 설정을 미리 합니다. 문장 최대 길이, 에포크, 러닝레이트 등등을 미리 지정합니다. . 또 kobert 패키지 내 BERT 모델, 토크나이저, 어휘 집합을 입력받습니다. . class BERTDataset(Dataset): def __init__(self, dataset, sen_idx, label_idx, bert_tokenizer, max_len, pad, pair, mode = &#39;train&#39;): self.mode = mode transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length = max_len, pad = pad, pair = pair) # 문장 쌍(pair = True)을 학습하는 트렌스포머를 만듭니다. if self.mode == &#39;train&#39;: self.sentence = [transform(i[sen_idx]) for i in dataset] self.labels = [np.int32(i[label_idx]) for i in dataset] else: self.sentence = [transform(i[sen_idx]) for i in dataset] def __getitem__(self, i): if self.mode == &#39;train&#39;: return (self.sentence[i] + (self.labels[i],)) else: return self.sentence[i] def __len__(self): return (len(self.sentence)) . 파이토치 내 Dataset 클래스를 상속받아서 데이터 셋을 클래스로 만듭니다. . Dataset 클래스를 상속했기 때문에 향후 DataLoader 내에 실을 수 있겠죠. . 리스트 형태인 데이터 내 문장을 트랜스포머를 이용해서 변환 한 뒤 sentence 내 리스트 형태로 저장합니다. . 라벨 값 또한 labels 내 리스트 형태로 저장합니다. . data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, True, mode = &#39;train&#39;) data_valid = BERTDataset(dataset_valid, 0, 1, tok, max_len, True, True, mode = &#39;train&#39;) data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, True, mode = &#39;test&#39;) train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 0) valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size = batch_size, num_workers = 0) test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 0) . 앞서 정의한 BERTDataset 클래스를 이용해 문장을 트랜스포머로 변형하고, 로더에 실겠습니다. . data_train[0] . (array([ 2, 3088, 6117, 7086, 2658, 5439, 6708, 6080, 4059, 7245, 1442, 6965, 1423, 5939, 1678, 1504, 7096, 6081, 517, 46, 2822, 5712, 7098, 3954, 7227, 5940, 1459, 5439, 4841, 7724, 7828, 2298, 6493, 7178, 7098, 1907, 5804, 6903, 2064, 2720, 5211, 5468, 2948, 5573, 517, 5411, 6095, 5760, 913, 517, 54, 3, 3088, 6117, 7095, 3318, 5939, 1504, 7096, 7100, 517, 54, 3, 1, 1, 1, 1, 1, 1, 1], dtype=int32), array(63, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=int32), 1) . 이 데이터 셋은 문장을 트랜스포머를 통해 숫자로 바꾼 값, 전체 문장이 끝나는 위치, 두번째 문장을 1로 표기한 리스트, 라벨값으로 구성됩니다. . &#47784;&#45944; &#44396;&#52629;&#54616;&#44592; . class BERTClassifier(nn.Module): def __init__(self, bert, hidden_size = 768, num_classes = 3, dr_rate = None, params = None): super(BERTClassifier, self).__init__() self.bert = bert self.dr_rate = dr_rate self.classifier = nn.Linear(hidden_size, num_classes) if dr_rate: self.dropout = nn.Dropout(p = dr_rate) def gen_attention_mask(self, token_ids, valid_length): attention_mask = torch.zeros_like(token_ids) for i, v in enumerate(valid_length): attention_mask[i][:v] = 1 return attention_mask.float() def forward(self, token_ids, valid_length, segment_ids): attention_mask = self.gen_attention_mask(token_ids, valid_length) _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device)) if self.dr_rate: out = self.dropout(pooler) return self.classifier(out) . 파이토치 내 nn.Module 클래스를 상속받아 모델을 클래스로 구현했습니다. init 함수로 정의를 하고 forward 함수에서 실행을 합니다. . 특이한 점은 bert 모델에 입력값으로 input_ids(숫자로 변환된 문장), token_type_ids(두번째 문장을 1로 표기한 리스트), attention_mask(문장부분을 1로, 문장 아닌부분을 0으로 표기한 리스트) 세가지를 받습니다. . 현재 데이터 로더는 input_ids, token_type_ids는 bert 입력값과 같은형태이나, attention_mask가 아닌 문장이 끝나는 부분을 숫자로 알려줍니다. . 문장이 끝나는 부분 값을 attention_mask 형식으로 바꾸기 위해 gen_attention_mask 함수를 사용했습니다. . model = BERTClassifier(bertmodel, dr_rate = 0.5).to(device) # 가중치 감퇴를 위한 부분 (오버피팅 방지를 위해) no_decay = [&#39;bias&#39;, &#39;LayerNorm.weight&#39;] # 이 부분은 가중치 감퇴 x optimizer_group_parameters = [ {&#39;params&#39;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], &#39;weight_decay&#39; : 0.01}, {&#39;params&#39;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], &#39;weight_decay&#39; : 0.0} ] optimizer = AdamW(optimizer_group_parameters, lr = learning_rate) loss_fn = nn.CrossEntropyLoss() # warnup 값을 설정해줍니다. t_total = len(train_dataloader) * num_epochs warmup_step = int(t_total * warmup_ratio) # warnup 값보다 스텝이 작을경우 러닝레이트를 선형증가, 클경우 러닝레이트 임의에 방법으로 업데이트 합니다. scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total) . 모델을 정의하고, 자연어 분류문제에 자주 사용되는 AdamW 옵티마이저와 크로스엔트로피 손실함수를 정의합니다. . 오버피팅 방지를 위해 가중치를 규제하는 부분도 정의했으며, 러닝레이트 또한 스케줄러를 통해 유동적으로 조정해줍니다. . &#47784;&#45944; &#54617;&#49845;&#54616;&#44592; . def calc_accuracy(x,y): max_vals, max_indices = torch.max(x, 1) train_acc = (max_indices == y).sum().data.cpu().numpy() / max_indices.size()[0] return train_acc . 모델 성능 확인을 위해 정확도를 계산해주는 함수를 만들었습니다. . import enum for e in range(num_epochs): train_acc = 0.0 valid_acc = 0.0 model.train() for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total = len(train_dataloader)): optimizer.zero_grad() # 옵티마이저 파라미터 배치단위로 초기화 token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length = valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) loss = loss_fn(out, label) loss.backward() # Gradient Vanishing 또는 Exploding 방지 torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) optimizer.step() scheduler.step() # 러닝 레이트 업데이트 train_acc += calc_accuracy(out, label) print(&quot;epoch {} train acc {}&quot;.format(e+1, train_acc / (batch_id+1))) model.eval() for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(valid_dataloader), total = len(valid_dataloader)): token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length = valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) valid_acc += calc_accuracy(out, label) print(&quot;epoch {} valid acc {}&quot;.format(e+1, valid_acc / (batch_id+1))) . 100%|██████████| 313/313 [07:05&lt;00:00, 1.36s/it] . epoch 1 train acc 0.7162040734824281 . 100%|██████████| 79/79 [00:39&lt;00:00, 2.01it/s] . epoch 1 valid acc 0.7140690928270043 . 100%|██████████| 313/313 [07:06&lt;00:00, 1.36s/it] . epoch 2 train acc 0.8008186900958466 . 100%|██████████| 79/79 [00:39&lt;00:00, 2.01it/s] . epoch 2 valid acc 0.7144646624472574 . 100%|██████████| 313/313 [07:06&lt;00:00, 1.36s/it] . epoch 3 train acc 0.8582268370607029 . 100%|██████████| 79/79 [00:38&lt;00:00, 2.05it/s] . epoch 3 valid acc 0.7225738396624473 . 100%|██████████| 313/313 [06:59&lt;00:00, 1.34s/it] . epoch 4 train acc 0.8917731629392971 . 100%|██████████| 79/79 [00:38&lt;00:00, 2.04it/s] . epoch 4 valid acc 0.7298918776371308 . 100%|██████████| 313/313 [07:01&lt;00:00, 1.35s/it] . epoch 5 train acc 0.9011581469648562 . 100%|██████████| 79/79 [00:38&lt;00:00, 2.04it/s] . epoch 5 valid acc 0.7172336497890296 . . 데이터 로더에서 배치단위로 값을 꺼내서 모델에 적용하고, 가중치를 업데이트 시켜줬습니다. . 한 에포크가 끝날때마다 정확도를 지속적으로 확인하는 모습입니다. . &#47784;&#45944; &#51060;&#50857;&#54616;&#44592; . result = [] model.eval() with torch.no_grad(): for batch_id, (token_ids, valid_length, segment_ids) in tqdm(enumerate(test_dataloader), total = len(test_dataloader)): token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length = valid_length result.append(model(token_ids, valid_length, segment_ids)) . 100%|██████████| 27/27 [00:12&lt;00:00, 2.11it/s] . 모델을 평가모드로 변환하고 테스트 데이터를 모델에 넣어 3개 라벨의 예측 확률을 출력한 것을 result 안에 넣습니다. . result_ = [] for i in result: for j in i: result_.append(int(torch.argmax(j))) out = [list(label_dict.keys())[_] for _ in result_] sample_submission[&#39;label&#39;] = out sample_submission.to_csv(&#39;sentence_4.csv&#39;,index=False) . 우리가 최종적으로 필요한 것은 라벨 이름입니다. 지금 result에 있는 값은 3개 라벨의 각각의 확률값이죠. . argmax 함수를 사용해 가장 확률이 높은 값을 추출하고, 앞서 정의한 딕셔너리를 이용해 라벨로 변환하여 파일로 저장합니다. . &#45712;&#45184;&#51216; . 파이토치 사용법을 익히기 위해 이해하기 쉽고 좋은 코드들을 직접 하나하나 따라치는 중입니다. . 이번 코드는 딥러닝에 한 분야인 자연어 처리쪽으로, 제가 가장 관심있는 분야인데요. 이전 코드보단 확실히 난이도가 있는 것 같습니다. . 가중치에 오버피팅, 폭주, 소실을 막기 위해 더 다양한 기술들을 사용하는데 시간이 날때 보다 자세히 관찰하려합니다. . GPU에 성능 체감을 처음 하는데 이것때문에 현질을 한 사람의 마음이 바로 이해가 되네요. . max_grad_norm 값을 실수로 -1로 했을 뿐인데 모델 성능이 극악으로 안좋아졌습니다. 이것때문에 고생 많이 했는데, 긍정적으로 보면 max_grad_norm 값이 매우 중요하다는 걸 피부로 느꼈네요. . 다음엔 이미지 분류 문제를 공부할 것 같습니다. .",
            "url": "https://ksy1526.github.io/myblog/jupyter/deep%20learning/pytorch/dacon/natural%20language/bert/tokenizer/classifier/2022/02/21/torch3.html",
            "relUrl": "/jupyter/deep%20learning/pytorch/dacon/natural%20language/bert/tokenizer/classifier/2022/02/21/torch3.html",
            "date": " • Feb 21, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "[DACON] 파이토치 항공사 고객 만족도 데이터로 연습하기",
            "content": ". &#54596;&#50836;&#54620; &#54056;&#53412;&#51648;&#50752; &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . 데이터가 저장되어 있는 구글 드라이브와 연동합니다. . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import DataLoader, Dataset device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; import random random.seed(777) torch.manual_seed(777) if device == &#39;cuda&#39;: torch.cuda.manual_seed_all(777) . 파이토치 관련 모듈들을 임포트합니다. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/airport/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id Gender Customer Type Age Type of Travel Class Flight Distance Seat comfort Departure/Arrival time convenient Food and drink Gate location Inflight wifi service Inflight entertainment Online support Ease of Online booking On-board service Leg room service Baggage handling Checkin service Cleanliness Online boarding Departure Delay in Minutes Arrival Delay in Minutes target . 0 1 | Female | disloyal Customer | 22 | Business travel | Eco | 1599 | 3 | 0 | 3 | 3 | 4 | 3 | 4 | 4 | 5 | 4 | 4 | 4 | 5 | 4 | 0 | 0.0 | 0 | . 1 2 | Female | Loyal Customer | 37 | Business travel | Business | 2810 | 2 | 4 | 4 | 4 | 1 | 4 | 3 | 5 | 5 | 4 | 2 | 1 | 5 | 2 | 18 | 18.0 | 0 | . 2 3 | Male | Loyal Customer | 46 | Business travel | Business | 2622 | 1 | 1 | 1 | 1 | 4 | 5 | 5 | 4 | 4 | 4 | 4 | 5 | 4 | 3 | 0 | 0.0 | 1 | . 3 4 | Female | disloyal Customer | 24 | Business travel | Eco | 2348 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 2 | 4 | 5 | 3 | 4 | 3 | 10 | 2.0 | 0 | . 4 5 | Female | Loyal Customer | 58 | Business travel | Business | 105 | 3 | 3 | 3 | 3 | 4 | 4 | 5 | 4 | 4 | 4 | 4 | 4 | 4 | 5 | 0 | 0.0 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 데이터 분석에 기본적으로 필요한 패키지를 임포트하고, 데이터 csv 파일 또한 임포트 합니다. . X = train.drop(columns = [&#39;id&#39;, &#39;target&#39;], axis = 1) X.shape . (3000, 22) . id랑 target은 분석하는데 필요하지 않거나, 라벨 값이기 때문에 제거합니다. . &#45936;&#51060;&#53552; &#51333;&#47448;&#48324;&#47196; &#44033;&#44033; &#51204;&#52376;&#47532;&#54616;&#44592; . binary_obj_columns = [&#39;Gender&#39;, &#39;Customer Type&#39;, &#39;Type of Travel&#39;] numerical_columns = [&#39;Age&#39;, &#39;Departure Delay in Minutes&#39;, &#39;Arrival Delay in Minutes&#39;, &#39;Flight Distance&#39;] multical_obj_columns = list(set(X.columns) - set(binary_obj_columns) - set(numerical_columns)) . 데이터를 이진분류가 되는 데이터, 두 개를 넘는 항목이 존재하는 데이터, 연속형 데이터 세개로 나눠서 정리합니다. . 이진분류 데이터는 그대로 유지하고 다중분류 데이터는 라벨인코딩, 연속형 데이터는 4개의 항목으로 그룹핑 하겠습니다. . X_train_num = pd.DataFrame() X_test_num = pd.DataFrame() for col in binary_obj_columns : map_dict = {key : num for num,key in enumerate(train[col].unique())} X_train_num[col] = train[col].map(map_dict) X_test_num[col] = test[col].map(map_dict) X_train_group = pd.DataFrame() X_test_group = pd.DataFrame() for col in numerical_columns : data = train[col] _, bins = pd.qcut(data, 4, retbins=True, labels=False, duplicates=&#39;drop&#39;) X_train_group[col+&#39;_group&#39;] = train[col].apply(lambda x : sum([x &gt;= a for a in bins])) X_test_group[col+&#39;_group&#39;] = test[col].apply(lambda x : sum([x &gt;= a for a in bins])) for col in multical_obj_columns : map_dict = {key : num for num, key in enumerate(sorted(train[col].unique()))} X_train_group[col] = train[col].map(map_dict) X_test_group[col] = test[col].map(map_dict) num_cols = list(X_train_num.columns) cat_cols = list(X_train_group.columns) X_train = pd.concat([X_train_num, X_train_group], axis = 1) X_test = pd.concat([X_test_num, X_test_group], axis = 1) Y = train[&#39;target&#39;].values X_train.head() . Gender Customer Type Type of Travel Age_group Departure Delay in Minutes_group Arrival Delay in Minutes_group Flight Distance_group Leg room service Inflight entertainment Seat comfort On-board service Class Baggage handling Ease of Online booking Cleanliness Checkin service Online support Online boarding Gate location Food and drink Departure/Arrival time convenient Inflight wifi service . 0 0 | 0 | 0 | 1 | 1 | 1 | 2 | 4 | 3 | 3 | 4 | 1 | 3 | 4 | 4 | 3 | 3 | 4 | 2 | 3 | 0 | 4 | . 1 0 | 1 | 0 | 2 | 2 | 2 | 4 | 4 | 4 | 2 | 4 | 0 | 1 | 5 | 4 | 0 | 2 | 2 | 3 | 4 | 4 | 1 | . 2 1 | 1 | 0 | 3 | 1 | 1 | 4 | 4 | 5 | 1 | 3 | 0 | 3 | 4 | 3 | 4 | 4 | 3 | 0 | 1 | 1 | 4 | . 3 0 | 0 | 0 | 1 | 1 | 1 | 3 | 4 | 3 | 3 | 1 | 1 | 4 | 3 | 3 | 2 | 2 | 3 | 2 | 3 | 3 | 3 | . 4 0 | 1 | 0 | 4 | 1 | 1 | 1 | 4 | 4 | 3 | 3 | 0 | 3 | 4 | 3 | 3 | 4 | 5 | 2 | 3 | 3 | 4 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 앞서 말한 방식대로 데이터 종류별로 전처리를 진행했습니다. . 문자 형식 데이터의 경우 enumerate 함수를 통해 딕셔너리 형태로 바꾼 뒤 map 함수로 매핑시켜 라벨인코딩을 진행했습니다. . 연속형 데이터의 경우 판다스 내 qcut 함수를 이용해 항목 내 데이터를 4등분하고, apply 함수로 값을 넣어주었습니다. . &#45936;&#51060;&#53552; &#47196;&#45908; &#47564;&#46308;&#44592; . class CustomDataset(Dataset): def __init__(self, x, y, cat_cols, num_cols): # 데이터 셋 정의하는 곳 self.x_cat = x[cat_cols].copy().values.astype(np.int64) self.x_num = x[num_cols].copy().values.astype(np.float32) self.y = y.astype(np.float32) def __len__(self): # 길이 출력 return len(self.y) def __getitem__(self, idx): # 특정 1개 샘플 가져오는 곳 return self.x_cat[idx], self.x_num[idx], self.y[idx] . torch.utils.data 내 Dataset 클래스를 상속받아 나만의 데이터 셋 클래스를 만들었습니다. . len과 getitem 함수만 정의해준다면 클래스 형식은 자유로우며, 데이터 셋을 x/y로 분류해서 저장할 수 있습니다. . 이렇게 데이터 셋을 Dataset 클래스를 상속받아 정의한다면 torch.utils.data 내 DataLoader를 사용할 수 있는 것 또한 장점입니다. . from sklearn.model_selection import train_test_split def return_dataloaders(batch_size, random_state = 0): data_range = X_train_group.index train_idx, valid_idx = train_test_split(data_range, shuffle = True, stratify = Y, test_size = .5, random_state = random_state) X_tr, y_tr = X_train.iloc[train_idx], Y[train_idx] X_val, y_val = X_train.iloc[valid_idx], Y[valid_idx] train_ds = CustomDataset(X_tr, y_tr, cat_cols, num_cols) valid_ds = CustomDataset(X_val, y_val, cat_cols, num_cols) # 데이터 셋을 로더 내에 저장합니다. train_dl = DataLoader(train_ds, batch_size = batch_size) valid_dl = DataLoader(valid_ds, batch_size = batch_size) dataloaders = {} dataloaders[&#39;train&#39;] = train_dl dataloaders[&#39;valid&#39;] = valid_dl return dataloaders batch_size = 32 dataloaders = return_dataloaders(batch_size) . 앞서 정의한 나만의 데이터 셋 CustomDataset 클래스를 이용해 파이토치에서 지원하는 DataLoader를 정의했습니다. . &#47784;&#45944; &#51221;&#51032; . def weights_init(m): classname = m.__class__.__name__ if classname.find(&#39;BatchNorm&#39;) != -1: # 클레스의 이름이 BatchNorm 이라면 nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) print(&#39;hello world!&#39;) . 코드 작성자가 정의한 함수입니다. 가중치 초기값을 주는 함수인 것 같은데 조금 더 공부해야겠습니다. . class FC_Block(nn.Module): def __init__(self, inp_dim, out_dim): super(FC_Block, self).__init__() self.linear = nn.Linear(inp_dim, out_dim) self.batch = nn.BatchNorm1d(out_dim) def forward(self, x): x = self.linear(x) x = self.batch(x) return x . nn.Module 클래스를 상속받아 FC_Block 클래스를 만들었습니다. . 이 클래스는 inp, out 두 입력값을 받아 히든층을 구현한 뒤 배치 정규화를 진행합니다. 한번에 사용하면 편리하겠죠? . inp_oup_dims = [[x , x//2] for x in X_train[cat_cols].nunique()] # nunique() 함수는 데이터 프레임 내 항목 개수를 출력해줍니다. # 범주형 변수 항목 수, 항목 수의 절반으로 구성됩니다. class EMBNN(nn.Module): def __init__(self, inp_oup_dims, num_continuous): super(EMBNN, self).__init__() # nn.ModuleList : nn.Module 형태를 리스트로 저장하는 방식. self.embeddings = nn.ModuleList([ nn.Embedding(inp_dim + 1, out_dim) for inp_dim, out_dim, in inp_oup_dims ]) self.n_emb = sum(e.embedding_dim for e in self.embeddings) self.emb_drop = nn.Dropout(0.3) self.cont_norm = nn.BatchNorm1d(num_continuous) self.n_con = num_continuous self.FFC = nn.Sequential( FC_Block(self.n_emb + self.n_con, 32), nn.Dropout(0.2), FC_Block(32, 8), nn.Dropout(0.2), nn.Linear(8,1) ) def forward(self, x_cat, x_cont): # 범주형 자료들의 입력값을 각각 임베딩 합니다. x_cat = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)] # 임베딩 한 값(리스트 형태)를 가로 방향으로 합쳐줍니다. x_cat = torch.cat(x_cat, 1) # 드롭 아웃 진행합니다. x_cat = self.emb_drop(x_cat) # 연속형 자료들에 배치 정규화 진행합니다. x_cont = self.cont_norm(x_cont) # 범주형 자료와 연속형 자료를 합칩니다. x = torch.cat([x_cat, x_cont], 1) # 히든 층을 진행합니다. x = self.FFC(x) return F.sigmoid(x) . 범주형 변수들을 특정 층에 먼저 통과시켜 임베딩을 진행 한 뒤 연속형 변수들과 합쳐줍니다. 그 이후 히든층을 진행시킵니다. . 부분 부분마다 각주를 달아놓았습니다. . &#47784;&#45944; &#54617;&#49845; . def train_model(model, dataloader, optimizer, criterion, num_epoch, early_stop, model_path): best_val_loss = np.float(&#39;inf&#39;) early_stop_epoch = 0 for epoch in range(num_epoch): for phase in [&#39;train&#39;, &#39;valid&#39;]: # 트레인, 벨리드 2가지 모드로. if phase == &#39;train&#39;: model.train() elif phase == &#39;valid&#39;: model.eval() running_loss = 0 running_corr = 0 total = 0 for x_cat, x_num, y in dataloader[phase]: # 데이터들을 디바이스에 실음. x_cat = x_cat.to(device) x_num = x_num.to(device) y = y.to(device) optimizer.zero_grad() # 배치마다 옵티마이저 초기화. total += x_cat.size(0) with torch.set_grad_enabled(phase == &#39;train&#39;): # 트레인 모드 일때만 가중치 계산을 합니다. # 이부분이 조금 신기한게 모델(입력값)을 통해 forward 함수가 어떻게 실행되는지 모르겠습니다. output = model(x_cat, x_num) loss = criterion(output.squeeze(), y) # 트레인 모드일때 역전파 + 최적화 if phase == &#39;train&#39;: loss.backward() optimizer.step() running_loss += loss.item() # loss 내 item 함수를 사용해 로스 값 출력 running_corr += (output.round() == y.unsqueeze(1)).sum().item() epoch_loss = running_loss / total epoch_acc = running_corr / total if phase == &#39;valid&#39; and epoch_loss &lt; best_val_loss: best_val_loss = epoch_loss best_acc = epoch_acc torch.save(model.state_dict(), model_path) early_stop_epoch = 0 best_epoch = epoch elif phase == &#39;valid&#39;: early_stop_epoch += 1 if (early_stop_epoch &gt;= early_stop) or (epoch == num_epoch-1) : &quot;Early Stop Occured on epoch&quot; + str(epoch) print(f&#39;On Epoch {best_epoch}, Best Model Saved with Valid Loss {round(epoch_loss, 6)} and Acc {round(epoch_acc, 4)*100}%&#39;) break; model.load_state_dict(torch.load(model_path)) return model . 모델을 실제로 실행시켜주는 함수를 구현했습니다. 짚고 넘어갈 건 데이터와 모델 모두 디바이스에 실어줘야 한다는 점 입니다. . 상세한 코드 설명은 주석을 달았습니다. . def predict(model): with torch.no_grad(): # 평가하는 부분이기 때문에 가중치 업데이트 기능은 꺼둡니다. test_cat = torch.LongTensor(X_test[cat_cols].values) # int 형 데이터 test_num = torch.FloatTensor(X_test[num_cols].values) # 실수형 데이터 # squeeze는 1인 차원을 제거해줍니다. pred = model(test_cat, test_num).squeeze() return pred.cpu().detach().numpy() . 테스트 데이터를 평가하는 부분을 함수로 구현했습니다. . def return_pred_with_random_state(random_state = 0) : batch_size = 32 model = EMBNN(inp_oup_dims, len(num_cols)).to(device) model.apply(weights_init) dataloader = return_dataloaders(batch_size, random_state) optimizer = optim.Adam(model.parameters(), lr = 0.005) criterion = nn.BCELoss() trained_model = train_model(model, dataloader, optimizer, criterion, num_epoch = 300, early_stop = 10, model_path = &#39;EMBNN.pth&#39;) pred = predict(trained_model) return pred . 배치 사이즈를 정하고, 모델 정의하고, 배치 정규화시 초기 가중치를 넣어주고 데이터를 로더화 시켰습니다. . 다음으로 옵티마이저 아담을 정의하고, BCELoss 손실함수를 사용했습니다. 그 후 train_model 함수를 이용해 모델 학습을 시켰습니다. . 마지막으로 predict 함수를 사용해 테스트 데이터의 라벨값을 추출합니다. 작성자가 함수화 한 덕에 이해가 한번에 됩니다. . pred = return_pred_with_random_state(42) sample_submission[&#39;target&#39;] = pred.round() sample_submission.to_csv(&#39;airport.csv&#39;,index=False) . hello world! hello world! hello world! On Epoch 29, Best Model Saved with Valid Loss 0.007403 and Acc 90.53% . 방금 정의한 함수를 사용하기만 하면 결과물이 바로 나옵니다. . &#45712;&#45184;&#51216; . 파이토치를 연습하고자 그동안 튜토리얼 위주로 공부했는데, 실제 대회 데이터에 적용된 코드를 공부하니 느낌이 달랐습니다. . 이상적으로 잘 전처리 된 튜토리얼 내 데이터와 달리 딥러닝에 적용하기 위해 쓰는 전처리 과정에서 많이 공부 했습니다. . 실제 데이터 분석은 깨끗한 데이터로 하는 경우는 많이 없으니깐요. . 100% 이해하지 못한 부분도 일부 존재하는데, 이번이 끝이 아니고 계속 공부할 것이기 때문에 열심히 해보겠습니다. . 다음에는 자연어 처리 부분을 파이토치를 이용한 공부를 해보겠습니다. . 끝으로 코드 출처를 밝힙니다. 작성자분에게 감사드립니다. . https://dacon.io/competitions/official/235871/codeshare/4517?page=2&amp;dtype=recent .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/deep%20learning/pytorch/dacon/2022/02/19/torch2.html",
            "relUrl": "/ssuda/jupyter/deep%20learning/pytorch/dacon/2022/02/19/torch2.html",
            "date": " • Feb 19, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "[PyTorch] 파이토치 기초 연습하기",
            "content": ". &#53584;&#49436; . import torch import numpy as np data = [[1, 2], [3, 4]] x_data = torch.tensor(data) np_array = np.array(data) x_np = torch.from_numpy(np_array) print(x_data) print(x_np) . tensor([[1, 2], [3, 4]]) tensor([[1, 2], [3, 4]]) . 일반 리스트 데이터, 넘파이 데이터를 텐서로 만들 수 있습니다. . tensor = torch.rand(3, 4) print(f&quot;Shape of tensor: {tensor.shape}&quot;) print(f&quot;Datatype of tensor: {tensor.dtype}&quot;) print(f&quot;Device tensor is stored on: {tensor.device}&quot;) if torch.cuda.is_available(): tensor = tensor.to(&#39;cuda&#39;) print(f&quot;Device tensor is stored on: {tensor.device}&quot;) . Shape of tensor: torch.Size([3, 4]) Datatype of tensor: torch.float32 Device tensor is stored on: cpu Device tensor is stored on: cuda:0 . 텐서의 속성에는 모양, 자료형, 어느 장치에 저장되는지가 있습니다. . 사용할 수 있는 gpu가 있다면 사용이 되는 모습입니다. . tensor = torch.ones(4, 4) tensor[:,1] = 0 print(tensor) print(f&quot;tensor.matmul(tensor.T) n {tensor.matmul(tensor.T)} n&quot;) # 다른 문법: print(f&quot;tensor @ tensor.T n {tensor @ tensor.T}&quot;) . tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) tensor.matmul(tensor.T) tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) tensor @ tensor.T tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . 텐서는 넘파이와 같이 값을 변경해줄 수 있습니다. 또한 텐서간 @ 연산자를 사용하면 행렬 곱 연산이 가능합니다. . &#48516;&#47448;&#44592; &#54617;&#49845;&#54616;&#44592; . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44256; &#51221;&#44508;&#54868;&#54616;&#44592; . import torchvision import torchvision.transforms as transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] ) . 데이터를 불러왔을때 바로 전처리 할수 있는 툴을 transforms 클래스를 이용해 구현했습니다. . trainset = torchvision.datasets.CIFAR10(root = &#39;./data&#39;, train = True, download = True, transform = transform) testset = torchvision.datasets.CIFAR10(root = &#39;./data&#39;, train = False, download = True, transform = transform) . Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified . CIFAR10 데이터를 불러옵니다. 이때 앞서 구현한 transform을 이용해 -1 ~ 1 범위로 정규화한 텐서로 변환합니다. . batch_size = 4 trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers = 2) testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = True, num_workers = 2) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) . 데이터를 배치단위로 묶어서 로더로 만듭니다. . import matplotlib.pyplot as plt import numpy as np def imshow(img): img = img / 2 + 0.5 # -1 ~ 1 사이 값을 0 ~ 1 사이 값으로 변환 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() dataiter = iter(trainloader) images, labels = dataiter.next() imshow(torchvision.utils.make_grid(images)) print(&#39; &#39;.join(f&#39;{classes[labels[j]]:5s}&#39; for j in range(batch_size))) . horse truck truck horse . 학습용 이미지에는 무엇이 있는지 실제로 시각적으로 관찰하는 코드입니다. . 우선 먼저 봐야할께 iter과 next 함수입니다. iter은 iterable 한 객체(반복가능한)에 적용하는 함수로 iterator 객체로 변환시킵니다. . iterator 객체는 한번에 하나씩 객체 내 요소를 순서대로 엑세스가 가능합니다. 자료를 가져온 이후 폐기하기 때문에 메모리 절약이 가능합니다. . 그 뒤 next 함수를 통해 iterator 객체 값을 다음값으로 넘기고, 이전값을 반환하게 됩니다. . 다음으로 torchvision 내 utils.make_grid 함수를 이용해 이미지 배치 데이터(4차원 형식)를 입력받으면 실제 이미지를 출력합니다. . &#54633;&#49457;&#44273; &#49888;&#44221;&#47581; &#51221;&#51032;&#54616;&#44592; . import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) # 인풋 채널, 아웃풋 채널, 커널 사이즈 self.pool = nn.MaxPool2d(2, 2) # 커널, 스트라이드 값, 특징 맵 크기가 반이됨. self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) # 3 * 32 * 32 =&gt; 6 * 28 * 28 =&gt; 6 * 14 * 14 x = self.pool(F.relu(self.conv2(x))) # 6 * 14 * 14 =&gt; 16 * 10 * 10 =&gt; 16 * 5 * 5 x = torch.flatten(x, 1) # 채널 포함 1차원화시킴. x = F.relu(self.fc1(x)) # 16 * 5 * 5 =&gt; 120 x = F.relu(self.fc2(x)) # 120 =&gt; 84 x = self.fc3(x) # 84 =&gt; 10(10개로 분류하기 때문에 원핫 인코딩 꼴로 변환) return x net = Net() . nn.Module 클래스를 상속해 합성곱 신경망을 정의했습니다. . init 함수 부분에는 신경망 함수를 선언하고, forward 함수 부분에서 선언한 신경망 함수를 실행했습니다. . torch.nn.functional 에서는 relu 등 여러가지 활성화 함수들이 있습니다. . import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9) . 손실 함수로 크로스 엔트로피 함수를, 옵티마이저로 SGD를 사용했습니다. . 손실 함수는 nn 클래스 내 존재하고 옵티마이저는 torch.optim 클래스 내 존재합니다. . &#49888;&#44221;&#47581; &#54617;&#49845;&#54616;&#44592; . for epoch in range(2): running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data optimizer.zero_grad() # 매개변수를 0으로 만듭니다. outputs = net(inputs) # 입력값을 넣어 순전파를 진행시킨뒤 결과값 배출 loss = criterion(outputs, labels) # 결과와 실제 값을 손실함수에 대입 loss.backward() # 손실함수에서 역전파 수행 optimizer.step() # 옵티마이저를 사용해 매개변수 최적화 running_loss += loss.item() if i % 2000 == 1999: print(f&#39;[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}&#39;) running_loss = 0.0 . [1, 2000] loss: 2.222 [1, 4000] loss: 1.880 [1, 6000] loss: 1.696 [1, 8000] loss: 1.602 [1, 10000] loss: 1.537 [1, 12000] loss: 1.480 [2, 2000] loss: 1.409 [2, 4000] loss: 1.379 [2, 6000] loss: 1.358 [2, 8000] loss: 1.358 [2, 10000] loss: 1.308 [2, 12000] loss: 1.298 . 2 에포크로 트레인 데이터 로더를 사용하고, 앞서 정의한 모델, 옵티마이저, 손실함수를 사용합니다. . 신경망 학습 과정은 옵티마이저 초기화하기 =&gt; 순전파 진행으로 output 값 배출 =&gt; 손실함수 사용해서 loss값 배출 =&gt; 손실함수 역전파 수행 =&gt; 옵티마이저 사용 매개변수 최적화 과정으로 진행됩니다. . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552;&#47196; &#47784;&#45944; &#44160;&#51221;&#54616;&#44592; . dataiter = iter(testloader) images, labels = dataiter.next() imshow(torchvision.utils.make_grid(images)) print(&#39;GroundTruth: &#39;, &#39; &#39;.join(f&#39;{classes[labels[j]]:5s}&#39; for j in range(4))) . GroundTruth: ship plane deer ship . 테스트 데이터 로더 내 첫번째 배치 데이터를 사용해 시각적으로 검정하겠습니다. 실제 값을 출력한 모습이죠. . outputs = net(images) # 모델 내 인풋값을 넣으면 원핫인코딩 방식으로 출력됩니다. _, predicted = torch.max(outputs, 1) # torch.max 함수를 사용해 배치 내 데이터 당 최댓 값을 찾아줍니다. # 첫 출력은 최댓값 그 자체를, 두 번째 출력은 몇번 레이블인지 찾아줍니다. # 첫 출력은 관심대상이 아니므로 &#39;_&#39;를 사용하여 메모리를 절약합니다. print(&#39;Predicted: &#39;, &#39; &#39;.join(f&#39;{classes[predicted[j]]:5s}&#39; for j in range(4))) . Predicted: ship ship deer ship . 테스트 첫번째 배치 데이터를 모델에 넣어서 레이블을 예측했습니다. 2번째 plane 빼고 맞췄습니다! . correct = 0 total = 0 with torch.no_grad(): # 이 내부서에 생성된 텐서들은 requires_grad=False 상태가 되어 gradient 연산이 불가능해집니다. # 가중치 업데이트가 필요한 부분이 아니기 때문에 메모리 절약 차원입니다. for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(f&#39;Accuracy of the network on the 10000 test images: {100 * correct // total} %&#39;) . Accuracy of the network on the 10000 test images: 53 % . 전체 테스트 데이터 중 53% 정도를 맞췄습니다. 분류 레이블 개수가 10개인걸 감안하면 엄청 나쁜 수치는 아닙니다. . correct_pred = {classname: 0 for classname in classes} total_pred = {classname: 0 for classname in classes} # 변화도는 여전히 필요하지 않습니다 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predictions = torch.max(outputs, 1) # 각 분류별로 올바른 예측 수를 모읍니다 for label, prediction in zip(labels, predictions): if label == prediction: correct_pred[classes[label]] += 1 total_pred[classes[label]] += 1 # 각 분류별 정확도(accuracy)를 출력합니다 for classname, correct_count in correct_pred.items(): accuracy = 100 * float(correct_count) / total_pred[classname] print(f&#39;Accuracy for class: {classname:5s} is {accuracy:.1f} %&#39;) . Accuracy for class: plane is 67.6 % Accuracy for class: car is 56.8 % Accuracy for class: bird is 47.7 % Accuracy for class: cat is 33.9 % Accuracy for class: deer is 57.0 % Accuracy for class: dog is 33.1 % Accuracy for class: frog is 71.5 % Accuracy for class: horse is 50.5 % Accuracy for class: ship is 74.3 % Accuracy for class: truck is 42.5 % . 어느 클래스를 더 잘 분류하고, 어느 클래스는 잘 분류하지 못했는지 찾아봤습니다. . 다만 정확도의 한계로 단순히 한 클래스를 많이 예측한 경우도 있기 때문에 전적으로 신뢰할 결과는 아닙니다. . GPU&#50640;&#49436; &#54617;&#49845;&#54616;&#44592; . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) . cuda:0 . GPU를 사용하고 있군요. . net.to(device) inputs, labels = data[0].to(device), data[1].to(device) . 모델과 입력, 레이블 값들을 GPU로 보내면 정상적인 GPU 연산이 가능해집니다. . &#45712;&#45184;&#51216; . 파이토치라는 딥러닝 도구 사용법을 익히기 위해 쉬운 예제부터 시작했습니다. . 예제 자체는 무슨말인지 알고 있으나, 파이토치 내 어느 클래스에서 어떤 함수를 가져오는지를 중점적으로 학습했습니다. . 생각보다 어렵네요. 낯선 부분이 다소 있습니다. 하지만 하나하나 알아가는 기분이 좋네요. . 간단한 예제를 학습했는데, 다음엔 상대적으로 더 복잡한 다른 코드를 리뷰해보도록 하겠습니다. . 참고 : https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/deep%20learning/pytorch/2022/02/16/torch1.html",
            "relUrl": "/ssuda/jupyter/deep%20learning/pytorch/2022/02/16/torch1.html",
            "date": " • Feb 16, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "[SSUDA] GAN에 대해 가볍게 알아보기",
            "content": ". GAN (Generative Adversarial Network) &#49373;&#49328;&#51201; &#51201;&#45824; &#49888;&#44221;&#47581;&#51060;&#46976;? . GAN을 간단히 설명하면 저번에 공부했던 오토인코더의 뒷부분만 뗀것과 유사합니다. . 임의에 특성들로부터 이미지 등을 복구한 디코더를 얘기합니다. 다만 이 구조로는 원래 데이터를 모르기 때문에 학습이 불가능한데요. . . Discriminator(구별자)를 사용해서 학습을 진행하게 됩니다. . 자세히 설명하면 실제 데이터 내에서 뽑고, Generator(생성자)를 이용해서 유사 데이터를 뽑습니다. . 그 뒤 Discriminator를 이용해서 주어진 데이터가 실제 데이터인지 Generator를 이용해서 만든 가짜 데이터인지 구분합니다. . 이 과정을 반복해서 Generator가 생성하는 데이터를 실제 데이터와 유사하게 만들어서 Discriminator가 구분하지 못할 정도로 만드는 것이 목표입니다. . . 방금 말한 이론을 현실세계에 간단한 예시를 통해 관찰하겠습니다. 지폐위조범과 경찰의 관계입니다. . 지폐위조범이 가짜돈을 찍어냅니다 -&gt; 그럼 경찰이 이걸 보고 가짜인지 진짜인지 구별합니다. . 이때 처음 찍어낸 돈은 품질이 안좋아서 경찰이 쉽게 가짜를 구별합니다. . 하지만 지폐위조범은 점점 더 정교하게 가짜돈을 만들고, 경찰은 그걸 보고 계속 돈을 구분할 것입니다. . 이 상황에서 경찰은 최대한 가짜 돈을 잘 구분하려고 노력하겠죠. . 반복되면 경찰이 지폐위조범이 만든 돈을 구분 못하는 상황이 올겁니다. 가짜 돈을 잘 만드는 생성자를 만들어 낸 것입니다. . (참고자료 : https://lifeignite.tistory.com/53) . GAN&#45800;&#54620; &#49892;&#49845; . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers import matplotlib.pyplot as plt import numpy as np import time (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data() train_images.shape . (60000, 28, 28) . 텐서플로 패키지를 다운하고, 텐서플로우 내 mnist 데이터를 불러옵니다. . 테스트 데이터의 개수는 6만개이고 28 * 28 형태이며 값은 0~255 사이인 이미지 데이터 입니다. . train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(&#39;float32&#39;) train_images = (train_images - 127.5) / 127.5 # 값이 0~255에 존재하는데, -1~1 사이 데이터로 변환해줍니다. test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype(&#39;float32&#39;) test_images = (test_images - 127.5) / 127.5 . 입력된 데이터를 형변환하고, 값을 -1 ~ 1 내에 존재하도록 전처리합니다. . BUFFER_SIZE = 60000 BATCH_SIZE = 128 train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) . 텐서형태로 데이터를 변환해 넣어줍니다. 이때 데이터를 섞어주며, 배치사이즈로 분리합니다. . Generator(&#49373;&#49457;&#51088;), Discriminator(&#44396;&#48324;&#51088;) &#47784;&#45944; &#49373;&#49457; . inputs = keras.Input(shape=(100,)) x = inputs x = layers.Dense(256)(x) x = layers.LeakyReLU()(x) x = layers.Dense(28*28, activation = &#39;tanh&#39;)(x) outputs = layers.Reshape((28, 28))(x) G = keras.Model(inputs, outputs) G.summary() . Model: &#34;model_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_6 (InputLayer) [(None, 100)] 0 dense_10 (Dense) (None, 256) 25856 leaky_re_lu_5 (LeakyReLU) (None, 256) 0 dense_11 (Dense) (None, 784) 201488 reshape_3 (Reshape) (None, 28, 28) 0 ================================================================= Total params: 227,344 Trainable params: 227,344 Non-trainable params: 0 _________________________________________________________________ . 100차원의 값을 입력받아 이미지 데이터와 같은 형태인 28 * 28 형태로 출력하는 모델을 만들었습니다. . 더 자세히 살펴보면 100개의 입력에서 256개의 특성으로, 또 다음 층을 지나면 784개의 특성을 만들어내는 모델입니다. . 이 모델은 Generator(생성자) 모델로, 가짜 이미지를 만드는 모델입니다. . inputs = keras.Input(shape=(28, 28)) x = layers.Flatten()(inputs) x = layers.Dense(256)(x) x = layers.LeakyReLU()(x) x = layers.Dropout(0.3)(x) outputs = layers.Dense(1)(x) D = keras.Model(inputs, outputs) D.summary() . Model: &#34;model_6&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_7 (InputLayer) [(None, 28, 28)] 0 flatten_2 (Flatten) (None, 784) 0 dense_12 (Dense) (None, 256) 200960 leaky_re_lu_6 (LeakyReLU) (None, 256) 0 dropout_2 (Dropout) (None, 256) 0 dense_13 (Dense) (None, 1) 257 ================================================================= Total params: 201,217 Trainable params: 201,217 Non-trainable params: 0 _________________________________________________________________ . 28 * 28 형태의 이미지 데이터를 입력받아 256 차원으로 바꾼뒤, 최종 1개의 값을 내보내는 모델입니다. . 이미지를 입력받아 실제 이미지인지, 생성자에서 만든 이미지인지 구분하는 Discriminator(구별자) 입니다. . test_noise = tf.random.normal([1, 100]) fake_image_test = G(test_noise, training = False) plt.imshow(fake_image_test[0], cmap = &#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f44f725ee50&gt; . 임의에 값 100개를 사용하여 생성자 G 모델을 통과시켜서 28 * 28 이미지를 만들었습니다. . 랜덤 값으로 만든 이미지이기 때문에 아무런 특이점이 없는 형태이죠. . decision = D(fake_image_test, training = False) print(decision) . tf.Tensor([[0.4609413]], shape=(1, 1), dtype=float32) . 방금 만든 이미지 데이터를 가지고 실제 데이터가 맞는지 판단하는 구별자 D 모델을 통과시킨 모습입니다. . 아직 학습을 안시켰는데 우연인건지 0에 가까운 정확한 결과를 출력시켰네요. . &#47784;&#45944; &#54984;&#47144; . EPOCHS = 50 noise_dim = 100 seed = tf.random.normal([BATCH_SIZE, noise_dim]) G_optimizer = tf.keras.optimizers.Adam(1e-4) D_optimizer = tf.keras.optimizers.Adam(1e-4) . 에포크는 50으로, 시드값도 넣어주고 옵티마이저는 생성자 구별자 모두 Adam을 사용했습니다. . cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True) def G_loss(fake_output): return cross_entropy(tf.ones_like(fake_output), fake_output) def D_loss(real_output, fake_output): real_loss = cross_entropy(tf.ones_like(real_output), real_output) fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) total_loss = real_loss + fake_loss return total_loss . 크로스 엔트로피 방식으로 손실함수를 정의했습니다. 여기서 진짜 이미지를 의미하는 값은 1, 가짜 이미지를 의미하는 값은 0인데요. . 생성자는 최대한 생성한 이미지가 판별자에게 1값을 배출하는 이미지를 만들기 위해 파라미터를 업데이트 해야합니다. . 구별자는 최대한 진짜 이미지는 1, 생성자가 만든 이미지는 0값을 출력해주도록 파라미터를 업데이트 해야합니다. . 두 모델의 최종 목적이 상충되는 것이 있는데, 두 모델을 경쟁시키며 둘 다 우수한 모델이 되는 것이 목표입니다. . @tf.function # 데코레이션? def train_step(real_images): # 100개의 변수는 랜덤값을 계속 줌. 이건 중요한 값이 아닙니다. noises = tf.random.normal([BATCH_SIZE, noise_dim]) with tf.GradientTape() as gen_tape, tf.GradientTape() as dsc_tape: # 생성자로 가짜 이미지를 생성합니다. fake_images = G(noises, training = True) # 진짜 이미지를 구별자에 학습시킵니다. real_output = D(real_images, training = True) # 가짜 이미지를 구별자에 학습시킵니다. fake_output = D(fake_images, training = True) # 로스를 출력합니다. gen_loss = G_loss(fake_output) dsc_loss = D_loss(real_output, fake_output) # 로스 값을 통해 각 모델의 파라미터를 개산해줍니다. gen_gradients = gen_tape.gradient(gen_loss, G.trainable_variables) dsc_gradients = dsc_tape.gradient(dsc_loss, D.trainable_variables) # 옵티마이저를 이용해 파라미터를 업데이트 해줍니다. G_optimizer.apply_gradients(zip(gen_gradients, G.trainable_variables)) D_optimizer.apply_gradients(zip(dsc_gradients, D.trainable_variables)) . 배치 한 개의 값이 입력됬을때 수행하는 일을 모은 train_step 함수를 만듭니다. . gen_losses = [] dsc_losses = [] def test_step(real_images): noises = tf.random.normal([BATCH_SIZE, noise_dim]) fake_images = G(noises, training = False) real_output = D(real_images, training = False) fake_output = D(fake_images, training = False) gen_loss = G_loss(fake_output) gen_losses.append(gen_loss) dsc_loss = D_loss(real_output, fake_output) dsc_losses.append(dsc_loss) print(&#39;Generator loss:&#39;, gen_loss.numpy(), &#39;, Discriminator loss:&#39;, dsc_loss.numpy()) . 테스트 하는 부분도 비슷하게 test_step 함수를 만듭니다. . def train(dataset, epochs): for epoch in range(epochs): start = time.time() for i, image_batch in enumerate(dataset): train_step(image_batch) if i == 0: test_step(image_batch) print (&#39;Time for epoch {} is {} sec&#39;.format(epoch + 1, time.time()-start)) . 앞서 정의한 train_step과 test_step 함수를 작동시키는 train 함수를 만듭니다. . train(train_dataset, EPOCHS) . Generator loss: 0.71649 , Discriminator loss: 1.5534838 Time for epoch 1 is 7.659214973449707 sec Generator loss: 1.4269571 , Discriminator loss: 0.34772688 Time for epoch 2 is 6.5700695514678955 sec Generator loss: 1.5954232 , Discriminator loss: 0.30279088 Time for epoch 3 is 8.152190446853638 sec Generator loss: 1.36361 , Discriminator loss: 0.4483151 Time for epoch 4 is 6.410968065261841 sec Generator loss: 1.165875 , Discriminator loss: 0.6234964 Time for epoch 5 is 6.4502575397491455 sec Generator loss: 1.0780177 , Discriminator loss: 0.6841619 Time for epoch 6 is 6.431274652481079 sec Generator loss: 1.3505816 , Discriminator loss: 0.5721281 Time for epoch 7 is 6.4662909507751465 sec Generator loss: 1.2779344 , Discriminator loss: 0.62072957 Time for epoch 8 is 6.369911193847656 sec Generator loss: 1.5677459 , Discriminator loss: 0.4794592 Time for epoch 9 is 6.448152780532837 sec Generator loss: 1.8098621 , Discriminator loss: 0.37730467 Time for epoch 10 is 6.668363332748413 sec Generator loss: 1.7685666 , Discriminator loss: 0.46769577 Time for epoch 11 is 9.194777011871338 sec Generator loss: 1.5252336 , Discriminator loss: 0.56575656 Time for epoch 12 is 7.199241399765015 sec Generator loss: 1.7163453 , Discriminator loss: 0.44394356 Time for epoch 13 is 6.342915296554565 sec Generator loss: 1.5267828 , Discriminator loss: 0.4659177 Time for epoch 14 is 6.351556777954102 sec Generator loss: 1.8910414 , Discriminator loss: 0.46936297 Time for epoch 15 is 6.366145372390747 sec Generator loss: 1.5646715 , Discriminator loss: 0.50378096 Time for epoch 16 is 6.389109134674072 sec Generator loss: 1.7325975 , Discriminator loss: 0.5011499 Time for epoch 17 is 6.3877105712890625 sec Generator loss: 1.6206157 , Discriminator loss: 0.53547317 Time for epoch 18 is 6.335543632507324 sec Generator loss: 1.4234588 , Discriminator loss: 0.6910653 Time for epoch 19 is 6.378859043121338 sec Generator loss: 1.5831845 , Discriminator loss: 0.62829065 Time for epoch 20 is 6.430606842041016 sec Generator loss: 1.8133811 , Discriminator loss: 0.4704691 Time for epoch 21 is 6.33375358581543 sec Generator loss: 1.5101905 , Discriminator loss: 0.6232215 Time for epoch 22 is 6.405717372894287 sec Generator loss: 1.4687159 , Discriminator loss: 0.6265195 Time for epoch 23 is 6.3467371463775635 sec Generator loss: 1.723053 , Discriminator loss: 0.49364492 Time for epoch 24 is 6.364705801010132 sec Generator loss: 1.610209 , Discriminator loss: 0.63721573 Time for epoch 25 is 6.332318067550659 sec Generator loss: 1.7356936 , Discriminator loss: 0.52721524 Time for epoch 26 is 6.364203453063965 sec Generator loss: 1.9121637 , Discriminator loss: 0.5568584 Time for epoch 27 is 6.319864273071289 sec Generator loss: 1.6766417 , Discriminator loss: 0.5997259 Time for epoch 28 is 6.387562990188599 sec Generator loss: 1.75248 , Discriminator loss: 0.62925696 Time for epoch 29 is 6.367716312408447 sec Generator loss: 1.7534504 , Discriminator loss: 0.5443964 Time for epoch 30 is 6.331058979034424 sec Generator loss: 1.5471501 , Discriminator loss: 0.6470097 Time for epoch 31 is 6.286182641983032 sec Generator loss: 1.5621777 , Discriminator loss: 0.6091302 Time for epoch 32 is 6.333756446838379 sec Generator loss: 1.6764431 , Discriminator loss: 0.6616212 Time for epoch 33 is 6.318872690200806 sec Generator loss: 1.4045196 , Discriminator loss: 0.7134573 Time for epoch 34 is 6.299607038497925 sec Generator loss: 1.4713526 , Discriminator loss: 0.7262584 Time for epoch 35 is 6.3872082233428955 sec Generator loss: 1.3149565 , Discriminator loss: 0.8259001 Time for epoch 36 is 6.349704265594482 sec Generator loss: 1.110297 , Discriminator loss: 0.9021453 Time for epoch 37 is 9.245652437210083 sec Generator loss: 1.3754975 , Discriminator loss: 0.82389057 Time for epoch 38 is 6.385930061340332 sec Generator loss: 1.1597505 , Discriminator loss: 0.8702109 Time for epoch 39 is 6.327707529067993 sec Generator loss: 1.2481728 , Discriminator loss: 0.8911562 Time for epoch 40 is 6.268001079559326 sec Generator loss: 1.3623071 , Discriminator loss: 0.8037815 Time for epoch 41 is 6.32326602935791 sec Generator loss: 1.0813665 , Discriminator loss: 0.92148185 Time for epoch 42 is 6.305378437042236 sec Generator loss: 1.1077981 , Discriminator loss: 0.82926035 Time for epoch 43 is 6.32554030418396 sec Generator loss: 1.3075023 , Discriminator loss: 0.87365437 Time for epoch 44 is 6.315555810928345 sec Generator loss: 1.1911769 , Discriminator loss: 0.8892145 Time for epoch 45 is 6.272730350494385 sec Generator loss: 1.1509132 , Discriminator loss: 0.94256556 Time for epoch 46 is 6.288074493408203 sec Generator loss: 1.207123 , Discriminator loss: 0.82477653 Time for epoch 47 is 6.279074430465698 sec Generator loss: 1.1665838 , Discriminator loss: 0.8166743 Time for epoch 48 is 6.324838876724243 sec Generator loss: 1.5069995 , Discriminator loss: 0.73425233 Time for epoch 49 is 6.327970266342163 sec Generator loss: 1.2253973 , Discriminator loss: 0.951316 Time for epoch 50 is 6.335601329803467 sec . &#44208;&#44284; &#44288;&#52272; . epo = range(1,EPOCHS + 1) plt.plot(epo, gen_losses) plt.plot(epo, dsc_losses) plt.show() . 두 로스 값이 에포크마다 어떻게 변하는지 관찰해봤습니다. 파란색이 생성자 로스, 노란색이 구별자 로스입니다. . 다른 딥러닝 로스 값은 에포크가 진행될 때 마다 계속 감소하는 모양을 띄거나 과적합이 되면 더 이상 감소되지 않는 모양을 보입니다. . 반면 이 두 모델의 로스 값은 다소 특이하죠. 로스 값이 증가와 감소를 계속 반복합니다. . 생성자가 계속해서 구별자를 햇갈리계 하면서 빈틈을 노리고, 구별자는 진화하는 생성자에 대항하기 위해 업데이트 되는 재밌는 현상이네요. . noises = tf.random.normal([50, 100]) generated_image = G(noises, training = False) fig, axes = plt.subplots(nrows = 3, ncols = 2, figsize = (10, 10)) for ax in axes.flat: ax.axis(&#39;off&#39;) axes[0,0].imshow(generated_image[0], cmap = &#39;gray&#39;) axes[0,1].imshow(generated_image[1], cmap = &#39;gray&#39;) axes[1,0].imshow(generated_image[2], cmap = &#39;gray&#39;) axes[1,1].imshow(generated_image[3], cmap = &#39;gray&#39;) axes[2,0].imshow(generated_image[4], cmap = &#39;gray&#39;) axes[2,1].imshow(generated_image[5], cmap = &#39;gray&#39;) plt.show() . 이미지가 원하는 만큼까진 아니지만 어느정도 MNIST 그림과 유사하려고 노력한 것 같습니다. . 심플한 GAN모델을 사용했기 때문이며, 더 깊은 층을 쌓거나 비지도 학습에서 벗어나 label 정보를 더 준다면(숫자 값) 더 정교한 가짜 이미지도 만들 수 있을 것입니다. . 출처 : https://dataplay.tistory.com/39 . &#45712;&#45184;&#51216; . 딥러닝의 개넘, 구조는 많이 학습했지만 텐서플로/파이토치 같은 도구들을 활용하는데 아직 어색합니다. . 때문에 GAN이라는 더 응용된 분야를 마음껏 학습하는데 막히는 것이 많았습니다. 오늘 공부한 내용은 GAN이 무엇인지, 간단한 실습 정도 해봤는데요. . 개인적으로 아쉽습니다. 더 디테일하게 공부하지 못해서. 빨리 텐서플로/파이토치와 친해져야겠다는 생각이 들었습니다. . 오늘 느낀것은 딥러닝구조를 잘 이해만 한다면 이런식의 응용이 또 있을 수 있겠다는 생각이 들었습니다. . 딥러닝 기술을 어떻게 응용할지 어느 산업에서 일을 하던 끊임없이 생각하도록 노력해야겠습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/tensorflow/gan/2022/02/15/handssu6.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/tensorflow/gan/2022/02/15/handssu6.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "[SSUDA] 간단한 오토인코더 실습해보기",
            "content": ". 지금까지 공부 했던 지도 학습 문제를 해결하기 위한 모델들은 당연히 레이블 정보가 필요했습니다. . 목표로 하는 것은 레이블 정보 없이도 유용한 표현을 학습하는 것인데요. 만약 선형 활성화 함수만 사용하고 비용 함수가 MSE라면 PCA와 동일합니다. . 이를 조금 응용한 것을 오토인코더라고 할 수 있습니다. 입력을 차원이 줄어든 압축된 표현으로 나타내는 층(부호화층)과 압축된 표현을 다시 원래의 차원을 가진 최초 입력 데이터로 복원하는 층(복호화층)으로 구성됩니다. . 이 복호화층에서 입력을 재구성하는데 유용한 저차원 표현이 학습됩니다. . import numpy as np import pandas as pd import torch from torch import Tensor import torch.optim as optim from torch.optim import lr_scheduler from torch.optim import Optimizer import torch.nn as nn import torch.nn.functional as F from torch.nn.modules.loss import _Loss import matplotlib.pyplot as plt import matplotlib import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) . &#52789; &#44396;&#54788;&#54616;&#44592; . class DenseLayer(nn.Module): def __init__(self, input_size: int, neurons: int, dropout: float = 1.0, activation: nn.Module = None) -&gt; None: super().__init__() self.linear = nn.Linear(input_size, neurons) self.activation = activation if dropout &lt; 1.0: self.dropout = nn.Dropout(1 - dropout) def forward(self, x: Tensor) -&gt; Tensor: # 모든 파이토치 연산은 nn.Module를 상속하므로 역전파 연산를 자동으로 처리합니다. x = self.linear(x) # 가중치를 곱하고 편향을 더함 if self.activation: x = self.activation(x) if hasattr(self, &#39;dropout&#39;): x = self.dropout(x) return x . 파이토치를 이용해 히든층을 구현했습니다. 역시 파이토치를 쓰니 식이 훨씬 간편해졌습니다. . 특히 역전파 연산을 자동으로 해주기 때문에 forward 함수만 잘 구현하면 되겠습니다. . 이전에 제가 공부했던 방식과 유사하게 구현하기 위해 이런식의 코드를 썼으며, 실제 파이토치 사용시 더 간편하다고 합니다. . 다음 글은 파이토치 사용법을 학습해볼까 해요. . class ConvLayer(nn.Module): def __init__(self, in_channels : int, out_channels : int, filter_size: int, activation = None, dropout: float = 1.0, flatten : bool = False) -&gt; None: super().__init__() self.conv = nn.Conv2d(in_channels, out_channels, filter_size, padding = filter_size // 2) self.activation = activation self.flatten = flatten if dropout &lt; 1.0: self.dropout = nn.Dropout(1 - dropout) def forward(self, x: Tensor) -&gt; Tensor: x = self.conv(x) # 합성곱 연산 수행 if self.activation: # 활성화 함수 적용 x = self.activation(x) if self.flatten: # 1차원으로 펴주는 경우 x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]) if hasattr(self, &#39;dropout&#39;): # 드롭아웃이 있는 경우 x = self.dropout(x) return x . 히든층과 비슷한 구조인 합성곱 층입니다. nn.Conv2d 함수를 실제로 이용하여 사용합니다. . &#51064;&#53076;&#45908;, &#46356;&#53076;&#45908; &#44396;&#54788;&#54616;&#44592; . class Encoder(nn.Module): def __init__(self, hidden_dim: int = 28): super(Encoder, self).__init__() self.conv1 = ConvLayer(1, 14, 5, activation = nn.Tanh()) self.conv2 = ConvLayer(14, 7, 5, activation = nn.Tanh(), flatten = True) self.dense1 = DenseLayer(7 * 28 * 28, hidden_dim, activation = nn.Tanh()) def forward(self, x: Tensor) -&gt; Tensor: x = self.conv1(x) x = self.conv2(x) x = self.dense1(x) return x . 인코더 역할을 하는 인코더 클래스 입니다. . 입력을 1채널에서 14채널로 변환하는 합성곱층, 14채널을 다시 7채널(각 체널은 28*28 뉴런)로 변환한 뒤 데이터를 1차원으로 펼칩니다. . 그 후 히든층에 값을 넣어 28개의 특성을 최종 배출하게 되며 모든 층에서 하이퍼탄전트 함수를 활성화함수로 사용합니다. . class Decoder(nn.Module): def __init__(self, hidden_dim: int = 28): super(Decoder, self).__init__() self.dense1 = DenseLayer(hidden_dim, 7 * 28 * 28, activation = nn.Tanh()) self.conv1 = ConvLayer(7, 14, 5, activation = nn.Tanh()) self.conv2 = ConvLayer(14, 1, 5, activation = nn.Tanh()) def forward(self, x: Tensor) -&gt; Tensor: x = self.dense1(x) x = x.view(-1, 7, 28, 28) # -1은 알맞은 값을 계산해서 대입하라 x = self.conv1(x) x = self.conv2(x) return x . 디코더 역할을 하는 디코더 클래스입니다. 구성을 보시면 알겠지만, 인코더와 반대로 대칭되는 구조입니다. . 밀집층에 28개의 특성을 입력받아 7 28 28 개의 특성을 출력합니다. 그 후 7채널과 2차원 구조를 만들어 줍니다. . 그 후 2번의 합성곱 층을 거치는데 채널을 14개로 늘려주었다가 1채널로 다시 줄여준 것을 출력합니다. . 결국 인코더의 입력값과 디코더의 출력값은 같은 형태를 유지하게 됩니다. . class Autoencoder(nn.Module): def __init__(self, hidden_dim: int = 28): super(Autoencoder, self).__init__() self.encoder = Encoder(hidden_dim) self.decoder = Decoder(hidden_dim) def forward(self, x: Tensor) -&gt; Tensor: encoding = self.encoder(x) x = self.decoder(encoding) return x, encoding . 앞서 구현한 인코더와 디코더를 같이 실행시키는 클래스를 만들었습니다. . &#53944;&#47112;&#51060;&#45320; &#44396;&#54788;&#54616;&#44592; . from typing import Optional, Tuple def permute_data(X: Tensor, y: Tensor): perm = torch.randperm(X.shape[0]) # 데이터 셔플 return X[perm], y[perm] class PyTorchTrainer(object): def __init__(self, model, optim, criterion): self.model = model self.optim = optim self.loss = criterion def _generate_batches(self, x: Tensor, y: Tensor, size: int = 32): N = x.shape[0] for ii in range(0, N, size): x_batch, y_batch = x[ii:ii+size], y[ii:ii+size] yield x_batch, y_batch # 제너레이터 관련 def fit(self, x_train, y_train, x_test, y_test, epochs: int = 100, eval_every: int = 10, batch_size: int = 32): for e in range(epochs): x_train, y_train = permute_data(x_train, y_train) # 배치 크기별로 데이터 분리함. batch_generator = self._generate_batches(x_train, y_train, batch_size) for ii, (x_batch, y_batch) in enumerate(batch_generator): self.optim.zero_grad() # 매개변수 초기화 output = self.model(x_batch)[0] # 배치값 모델에 대입 loss = self.loss(output, y_batch) # 로스값 출력 loss.backward() # 역전파 계산 수행. self.optim.step() # 매개변수 갱신 # 한 에포크 끝난 뒤 결과 출력. output = self.model(x_test)[0] loss = self.loss(output, y_test) print(e, loss) . 트레이너 또한 파이토치 클래스를 상속받아 직접 구현했습니다. 이전에 트레이너를 밑바닥부터 구현했기 때문에 어렵지는 않았습니다. . permute_data 함수는 데이터 순서를 섞어주는 역할을 하고, _generate_batches 함수는 배치 크기로 데이터를 분리합니다. . 이때 파이썬에서 for문 내 yield 은 제너레이터를 사용한다고 하는데 정확히는 모르겠지만 메모리와 속도 차원에서 유용한 방식이다로 이해했습니다. . 배치별로 zero_grad 함수를 시작 전에 수행하여 매개변수를 초기화시켜줘야 한다고 합니다. . 딥러닝 모델에 값 대입하고, 로스 값 출력하고 역전파 계산을 통해 파라미터 업데이트를 진행하여 더 좋은 모델을 만들어 갑니다. . &#44036;&#45800;&#54620; &#49892;&#49845; &#54644;&#48372;&#44592; . import torchvision from torchvision.datasets import MNIST import torchvision.transforms as transforms img_transforms = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1305,), (0.3081,)) ]) train_dataset = MNIST(root=&#39;../mnist_data/&#39;, train=True, download=True, transform=img_transforms) test_dataset = MNIST(root=&#39;../mnist_data/&#39;, train=False, download=True, transform=img_transforms) mnist_train = ((train_dataset.data.type(torch.float32).unsqueeze(3).permute(0, 3, 1, 2) / 255.0) - 0.1305) / 0.3081 mnist_test = ((test_dataset.data.type(torch.float32).unsqueeze(3).permute(0, 3, 1, 2) / 255.0) - 0.1305) / 0.3081 X_train = mnist_train X_test = mnist_test # 모든 데이터를 -1 ~ 1 사이로 변환 X_train_auto = (X_train - X_train.min()) / (X_train.max() - X_train.min()) * 2 - 1 X_test_auto = (X_test - X_train.min()) / (X_train.max() - X_train.min()) * 2 - 1 . Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw/train-images-idx3-ubyte.gz Extracting ../mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz Extracting ../mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ../mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ../mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw . 실습 데이터로 유명한 MNIST 데이터를 불러와서 전처리를 수행했습니다. . model = Autoencoder(hidden_dim = 28) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9) trainer = PyTorchTrainer(model, optimizer, criterion) trainer.fit(X_train_auto, X_train_auto, X_test_auto, X_test_auto, epochs = 1, batch_size = 60) . 0 tensor(0.0679, grad_fn=&lt;MseLossBackward0&gt;) . 오토인코더의 여러가지 응용이 있겠지만, 입력값을 그대로 복원하는 방식을 진행하겠습니다. . 그렇게 하려면 입력 데이터와 타겟 데이터를 같게 넣으면 되겠죠. 수행 결과 실제 로스값도 상당히 낮은 수치를 보입니다. . 실제로 오토인코더는 비지도 학습을 수행한다고도 생각할 수 있고, 원본 데이터를 압축시키는 개념으로도 적용할 수 있겠습니다. . reconstructed_images, image_representations = model(X_test_auto) def display_image(ax, t: Tensor): n = t.detach().numpy() ax.imshow(n.reshape(28, 28)) a = np.random.randint(0, 10000) f, axarr = plt.subplots(1,2) display_image(axarr[0], X_test[a]) display_image(axarr[1], reconstructed_images[a]) axarr[0].set_title(&quot;Originally&quot;) axarr[1].set_title(&quot;AutoEncoder&quot;) axarr[0].axis(&#39;off&#39;) axarr[1].axis(&#39;off&#39;) . (-0.5, 27.5, 27.5, -0.5) . 원본 그림과 꽤 비슷한 그림이 유지됩니다! 인코더 후 28개의 특징이 중요한 값을 잘 기억을 한 모양이죠. . t-SNE&#47484; &#51060;&#50857;&#54620; &#49884;&#44033;&#54868; . from sklearn.manifold import TSNE tsne_result = TSNE(n_components=2, random_state=20190405).fit_transform(image_representations.detach().numpy()) . t-SNE 기술을 이용해 2차원으로 차원을 축소해보겠습니다. . 더 자세히 얘기하면 오토인코더로 28개의 특징으로 원본이미지를 압축한 뒤 그 결과에 다시 t-SNE를 적용해 2차원으로 특징을 축소합니다. . tsne_df = pd.DataFrame({&#39;tsne_dim_1&#39;: tsne_result[:,0], &#39;tsne_dim_2&#39;: tsne_result[:,1], &#39;category&#39;: test_dataset.targets}) groups = tsne_df.groupby(&#39;category&#39;) # Plot fig, ax = plt.subplots(figsize=(25,25)) ax.margins(0.05) # 자동 스케일링을 위한 5% 패딩 추가 for name, group in groups: ax.scatter(group[&#39;tsne_dim_1&#39;], group[&#39;tsne_dim_2&#39;], marker=&#39;o&#39;, label=name) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f111f422e10&gt; . 2차원으로 축소하게 되면 위의 그림과 같이 이미지를 2차원 그래프에 시각화가 가능해집니다. . 그림에 있는 색깔은 실제 숫자 값 레이블에 따라 다르게 색칠했습니다. 이 레이블은 오토인코더 모델 학습 시 적용하지 않았었죠. . 압축된 2개의 특징으로도 색깔 별로 꽤 잘 구분하는 모습이니 28개 특징으로는 레이블을 더 잘 구분하겠죠. . 또 다른 의의는 레이블 없이 학습을 했는데도 레이블을 꽤 잘 구분한다는 점입니다. PCA를 딥러닝 버전으로 한 것 같네요. . &#45712;&#45184;&#51216; . 지나가는 말로 오토인코더를 들어봤는데 직접 학습하니 남들에게 오토인코더가 뭔지 자신있게 말할 정도로는 학습한 것 같습니다. . 비지도 학습 분야에서도 딥러닝이 잘 활용되는걸 관찰하니 신기하네요. 아직 맛보기만 했지만. . 딥러닝에 대한 이론적인 이해가 꽤 진행된거 같습니다. 이제 그 도구인 파이토치, 텐서플로를 다루는 법을 공부하는게 좋겠군요. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/pytorch/autoencoder/2022/02/11/handssu5.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/pytorch/autoencoder/2022/02/11/handssu5.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "[DACON] 항공사 고객 만족도 예측 예측 경진대회",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/airport/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id Gender Customer Type Age Type of Travel Class Flight Distance Seat comfort Departure/Arrival time convenient Food and drink Gate location Inflight wifi service Inflight entertainment Online support Ease of Online booking On-board service Leg room service Baggage handling Checkin service Cleanliness Online boarding Departure Delay in Minutes Arrival Delay in Minutes target . 0 1 | Female | disloyal Customer | 22 | Business travel | Eco | 1599 | 3 | 0 | 3 | 3 | 4 | 3 | 4 | 4 | 5 | 4 | 4 | 4 | 5 | 4 | 0 | 0.0 | 0 | . 1 2 | Female | Loyal Customer | 37 | Business travel | Business | 2810 | 2 | 4 | 4 | 4 | 1 | 4 | 3 | 5 | 5 | 4 | 2 | 1 | 5 | 2 | 18 | 18.0 | 0 | . 2 3 | Male | Loyal Customer | 46 | Business travel | Business | 2622 | 1 | 1 | 1 | 1 | 4 | 5 | 5 | 4 | 4 | 4 | 4 | 5 | 4 | 3 | 0 | 0.0 | 1 | . 3 4 | Female | disloyal Customer | 24 | Business travel | Eco | 2348 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 2 | 4 | 5 | 3 | 4 | 3 | 10 | 2.0 | 0 | . 4 5 | Female | Loyal Customer | 58 | Business travel | Business | 105 | 3 | 3 | 3 | 3 | 4 | 4 | 5 | 4 | 4 | 4 | 4 | 4 | 4 | 5 | 0 | 0.0 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 파일이 저장된 위치를 path로 지정하여 불러왔습니다. 이 코드를 사용하신다면 path 값을 파일 저장 위치로 지정하시면 잘 작동됩니다. . 데이터를 간단히 살펴보면 만족여부를 판단하는 분류문제이며, 범주형 변수가 상당히 많은 것을 알 수 있습니다. . print(train.shape) print(test.shape) . (3000, 24) (2000, 23) . 트레인 데이터는 3천개, 테스트 데이터는 2천개이며 특성 개수는 총 23개입니다. . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3000 entries, 0 to 2999 Data columns (total 24 columns): # Column Non-Null Count Dtype -- -- 0 id 3000 non-null int64 1 Gender 3000 non-null object 2 Customer Type 3000 non-null object 3 Age 3000 non-null int64 4 Type of Travel 3000 non-null object 5 Class 3000 non-null object 6 Flight Distance 3000 non-null int64 7 Seat comfort 3000 non-null int64 8 Departure/Arrival time convenient 3000 non-null int64 9 Food and drink 3000 non-null int64 10 Gate location 3000 non-null int64 11 Inflight wifi service 3000 non-null int64 12 Inflight entertainment 3000 non-null int64 13 Online support 3000 non-null int64 14 Ease of Online booking 3000 non-null int64 15 On-board service 3000 non-null int64 16 Leg room service 3000 non-null int64 17 Baggage handling 3000 non-null int64 18 Checkin service 3000 non-null int64 19 Cleanliness 3000 non-null int64 20 Online boarding 3000 non-null int64 21 Departure Delay in Minutes 3000 non-null int64 22 Arrival Delay in Minutes 3000 non-null float64 23 target 3000 non-null int64 dtypes: float64(1), int64(19), object(4) memory usage: 562.6+ KB . 결측치는 관찰되지 않습니다. . &#44036;&#45800;&#55176; &#48320;&#49688; &#44288;&#52272; . train.describe() . id Age Flight Distance Seat comfort Departure/Arrival time convenient Food and drink Gate location Inflight wifi service Inflight entertainment Online support Ease of Online booking On-board service Leg room service Baggage handling Checkin service Cleanliness Online boarding Departure Delay in Minutes Arrival Delay in Minutes target . count 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.00000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | 3000.000000 | . mean 1500.500000 | 39.203000 | 1983.079333 | 2.863333 | 3.009667 | 2.874333 | 3.016667 | 3.259667 | 3.352333 | 3.50500 | 3.488000 | 3.497000 | 3.485000 | 3.728667 | 3.370000 | 3.728667 | 3.356333 | 15.634333 | 15.922000 | 0.556000 | . std 866.169729 | 15.108802 | 1028.109117 | 1.394981 | 1.519543 | 1.431511 | 1.294713 | 1.322683 | 1.352826 | 1.31068 | 1.302211 | 1.283436 | 1.294218 | 1.154190 | 1.258158 | 1.161678 | 1.294057 | 45.083228 | 45.203411 | 0.496937 | . min 1.000000 | 7.000000 | 52.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 1.00000 | 0.000000 | 1.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 750.750000 | 27.000000 | 1348.250000 | 2.000000 | 2.000000 | 2.000000 | 2.000000 | 2.000000 | 2.000000 | 3.00000 | 2.000000 | 3.000000 | 2.000000 | 3.000000 | 3.000000 | 3.000000 | 2.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 1500.500000 | 39.000000 | 1937.000000 | 3.000000 | 3.000000 | 3.000000 | 3.000000 | 3.000000 | 4.000000 | 4.00000 | 4.000000 | 4.000000 | 4.000000 | 4.000000 | 3.000000 | 4.000000 | 4.000000 | 0.000000 | 0.000000 | 1.000000 | . 75% 2250.250000 | 51.000000 | 2547.250000 | 4.000000 | 4.000000 | 4.000000 | 4.000000 | 4.000000 | 4.000000 | 5.00000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 4.000000 | 5.000000 | 4.000000 | 12.000000 | 13.000000 | 1.000000 | . max 3000.000000 | 80.000000 | 6882.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.00000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 1128.000000 | 1115.000000 | 1.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Categorical = [&#39;Gender&#39;, &#39;Customer Type&#39;, &#39;Type of Travel&#39;, &#39;Class&#39;] Order = [&#39;Seat comfort&#39;, &#39;Departure/Arrival time convenient&#39;, &#39;Food and drink&#39;,&#39;Gate location&#39;, &#39;Inflight wifi service&#39;, &#39;Inflight entertainment&#39;, &#39;Online support&#39;, &#39;Ease of Online booking&#39;, &#39;On-board service&#39;, &#39;Leg room service&#39;, &#39;Baggage handling&#39;, &#39;Checkin service&#39;, &#39;Cleanliness&#39;, &#39;Online boarding&#39;] Continuous = [&#39;Age&#39;, &#39;Flight Distance&#39;, &#39;Departure Delay in Minutes&#39;, &#39;Arrival Delay in Minutes&#39;] . 변수를 질적 변수, 이산형 변수, 연속형 변수로 구분해서 관찰할 수 있을 거 같아요. . 질적 변수 . Gender : 성별 (M, F), Customer Type : Loyal 여부 (Disloyal 또는 Loyal), Type of Travel : 여행 목적 (Business 또는 Personal Travel), Class : 좌석 종류 ( Eco &lt; Eco Plus &lt; Business) . 이산형 변수 (모두 만족도 변수이기 때문에 값이 0~5 사이 입니다.) . Seat comfort : 좌석 만족도, Departure/Arrival time convenient : 출발/도착 시간 편의성 만족도, Food and drink : 식음료 만족도, Gate location : 게이트 위치 만족도, Inflight wifi service : 기내 와이파이 서비스 만족도, Inflight entertainment : 기내 엔터테인먼트 만족도, Online support : 온라인 지원 만족도, Ease of Online booking : 온라인 예매 편리성 만족도, On-board service : 탑승 서비스 만족도, Leg room service : Leg room 서비스 만족도, Baggage handling : 수하물 처리 만족도, Checkin service : 체크인 서비스 만족도,Cleanliness : 청결도 만족도, Online boarding : 온라인보딩 만족도 . 연속형 변수 . Age : 나이, Flight Distance : 비행거리, Departure Delay in Minutes : 출발 지연 시간, Arrival Delay in Minutes : 도착 지연 시간 . &#45936;&#51060;&#53552; &#49884;&#44033;&#51201;&#51004;&#47196; &#44288;&#52272;&#54616;&#44592; . &#53440;&#44191; &#48320;&#49688; &#44288;&#52272; . plt.figure(figsize=[12,8]) plt.text(s=&quot;Target variables&quot;,x=0,y=1.3, va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;,fontsize=25) plt.pie(train[&#39;target&#39;].value_counts(),autopct=&#39;%1.1f%%&#39;, pctdistance=1.1) plt.legend([&#39;Good&#39;, &#39;Bad&#39;], loc = &quot;upper right&quot;,title=&quot;Programming Languages&quot;, prop={&#39;size&#39;: 15}) plt.show() . 제 개인적으로 가장 먼저 관찰해야한다고 생각하는 변수인 반응변수 입니다. 만족하는 비율이 조금 높긴 하지만 빈도차이가 크지 않습니다. . 따로 오버샘플링 등 조치를 취할 필요는 없을 것 같습니다. . train_0 = train[train[&#39;target&#39;]==0] train_1 = train[train[&#39;target&#39;]==1] . 향후 코드 분석을 위해 타겟 값에 따라 트레인 데이터를 두 그룹으로 분리합니다. . &#48276;&#51452;&#54805; &#48320;&#49688; &#44288;&#52272; . def cat_plot(column): f, ax = plt.subplots(1, 3, figsize=(16, 6)) sns.countplot(x = column, data = train, ax = ax[0], order = train[column].value_counts().index) ax[0].tick_params(labelsize=12) ax[0].set_title(&#39;Full train data&#39;) ax[0].set_ylabel(&#39;count&#39;) ax[0].tick_params(rotation=50) sns.countplot(x = column, data = train_1, ax = ax[1], order = train_1[column].value_counts().index) ax[1].tick_params(labelsize=12) ax[1].set_title(&#39;target = 1&#39;) ax[1].set_ylabel(&#39;count&#39;) ax[1].tick_params(rotation=50) sns.countplot(x = column, data = train_0, ax = ax[2], order = train_0[column].value_counts().index) ax[2].tick_params(labelsize=12) ax[2].set_title(&#39;target = 0&#39;) ax[2].set_ylabel(&#39;count&#39;) ax[2].tick_params(rotation=50) plt.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() cat_plot(&quot;Gender&quot;) . 범주형 변수들을 먼저 시각적으로 관찰하겠습니다. 주요 목표는 이 변수가 과연 타겟값에 영향을 주는지 입니다. . 이를 확인하는 방법으로 원본 데이터, 타겟 값이 1인 데이터, 타겟 값이 0인 데이터 각각에서 특정 변수가 다른 모양을 가지고 있는지를 관찰합니다. . 먼저 성별 변수를 관찰해보면 타겟이 1인 데이터에서는 여성이 많고, 타겟이 0인 데이터에서는 남성이 많습니다. . 그래프 차이가 눈에 띄기 때문에 성별 변수는 타겟 변수에 유의미한 영향이 있다, 여성이 긍정적 응답을 유의미하게 많이 했다라고 판단할 수 있겠습니다. . cat_plot(&quot;Customer Type&quot;) . Customer Type 로얄 여부 변수 입니다. 우선 전체 데이터에서 Loyal 항목이 disloyal 항목보다 훨씬 많습니다. . 다만 타겟이 1인 데이터와 타겟이 0인 데이터를 비교하면 타겟이 0인 데이터에서 disloyal 항목의 빈도가 높게 나옵니다. . 통계적으로 검증까진 하진 않았지만, 시각적으로 봐도 두 집단 간 유의미한 차이가 있어보입니다. 즉 이 변수는 유의미한 변수입니다. . cat_plot(&quot;Type of Travel&quot;) . Type of Travel 여행 목적 변수 입니다. 전체 데이터를 보면 비지니스 목적의 비행이 더 많습니다. . 다만 타겟이 0인 데이터, 즉 불만족하다는 응답을 준 데이터를 살펴보면 개인적인 여행을 한 사람에 비중이 조금 높아지는데요. . 여행을 목적으로 비행을 한 손님은 만족의 기준이 상대적으로 높다고 볼 수 있겠네요. . 이 변수 역시 통계적 검증은 하지 않았지만 시각적으로 봤을때 유의미하게 타겟 값에 영향을 주는 변수인 것 같습니다. . cat_plot(&quot;Class&quot;) . Class 항공 좌석 변수입니다. 우선 변수에 대해 설명을 하면 이코노미 &lt; 이코노미 플러스 &lt; 비지니스 순으로 높은 등급의 좌석입니다. . 데이터를 살펴보면 비지니스 좌석을 사용한 사람은 대부분 만족하고, 이코노미 좌석을 사용한 사람은 대부분 불만족하는 것 같습니다. . 이코노미 플러스 좌석 같은 경우 중간 등급 좌석이기 때문에 두 그룹간 차이가 눈에 띄진 않으나 불만족한 비율이 조금 높군요. . 타겟 변수에 따른 두 그룹간 그래프에 모양이 아에 달라지기 때문에 이 변수는 상당히 유의미한 변수 입니다. . train = pd.get_dummies(train) test = pd.get_dummies(test) train.head() . id Age Flight Distance Seat comfort Departure/Arrival time convenient Food and drink Gate location Inflight wifi service Inflight entertainment Online support Ease of Online booking On-board service Leg room service Baggage handling Checkin service Cleanliness Online boarding Departure Delay in Minutes Arrival Delay in Minutes target Gender_Female Gender_Male Customer Type_Loyal Customer Customer Type_disloyal Customer Type of Travel_Business travel Type of Travel_Personal Travel Class_Business Class_Eco Class_Eco Plus . 0 1 | 22 | 1599 | 3 | 0 | 3 | 3 | 4 | 3 | 4 | 4 | 5 | 4 | 4 | 4 | 5 | 4 | 0 | 0.0 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | . 1 2 | 37 | 2810 | 2 | 4 | 4 | 4 | 1 | 4 | 3 | 5 | 5 | 4 | 2 | 1 | 5 | 2 | 18 | 18.0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | . 2 3 | 46 | 2622 | 1 | 1 | 1 | 1 | 4 | 5 | 5 | 4 | 4 | 4 | 4 | 5 | 4 | 3 | 0 | 0.0 | 1 | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | . 3 4 | 24 | 2348 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 2 | 4 | 5 | 3 | 4 | 3 | 10 | 2.0 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | . 4 5 | 58 | 105 | 3 | 3 | 3 | 3 | 4 | 4 | 5 | 4 | 4 | 4 | 4 | 4 | 4 | 5 | 0 | 0.0 | 1 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 모든 범주형 변수가 유의미 하기 때문에 제외하는 변수 없이 사용하고자 합니다. . 밑에 다루는 이산형 변수들은 순서라는게 있지만, 범주형 변수들은 순서가 없는 쌩 범주이기 때문에 라벨인코딩 보다는 원핫인코딩이 적절합니다. . 판다스에 get_dummies 함수를 사용하면 데이터 내 범주형 변수만 알아서 뽑아서 자동으로 원핫인코딩을 해줍니다. . &#51060;&#49328;&#54805; &#48320;&#49688; &#44288;&#52272; . cat_plot(&quot;Seat comfort&quot;) . 이산형 변수에 경우 0~5까지 크기 순서가 정해저 있다는 점에서 범주형 변수와 차이가 있으나 항목 별로 그래프를 관찰할 수 있다는 점에서 범주형 변수와 동일하게 생각할 수 있습니다. 그렇기 때문에 범주형 변수와 동일하게 시각화하여 관찰하겠습니다. . Seat comfort 좌석 만족도 변수인데요. 우리가 보통 만족도 설문조사를 할 때 양 극단으로 답변(5, 0)하는 개인의 소신을 보여주는 기입은 잘 하지 않고 평범한 대답(2,3,4)을 하려는 특성이 있습니다. . 여기서도 앞서 말한 특성이 잘 들어나 있는 것 같습니다. 다만 분석에서 중요한 건 이 부분이 아니라 타겟 변수가 다를 때 응답의 형태가 다른가 인데요. . 좌석 만족도가 4/5로 높은 경우 대부분 만족한다는 응답을 많이 보였고, 좌석 만족도가 1/2/3인 경우 고객 만족도가 불만족인 경우가 많습니다. . 특이한 점은 0인데요. 0을 응답한 거의 대부분의 사람이 고객 만족도에서 만족한다는 응답을 보였습니다. . 0이 가장 안좋은 응답이라고 생각 했었는데, 안좋은 응답이라기 보단 결측치를 표기한 것으로 생각 됩니다. 변수를 더 확인해야겠습니다. . cat_plot(&#39;Departure/Arrival time convenient&#39;) . 다음 변수는 Departure/Arrival time convenient 출발/도착시간 만족도 입니다. 대부분 4,5점을 주어 만족한다는 응답입니다. . 전체 트레인 데이터 분포로 보아 출발/도착시간이 연착되지 않았다면 대부분 만족한다고 답변한 것 같습니다. . 다만 두 그룹 간 그래프 차이가 크게 나지 않는데요. 2, 3번 항목이 순서가 뒤바뀐 것 이외에는 눈에 띄는 차이가 안보입니다. . 특히 1번 항목은 출발/도착시간 만족도가 형편 없었다는 건데, 실제 만족도는 더 높은 것을 보면 이 변수가 의미가 없다고 생각되네요. . 앞서 말한 0번 항목은 여기선 반반 분포가 되있습니다. 조금 이상한데, 다음 변수를 또 봐야할 것 같습니다. . cat_plot(&#39;Food and drink&#39;) . Food and drink 식/음료 만족도 변수 입니다. . 값이 클 수록 타겟 1에 속할 확률이 늘어나는 모습을 보입니다. 하지만 1 항목의 경우 타겟 1일 확률과 0일 확률이 반반입니다. . 또 0 항목의 경우 전체 개수가 100개 이상으로 적지 않은 표본임에도 타겟 1일 확률이 매우 높은 것이 5 항목과 유사한 정도입니다. . 직관적으로 이해되진 않으나 조정이 필요해보입니다. . cat_plot(&#39;Gate location&#39;) . Gate location 게이트 위치 만족도 변수 입니다. 앞서 말한 대로 이 변수 또한 가운데 응답이 몰려있습니다. . 조금 특이한 것은 3번 항목은 대부분 타겟 0 데이터이고, 1/2 항목은 타겟 1 데이터 입니다. . 타겟 별 데이터의 그래프가 눈에 띄게 다르지만 해석하는데는 어려움이 있습니다. 답변자가 응답을 성실히 했는지도 의심해봐야겠습니다. . 또 특이한건 0 응답이 없습니다. 이 부분도 조금 이상하네요. . cat_plot(&#39;Inflight wifi service&#39;) . Inflight wifi service 기내 와이파이 만족도 변수 입니다. . 그래프를 관찰하면 대채로 와이파이 만족도가 높을 수록 타겟 값이 1일 확률이 유의미하게 높은 것 같습니다. . 특이한 점은 0 항목이 적은 수로 존재하는데 저 값은 이상치로 추정되므로 다른 값으로 대체해야겠습니다. . cat_plot(&#39;Inflight entertainment&#39;) . Inflight entertainment 기내 엔터테이먼트 만족도 변수 입니다. . 와이파이 만족도 변수와 비슷하게 4/5 항목일수록 타겟 값이 1일 확률이 높아집니다. . 이 변수에서도 0이 일부 관찰되는데, 0의 활용을 고민해야겠습니다. 가장 많이 나온 4로 대체하는 것도 하나에 방법입니다. . cat_plot(&#39;Online support&#39;) . Online support 온라인 지원 만족도 변수 입니다. . 앞선 두 변수와 비슷하게 전반적으로 4/5 항목이 많으며 4/5 항목일수록 타겟 1일 확률이 많이 높아집니다. . cat_plot(&#39;Ease of Online booking&#39;) . Ease of Online booking 온라인 예매 편의성 만족도 변수 입니다. . 앞선 세 변수와 마찬가지로 4/5 항목일수록 타겟 1일 확률이 높아집니다. . 여기서도 0 항목이 극소수로 존재하는데 빈도가 높은 항목인 4 항목으로 대체하는게 좋을 것 같아요. . cat_plot(&#39;On-board service&#39;) . On-board service 탑승 서비스 만족도 변수 입니다. . 높은 점수를 받을 수록 타겟 1을 받을 확률이 점점 높아지는 형태가 뚜렷한 것을 그래프를 보면 알 수 있습니다. . 계속 같은 말을 반복하는데, 이 말을 하는 것은 타겟을 판단하는데 굉장히 좋은 변수라는 것 입니다. . cat_plot(&#39;Leg room service&#39;) . Leg room service 발이 편안했는지 묻는 변수 입니다. . 일반적으로는 숫자가 커질수록 타겟 1이 될 가능성이 높습니다만, 여기서는 조금 의외인 점이 3 항목 보다 2 항목이 타겟 1이 될 확률이 높습니다. . 표본이 조금 튄 것으로 생각할 수 있겠습니다만, 다르게 말하면 2나 3이나 타겟을 가리는데 별 차이가 없다고도 생각할 수 있겠죠. . 2/3 항목을 병합하는 것도 좋은 아이디어인 것 같아요. 여기서도 0이 극소수 관찰되는데 가장 큰 빈도인 4로 바꿔주겠습니다. . cat_plot(&#39;Baggage handling&#39;) . Baggage handling 수하물 처리 만족도 변수 입니다. . 이 변수 또한 윗 변수와 비슷하게 숫자가 커질수록 타겟 1일 확률이 늘어나나 2/3 항목은 다소 뒤바뀐 결과입니다. . 이 변수 또한 2/3 항목을 병합하겠습니다. . cat_plot(&#39;Checkin service&#39;) . Checkin service 체크인 서비스 만족도 변수 입니다. . 전반적으로 높은 만족도 점수를 기록하며, 점수가 클 수록 타겟 1 값을 가질 확률이 뚜렷히 높아지는 것을 확인할 수 있습니다. . cat_plot(&#39;Cleanliness&#39;) . Cleanliness 청결도 만족도 변수 입니다. . 윗 변수와 마찬가지로 대체로 높은 만족도 점수를 기록하며, 점수가 클수록 타겟 1 값을 가질 확률이 높아지는 것으로 보입니다. . cat_plot(&#39;Online boarding&#39;) . Online boarding 온라인 보딩 만족도 변수 입니다. . 이 변수 또한 값이 커질수록 타겟 1 값을 가질 확률이 높아집니다. . 0 값이 관찰되고 있는데, 마찬가지로 최고 빈도 항목으로 대체하겠습니다. . train[&#39;Seat comfort&#39;][train[&#39;Seat comfort&#39;] == 0] = 5 test[&#39;Seat comfort&#39;][test[&#39;Seat comfort&#39;] == 0] = 5 train[&#39;Inflight wifi service&#39;][train[&#39;Inflight wifi service&#39;] == 0] = 4 test[&#39;Inflight wifi service&#39;][test[&#39;Inflight wifi service&#39;] == 0] = 4 train[&#39;Ease of Online booking&#39;][train[&#39;Ease of Online booking&#39;] == 0] = 4 test[&#39;Ease of Online booking&#39;][test[&#39;Ease of Online booking&#39;] == 0] = 4 train[&#39;On-board service&#39;][train[&#39;On-board service&#39;] == 0] = 4 test[&#39;On-board service&#39;][test[&#39;On-board service&#39;] == 0] = 4 # 1,2 항목 병합 필요한 변수들 train[&#39;Inflight entertainment&#39;][train[&#39;Inflight entertainment&#39;] == 1] = 2 train[&#39;Inflight entertainment&#39;][train[&#39;Inflight entertainment&#39;] == 0] = 4 test[&#39;Inflight entertainment&#39;][test[&#39;Inflight entertainment&#39;] == 1] = 2 test[&#39;Inflight entertainment&#39;][test[&#39;Inflight entertainment&#39;] == 0] = 4 train[&#39;Online support&#39;][train[&#39;Online support&#39;] == 1] = 2 train[&#39;Online support&#39;][train[&#39;Online support&#39;] == 0] = 4 test[&#39;Online support&#39;][test[&#39;Online support&#39;] == 1] = 2 test[&#39;Online support&#39;][test[&#39;Online support&#39;] == 0] = 4 train[&#39;Checkin service&#39;][train[&#39;Checkin service&#39;] == 1] = 2 train[&#39;Checkin service&#39;][train[&#39;Checkin service&#39;] == 0] = 4 test[&#39;Checkin service&#39;][test[&#39;Checkin service&#39;] == 1] = 2 test[&#39;Checkin service&#39;][test[&#39;Checkin service&#39;] == 0] = 4 train[&#39;Cleanliness&#39;][train[&#39;Cleanliness&#39;] == 1] = 2 train[&#39;Cleanliness&#39;][train[&#39;Cleanliness&#39;] == 0] = 4 test[&#39;Cleanliness&#39;][test[&#39;Cleanliness&#39;] == 1] = 2 test[&#39;Cleanliness&#39;][test[&#39;Cleanliness&#39;] == 0] = 4 train[&#39;Online boarding&#39;][train[&#39;Online boarding&#39;] == 1] = 2 train[&#39;Online boarding&#39;][train[&#39;Online boarding&#39;] == 0] = 4 test[&#39;Online boarding&#39;][test[&#39;Online boarding&#39;] == 1] = 2 test[&#39;Online boarding&#39;][test[&#39;Online boarding&#39;] == 0] = 4 # 2,3 항목 변환 필요한 변수들 train[&#39;Leg room service&#39;][train[&#39;Leg room service&#39;] == 2] = 3 train[&#39;Leg room service&#39;][train[&#39;Leg room service&#39;] == 0] = 4 test[&#39;Leg room service&#39;][test[&#39;Leg room service&#39;] == 2] = 3 test[&#39;Leg room service&#39;][test[&#39;Leg room service&#39;] == 0] = 4 train[&#39;Baggage handling&#39;][train[&#39;Baggage handling&#39;] == 2] = 3 train[&#39;Baggage handling&#39;][train[&#39;Baggage handling&#39;] == 0] = 4 test[&#39;Baggage handling&#39;][test[&#39;Baggage handling&#39;] == 2] = 3 test[&#39;Baggage handling&#39;][test[&#39;Baggage handling&#39;] == 0] = 4 # 조금 특별한 변환 필요한 변수들 train[&#39;Food and drink&#39;][train[&#39;Food and drink&#39;] == 1] = -1 train[&#39;Food and drink&#39;][train[&#39;Food and drink&#39;] == 2] = 1 train[&#39;Food and drink&#39;][train[&#39;Food and drink&#39;] == 3] = 2 train[&#39;Food and drink&#39;][train[&#39;Food and drink&#39;] == -1] = 3 train[&#39;Food and drink&#39;][train[&#39;Food and drink&#39;] == 0] = 5 test[&#39;Food and drink&#39;][test[&#39;Food and drink&#39;] == 1] = -1 test[&#39;Food and drink&#39;][test[&#39;Food and drink&#39;] == 2] = 1 test[&#39;Food and drink&#39;][test[&#39;Food and drink&#39;] == 3] = 2 test[&#39;Food and drink&#39;][test[&#39;Food and drink&#39;] == -1] = 3 test[&#39;Food and drink&#39;][test[&#39;Food and drink&#39;] == 0] = 5 train[&#39;Gate location&#39;][train[&#39;Gate location&#39;] == 1] = 5 train[&#39;Gate location&#39;][train[&#39;Gate location&#39;] == 2] = 5 train[&#39;Gate location&#39;][train[&#39;Gate location&#39;] == 0] = 3 test[&#39;Gate location&#39;][test[&#39;Gate location&#39;] == 1] = 5 test[&#39;Gate location&#39;][test[&#39;Gate location&#39;] == 2] = 5 test[&#39;Gate location&#39;][test[&#39;Gate location&#39;] == 0] = 3 # 삭제할 변수 train.drop([&#39;Departure/Arrival time convenient&#39;], axis = 1, inplace = True) test.drop([&#39;Departure/Arrival time convenient&#39;], axis = 1, inplace = True) . 우선 0이 관찰되는 변수도 있고 아닌 변수도 있는데, 관찰되지 않더라도 테스트 데이터 있을 수 있으므로 공통적으로 적용하겠습니다. . 대부분 변수의 가장 큰 빈도인 항목이 4입니다. 특별한 언급이 없는 변수는 0 항목을 4로 대체하였습니다. . 이산형 변수를 시각적으로 다루면서 느낀점은 역시 사람이 하는 설문조사다 보니깐 데이터의 질이 높진 못한 것 같아요. . 이산형 변수를 앞서 관찰한 결과를 통해 다섯 가지 종류로 나눠서 전처리 하였습니다. 다섯 가지 종류는 다음과 같습니다. . 깔끔한 변수들 . Seat comfort (0은 5로), Inflight wifi service, Ease of Online booking, On-board service . 1,2 항목 병합 필요한 변수들 . Inflight entertainment, Online support, Checkin service, Cleanliness, Online boarding . 2,3 항목 병합 필요한 변수들 . Leg room service, Baggage handling . 형태가 다소 이상하지만 유의미한 변수들 . Food and drink(1은 3으로, 3은 2로, 2는 1로, 0은 5로), Gate location(2,1을 5로, 0은 3으로) . 유의미 하지 않은 변수(사용하지 않을 변수) . Departure/Arrival time convenient . &#50672;&#49549;&#54805; &#48320;&#49688; &#44288;&#52272; . def num_plot(column): fig, axes = plt.subplots(1, 3, figsize=(16, 6)) sns.distplot(train[column], ax = axes[0]) axes[0].tick_params(labelsize=12) axes[0].set_title(&#39;Full train data&#39;) axes[0].set_ylabel(&#39;count&#39;) sns.distplot(train_1[column], ax = axes[1]) axes[1].tick_params(labelsize=12) axes[1].set_title(&#39;target = 1&#39;) axes[1].set_ylabel(&#39;count&#39;) sns.distplot(train_0[column], ax = axes[2]) axes[2].tick_params(labelsize=12) axes[2].set_title(&#39;target = 0&#39;) axes[2].set_ylabel(&#39;count&#39;) plt.subplots_adjust(wspace=0.3, hspace=0.3) print(&#39;타겟 1 데이터의 평균 :&#39;, (train_1[column]).mean()) print(&#39;타겟 0 데이터의 평균 :&#39;, (train_0[column]).mean()) print(&#39;데이터의 표준오차 :&#39;, train[column].std() / np.sqrt(3000)) num_plot(&quot;Age&quot;) . 타겟 1 데이터의 평균 : 40.65047961630695 타겟 0 데이터의 평균 : 37.390390390390394 데이터의 표준오차 : 0.2758477133981643 . 연속형 변수는 데이터를 타겟이 1과 0인 두 그룹으로 나눠서 그룹 별 히스토그램이 차이가 있는지를 시각적으로 관찰하겠습니다. . 우선 전체 트레인 데이터의 나이 변수는 정규분포와 유사합니다. 변수로써 좋은 성질이죠. . 타겟 값이 1인 데이터의 나이 평균은 40살, 타겟 값이 0인 데이터의 나이 평균은 37살로 크게 차이나진 않습니다. . 다만 표본이 3천개면 통계적으로 상당히 많은 편인데(요즘 많이 수행하는 대통령 여론조사도 천명 뽑습니다.) 3살 차이는 두 그룹간 나이 평균이 유의미하게 난다고 볼 수 있습니다. . 또 그래프도 봉우리가 있는 위치가 조금 다른 것이 보이기도 합니다. 그렇기 때문에 이 변수는 사용하겠습니다. . num_plot(&quot;Flight Distance&quot;) . 타겟 1 데이터의 평균 : 1935.2583932853718 타겟 0 데이터의 평균 : 2042.9632132132133 데이터의 표준오차 : 18.770618492960658 . Flight Distance 비행거리 변수 입니다. 단순하게 생각했을때 비행 거리가 길면 만족도가 떨어질 확률이 높겠죠. . 전체 트레인 데이터의 분포가 정규분포와 유사하나 우측 꼬리가 조금 길어보입니다. 로그변환 해주는 것도 좋겠군요. . 실제 데이터도 예측한대로 만족도가 1인 그룹의 평균 비행 시간이 짧게 나옵니다. 그래프를 봐도 타겟 1인 그래프가 앞쪽에 값이 많이있어보이죠. . 저 차이가 유의미한 것인지도 생각해야하는데, 표준오차 대비 두 그룹간 차이가 꽤 있으므로 이 변수도 유의미한 변수로 취급하겠습니다. . num_plot(&quot;Departure Delay in Minutes&quot;) . 타겟 1 데이터의 평균 : 11.405875299760192 타겟 0 데이터의 평균 : 20.92942942942943 데이터의 표준오차 : 0.8231033660699084 . num_plot(&quot;Arrival Delay in Minutes&quot;) . 타겟 1 데이터의 평균 : 11.384892086330936 타겟 0 데이터의 평균 : 21.603603603603602 데이터의 표준오차 : 0.8252975959121616 . print(&#39;두 변수간 상관계수:&#39;, train[&#39;Arrival Delay in Minutes&#39;].corr(train[&quot;Departure Delay in Minutes&quot;])) print(&#39;출발 지연시간이 0인 값:&#39;, sum(train[&#39;Departure Delay in Minutes&#39;] == 0)) print(&#39;도착 지연시간이 0인 값:&#39;, sum(train[&#39;Arrival Delay in Minutes&#39;] == 0)) . 두 변수간 상관계수: 0.9768732919464286 출발 지연시간이 0인 값: 1705 도착 지연시간이 0인 값: 1661 . Departure Delay in Minutes, Arrival Delay in Minutes 출발 지연 시간, 도착 지연 시간 변수 입니다. . 근데 비행기가 출발이 지연되면 도착도 자연스럽게 지연이 되겠죠? 즉 두 변수간 상관계수가 매우 높을 것으로 추축되고 실제로도 그러합니다. . 이 말은 굳이 두 변수를 사용할 필요가 없다, 오히려 다중공선성 문제를 가져오게 됩니다. 0.97이면 거의 한 변수나 다름 없죠. . 또 지연이 됬는지 안됬는지를 구분할 수도 있습니다. 실제로 절반 이상의 값이 0을 기록했는데 지연이 안됬음을 의미합니다. . 다만 출발은 정상적으로 했는데, 도착은 연착될 수도 있으므로 도착 지연 시간 변수를 사용하도록 하겠습니다. . 그리고 큰 값은 엄청 큰 우측 꼬리가 긴 분포형태이기 때문에 로그변환을 해야합니다. . 이때 0은 로그변환이 안되므로 전체 값에 1을 더한 뒤 로그변환 하는 log1p 함수를 사용해야 한다는 것도 유의해야합니다. . train[&#39;Flight Distance&#39;] = np.log1p(train[&#39;Flight Distance&#39;]) train[&#39;Arrival Delay in Minutes&#39;] = np.log1p(train[&#39;Arrival Delay in Minutes&#39;]) test[&#39;Flight Distance&#39;] = np.log1p(test[&#39;Flight Distance&#39;]) test[&#39;Arrival Delay in Minutes&#39;] = np.log1p(test[&#39;Arrival Delay in Minutes&#39;]) train.drop([&#39;Departure Delay in Minutes&#39;], axis = 1, inplace = True) test.drop([&#39;Departure Delay in Minutes&#39;], axis = 1, inplace = True) . 연속형 변수를 시각적으로 관찰하면서 해결하려 했던 부분을 적은 코드입니다. . &#44036;&#45800;&#54620; &#47004;&#45924;&#54252;&#47112;&#49828;&#53944; &#47784;&#45944; &#51201;&#54633; . train_label = train[&#39;target&#39;] train.drop([&#39;id&#39;, &#39;target&#39;], axis = 1, inplace= True) test.drop([&#39;id&#39;], axis = 1, inplace= True) . 타겟 값을 라벨이라는 변수에 따로 뺀 뒤 분석에 의미 없는 변수인 id와 함께 지웁니다. . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;target&#39;] = rf.predict(test) sample_submission.to_csv(&#39;airport_1.csv&#39;,index=False) . 노말한 랜덤포레스트 분류 모델에 적용하여 결과 값을 csv 파일로 저장하였습니다. . 결과는 public 기준 0.921 인데요. 조금 더 좋은 모델을 사용하거나 앙상블을 시키면 더 점수가 좋아지지 않을가 생각되네요. . EDA 중심에 코드이니 우선 이정도 결과로 만족하겠습니다. 변수를 하나하나 살펴봤는데 많은 인사이트 얻으셨으면 좋겠습니다! .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/eda/classifier/2022/02/08/dacon_airport.html",
            "relUrl": "/dacon/jupyter/eda/classifier/2022/02/08/dacon_airport.html",
            "date": " • Feb 8, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "[SSUDA] 자연어 처리 문장 쌍 분류 실습하기 with DACON",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592;, &#54056;&#53412;&#51648; &#49444;&#52824;&#54616;&#44592; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-1.0.1-py3-none-any.whl (42 kB) |████████████████████████████████| 42 kB 1.2 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 41.6 MB/s Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 7.0 MB/s Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 63.7 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 73.3 MB/s Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.7.0-py3-none-any.whl (396 kB) |████████████████████████████████| 396 kB 70.6 MB/s Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 73.8 MB/s Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB) |████████████████████████████████| 133 kB 68.4 MB/s Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting sacremoses Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 45.7 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.10.1) Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB) |████████████████████████████████| 67 kB 6.9 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 66.8 MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.2) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 66.1 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 7.7 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.7) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.0.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.1) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.43.0) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.1) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.7.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.11) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB) |████████████████████████████████| 94 kB 4.2 MB/s Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 67.5 MB/s Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.4.0) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB) |████████████████████████████████| 144 kB 54.0 MB/s Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=3f1aeef72f51ff19e32178a95e240969639ff6bca15732e4f6f3c316c125c369 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, pyDeprecate, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.3.0 fsspec-2022.1.0 future-0.18.2 huggingface-hub-0.4.0 multidict-6.0.2 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-1.0.1 sacremoses-0.0.47 tokenizers-0.10.3 torchmetrics-0.7.0 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/sentence/&#39; train = pd.read_csv(path + &#39;train_data.csv&#39;) test = pd.read_csv(path + &#39;test_data.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . index premise hypothesis label . 0 0 | 씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이... | 씨름의 여자들의 놀이이다. | contradiction | . 1 1 | 삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,... | 자작극을 벌인 이는 3명이다. | contradiction | . 2 2 | 이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다. | 예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다. | entailment | . 3 3 | 광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ... | 원주민들은 종합대책에 만족했다. | neutral | . 4 4 | 진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는... | 이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다. | neutral | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 필요한 패키지를 설치하고, 데이터를 불러옵니다. . BERT &#53664;&#53356;&#45208;&#51060;&#51200;&#50640; &#45824;&#54644; . from Korpora import Korpora import os nsmc = Korpora.load(&#39;nsmc&#39;, force_download=True) def write_lines(path, lines): with open(path, &#39;w&#39;, encoding = &#39;utf-8&#39;) as f: for line in lines: f.write(f&#39;{line} n&#39;) write_lines(&#39;/root/train.txt&#39;, nsmc.train.get_all_texts()) write_lines(&#39;/root/test.txt&#39;, nsmc.test.get_all_texts()) . Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을 손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다. 말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다. 해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고, 해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다. # Description Author : e9t@github Repository : https://github.com/e9t/nsmc References : www.lucypark.kr/docs/2015-pyconkr/#39 Naver sentiment movie corpus v1.0 This is a movie review dataset in the Korean language. Reviews were scraped from Naver Movies. The dataset construction is based on the method noted in [Large movie review dataset][^1] from Maas et al., 2011. [^1]: http://ai.stanford.edu/~amaas/data/sentiment/ # License CC0 1.0 Universal (CC0 1.0) Public Domain Dedication Details in https://creativecommons.org/publicdomain/zero/1.0/ . [nsmc] download ratings_train.txt: 14.6MB [00:00, 218MB/s] [nsmc] download ratings_test.txt: 4.90MB [00:00, 94.3MB/s] . NSMC는 네이버 영화 리뷰 자료입니다. 실습을 위해 자료를 다운 받고 텍스트 형태로 저장했습니다. . from tokenizers import BertWordPieceTokenizer wordpiece_tokenizer = BertWordPieceTokenizer(lowercase = False) wordpiece_tokenizer.train( files=[&#39;/root/train.txt&#39;, &#39;/root/test.txt&#39;], vocab_size = 10000, ) os.makedirs(&#39;/gdrive/My Drive/nlpbook/wordpiece&#39;, exist_ok= True) wordpiece_tokenizer.save_model(&#39;/gdrive/My Drive/nlpbook/wordpiece&#39;) . [&#39;/gdrive/My Drive/nlpbook/wordpiece/vocab.txt&#39;] . BERT 방식으로 토크나이저를 구축했습니다. 토크나이저란 문장을 토큰 시퀀스로 나눈것을 의미하는데요. . BERT 방식은 말뭉치에서 자주 등장하는 문자열을 토큰으로 인식한 뒤 문자열을 병합해 어휘 집합을 구축합니다. . 이때 말뭉치의 우도를 가장 높이는 쌍을 먼저 병합하게 됩니다. 두 말뭉치가 동시에 자주 등장할 수록 병합 될 가능성이 높겠죠. . BERT 방식을 간단히 설명하긴 힘들기 때문에 댓글로 질문 남겨주시면 감사하겠습니다. . !head /gdrive/My Drive/nlpbook/wordpiece/vocab.txt . [PAD] [UNK] [CLS] [SEP] [MASK] ! &#34; % &amp; &#39; . BERT로 구성된 어휘집합의 앞 내용들입니다. 여기서 PAD은 더미 토큰으로 길이를 맞춰주는 역할을 합니다. . from transformers import BertTokenizer tokenizer_bert = BertTokenizer.from_pretrained( &#39;/gdrive/My Drive/nlpbook/wordpiece&#39;, do_lower_case = False, ) sentences = [ &quot;아 더빙.. 진짜 짜증나네요 목소리&quot;, &quot;흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나&quot;, &quot;별루 였다..&quot;, ] tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences] tokenized_sentences . file /gdrive/My Drive/nlpbook/wordpiece/config.json not found . [[&#39;아&#39;, &#39;더빙&#39;, &#39;.&#39;, &#39;.&#39;, &#39;진짜&#39;, &#39;짜증나&#39;, &#39;##네요&#39;, &#39;목소리&#39;], [&#39;흠&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;포스터&#39;, &#39;##보고&#39;, &#39;초딩&#39;, &#39;##영화&#39;, &#39;##줄&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;오버&#39;, &#39;##연기&#39;, &#39;##조차&#39;, &#39;가볍&#39;, &#39;##지&#39;, &#39;않&#39;, &#39;##구나&#39;], [&#39;별루&#39;, &#39;였다&#39;, &#39;.&#39;, &#39;.&#39;]] . BERT 토크나이저 모델로 예시 문장을 토큰화 시켰습니다. 이때 ## 기호는 문장의 시작이 아닌 것을 의미합니다. . batch_inputs = tokenizer_bert( sentences, padding = &#39;max_length&#39;, max_length = 12, truncation = True, ) batch_inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]) . 앞선 코드는 시각적으로 살펴보기 위해 한 것이고 이 부분은 실제 모델 입력값입니다. . &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, 3가지 입력값이 나옵니다. . input_ids는 실제 문장을 의미하고, token_type_ids는 문장이 2개 입력됬을 경우 0과 1로 두 문장을 구분해줍니다. . 마지막으로 attention_mask는 어디까지가 실제 문장인지, 공백 문장이 어딨는지를 알려줍니다. . batch_inputs[&#39;input_ids&#39;] . [[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0], [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1168, 16, 3], [2, 3274, 9508, 16, 16, 3, 0, 0, 0, 0, 0, 0]] . 어휘 집합에 있는 토큰과 매칭되는 숫자가 실제로 출력됩니다. . &#47784;&#45944; &#44592;&#52488; &#54872;&#44221;&#49444;&#51221; . from torch.cuda import is_available import torch from ratsnlp.nlpbook.classification import ClassificationTrainArguments args = ClassificationTrainArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name = &#39;pair-classification&#39;, # 문장 쌍 분류를 할 예정이므로 downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, batch_size = 32 if torch.cuda.is_available() else 4, learning_rate=5e-5, max_seq_length=64, epochs = 5, tpu_cores=0 if torch.cuda.is_available() else 8, seed = 7, ) . beomi/kcbert-base 라는 프리트레인을 마친 언어 모델을 사용하여 분석을 진행합니다. . 여기서는 ClassificationTrainArguments 클래스를 활용해 사용하는 파라미터 값을 정해집니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;pair-classification&#39;, downstream_corpus_name=None, downstream_corpus_root_dir=&#39;/content/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;pair-classification&#39;, downstream_corpus_name=None, downstream_corpus_root_dir=&#39;/content/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;pair-classification&#39;, downstream_corpus_name=None, downstream_corpus_root_dir=&#39;/content/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;pair-classification&#39;, downstream_corpus_name=None, downstream_corpus_root_dir=&#39;/content/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) . set seed: 7 . 같은 결과를 재현하기 위해 시드값을 고정해줍니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( &#39;beomi/kcbert-base&#39;, do_lower_case = False, ) . 실제 kcbert-base 모델이 사용하는 토크나이저를 선언합니다. 이를 통해 입력문장을 토큰화 시킵니다. . np.random.seed(42) tem = np.random.choice(train.shape[0], 5000, replace=False) val = train.iloc[tem] . 평가 데이터 셋을 트레인 데이터 셋중 일부를 사용해 추출합니다. . from ratsnlp.nlpbook.classification import ClassificationExample from ratsnlp.nlpbook.classification import ClassificationFeatures train_dataset = [] val_dataset = [] label = {&quot;entailment&quot; : 0, &quot;contradiction&quot; : 1, &quot;neutral&quot; : 2} # for i in range(train.shape[0]): # text_a = train[&#39;premise&#39;].loc[i] # text_b = train[&#39;hypothesis&#39;].loc[i] # label = train[&#39;label&#39;].loc[i] # examples.append(ClassificationExample(text_a=text_a, text_b=text_b, label=label)) for i in range(train.shape[0]): token = tokenizer( train[&#39;premise&#39;].iloc[i], train[&#39;hypothesis&#39;].iloc[i], padding = &#39;max_length&#39;, max_length = 64, truncation = True, ) train_dataset.append(ClassificationFeatures(token.input_ids, token.attention_mask, token.token_type_ids, label = label[train[&#39;label&#39;].iloc[i]])) for i in range(val.shape[0]): token = tokenizer( val[&#39;premise&#39;].iloc[i], val[&#39;hypothesis&#39;].iloc[i], padding = &#39;max_length&#39;, max_length = 64, truncation = True, ) val_dataset.append(ClassificationFeatures(token.input_ids, token.attention_mask, token.token_type_ids, label = label[val[&#39;label&#39;].iloc[i]])) . 입력된 문장을 앞서 정의한 토크나이저 모델을 이용해 토큰화를 합니다. . 이때 ClassificationFeatures 클래스를 이용하면 분류를 위한 라벨까지 한 객체에 넣을 수 있습니다. . max_length = 64에 의미는 문장당 최대 토큰의 길이를 64로 설정한 것 입니다. truncation = True 은 64가 넘는 문장을 잘라낸다는 설정입니다. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = 32, sampler = RandomSampler(train_dataset, replacement = False), collate_fn = nlpbook.data_collator, # 뽑은 인스턴스를 배치로 바꿔줌(텐서 형태로) drop_last = False, num_workers = 1, ) val_dataloader = DataLoader( val_dataset, batch_size = 32, sampler = RandomSampler(val_dataset, replacement = False), collate_fn = nlpbook.data_collator, # 뽑은 인스턴스를 배치로 바꿔줌(텐서 형태로) drop_last = False, num_workers = 1, ) . 토큰화한 데이터 셋을 DataLoader 함수를 이용해 배치화 시킵니다. . from transformers import BertConfig, BertForSequenceClassification pretrained_model_config = BertConfig.from_pretrained( &#39;beomi/kcbert-base&#39;, num_labels = 3, # 레이블이 3개이기 때문에 ) model = BertForSequenceClassification.from_pretrained( &#39;beomi/kcbert-base&#39;, config = pretrained_model_config ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.seq_relationship.weight&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . 사용할 모델은 kcbert-base 모델로 토크나이저를 수행했으며 두 문장의 관계를 모순/중립/참 세가지로 분류하는 것이 목적입니다. . from transformers import PreTrainedModel from transformers.optimization import AdamW from ratsnlp.nlpbook.metrics import accuracy from pytorch_lightning import LightningModule from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts from ratsnlp.nlpbook.classification.arguments import ClassificationTrainArguments class ClassificationTask(LightningModule): def __init__(self, model, learning_rate): super().__init__() self.model = model self.learning_rate = learning_rate def configure_optimizers(self): optimizer = AdamW(self.parameters(), lr=self.learning_rate) scheduler = ExponentialLR(optimizer, gamma=0.5) return { &#39;optimizer&#39;: optimizer, &#39;scheduler&#39;: scheduler, } def training_step(self, inputs, batch_idx): # outputs: SequenceClassifierOutput outputs = self.model(**inputs) preds = outputs.logits.argmax(dim=-1) labels = inputs[&quot;labels&quot;] acc = accuracy(preds, labels) self.log(&quot;loss&quot;, outputs.loss, prog_bar=False, logger=True, on_step=True, on_epoch=False) self.log(&quot;acc&quot;, acc, prog_bar=True, logger=True, on_step=True, on_epoch=False) return outputs.loss def validation_step(self, inputs, batch_idx): # outputs: SequenceClassifierOutput outputs = self.model(**inputs) preds = outputs.logits.argmax(dim=-1) labels = inputs[&quot;labels&quot;] acc = accuracy(preds, labels) self.log(&quot;val_loss&quot;, outputs.loss, prog_bar=True, logger=True, on_step=False, on_epoch=True) self.log(&quot;val_acc&quot;, acc, prog_bar=True, logger=True, on_step=False, on_epoch=True) return outputs.loss . task = ClassificationTask(model, 5e-5) . 파이토치 내 LightningModule 클래스를 상속해 ClassificationTask 를 만들었습니다. . &#47784;&#45944; &#49324;&#50857;&#54616;&#44592; . trainer = nlpbook.get_trainer(args) trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader ) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | model | BertForSequenceClassification | 108 M -- 108 M Trainable params 0 Non-trainable params 108 M Total params 435.683 Total estimated model params size (MB) . 파이토치 라이트닝 라이브러리를 상속받은 get_trainer로 GPU 설정, 체크포인트 등을 자동으로 설정해줍니다. . 그 후 학습을 진행하게 됩니다. 여기서 저는 모든 트레인 데이터를 학습에 사용하고 싶었습니다. . 다만 이렇게 되면 val 값을 신용할수는 없겠죠. (val 데이터가 학습 데이터 내부 값이기 때문) . model.eval() . BertForSequenceClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30000, 768, padding_idx=0) (position_embeddings): Embedding(300, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=3, bias=True) ) . 모델을 평가모드로 바꿔줍니다. . &#47784;&#45944; &#51060;&#50857;&#54616;&#44592; . for i in range(test.shape[0]): token = tokenizer( test[&#39;premise&#39;].iloc[i], test[&#39;hypothesis&#39;].iloc[i], padding = &#39;max_length&#39;, max_length = 64, truncation = True, ) token[&#39;input_ids&#39;] = [token[&#39;input_ids&#39;]] token[&#39;attention_mask&#39;] = [token[&#39;attention_mask&#39;]] token[&#39;token_type_ids&#39;] = [token[&#39;token_type_ids&#39;]] outputs = model(**{k: torch.tensor(v) for k, v in token.items()}) prob = outputs.logits.softmax(dim = 1) labels = torch.argmax(prob) if labels == 0: test[&#39;label&#39;].iloc[i] = &#39;entailment&#39; elif labels == 1: test[&#39;label&#39;].iloc[i] = &#39;contradiction&#39; else: test[&#39;label&#39;].iloc[i] = &#39;neutral&#39; . 테스트 데이터를 토크나이저를 이용해 토큰화를 한 뒤 모델에 입력해줍니다. . 모델의 출력값 outputs 은 길이가 3인 벡터를 출력해주는데, 이를 소프트맥스 함수에 통과시켜주면 각 범주의 확률값이 나옵니다. . 확률값을 이용해 테스트 데이터가 어느 범주에 속해있는지 구해줍니다. . test[&#39;label&#39;].value_counts() . neutral 589 contradiction 585 entailment 492 Name: label, dtype: int64 . train[&#39;label&#39;].value_counts() . entailment 8561 contradiction 8489 neutral 7948 Name: label, dtype: int64 . 범주당 비슷한 비율로 구분한 것을 보아 결과가 터무니없지는 않은 것 같습니다. . sample_submission[&#39;label&#39;] = test[&#39;label&#39;] sample_submission.to_csv(&#39;sentence_3.csv&#39;,index=False) . 나온 결과를 제출파일에 저장하여 제출했습니다. Public 결과는 약 0.624 정도로 앞선 모델이 다소 과적합되어 보입니다. . &#45712;&#45184;&#51216; . 우선 딥러닝 관련 이론 위주로 항상 공부하다가 실전 데이터를 사용했는데, 확실히 흥미로웠습니다. . 직접 구현했다기 보단 라이브러리를 불러온 형식이 많고, 코드 부분부분까지 정확하게 이해하진 못했습니다. . 클래스나 함수 내 입력값 형식 관련해서 진행할 때 많은 오류를 겪었습니다. 어짜피 한번 겪을 과정이라 생각했지만 조금 힘들었네요. . 다음에는 같은 데이터를 가지고 데이콘 운영자님이 작성하신 베이스라인 코드를 이해가 되는 한에서 리뷰를 해보겠습니다. .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/ssuda/natural%20language/bert/tokenizer/classifier/2022/02/03/dacon_sentence.html",
            "relUrl": "/dacon/jupyter/ssuda/natural%20language/bert/tokenizer/classifier/2022/02/03/dacon_sentence.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "[DACON] 집값 예측 경진대회",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/house2/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id Overall Qual Gr Liv Area Exter Qual Garage Cars Garage Area Kitchen Qual Total Bsmt SF 1st Flr SF Bsmt Qual Full Bath Year Built Year Remod/Add Garage Yr Blt target . 0 1 | 10 | 2392 | Ex | 3 | 968 | Ex | 2392 | 2392 | Ex | 2 | 2003 | 2003 | 2003 | 386250 | . 1 2 | 7 | 1352 | Gd | 2 | 466 | Gd | 1352 | 1352 | Ex | 2 | 2006 | 2007 | 2006 | 194000 | . 2 3 | 5 | 900 | TA | 1 | 288 | TA | 864 | 900 | TA | 1 | 1967 | 1967 | 1967 | 123000 | . 3 4 | 5 | 1174 | TA | 2 | 576 | Gd | 680 | 680 | TA | 1 | 1900 | 2006 | 2000 | 135000 | . 4 5 | 7 | 1958 | Gd | 3 | 936 | Gd | 1026 | 1026 | Gd | 2 | 2005 | 2005 | 2005 | 250000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 이 코드를 그대로 사용하신다면 path 위치를 파일 저장하신 곳으로 지정하시면 됩니다. . print(train.shape) print(test.shape) . (1350, 15) (1350, 14) . 학습 데이터, 테스트 데이터 모두 1350개이며 변수는 총 14개입니다. . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1350 entries, 0 to 1349 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 id 1350 non-null int64 1 Overall Qual 1350 non-null int64 2 Gr Liv Area 1350 non-null int64 3 Exter Qual 1350 non-null object 4 Garage Cars 1350 non-null int64 5 Garage Area 1350 non-null int64 6 Kitchen Qual 1350 non-null object 7 Total Bsmt SF 1350 non-null int64 8 1st Flr SF 1350 non-null int64 9 Bsmt Qual 1350 non-null object 10 Full Bath 1350 non-null int64 11 Year Built 1350 non-null int64 12 Year Remod/Add 1350 non-null int64 13 Garage Yr Blt 1350 non-null int64 14 target 1350 non-null int64 dtypes: int64(12), object(3) memory usage: 158.3+ KB . 결측치는 없는 것으로 관찰됩니다. . &#53440;&#44191; &#48320;&#49688; &#44288;&#52272; . 회귀 문제를 다룰때는 개인적인 경험으로 타겟 변수의 분포가 상당히 중요하다고 생각하기 때문에 가장 먼저 살펴보는 편 입니다. . 이 데이터의 타겟 변수는 집값 변수입니다. 특이한 이상치가 없는지, 정규분포와 유사한 모양을 따르는지를 중점적으로 관찰하겠습니다. . plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train[&#39;target&#39;].values)) plt.show() . 집값을 크기순으로 정렬한 뒤 그래프로 관찰했습니다. . 과하게 0과 가까운 점이 하나 관찰되고, 50만보다 큰 데이터 또한 띄엄띄엄 관찰됩니다. . 데이터 크기가 거대하지 않기 때문에 과연 이상치인지 다른 변수를 살펴본 뒤 다시 관찰하겠습니다. . sns.distplot(train[&#39;target&#39;], fit=stats.norm) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdbe8521250&gt; . 타겟 변수의 정규성에 대해서 살펴봤습니다. 파란색이 실제 데이터의 히스토그램이고 검은색이 정규분포임을 가정한 그래프입니다. . 확실히 오른쪽 꼬리가 긴 모습인데, 집 값 데이터임을 감안하고 보면 납득이 되는 결과입니다. . 하지만 데이터 분석을 할때는 다소 부적절할 것 같아서 로그변환을 적극적으로 검토해보겠습니다. . sns.distplot(np.log1p(train[&#39;target&#39;]), fit=stats.norm) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdbe842ab50&gt; . 타겟 변수를 로그변환한 뒤 정규성을 확인했습니다. 이전보다 확실히 정규분포에 가깝습니다. . 다만 이전에 관찰한 0과 가까운 집값 데이터 때문인지 9 근처에 데이터 하나가 잡히는게 신경쓰이네요. . &#51060;&#49328;&#54805; &#48320;&#49688; &#44288;&#52272; . Overall Qual &#48320;&#49688; . train[&#39;Overall Qual&#39;].value_counts() . 5 379 6 351 7 307 8 156 4 75 9 56 10 15 3 8 2 3 Name: Overall Qual, dtype: int64 . Overall Qual 변수는 전반적 재료와 마감품질 변수입니다. . 이 변수는 2부터 10까지 값으로 표현되어있으며 중앙값인 5,6,7에 변수들이 몰려있는 걸 알 수 있습니다. . 제가 아는 통계이론으로, 개수가 30 이하의 범주는 병합하는 것이 좋기 때문에 병합을 생각하고 그래프를 그려보겠습니다. . def discrete_plot(variable): plt.figure(figsize=(12,8)) sns.violinplot(x= train[variable], y= train[&#39;target&#39;]) plt.tight_layout(rect=[0, 0.03, 1, 0.95]) plt.show() discrete_plot(&#39;Overall Qual&#39;) . Overall Qual 변수는 재료 품질변수이기 때문에 값이 클 수록 집 값의 분포 또한 높게 나오는 걸 쉽계 예측할 수 있습니다. . 예측한 것과 실제 데이터가 일치한 지를 시각화를 해보았습니다. 예상한 결과와 크게 다르지 않습니다. . 또한 변수 값이 커질수록 박스 플랏이 길어진다, 즉 분산이 커지는 것도 관찰할 수 있습니다. . 앞서 말한대로 표본 개수가 적은 2,3 그리고 10 항목은 4와 9 항목과 병합하겠습니다. . 이때 테스트 데이터의 정확한 항목을 모르기 때문에 코딩은 9 이상 값은 9로, 4이하 값은 4로 하겠습니다. . 이후 연속형 변수들을 표준화 할 것이기 때문에 이 변수도 평균을 0과 근사하게 하기 위해 모든 값을 6.5로 빼주겠습니다. . train[&#39;Overall Qual&#39;][train[&#39;Overall Qual&#39;] &gt; 9] = 9 train[&#39;Overall Qual&#39;][train[&#39;Overall Qual&#39;] &lt; 4] = 4 train[&#39;Overall Qual&#39;] = train[&#39;Overall Qual&#39;] - 6.5 test[&#39;Overall Qual&#39;][test[&#39;Overall Qual&#39;] &gt; 9] = 9 test[&#39;Overall Qual&#39;][test[&#39;Overall Qual&#39;] &lt; 4] = 4 test[&#39;Overall Qual&#39;] = test[&#39;Overall Qual&#39;] - 6.5 . Exter Qual &#48320;&#49688; . train[&#39;Exter Qual&#39;].value_counts() . TA 808 Gd 485 Ex 49 Fa 8 Name: Exter Qual, dtype: int64 . test[&#39;Exter Qual&#39;].value_counts() . TA 794 Gd 489 Ex 58 Fa 9 Name: Exter Qual, dtype: int64 . Exter Qual변수는 외관 재료 품질 변수입니다. 4등급으로 나눠진 것 같아요. . 구체적으로 Fair -&gt; Typical/Average -&gt; Good -&gt; Excellent 순인 것 같습니다. (yun99님 코드 공유 내용을 참고했습니다.) . TA, Gd. 중간 부분 등급이 많이 관찰된 모습입니다. 다만 Fa는 8개로 표본 개수가 많이 부족해보입니다. . discrete_plot(&#39;Exter Qual&#39;) . 앞서 언급한 등급 순서대로 집값이 높은 것을 관찰할 수 있습니다. 예상한 결과와 같죠. . Fa 항목이 표본이 작아 걱정됬는데 합리적인 결과가 나오긴 한 것 같아요. . 하지만 TA 항목과 큰 차이가 없기 때문에 항목 개수를 줄여준다는 차원에서 두 항목을 병합하겠습니다. . mapping={&#39;Ex&#39;:1, &#39;Gd&#39;:0, &#39;TA&#39;:-1, &#39;Fa&#39;:-1, &#39;Po&#39;:-1} train[&#39;Exter Qual&#39;] = train[&#39;Exter Qual&#39;].map(mapping) test[&#39;Exter Qual&#39;] = test[&#39;Exter Qual&#39;].map(mapping) . (jujukwakwkak님의 코드 공유 내용을 참고했습니다.) . 범주형 값이기 때문에 분석 가능한 숫자 값으로 변환했습니다. 좋은 항목일 수록 큰 값을 배치했는데 일종의 라벨-인코딩입니다. . 추후에 연속형 변수들을 표준화 할 예정이기 때문에 이 변수도 표준화된 값과 비슷한 형태를 유지하기 위해 값을 1, 0, -1 값을 썼습니다. . 또한 Ta보다 아랫 등급의 항목들은 앞서 설명한 대로 Ta와 같은 값으로 취급했습니다. . Garage Cars &#48320;&#49688; . train[&#39;Garage Cars&#39;].value_counts() . 2 794 1 372 3 172 4 11 5 1 Name: Garage Cars, dtype: int64 . Garage Cars 변수는 차고 자리 개수를 나타냅니다. 2대가 보편적인 값으로 보입니다. . 0이 없는걸로 봐서 표본에 있는 집은 모두 차고지가 있는 집인 것 같아요. 우리나라랑 주택 환경이 다른것 같습니다. . 4, 5 값은 표본이 많이 작기 때문에 값이 튈 수 있습니다. 1대, 2대, 3대 이상 방식으로 범주를 병합하는게 좋겠습니다. . discrete_plot(&#39;Garage Cars&#39;) . 차고 자리 개수가 많을 수록 더 높은 집값을 가지는 것 같아요. 우리가 쉽게 예측할 수 있는 결과입니다. . 4,5 항목은 개수가 11개, 1개로 작기 때문에 값이 튀었다고 생각하고 3 항목에 포함시키는 방향이 맞을 것 같습니다. . train[&#39;Garage Cars&#39;][train[&#39;Garage Cars&#39;] &gt; 3] = 3 train[&#39;Garage Cars&#39;] = train[&#39;Garage Cars&#39;] - 2 test[&#39;Garage Cars&#39;][test[&#39;Garage Cars&#39;] &gt; 3] = 3 test[&#39;Garage Cars&#39;] = test[&#39;Garage Cars&#39;] - 2 . 테스트 데이터에 어느 값이 있을 지 모르기 때문에 (5보다 큰 값이 있을수도) 그 부분을 고려하여 코딩했습니다. . 마찬가지로 평균을 0에 근사시키기 위해 전체 값에 2를 빼주었습니다. . Kitchen Qual &#48320;&#49688; . train[&#39;Kitchen Qual&#39;].value_counts() . TA 660 Gd 560 Ex 107 Fa 23 Name: Kitchen Qual, dtype: int64 . test[&#39;Kitchen Qual&#39;].value_counts() . TA 666 Gd 566 Ex 94 Fa 23 Po 1 Name: Kitchen Qual, dtype: int64 . Kitchen Qual 변수는 부엌 품질 변수입니다. 앞서 다룬 외관 품질 변수와 항목이 같습니다. . 이 변수 또한 중간 등급의 개수가 많이 나왔습니다. 반면 최하 등급인 Fa 항목은 23개로 개수가 많이 낮습니다. . 테스트 데이터에서 특이하게 Po 항목이 1개 있는데 &#39;poor&#39;로 Fa보다 낮은 등급이라고 생각하고 Fa 항목과 같이 취급하겠습니다. . discrete_plot(&#39;Kitchen Qual&#39;) . 앞선 변수와 마찬가지로 품질이 좋을 수록 집값이 더 높게 나오는 쉽게 예측 가능한 결과가 나왔습니다. . Fa 항목의 개수가 23개로 작은 편인데 윗 등급인 TA와 모양에 큰 차이가 없으므로 두 항목을 병합하겠습니다. . mapping={&#39;Ex&#39;:1, &#39;Gd&#39;:0, &#39;TA&#39;:-1, &#39;Fa&#39;:-1, &#39;Po&#39;:-1} train[&#39;Kitchen Qual&#39;] = train[&#39;Kitchen Qual&#39;].map(mapping) test[&#39;Kitchen Qual&#39;] = test[&#39;Kitchen Qual&#39;].map(mapping) . Bsmt Qual &#48320;&#49688; . train[&#39;Bsmt Qual&#39;].value_counts() . TA 605 Gd 582 Ex 134 Fa 28 Po 1 Name: Bsmt Qual, dtype: int64 . Bsmt Qual 변수는 지하실 높이를 나타내는 변수라고 소개되어있으나 윗 품질 변수와 마찬가지로 생각하면 될 것 같습니다. . 저는 지하실 품질이라고 생각하고 변수를 분석하겠습니다. . 마찬가지로 중간품질의 개수가 많으며 Fa, Po 항목이 28개, 1개로 개수가 많이 적은 모습입니다. . discrete_plot(&#39;Bsmt Qual&#39;) . 계속 비슷한 얘기를 하는 것 같은데 품질이 높을 수록 집값이 비싼 쉽계 예측되는 결과를 보여줍니다. . Ta 항목과 Fa, Po 항목간 값의 차이가 크진 않다고 생각해서 항목의 개수가 작은 Fa, Po 항목은 Ta 항목과 병합하겠습니다. . mapping={&#39;Ex&#39;:1, &#39;Gd&#39;:0, &#39;TA&#39;:-1, &#39;Fa&#39;:-1, &#39;Po&#39;:-1} train[&#39;Bsmt Qual&#39;] = train[&#39;Bsmt Qual&#39;].map(mapping) test[&#39;Bsmt Qual&#39;] = test[&#39;Bsmt Qual&#39;].map(mapping) . Full Bath &#48320;&#49688; . train[&#39;Full Bath&#39;].value_counts() . 2 703 1 612 3 27 0 6 4 2 Name: Full Bath, dtype: int64 . Full Bath 변수는 지상층 화장실 개수 변수 입니다. 한국 집과 비슷하게 화장실이 1, 2개인 집이 대부분이네요. . 화장실이 3개있는 집이 27개로 꽤 적은 표본 개수이며 4개 혹은 0개 있는 집의 개수는 각각 6개/2개로 표본이 매우 적습니다. . 표본이 적은 항목은 병합을 해주는 것이 분석에도 용이하고 직관성도 올라가기 때문에 병합을 고려하고 밑 그래프로 값을 관찰하겠습니다. . discrete_plot(&#39;Full Bath&#39;) . 우선 표본이 많은 화장실 개수가 1개, 2개인 두 항목의 집값을 비교해보면 직관적 관찰로는 화장실이 2개인 집이 유의미하게 집값이 비싼 것 같아요. . 큰 차이는 아닙니다만, 표본에 개수가 6~7백개인데 저정도 차이면 아마 통계적 가설검정을 한다면 꽤 유의미한 수치일 것입니다. . 0개, 3개, 4개인 항목또한 화장실 개수가 많아질수록 집 값의 평균값이 증가하는게 보이지만, 표본에 개수가 많이 작죠. . 0 항목은 1 항목과, 2보다 큰 항목은 2 항목과 병합을 하겠습니다. 테스트 데이터에 4보다 큰 값이 있을 수 있으니 유의해서 코딩하겠습니다. . 또한 평균을 0에 근사시키기 위해 값을 -1과 1로 두겠습니다. . train[&#39;Full Bath&#39;][train[&#39;Full Bath&#39;] &lt;= 1] = -1 train[&#39;Full Bath&#39;][train[&#39;Full Bath&#39;] &gt;= 2] = 1 test[&#39;Full Bath&#39;][test[&#39;Full Bath&#39;] &lt;= 1] = -1 test[&#39;Full Bath&#39;][test[&#39;Full Bath&#39;] &gt;= 2] = 1 . &#50672;&#49549;&#54805; &#48320;&#49688; &#44288;&#52272; . &#50672;&#49549;&#54805; &#48320;&#49688; &#55176;&#49828;&#53664;&#44536;&#47016; . 연속형 변수는 총 7개로 지상층 면적, 차고 면적, 지하층 면적, 1층 면적, 완공 연도, 리모델링 연도, 차고 완공 연도 로 구성되어 있습니다. . 크게 4개의 면적과 3개의 연도로 구분할 수 있을 것 같아요. . continuous_names = [&#39;Gr Liv Area&#39;, &#39;Garage Area&#39;, &#39;Total Bsmt SF&#39;, &#39;1st Flr SF&#39;, &#39;Year Built&#39;, &#39;Year Remod/Add&#39;, &#39;Garage Yr Blt&#39;] plt.figure(figsize=(20,15)) plt.suptitle(&quot;Histogram&quot;, fontsize=40) for i in range(len(continuous_names)): plt.subplot(2,4,i+1) plt.title(continuous_names[i]) plt.hist(train[continuous_names[i]]) . 연속형 변수별 히스토그램입니다. 대체로 면적 변수는 오른쪽 꼬리가 긴 형태이며 연도 변수는 최근 년도의 빈도가 높은 모습입니다. . 차고 완공 연도 변수의 경우 2200의 값이 있는데 연도 변수이므로 이상치로 생각해야겠습니다. . train[train[&#39;Garage Yr Blt&#39;] &gt; 2020] . id Overall Qual Gr Liv Area Exter Qual Garage Cars Garage Area Kitchen Qual Total Bsmt SF 1st Flr SF Bsmt Qual Full Bath Year Built Year Remod/Add Garage Yr Blt target . 254 255 | 1.5 | 1564 | 0 | 0 | 502 | 1 | 1546 | 1564 | 0 | 1 | 2006 | 2007 | 2207 | 267300 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 이상치로 추정되는 변수를 직접 관찰해보면 리모델링된 년도가 2007년 이므로 2007년 입력을 잘못해 2207로 입력한 것 같아요. . 이에 맞게 수정해주었습니다. . train[&#39;Garage Yr Blt&#39;][train[&#39;Garage Yr Blt&#39;] &gt; 2020] = 2007 . &#50672;&#49549;&#54805; &#48320;&#49688; &#49345;&#44288;&#44228;&#49688; &#54665;&#47148; . plt.figure(figsize=(15,10)) ax = sns.heatmap(train[continuous_names+[&#39;target&#39;]].corr(), annot=True) plt.show() . 전반적으로 상관계수가 상당히 높은 편입니다. 특히 면적과 연도를 나타내는 변수끼리 상관계수가 높습니다. . 먼저 눈에 띄는 부분은 Total Bsmt SF 와 1st Flr SF의 상관계수가 0.87로 너무나도 높다는 점 입니다. 이후에 자세히 관찰해보죠. . 또 완공 년도와 리모델링 년도/차고 완공 년도 간 상관계수도 상당히 높은 편인데 이것 또한 자세히 살펴보겠습니다. . &#47732;&#51201; &#48320;&#49688;&#46308; . print(sum(train[&#39;Total Bsmt SF&#39;] &gt; train[&#39;1st Flr SF&#39;])) print(sum(train[&#39;Total Bsmt SF&#39;] == train[&#39;1st Flr SF&#39;])) print(sum(train[&#39;Total Bsmt SF&#39;] &lt; train[&#39;1st Flr SF&#39;])) print(train[&#39;target&#39;][train[&#39;Total Bsmt SF&#39;] &gt; train[&#39;1st Flr SF&#39;]].mean()) print(train[&#39;target&#39;][train[&#39;Total Bsmt SF&#39;] == train[&#39;1st Flr SF&#39;]].mean()) print(train[&#39;target&#39;][train[&#39;Total Bsmt SF&#39;] &lt; train[&#39;1st Flr SF&#39;]].mean()) . 41 690 619 225367.0487804878 178311.39420289855 192849.1292407108 . Total Bsmt SF 변수는 지상층 넓이, 1st Flr SF 변수는 1층 넓이 라고 소개되어 있습니다. . 직관적으로 생각했을때 지상층 넓이가 1층, 2층까지 포함하는게 아닌가 생각했는데 데이터를 관찰하니 예상과 다른 결과가 나옵니다. . 지상층 넓이 &gt; 1층 넓이에 해당하는 개수가 41개로 매우 적고 두 변수가 같은 경우가 690개, 지상층 &lt; 1층 개수는 619개 입니다. . 잘 이해가 되지 않는 결과인데, 세 집단간 집 값의 평균값이 유의미하게 차이나는 것으로 관찰됩니다. . 지상층 &gt; 1층, 지상층 == 1층, 지상층 &lt; 1층 세 그룹으로 나눈 변수를 추가하는 것도 괜찮을 것 같아요. . 또 두 변수간 상관계수가 높기 때문에 한 변수만 써야하는데, 타겟 값과 상관계수가 조금 높은 Total Bsmt SF 변수를 사용하겠습니다. . train[&#39;1st Flr SF&#39;][train[&#39;Total Bsmt SF&#39;] &gt; train[&#39;1st Flr SF&#39;]] = -1 train[&#39;1st Flr SF&#39;][train[&#39;Total Bsmt SF&#39;] == train[&#39;1st Flr SF&#39;]] = 0 train[&#39;1st Flr SF&#39;][train[&#39;Total Bsmt SF&#39;] &lt; train[&#39;1st Flr SF&#39;]] = 1 train.rename(columns={&#39;1st Flr SF&#39;:&#39;1st - Total&#39;}, inplace=True) test[&#39;1st Flr SF&#39;][test[&#39;Total Bsmt SF&#39;] &gt; test[&#39;1st Flr SF&#39;]] = -1 test[&#39;1st Flr SF&#39;][test[&#39;Total Bsmt SF&#39;] == test[&#39;1st Flr SF&#39;]] = 0 test[&#39;1st Flr SF&#39;][test[&#39;Total Bsmt SF&#39;] &lt; test[&#39;1st Flr SF&#39;]] = 1 test.rename(columns={&#39;1st Flr SF&#39;:&#39;1st - Total&#39;}, inplace=True) . 1st Flr SF 변수를 활용하여 원하는 코딩을 했습니다. . &#50672;&#46020; &#48320;&#49688;&#46308; . print(sum(train[&#39;Year Built&#39;] == train[&#39;Year Remod/Add&#39;])) print(sum(train[&#39;Year Built&#39;] &gt; train[&#39;Year Remod/Add&#39;])) print(sum(train[&#39;Year Built&#39;] &lt; train[&#39;Year Remod/Add&#39;])) print(sum(train[&#39;Year Built&#39;] == train[&#39;Garage Yr Blt&#39;])) print(sum(train[&#39;Year Built&#39;] &gt; train[&#39;Garage Yr Blt&#39;])) print(sum(train[&#39;Year Built&#39;] &lt; train[&#39;Garage Yr Blt&#39;])) . 743 0 607 1075 11 264 . 완공 연도와 리모델링 연도가 같다면 리모델링이 없었고, 리모델링 연도가 크다면 리모델링이 있던 집입니다. . 리모델링 연도가 더 작다, 즉 완공연도보다 리모델링이 더 빨랐다는 비 상식적인 데이터는 다행이 관찰되지 않습니다. . 완공 연도와 차고 완공 연도가 같은 경우는 1075개로 매우 많은 것이 두 변수에 상관계수 관찰에서 높은 값(0.83)의 이유인 것 같습니다. . 특이하게 차고 완공 연도가 완공 연도보다 이른 데이터가 11개 있는데 차고를 미리 지은 집으로 생각할 수 있습니다. . 다만 개수가 작기 때문에 차고 완공 연도와 완공 연도가 같은 경우와 같게 생각을 하겠습니다. . print(train[&#39;target&#39;][train[&#39;Year Built&#39;] &lt; train[&#39;Year Remod/Add&#39;]].mean()) print(train[&#39;target&#39;][train[&#39;Year Built&#39;] == train[&#39;Year Remod/Add&#39;]].mean()) print(train[&#39;target&#39;][train[&#39;Year Built&#39;] &lt; train[&#39;Garage Yr Blt&#39;]].mean()) print(train[&#39;target&#39;][train[&#39;Year Built&#39;] &gt;= train[&#39;Garage Yr Blt&#39;]].mean()) . 183097.6589785832 189109.34454912518 151515.625 194888.02670349908 . 리모델링을 한 주택과 안한 주택간 집 값 차이가 약 6천으로 크진 않으나 표본이 743, 607이기 때문에 유의미 하다고 생각하겠습니다. . 또 차고 완공 연도가 늦은 주택의 경우 집 값이 다소 낮은 것으로 관찰됩니다. . 두 요소를 각각 변수로 만들고, 집 값 변수와 상관계수가 가장 높은 완공 연도 변수를 제외하고 나머지 두 변수는 사용하지 않겠습니다. . train[&#39;Year Remod/Add&#39;][train[&#39;Year Built&#39;] &lt; train[&#39;Year Remod/Add&#39;]] = -1 train[&#39;Year Remod/Add&#39;][train[&#39;Year Built&#39;] == train[&#39;Year Remod/Add&#39;]] = 1 train.rename(columns={&#39;Year Remod/Add&#39;:&#39;Remod&#39;}, inplace=True) train[&#39;Garage Yr Blt&#39;][train[&#39;Year Built&#39;] &gt;= train[&#39;Garage Yr Blt&#39;]] = 1 train[&#39;Garage Yr Blt&#39;][train[&#39;Year Built&#39;] &lt; train[&#39;Garage Yr Blt&#39;]] = -1 train.rename(columns={&#39;Garage Yr Blt&#39;:&#39;Garage&#39;}, inplace=True) test[&#39;Year Remod/Add&#39;][test[&#39;Year Built&#39;] &lt; test[&#39;Year Remod/Add&#39;]] = -1 test[&#39;Year Remod/Add&#39;][test[&#39;Year Built&#39;] == test[&#39;Year Remod/Add&#39;]] = 1 test.rename(columns={&#39;Year Remod/Add&#39;:&#39;Remod&#39;}, inplace=True) test[&#39;Garage Yr Blt&#39;][test[&#39;Year Built&#39;] &gt;= test[&#39;Garage Yr Blt&#39;]] = 1 test[&#39;Garage Yr Blt&#39;][test[&#39;Year Built&#39;] &lt; test[&#39;Garage Yr Blt&#39;]] = -1 test.rename(columns={&#39;Garage Yr Blt&#39;:&#39;Garage&#39;}, inplace=True) . train[&#39;Remod&#39;].value_counts() . 1 743 -1 607 Name: Remod, dtype: int64 . train[&#39;Garage&#39;].value_counts() . 1 1086 -1 264 Name: Garage, dtype: int64 . 앞선 방식과 비슷하게 코딩했습니다. value_counts 함수를 통해 원하는 목적대로 코드가 작동했는지 확인했습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . from sklearn.preprocessing import StandardScaler continuous_names = [&#39;Gr Liv Area&#39;, &#39;Garage Area&#39;, &#39;Total Bsmt SF&#39;, &#39;Year Built&#39;] scaler = StandardScaler() train_scaler = scaler.fit_transform(train[continuous_names]) train[continuous_names] = pd.DataFrame(data=train_scaler, columns=continuous_names) test_scaler = scaler.transform(test[continuous_names]) test[continuous_names] = pd.DataFrame(data=test_scaler, columns=continuous_names) . 남은 연속형 변수들을 모두 표준화 시켜줍니다. 표준화의 장점은 변수 간 스케일 차이가 모델에 영향을 주지 않기 위함인데요. . 머신러닝 방식에 따라 표준화가 반드시 필요할 수도 있고 아닐수도 있지만, 모든 머신러닝 방식에 적용하기 위해 표준화를 시켜줬습니다. . 이전에 범주형 변수에서도 레이블 인코딩 방식을 적용할때 평균을 0에 가까이 하기 위해 노력한 것도 표준화의 일종입니다. . 또 중요한 점이 스케일링을 진행할 때 테스트 데이터는 모르는 상태로 진행해야한다는 것 입니다. . 그래서 스케일러를 만들때는 트레인 데이터만을 사용해서 만들었고, 테스트 데이터는 스케일러에 적용만 했습니다. . 이 부분에 더 자세한 설명은 제 이전글을 참고하시면 좋을 것 같아요! . train.describe() . id Overall Qual Gr Liv Area Exter Qual Garage Cars Garage Area Kitchen Qual Total Bsmt SF 1st - Total Bsmt Qual Full Bath Year Built Remod Garage target . count 1350.000000 | 1350.000000 | 1.350000e+03 | 1350.000000 | 1350.000000 | 1.350000e+03 | 1350.000000 | 1.350000e+03 | 1350.000000 | 1350.000000 | 1350.000000 | 1.350000e+03 | 1350.000000 | 1350.000000 | 1350.000000 | . mean 675.500000 | -0.291852 | -2.222091e-16 | -0.568148 | -0.139259 | -7.056084e-17 | -0.426667 | 1.773478e-16 | 0.428148 | -0.370370 | 0.084444 | 3.062735e-15 | 0.100741 | 0.608889 | 186406.312593 | . std 389.855743 | 1.287020 | 1.000371e+00 | 0.564078 | 0.626697 | 1.000371e+00 | 0.635169 | 1.000371e+00 | 0.552996 | 0.657293 | 0.996797 | 1.000371e+00 | 0.995281 | 0.793549 | 78435.424758 | . min 1.000000 | -2.500000 | -2.120771e+00 | -1.000000 | -1.000000 | -2.101280e+00 | -1.000000 | -2.546444e+00 | -1.000000 | -1.000000 | -1.000000 | -3.174022e+00 | -1.000000 | -1.000000 | 12789.000000 | . 25% 338.250000 | -1.500000 | -7.582801e-01 | -1.000000 | -1.000000 | -7.004781e-01 | -1.000000 | -6.945214e-01 | 0.000000 | -1.000000 | -1.000000 | -6.139801e-01 | -1.000000 | 1.000000 | 135000.000000 | . 50% 675.500000 | -0.500000 | -1.396189e-01 | -1.000000 | 0.000000 | -9.416111e-02 | -1.000000 | -1.918197e-01 | 0.000000 | 0.000000 | 1.000000 | 1.028315e-01 | 1.000000 | 1.000000 | 165375.000000 | . 75% 1012.750000 | 0.500000 | 5.354708e-01 | 0.000000 | 0.000000 | 4.494335e-01 | 0.000000 | 5.908844e-01 | 1.000000 | 0.000000 | 1.000000 | 9.903125e-01 | 1.000000 | 1.000000 | 217875.000000 | . max 1350.000000 | 2.500000 | 6.078799e+00 | 1.000000 | 1.000000 | 5.153617e+00 | 1.000000 | 4.108494e+00 | 1.000000 | 1.000000 | 1.000000 | 1.263384e+00 | 1.000000 | 1.000000 | 745000.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 모든 변수가 이쁘게 표준화 된 것을 확인할 수 있습니다. . train_label = np.log1p(train[&#39;target&#39;]) train.drop([&#39;target&#39;, &#39;id&#39;], axis = 1, inplace = True) test.drop([&#39;id&#39;], axis = 1, inplace = True) . 목푯값을 로그변환 한 뒤 따로 저장해주고, 의미없는 id 변수는 제거해줍니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;target&#39;] = np.expm1(rf.predict(test)) sample_submission.to_csv(&#39;house_1.csv&#39;,index=False) . 간단한 랜덤포레스트 회귀 모델을 사용했습니다. . 리더보드 결과는 0.103정도 나옵니다. 다른 모델을 사용하거나, 앙상블을 진행한다면 더 좋은 결과가 나올 것으로 기대됩니다! .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/eda/scale/regression/2022/01/30/dacon_house.html",
            "relUrl": "/dacon/jupyter/eda/scale/regression/2022/01/30/dacon_house.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "[처음 시작하는 딥러닝] 4. 밑바닥부터 만들어보는 RNN",
            "content": ". &#51088;&#46041;&#48120;&#48516; . import numpy as np a = np.array([2,3,1,0]) print(a) print(&#39;__add__ 함수 사용:&#39;, a.__add__(4)) print(&#39;+ 연산자 사용:&#39;, a + 4) . [2 3 1 0] __add__ 함수 사용: [6 7 5 4] + 연산자 사용: [6 7 5 4] . 파이썬에서는 + 연산자를 사용할 때 내부에서 add 함수가 호출되는 방식입니다. . 다른 연산자도 마찬가지로 연결된 함수를 불러오는 방식으로 실행됩니다. . from typing import Union, List Numberable = Union[float, int] # 정수, 소수 둘다 함께하는 자료형 생성, 파이토치에서는 tensor가 비슷한 역할을 함. def ensure_number(num: Numberable): # -&gt; NumberWithGrad, 자료형으로 출력해줌. if isinstance(num, NumberWithGrad): # 자료형 확인함수 return num else: return NumberWithGrad(num) class NumberWithGrad(object): def __init__(self, num: Numberable, depends_on : List[Numberable] = None, creation_op: str = &#39;&#39;): self.num = num # 원래 값(숫자) 자체를 저장. self.grad = None self.depends_on = depends_on or [] self.creation_op = creation_op def __add__(self, other: Numberable = None): # NumberWithGrad 출력 return NumberWithGrad(self.num + ensure_number(other).num, # 입력된 값 NumberWithGrad 자료형으로 변환 후 더하기. depends_on = [self, ensure_number(other)], # 3 + 4이면 [3, 4]로 저장(NumberWithGrad 자료형으로) creation_op = &#39;add&#39;) def __mul__(self, other: Numberable = None): # NumberWithGrad 출력 return NumberWithGrad(self.num * ensure_number(other).num, depends_on = [self, ensure_number(other)], creation_op = &#39;mul&#39;) def backward(self, backward_grad: Numberable = None) -&gt; None: if backward_grad is None: # 이 함수가 처음 호출될때 self.grad = 1 # 이부분에서 기울기가 누적. else: if self.grad is None: # 기울기 정보가 아직 없다면 self.grad = backward_grad # backward_grad로 설정 else: self.grad += backward_grad # 있다면 기존 기울기에 backward_grad를 더함. # self.grad를 역방향으로 전달. # 둘 중 어느 요소를 증가시켜도 출력이 같은 값만큼 증가함. if self.creation_op == &#39;add&#39;: self.depends_on[0].backward(self.grad) self.depends_on[1].backward(self.grad) if self.creation_op == &#39;mul&#39;: # 첫번째 요소에 대한 미분 계산 new = self.depends_on[1] * self.grad # 미분을 역방향으로 전달 self.depends_on[0].backward(new.num) # 두번째 요소에 대한 미분 계산 new = self.depends_on[0] * self.grad #미분을 역방향으로 전달. self.depends_on[1].backward(new.num) a = NumberWithGrad(3) b = a * 4 c = b + 5 c.backward() print(a.grad) . 4 . 자동 미분을 간단하게 구현한 함수입니다. 저도 이해하는데 꽤 걸렸는데 단기간에 이해하기 쉽진 않습니다. . 우선 Numberable 자료형을 생성합니다. int, float 자료형을 통합했다고 보면 될 것 같아요. 또 ensure_number 함수는 입력되는 모든 값을 뒤에 나오는 NumberWithGrad 자료형으로 변환해준다고 생각하면 됩니다. . NumberWithGrad 클레스도 간단하게 하나의 자료형으로 이해하시는게 좋습니다. a = NumberWithGrad(3)은 3이라는 값을 NumberWithGrad 자료형으로 선언했다고 생각하면 됩니다. . 이후 a * 4 코드가 진행되면 파이썬에서는 a. mul (4) 함수를 써서 곱하기를 진행하는데, NumberWithGrad 자료형 내부에 mul 함수가 오버라이딩 되어있습니다. . 오버라이딩 된 NumberWithGrad 내부 함수는 값 부분에 정상적인 곱하기 연산을 수행하고, depends_on 부분에 연산에 사용된 숫자를 NumberWithGrad 자료형으로 기록, creation_op 부분에 곱하기 연산이 수행됬다는 것을 기록합니다. . 즉 b = a * 4 연산을 통해 b는 NumberWithGrad 자료형으로 바뀌고 연산에 사용된 숫자와 연산기호가 NumberWithGrad 자료형 내부에 저장됩니다. . c = b + 5 부분에서 c도 마찬가지로 NumberWithGrad 자료형으로 저장되며 b와 5가 depends_on 부분에 저장되는데 b 같은 경우 이전 NumberWithGrad 자료형 그대로 저장되는 것을 주목합시다. . 그리고 c.backward() 함수를 실행하면 backward_grad 입력값에 아무것도 안 넣었기 때문에 첫번째 if문에 걸립니다. c의 grad 값은 1로 선언됩니다. . 그 뒤 c를 만들기 위해 사용했던 연산기호가 + 이기 때문에 세번째 if문에 걸리게 되고 c를 만들어준 두 연산자가 기록되어 있는 depends_on을 이용해 다시 backward 함수를 실행시킵니다. . backward 함수를 실행시킬때 함수 입력값에 c.grad을 넣고 함수를 실행합니다. 상세히 말하면 c.depends_on[0] = b이기 때문에 b.backward(c.grad)를 실행합니다. . 함수 입력값에 c.grad가 있기 때문에 두번째 if문에 걸리게 되고 b.grad 값에 1을 넣게 됩니다. 그 후 아까 c에서 벌어졌던 연산을 다시 반복합니다. . 이후 내용을 한번 더 상세하게 설명하면 b를 만들기 위해 사용했던 연산기호가 * 이므로 네번째 if문에 걸리게 되고 여기서는 b.grad에 서로 반대되는 값을 곱해준 것을 backward 함수에 전달합니다. . 과정이 조금 복잡해서 저도 되집어본다고 생각하고 클레스 구조를 간단히 서술했는데 관심있는 분들은 스스로 코드를 해석해보시면서 제 설명을 참고하시면 보다 쉽게 익힐 수 있을거 같아요. 이해 안되는 부분은 댓글달아주시면 아는 선에서 설명드리겠습니다. . a = NumberWithGrad(3) b = a * 4 c = b + 3 d = (a + 2) # a 다시 사용. e = c * d e.backward() print(a.grad) . 35 . 아까보다 한단계 복잡한 자동미분 예시입니다. 실제 도함수를 통해 계산한 결과와 일치하는 것을 볼 수 있어요. . 예제는 d = (4a + 3) (a + 2) 입니다. . 자동 미분이 필요한 이유가 순방향 계산 과정의 중간 결과를 재사용 할 수 있습니다. 윗 예제도 a를 두번 써도 정상적으로 실행됩니다. . 이전에 했던 합성함수 미분법을 활용했을때는 윗 예제와 같은 식을 설명할 수 없기 때문에 자동 미분 방법이 중요합니다. . &#54596;&#50836;&#54620; &#46972;&#51060;&#48652;&#47084;&#47532; &#51076;&#54252;&#53944;, &#54876;&#49457;&#54868; &#54632;&#49688; &#49440;&#50616; . import numpy as np from numpy import ndarray from typing import Dict, List, Tuple import matplotlib.pyplot as plt from IPython import display plt.style.use(&#39;seaborn-white&#39;) %matplotlib inline from copy import deepcopy from collections import deque from scipy.special import logsumexp def assert_same_shape(output, output_grad): assert output.shape == output_grad.shape, &#39;&#39;&#39; 두 ndarray의 모양이 같아야 하는데, 첫 번째 ndarray의 모양은 {0}이고 두 번째 ndarray의 모양은 {1}이다. &#39;&#39;&#39;.format(tuple(output_grad.shape), tuple(output.shape)) return None def assert_dim(t, dim): assert len(t.shape) == dim, &#39;&#39;&#39; 이 텐서는 {0}차원이어야 하는데, {1}차원이다. &#39;&#39;&#39;.format(dim, len(t.shape)) return None def sigmoid(x: ndarray): return 1 / (1 + np.exp(-x)) def dsigmoid(x: ndarray): return sigmoid(x) * (1 - sigmoid(x)) def tanh(x: ndarray): return np.tanh(x) def dtanh(x: ndarray): return 1 - np.tanh(x) * np.tanh(x) def softmax(x, axis=None): return np.exp(x - logsumexp(x, axis=axis, keepdims=True)) def batch_softmax(input_array: ndarray): out = [] for row in input_array: out.append(softmax(row, axis=1)) return np.stack(out) . 필요한 라이브러리와 활성화 함수를 선언했습니다. . &#50741;&#54000;&#47560;&#51060;&#51200;, &#49552;&#49892;&#54632;&#49688; . class RNNOptimizer(object): def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -&gt; None: self.lr = lr self.gradient_clipping = gradient_clipping self.first = True def step(self) -&gt; None: for layer in self.model.layers: for key in layer.params.keys(): if self.gradient_clipping: np.clip(layer.params[key][&#39;deriv&#39;], -2, 2, layer.params[key][&#39;deriv&#39;]) self._update_rule(param=layer.params[key][&#39;value&#39;], grad=layer.params[key][&#39;deriv&#39;]) def _update_rule(self, **kwargs) -&gt; None: raise NotImplementedError() . RNNOptimizer 클레스를 통해 RNN에서 사용할 수 있는 옵티마이저의 기본 골격을 만들었습니다. . 코드 내 상세 내용은 저도 정확히 모르겠습니다. . class SGD(RNNOptimizer): def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -&gt; None: super().__init__(lr, gradient_clipping) def _update_rule(self, **kwargs) -&gt; None: update = self.lr*kwargs[&#39;grad&#39;] kwargs[&#39;param&#39;] -= update class AdaGrad(RNNOptimizer): def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -&gt; None: super().__init__(lr, gradient_clipping) self.eps = 1e-7 def step(self) -&gt; None: if self.first: self.sum_squares = {} for i, layer in enumerate(self.model.layers): self.sum_squares[i] = {} for key in layer.params.keys(): self.sum_squares[i][key] = np.zeros_like(layer.params[key][&#39;value&#39;]) self.first = False for i, layer in enumerate(self.model.layers): for key in layer.params.keys(): if self.gradient_clipping: np.clip(layer.params[key][&#39;deriv&#39;], -2, 2, layer.params[key][&#39;deriv&#39;]) self._update_rule(param=layer.params[key][&#39;value&#39;], grad=layer.params[key][&#39;deriv&#39;], sum_square=self.sum_squares[i][key]) def _update_rule(self, **kwargs) -&gt; None: # 이전 기울기의 제곱의 합을 계산 kwargs[&#39;sum_square&#39;] += (self.eps + np.power(kwargs[&#39;grad&#39;], 2)) # 이전 5개 기울기의 제곱의 합으로 학습률을 수정 lr = np.divide(self.lr, np.sqrt(kwargs[&#39;sum_square&#39;])) # 수정된 학습률을 적용 kwargs[&#39;param&#39;] -= lr * kwargs[&#39;grad&#39;] . 앞서 만든 RNNOptimizer를 상속받아 SGD와 AdaGrad 옵티마이저 클레스를 만들었습니다. . class Loss(object): def __init__(self): pass def forward(self, prediction: ndarray, target: ndarray) -&gt; float: assert_same_shape(prediction, target) self.prediction = prediction self.target = target self.output = self._output() return self.output def backward(self) -&gt; ndarray: self.input_grad = self._input_grad() assert_same_shape(self.prediction, self.input_grad) return self.input_grad def _output(self) -&gt; float: raise NotImplementedError() def _input_grad(self) -&gt; ndarray: raise NotImplementedError() class SoftmaxCrossEntropy(Loss): def __init__(self, eps: float=1e-9) -&gt; None: super().__init__() self.eps = eps self.single_class = False def _output(self) -&gt; float: out = [] for row in self.prediction: out.append(softmax(row, axis=1)) softmax_preds = np.stack(out) # 안정적인 계산을 위해 소프트맥스의 출력을 제한 self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps) # 손실을 실제로 계산 softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - (1.0 - self.target) * np.log(1 - self.softmax_preds) return np.sum(softmax_cross_entropy_loss) def _input_grad(self) -&gt; np.ndarray: return self.softmax_preds - self.target . 이 부분도 가벼운 마음으로 정리했습니다. 주가 아닌 부분이기 때문에 자세한 설명은 생략할께요. . &#49692;&#54872; &#49888;&#44221;&#47581; &#44396;&#54788; . class RNNLayer(object): def __init__(self, hidden_size, output_size, weight_scale = None): self.hidden_size = hidden_size # 은닉 뉴런수 self.output_size = output_size self.weight_scale = weight_scale self.start_H = np.zeros((1, hidden_size)) # 이 층에 내부상태 저장 self.first = True # 처음임을 선언 def _init_params(self, input_: ndarray): # input_ = x_seq_in self.vocab_size = input_.shape[2] # 사용되는 글자의 가짓수 if not self.weight_scale: self.weight_scale = 2 / (self.vocab_size + self.output_size) # 밑에 나오는 것은 파라미터를 저장하기 위한 형식입니다. # 키 value부분은 실제 파라미터, 키 deriv는 그에대한 기울깃값입니다. self.params = {} self.params[&#39;W_f&#39;] = {} self.params[&#39;B_f&#39;] = {} self.params[&#39;W_v&#39;] = {} self.params[&#39;B_v&#39;] = {} self.params[&#39;W_f&#39;][&#39;value&#39;] = np.random.normal(loc = 0.0, scale=self.weight_scale, size=(self.hidden_size + self.vocab_size, self.hidden_size)) self.params[&#39;B_f&#39;][&#39;value&#39;] = np.random.normal(loc = 0.0, scale=self.weight_scale, size=(1, self.hidden_size)) self.params[&#39;W_v&#39;][&#39;value&#39;] = np.random.normal(loc=0.0, scale=self.weight_scale, size=(self.hidden_size, self.output_size)) self.params[&#39;B_v&#39;][&#39;value&#39;] = np.random.normal(loc=0.0, scale=self.weight_scale, size=(1, self.output_size)) self.params[&#39;W_f&#39;][&#39;deriv&#39;] = np.zeros_like(self.params[&#39;W_f&#39;][&#39;value&#39;]) self.params[&#39;B_f&#39;][&#39;deriv&#39;] = np.zeros_like(self.params[&#39;B_f&#39;][&#39;value&#39;]) self.params[&#39;W_v&#39;][&#39;deriv&#39;] = np.zeros_like(self.params[&#39;W_v&#39;][&#39;value&#39;]) self.params[&#39;B_v&#39;][&#39;deriv&#39;] = np.zeros_like(self.params[&#39;B_v&#39;][&#39;value&#39;]) self.cells = [RNNNode() for x in range(input_.shape[1])] # input_shape[1] : 시간순서 # 시간 순서만큼 RNN 노드를 리스트형태로 만들어 cells에 저장. def _clear_gradients(self): for key in self.params.keys(): self.params[key][&#39;deriv&#39;] = np.zeros_like(self.params[key][&#39;deriv&#39;]) def forward(self, x_seq_in: ndarray): # 입력값은 배치, 순서(시간), 변수 크기 if self.first: # 처음이면 self._init_params(x_seq_in) # 윗 함수 실행할 것. self.first = False # 처음 아닌 상태 표시 batch_size = x_seq_in.shape[0] # 배치 크기 입력 H_in = np.copy(self.start_H) # 은닉층 내부 상태 저장. (1, 은닉층) H_in = np.repeat(H_in, batch_size, axis = 0) # 은닉층 배치크기만큼 확대. (배치크기, 은닉층) sequence_length = x_seq_in.shape[1] # 순서형 자료 개수 입력 x_seq_out = np.zeros((batch_size, sequence_length, self.output_size)) for t in range(sequence_length): # 순서형 자료 개수만큼 x_in = x_seq_in[:, t, :] # 특정 시점에 입력값들. # RNN 노드 내 forward함수 실행. H_in 값이 지속적으로 업데이트 됩니다. y_out, H_in = self.cells[t].forward(x_in, H_in, self.params) x_seq_out[:, t, :] = y_out # 출력층에 값을 넣어줍니다. self.start_H = H_in.mean(axis = 0, keepdims = True) # 마지막 은닉층 값을 기억합니다. return x_seq_out def backward(self, x_seq_out_grad: ndarray): batch_size = x_seq_out_grad.shape[0] h_in_grad = np.zeros((batch_size, self.hidden_size)) sequence_length = x_seq_out_grad.shape[1] x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size)) for t in reversed(range(sequence_length)): x_out_grad = x_seq_out_grad[:, t, :] grad_out, h_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, self.params) x_seq_in_grad[:, t, :] = grad_out return x_seq_in_grad . RNN 층을 직접 구현했습니다. 내용이 상당히 방대해 이해하기 쉽진 않습니다. . 쉽게 정리하면 입력되는 형태는 (배치크기, 순서, 특성 크기) 인 3차원 입니다. . 그 후 층 내부에서는 처음에 배치단위별로 한 순서씩 모든 특성크기가 입력 됩니다. . 층 내부 구조는 입력층, 히든층, 출력층으로 되어있으며 입력층은 입력된 특성크기 + 히든층 크기 로 구성되어있습니다. . 히든층은 사용자가 정의하며, 출력층은 원하는 결과물에 따라 개수를 정해주면 됩니다. . 이때 히든층은 지속적으로 업데이트되어 출력됩니다. 더 자세한 설명은 RNN 노드 부분에서 하겠습니다. . class RNNNode(object): def __init__(self): pass def forward(self, x_in: ndarray, H_in: ndarray, params_dict: Dict[str, Dict[str, ndarray]]) -&gt; Tuple[ndarray]: # params_dic 예시로 [B_f][value, ndarray] 로 생각하면 됩니다. self.X_in = x_in # (배치크기, 변수 크기) self.H_in = H_in # (배치크기, 은닉층 크기) self.Z = np.column_stack((x_in, H_in)) # column_stack 함수는 두 넘파이 배열을 입력받아 열을 기준(여기선 배치마다)으로 병합합니다. # 즉 Z는 (배치크기, 변수 + 은닉층 크기) 입니다. # 이 말은 입력값은 입력된 변수 + 이전 은닉층 값이라는 거죠. 이 부분이 핵심입니다. self.H_int = np.dot(self.Z, params_dict[&#39;W_f&#39;][&#39;value&#39;] + params_dict[&#39;B_f&#39;][&#39;value&#39;]) # W_f는 (은닉층 + 변수, 은닉층), B_f는 (1, 은닉층), 두 개 합 또한 넘파이 재활용 규칙으로 (은닉층 + 변수, 은닉층). # 출력값은 (배치, 은닉층) 이 됩니다. self.H_out = tanh(self.H_int) # 은닉층 값에 활성화 함수를 거칩니다. self.X_out = np.dot(self.H_out, params_dict[&#39;W_v&#39;][&#39;value&#39;] + params_dict[&#39;B_v&#39;][&#39;value&#39;]) # W_v는 (은닉층, 출력층), B_f는 (1, 출력층) 합은 재활용 규칙으로 (은닉층, 출력층) # 출력값은 (배치, 출력층)이 됩니다. return self.X_out, self.H_out # 출력층 값, 은닉층 값 돌려줍니다. def backward(self, X_out_grad: ndarray, H_out_grad: ndarray, params_dict: Dict[str, Dict[str, ndarray]]) -&gt; Tuple[ndarray]: assert_same_shape(X_out_grad, self.X_out) assert_same_shape(H_out_grad, self.H_out) params_dict[&#39;B_v&#39;][&#39;deriv&#39;] += X_out_grad.sum(axis = 0) params_dict[&#39;W_v&#39;][&#39;deriv&#39;] += np.dot(self.H_out.T, X_out_grad) dh = np.dot(X_out_grad, params_dict[&#39;W_v&#39;][&#39;value&#39;].T) dh += H_out_grad dH_int = dh * dtanh(self.H_int) params_dict[&#39;B_f&#39;][&#39;deriv&#39;] += dH_int.sum(axis = 0) params_dict[&#39;W_f&#39;][&#39;deriv&#39;] += np.dot(self.Z.T, dH_int) dz = np.dot(dH_int, params_dict[&#39;W_f&#39;][&#39;value&#39;].T) X_in_grad = dz[:, :self.X_in.shape[1]] H_in_grad = dz[:, self.X_in.shape[1]:] assert_same_shape(X_out_grad, self.X_out) assert_same_shape(H_out_grad, self.H_out) return X_in_grad, H_in_grad . 우선 역전파는 수리적으로 다소 어렵고 모델 구조 이해와 큰 연관이 없기 때문에 순전파 위주로 학습하겠습니다. . 여기서 주목할 건 학습할 때 입력값만 쓰는 것이 아니라 이전 은닉층 값도 사용한다는 것입니다. . 이전 은닉층 값은 이전 입력값과 연관이 있기 때문에 순서가 있는 데이터에서 상당히 유용합니다. . 나머지 부분은 코드 내 주석을 잘 읽어보시고, 이해가 안되시면 댓글 써주시면 아는 선에서 답변 드리겠습니다. . class RNNModel(object): def __init__(self, layers: List[RNNLayer], sequence_length, vocab_size, loss: Loss): self.layers = layers # 층을 리스트 단위로 입력 self.vocab_size = vocab_size self.sequence_length = sequence_length self.loss = loss for layer in self.layers: # 층마다 sequence_length 값 지정 setattr(layer, &#39;sequence_length&#39;, sequence_length) def forward(self, x_batch: ndarray): for layer in self.layers: # x_batch : (배치, 순서값, 특성값) x_batch = layer.forward(x_batch) return x_batch def backward(self, loss_grad: ndarray): for layer in reversed(self.layers): loss_grad = layer.backward(loss_grad) return loss_grad def single_step(self, x_batch, y_batch): # 순방향 계산 x_batch_out = self.forward(x_batch) # 손실 및 손실의 기울기 계산 loss = self.loss.forward(x_batch_out, y_batch) loss_grad = self.loss.backward() for layer in self.layers: layer._clear_gradients() # 역방향 계산 self.backward(loss_grad) return loss . 이 클래스는 입력과 목푯값으로 신경망을 실제로 학습해 손실을 계산합니다. . 층들을 리스트로 입력받으며 single_step 함수를 사용해 실제 연산을 진행합니다. . &#51088;&#50672;&#50612; &#44288;&#47144; &#49892;&#49845; . class RNNTrainer: def __init__(self, text_file: str, model: RNNModel, optim: RNNOptimizer, batch_size = 32): self.data = open(text_file, &#39;r&#39;).read() self.model = model self.chars = list(set(self.data)) self.vocab_size = len(self.chars) self.char_to_idx = {ch:i for i, ch in enumerate(self.chars)} self.idx_to_char = {i:ch for i, ch in enumerate(self.chars)} self.sequence_length = self.model.sequence_length self.batch_size = batch_size self.optim = optim setattr(self.optim, &#39;model&#39;, self.model) def _generate_inputs_targets(self, start_pos): inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int) targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int) for i in range(self.batch_size): inputs_indices[i, :] = np.array([self.char_to_idx[ch] for ch in self.data[start_pos + i: start_pos + self.sequence_length + i]]) targets_indices[i, :] = np.array([self.char_to_idx[ch] for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]]) return inputs_indices, targets_indices def _generate_one_hot_array(self, indices: ndarray): &#39;&#39;&#39; param indices: 모양이 (batch_size, sequence_length)인 넘파이 배열 return batch - 모양이 (batch_size, sequence_length, vocab_size)인 넘파이 배열 &#39;&#39;&#39; batch = [] for seq in indices: one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size)) for i in range(self.sequence_length): one_hot_sequence[i, seq[i]] = 1.0 batch.append(one_hot_sequence) return np.stack(batch) def sample_output(self, input_char, sample_length): &#39;&#39;&#39; 현재 학습된 모델로 한 글자씩 출력을 생성한다. param input_char: int - 연속열을 시작하는 글자의 인덱스에 해당하는 정수 param sample_length: int - 생성할 연속열의 길이 return txt: string - 길이가 sample_length이며 모델을 통해 생성한 문자열 &#39;&#39;&#39; indices = [] sample_model = deepcopy(self.model) for i in range(sample_length): input_char_batch = np.zeros((1, 1, self.vocab_size)) input_char_batch[0, 0, input_char] = 1.0 x_batch_out = sample_model.forward(input_char_batch) x_softmax = batch_softmax(x_batch_out) input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel()) indices.append(input_char) txt = &#39;&#39;.join(self.idx_to_char[idx] for idx in indices) return txt def train(self, num_iterations, sample_every = 100): &#39;&#39;&#39; &quot;글자 생성기&quot;를 학습 각 반복마다 신경망에 크기가 1인 배치가 입력된다 num_iterations회 반복한다. 매 반복마다 현재 학습된 모델로 생성한 텍스트가 출력된다. &#39;&#39;&#39; plot_iter = np.zeros((0)) plot_loss = np.zeros((0)) num_iter = 0 start_pos = 0 moving_average = deque(maxlen=100) while num_iter &lt; num_iterations: if start_pos + self.sequence_length + self.batch_size + 1 &gt; len(self.data): start_pos = 0 ## 모델 수정 inputs_indices, targets_indices = self._generate_inputs_targets(start_pos) inputs_batch, targets_batch = self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices) loss = self.model.single_step(inputs_batch, targets_batch) self.optim.step() moving_average.append(loss) ma_loss = np.mean(moving_average) start_pos += self.batch_size plot_iter = np.append(plot_iter, [num_iter]) plot_loss = np.append(plot_loss, [ma_loss]) if num_iter % 100 == 0: plt.plot(plot_iter, plot_loss) display.clear_output(wait=True) plt.show() sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], 200) print(sample_text) num_iter += 1 . 텍스트 파일과 모델을 전달받아 이어지는 글자를 예측하는 트레이너 입니다. . 자연어 관련 실습을 하기 위해 가져온 트레이너이기 때문에 자세한 설명은 생략하겠습니다. . layers = [RNNLayer(hidden_size=256, output_size=62)] mod = RNNModel(layers=layers, vocab_size=62, sequence_length=10, loss=SoftmaxCrossEntropy()) optim = SGD(lr=0.001, gradient_clipping=True) trainer = RNNTrainer(&#39;input.txt&#39;, mod, optim) trainer.train(1000, sample_every=100) . haglaw ahd Fh wo:&#39;ZiNJyFB yPsiEIiw :O woreyVVklEer y chimulmwofeoyTid?Phee IQSFZUinl;QhJvkP ikQw;KtUe ;khoahort fhaonZgb!ugor.;;wlZA oa;eyDbiT NlhbmoeLe Pneeh&#39;by,ne ajburPtUs zY xar ary jIsuor.wZwTDH . 간단한 실습을 해봤습니다. 사실 자세한 내용은 잘 이해가 안되는데 직접 구현한 코드로도 실제 실습이 된다는 점을 중요하게 생각하겠습니다. . &#45712;&#45184;&#51216; . 코드는 엄청 길지 않은데 진짜 이해하는데 한참걸렸던 것 같습니다. . RNN, 순환 신경망. 재사용한다는 것 까진 알겠는데 뭐가 어떻게 재사용이 되서 시계열, 자연어 처리에 쓰이는데? 하는 의문을 가졌는데요. . 100% 해결했다라고 자신있기 말하진 못하겠지만 어느정도 느낌은 받았던 것 같습니다. RNN 구조에 대해 직관적으로 머리 속에 들어온 것 같아요. . 특히 RNN 층 관련 내용 학습할 때, 긴 시간 씨름하다 이해가 됬을때 정말 기뻤습니다. 또 자동미분부분도 재밌게 했던 것 같아요. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/deep%20learning/matrix/math/class/2022/01/27/FirstDeep4.html",
            "relUrl": "/book/jupyter/deep%20learning/matrix/math/class/2022/01/27/FirstDeep4.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "[처음 시작하는 딥러닝] 3. 밑바닥부터 만들어보는 CNN",
            "content": ". &#44592;&#48376; &#49464;&#54021; . import numpy as np from numpy import ndarray . def assert_same_shape(output, output_grad): assert output.shape == output_grad.shape, &#39;&#39;&#39; 두 ndarray의 모양이 같아야 하는데, 첫 번째 ndarray의 모양은 {0}이고 두 번째 ndarray의 모양은 {1}이다. &#39;&#39;&#39;.format(tuple(output_grad.shape), tuple(output.shape)) return None def assert_dim(t, dim): assert len(t.shape) == dim, &#39;&#39;&#39; 이 텐서는 {0}차원이어야 하는데, {1}차원이다. &#39;&#39;&#39;.format(dim, len(t.shape)) return None . 필요한 차원을 잘 입력했는지 확인하는 함수를 선언합니다. . 1&#52264;&#50896; &#54633;&#49457; &#44273; . input_1d = np.array([1,2,3,4,5]) param_1d = np.array([1,1,1]) . def _pad_1d(inp, num): # 원래 데이터와 패딩 길이 입력 z = np.array([0]) z = np.repeat(z, num) return np.concatenate([z, inp, z]) _pad_1d(input_1d, 1) . array([0, 1, 2, 3, 4, 5, 0]) . 간단한 함수로 1차원 데이터를 패딩한 모습입니다. . 입력 데이터와 합성곱 연산을 한 출력 데이터의 크기를 같게하기 위해선 벗어나는 범위에 대해서 0 값을 채워주는 패딩을 하게 됩니다. . 패딩 크기는 필터 크기를 2로 나눈 값에 정수부분이 입력과 출력을 같게하는 패딩의 크기가 됩니다. . def conv_1d(inp, param): # 입력 값과 필터 값 입력 # 1차원 입력인지 확인합니다 assert_dim(inp, 1) assert_dim(param, 1) # 입력 값에 패딩을 덧붙입니다. param_len = param.shape[0] param_mid = param_len // 2 input_pad = _pad_1d(inp, param_mid) # 초기값 부여 out = np.zeros(inp.shape) # 1차원 합성곱 연산 수행 for o in range(out.shape[0]): for p in range(param_len): out[o] += param[p] * input_pad[o+p] # 출력 모양이 입력과 동일한지 확인 assert_same_shape(inp, out) return out conv_1d(input_1d, param_1d) . array([ 3., 6., 9., 12., 9.]) . 가중치가 [1,1,1] 인 간단한 합성곱 연산을 진행했습니다. . def conv_1d_sum(inp, param): out = conv_1d(inp, param) return np.sum(out) input_1d = np.array([1,2,3,4,5]) input_1d_2 = np.array([1,2,3,4,6]) input_1d_3 = np.array([1,2,3,5,5]) param_1d = np.array([1,1,1]) param_1d_2 = np.array([2,1,1]) print(conv_1d_sum(input_1d, param_1d)) print(conv_1d_sum(input_1d_2, param_1d)) print(conv_1d_sum(input_1d_3, param_1d)) print(conv_1d_sum(input_1d, param_1d_2)) . 39.0 41.0 42.0 49.0 . 입력값과 필터값이 달라짐에 따라 출력값의 합이 어떻게 바뀌는지 비교했습니다. . 끝 쪽 입력값이 1 증가할때는 출력값의 합이 2증가, 가운데 쪽 입력값(패딩 영향 안받는)이 1 증가할때는 출력값의 합이 3증가합니다. . 또 필터값이 1 증가할때 출력값의 합이 10 증가합니다. . def _param_grad_1d(inp, param, output_grad = None): # 입력값 패딩 추가 param_len = param.shape[0] param_mid = param_len // 2 input_pad = _pad_1d(inp, param_mid) if output_grad is None: # 출력값의 기울기를 입력하지 않으면 1로 초기화. # 왜냐하면 출력값의 합의 기울기이기 때문에 기울기를 유지하는 1을 쓰면됨. output_grad = np.ones_like(inp) else: assert_same_shape(inp, output_grad) # 모든 기울기의 초기값을 0으로 줍니다. param_grad = np.zeros_like(param) input_grad = np.zeros_like(inp) for o in range(inp.shape[0]): # 0~4 for p in range(param.shape[0]): # 0~2 # 필터값의 기울기는 실제 영향을 받는 입력값의 합으로 됨 param_grad[p] += input_pad[o+p] * output_grad[o] assert_same_shape(param_grad, param) return param_grad _param_grad_1d(input_1d, param_1d) . array([10, 15, 14]) . 1차원 합성 곱의 역방향 함수중 먼저 필터(파라미터) 기울기를 구하는 함수 입니다. . 조금 어려운데 결과값을 간단히 해석하면 파라미터가 1 증가했을때 출력값의 합이 각각 10, 15, 14 증가한다는 것 입니다. . def _input_grad_1d(inp, param, output_grad = None): # 입력값 패딩 추가 param_len = param.shape[0] param_mid = param_len // 2 input_pad = _pad_1d(inp, param_mid) if output_grad is None: # 출력값의 기울기를 입력하지 않으면 1로 초기화. # 왜냐하면 출력값의 합의 기울기이기 때문에 기울기를 유지하는 1을 쓰면됨. output_grad = np.ones_like(inp) else: assert_same_shape(inp, output_grad) # 원할한 연산을 위해 범위 내 값은 1을, 범위를 벗어나는 것들은 0으로함. # 패딩도 같은 효과를 냄. output_pad = _pad_1d(output_grad, param_mid) # 모든 기울기의 초기값을 0으로 줍니다. param_grad = np.zeros_like(param) input_grad = np.zeros_like(inp) for o in range(inp.shape[0]): # 0~4 for f in range(param.shape[0]): # 0~2 # 입력값의 기울기는 실제 영향을 받는 필터값의 합으로 됨 input_grad[o] += output_pad[o + param_len - f - 1] * param[f] assert_same_shape(param_grad, param) return input_grad _input_grad_1d(input_1d, param_1d) . array([2, 3, 3, 3, 2]) . 입력값에 따른 출력값의 변동이 얼마나 되는지를 나타내는 함수 입니다. . 첫번째와 마지막은 입력값이 1 증가할때 크기가 1 작은 2만큼 증가하고 나머지 값들은 3만큼 증가합니다. . 패딩한 것에 영향받는 값을 제외하고 필터의 개수(3)만큼 영향력이 있다고 생각하면 됩니다. . &#48176;&#52824; &#51077;&#47141; &#51201;&#50857;&#54616;&#44592; . input_1d_batch = np.array([[0,1,2,3,4,5,6], [1,2,3,4,5,6,7]]) def _pad_1d_batch(inp, num): outs = [_pad_1d(obs, num) for obs in inp] return np.stack(outs) _pad_1d_batch(input_1d_batch, 1) . array([[0, 0, 1, 2, 3, 4, 5, 6, 0], [0, 1, 2, 3, 4, 5, 6, 7, 0]]) . 입력값이 2개 이상인 배치에도 적용하기 위해 기존 구현한 함수를 확장하겠습니다. . 패딩의 경우 기존함수를 반복문을 이용해서 여러번 호출하면 됩니다. . def conv_1d_batch(inp, param): outs = [conv_1d(obs, param) for obs in inp] return np.stack(outs) conv_1d_batch(input_1d_batch, param_1d) . array([[ 1., 3., 6., 9., 12., 15., 11.], [ 3., 6., 9., 12., 15., 18., 13.]]) . 순방향 계산에 경우에도 같은 방식으로 확장했습니다. . def input_grad_1d_batch(inp, param): out = conv_1d_batch(inp, param) out_grad = np.ones_like(out) # 출력기울기 값의 형태가 배치이므로 이에 맞게 조정 batch_size = out_grad.shape[0] # 배치 크기가 나옴 grads = [_input_grad_1d(inp[i], param, out_grad[i]) for i in range(batch_size)] return np.stack(grads) input_grad_1d_batch(input_1d_batch, param_1d) . array([[2, 3, 3, 3, 3, 3, 2], [2, 3, 3, 3, 3, 3, 2]]) . 입력값에 따른 출력값이 얼마나 되는지 구하는 함수를 배치로 확장했습니다. . 기울기는 입력값에 영향이 있지 않기 때문에 어느 입력값이 입력되던 그대로 출력됩니다. . def param_grad_1d_batch(inp, param): output_grad = np.ones_like(inp) # 단순 합의 기울기이기 때문에 모든 값을 1로 둡니다. inp_pad = _pad_1d_batch(inp, 1) out_pad = _pad_1d_batch(inp, 1) param_grad = np.zeros_like(param) for i in range(inp.shape[0]): # 배치 크기만큼 for o in range(inp.shape[1]): # 인풋 길이만큼 for p in range(param.shape[0]): # 필터 길이만큼 # 전부 합해줍니다. param_grad[p] += inp_pad[i][o+p] * output_grad[i][o] return param_grad param_grad_1d_batch(input_1d_batch, param_1d) . array([36, 49, 48]) . 필터값에 따른 출력값이 얼마나 변하는지를 구하는 함수를 배치로 확장했습니다. . 이때 필터에 대한 기울기는 배치 단위인데 필터는 모든 관찰과 합성곱 연선이 이뤄지므로 모든 값을 다 더해야합니다. . 즉 모든 요소의 합이 필터 값이 바뀜에 따라서 얼마나 바뀌는지를 구하는 것 입니다. . 2&#52264;&#50896; &#54633;&#49457;&#44273; . imgs_2d_batch = np.random.randn(3, 28, 28) param_2d = np.random.randn(3,3) def _pad_2d_obs(inp, num): # 가로 단위로 앞 뒷 값 각각 패딩 inp_pad = _pad_1d_batch(inp, num) # 가로로 윗 2줄, 아래 2줄 패딩 other = np.zeros((num, inp.shape[0] + num * 2)) return np.concatenate([other, inp_pad, other]) def _pad_2d(inp, num): # 첫번째 차원은 배치 크기에 해당함. outs = [_pad_2d_obs(obs, num) for obs in inp] return np.stack(outs) _pad_2d(imgs_2d_batch, 1).shape . (3, 30, 30) . 2차원 단위에 입력값을 가지고 패딩을 진행했습니다. . _pad_2d_obs 함수는 패딩을 실질적으로 진행하는 함수이고, _pad_2d 함수는 배치 단위로 확장하는 함수 입니다. . def _compute_output_obs_2d(obs, param): param_mid = param.shape[0] // 2 obs_pad = _pad_2d_obs(obs, param_mid) out = np.zeros_like(obs) # 2차원 필터를 거처 출력값을 만듭니다. for o_w in range(out.shape[0]): # 출력값 가로길이 for o_h in range(out.shape[1]): # 출력값 세로길이 for p_w in range(param.shape[0]): # 필터 가로길이 for p_h in range(param.shape[1]): # 필터 세로길이 out[o_w][o_h] += param[p_w][p_h] * obs_pad[o_w+p_w][o_h+p_h] return out def _compute_output_2d(img_batch, param): assert_dim(img_batch, 3) outs = [_compute_output_obs_2d(obs, param) for obs in img_batch] return np.stack(outs) _compute_output_2d(imgs_2d_batch, param_2d).shape . (3, 28, 28) . 2차원 단위에 순방향 계산입니다. 패딩을 먼저 시킨 뒤 2차원 필터를 통과시켜 모든 값을 합친 값을 출력해줍니다. . def _compute_grads_obs_2d(input_obs, output_grad_obs, param): # 입력을 나타내는 2차원값, 출력 기울기를 나타내는 2차원값(여기선 모두 1을 사용), 2차원 필터 param_size = param.shape[0] # 2차원 필터의 가로 세로가 같다고 가정합니다. # 출력 기울기에 패딩을 먼저 덧붙입니다. 원본값은 1로, 나머지 값은 0으로하여 원본값만 유지하게 합니다. output_obs_pad = _pad_2d_obs(output_grad_obs, param_size // 2) input_grad = np.zeros_like(input_obs) # 초기 기울기는 0으로 합니다. for i_w in range(input_obs.shape[0]): # 입력값 가로길이 for i_h in range(input_obs.shape[1]): # 입력값 세로길이 for p_w in range(param_size): # 필터 가로길이 for p_h in range(param_size): # 필터 세로길이 input_grad[i_w][i_h] += output_obs_pad[i_w + param_size - p_w -1][i_h + param_size - p_h -1] * param[p_w][p_h] return input_grad def _compute_grads_2d(inp, output_grad, param): grads = [_compute_grads_obs_2d(inp[i], output_grad[i], param) for i in range(output_grad.shape[0])] return np.stack(grads) img_grads = _compute_grads_2d(imgs_2d_batch, np.ones_like(imgs_2d_batch), param_2d) img_grads.shape . (3, 28, 28) . 역방향 계산을 2차원으로 구현했습니다. 그 중 입력 기울기를 계산하는 절차인데요. . 출력 기울기에 패딩을 덧붙이고 해당하는 가중치와의 합 연산을 하면 입력 기울기를 계산할 수 있습니다. . def _param_grad_2d(inp, output_grad, param): # 입력을 나타내는 3차원값, 출력 기울기를 나타내는 3차원값(여기선 모두 1을 사용), 2차원 필터 param_size = param.shape[0] # 2차원 필터의 가로 세로가 같다고 가정합니다. inp_pad = _pad_2d(inp, param_size // 2) # 입력 값을 패딩합니다. param_grad = np.zeros_like(param) # 초기 가중치 기울기를 0으로 합니다. img_shape = output_grad.shape[1:] # 첫 값은 배치크기이므로 빼고 실행하기 위해. for i in range(inp.shape[0]): # 배치 크기 for o_w in range(img_shape[0]): # 입력값 가로길이 for o_h in range(img_shape[1]): # 입력값 세로길이 for p_w in range(param_size): # 필터 가로길이 for p_h in range(param_size): # 필터 세로길이 param_grad[p_w][p_h] += inp_pad[i][o_w+p_w][o_h+p_h] * output_grad[i][o_w][o_h] return param_grad param_grad = _param_grad_2d(imgs_2d_batch, np.ones_like(imgs_2d_batch), param_2d) param_grad . array([[108.03493534, 116.92134058, 112.40044695], [112.79682302, 122.16317892, 122.1838074 ], [106.24163816, 116.07072373, 116.86259153]]) . 역방향 계산 중 필터 기울기를 구하는 부분을 2차원으로 구현했습니다. . 여기서는 입력값에 패딩을 덧붙이고 합 연산을 했는데, 1차원하고 크게 다를게 없습니다. . 배치 연산까지 한번에 진행하는 함수를 구현했는데 모든 배치의 입력값을 순회합니다. . &#52292;&#45328; &#52628;&#44032;&#54616;&#44592; . def _compute_output_obs(obs, param): assert_dim(obs, 3) assert_dim(param, 4) param_size = param.shape[2] param_mid = param_size // 2 obs_pad = _pad_2d_channel(obs, param_mid) in_channels = param.shape[0] out_channels = param.shape[1] img_size = obs.shape[1] out = np.zeros((out_channels,) + obs.shape[1:]) for c_in in range(in_channels): for c_out in range(out_channels): for o_w in range(img_size): for o_h in range(img_size): for p_w in range(param_size): for p_h in range(param_size): out[c_out][o_w][o_h] += param[c_in][c_out][p_w][p_h] * obs_pad[c_in][o_w+p_w][o_h+p_h] return out def _output(inp, param): outs = [_compute_output_obs(obs, param) for obs in inp] return np.stack(outs) . 합성곱층은 2차원으로 서로 엮인 뉴런 외에도 특징 맵과 같은 수의 채널을 갖습니다. . 이런식으로 입력된 데이터를 다루기 위해 채널이 있는 순방향 연산을 구현했습니다. 역방향 연산은 생략합니다. . &#44592;&#53440; &#44032;&#48316;&#50868; &#51060;&#47200; . 이미지 데이터에서는 서로 가까운 픽셀 간에 유의미한 의미가 있는 조합이 나올 가능성이 높습니다. . 즉 픽셀간 얼마나 공간적으로 가까운지를 나타내기 위해 합성곱 연산을 수행합니다. . 풀링이란 각 특징 맵을 다운샘플링하여 데이터의 크기를 줄이는 방법입니다. 예시로 각 영역 픽셀 값의 최대값을 사용할 수 있습니다. . 계산양 감소라는 이점이 있으나, 정보 손실또한 크기 때문에 이 방법을 쓰는데 다양한 의견이 있습니다. . 스트라이드란 필터가 움직이는 간격이 지금까지는 1이였는데 이 값을 의미하는 용어로 커질수록 다운샘플링 효과가 커집니다. . 최근 제안되는 고급 합성곱 신경망 구조에서는 풀링 대신 스트라이드를 2이상으로 설정해 다운샘플링 효과를 얻습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/deep%20learning/matrix/math/class/2022/01/23/FirstDeep3.html",
            "relUrl": "/book/jupyter/deep%20learning/matrix/math/class/2022/01/23/FirstDeep3.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "[SSUDA] 텐서플로로 데이터 적재하기1",
            "content": ". &#45936;&#51060;&#53552; API . import sys import sklearn import tensorflow as tf from tensorflow import keras import numpy as np import pandas as pd import os import matplotlib.pyplot as plt . X = tf.range(10) dataset = tf.data.Dataset.from_tensor_slices(X) # 주어진 데이터 소스를 여러 텐서로 자릅니다. dataset # tf.data.Dataset.range(10)과 동등함. . &lt;TensorSliceDataset shapes: (), types: tf.int32&gt; . for item in dataset: print(item) . tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32) tf.Tensor(4, shape=(), dtype=int32) tf.Tensor(5, shape=(), dtype=int32) tf.Tensor(6, shape=(), dtype=int32) tf.Tensor(7, shape=(), dtype=int32) tf.Tensor(8, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32) . from_tensor_slices 함수는 텐서를 받아 10개의 아이템(0~9)으로 쪼갭니다. . 이 때 shapes가 () 인것은 크기가 1인 텐서 10개로 쪼개졌다는 것을 의미합니다. . dataset = dataset.repeat(3).batch(7) for item in dataset: print(item) . tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32) tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32) tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32) tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32) tf.Tensor([8 9], shape=(2,), dtype=int32) . 원래 dataset에 repeat 함수를 적용하면 원본 데이터 아이템을 세차레 반복하는 새로운 데이터셋을 반환합니다. . 그 뒤 batch 함수를 적용하면 아이템을 7개씩 그룹으로 묶습니다. 이 때 마지막 데이터셋은 2개가 됩니다. . dataset = dataset.map(lambda x: x*2) for item in dataset: print(item) . tf.Tensor([ 0 2 4 6 8 10 12], shape=(7,), dtype=int32) tf.Tensor([14 16 18 0 2 4 6], shape=(7,), dtype=int32) tf.Tensor([ 8 10 12 14 16 18 0], shape=(7,), dtype=int32) tf.Tensor([ 2 4 6 8 10 12 14], shape=(7,), dtype=int32) tf.Tensor([16 18], shape=(2,), dtype=int32) . map 함수로 데이터 셋 내 아이템을 변환할 수 있습니다. . tf.random.set_seed(42) dataset = tf.data.Dataset.range(10).repeat(3) dataset = dataset.shuffle(buffer_size=5, seed = 42).batch(7) for item in dataset: print(item) . tf.Tensor([0 1 6 5 7 3 9], shape=(7,), dtype=int64) tf.Tensor([8 2 1 0 4 6 4], shape=(7,), dtype=int64) tf.Tensor([7 2 5 9 2 1 3], shape=(7,), dtype=int64) tf.Tensor([4 3 8 7 9 5 0], shape=(7,), dtype=int64) tf.Tensor([8 6], shape=(2,), dtype=int64) . buffer_size 만큼 버퍼를 만든 뒤 데이터를 앞 부분부터 순서대로 버퍼 최대 크기만큼 뽑아 버퍼를 채웁니다. 그 후 버퍼 중 한 개 데이터를 뽑습니다. . 뽑은 데이터는 첫번째 결과값으로 지정하고 다시 데이터 셋에서 다음 순서 데이터를 뽑아 버퍼를 채웁니다. . 그 후 버퍼 중 렌덤하게 한 개 데이터를 뽑아 두번째 결과값으로 지정합니다. 이 과정을 반복하면 데이터가 셔플하게 됩니다. . 하지만 여전히 앞 부분 데이터는 앞 부분에 있을 확률이 커 셔플이 됬다고 하기 어렵습니다. 버퍼 크기가 현저히 작을 경우 그런 경향이 더 심합니다. . 이를 해결하기 위해 원본 데이터를 에포크마다 한번씩 섞어줘야 합니다. . &#50668;&#47084; CSV &#54028;&#51068;&#50640;&#49436; &#54620; &#51460;&#50473; &#48264;&#44040;&#50500; &#51069;&#44592; . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split( housing.data, housing.target.reshape(-1, 1), random_state=42) X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, random_state=42) scaler = StandardScaler() scaler.fit(X_train) X_mean = scaler.mean_ X_std = scaler.scale_ . 주택 데이터셋을 다운받습니다. 훈련/검증/테스트 로 나누고 스케일을 조정합니다. . def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10): housing_dir = os.path.join(&quot;datasets&quot;, &quot;housing&quot;) os.makedirs(housing_dir, exist_ok=True) path_format = os.path.join(housing_dir, &quot;my_{}_{:02d}.csv&quot;) filepaths = [] m = len(data) for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)): part_csv = path_format.format(name_prefix, file_idx) filepaths.append(part_csv) with open(part_csv, &quot;wt&quot;, encoding=&quot;utf-8&quot;) as f: if header is not None: f.write(header) f.write(&quot; n&quot;) for row_idx in row_indices: f.write(&quot;,&quot;.join([repr(col) for col in data[row_idx]])) f.write(&quot; n&quot;) return filepaths train_data = np.c_[X_train, y_train] valid_data = np.c_[X_valid, y_valid] test_data = np.c_[X_test, y_test] header_cols = housing.feature_names + [&quot;MedianHouseValue&quot;] header = &quot;,&quot;.join(header_cols) train_filepaths = save_to_multiple_csv_files(train_data, &quot;train&quot;, header, n_parts=20) valid_filepaths = save_to_multiple_csv_files(valid_data, &quot;valid&quot;, header, n_parts=10) test_filepaths = save_to_multiple_csv_files(test_data, &quot;test&quot;, header, n_parts=10) . 데이터를 여러개의 CSV파일로 쪼갰습니다. (이 부분은 핸즈온머신러닝 깃허브 코드를 복사했습니다.) . train_filepaths . [&#39;datasets/housing/my_train_00.csv&#39;, &#39;datasets/housing/my_train_01.csv&#39;, &#39;datasets/housing/my_train_02.csv&#39;, &#39;datasets/housing/my_train_03.csv&#39;, &#39;datasets/housing/my_train_04.csv&#39;, &#39;datasets/housing/my_train_05.csv&#39;, &#39;datasets/housing/my_train_06.csv&#39;, &#39;datasets/housing/my_train_07.csv&#39;, &#39;datasets/housing/my_train_08.csv&#39;, &#39;datasets/housing/my_train_09.csv&#39;, &#39;datasets/housing/my_train_10.csv&#39;, &#39;datasets/housing/my_train_11.csv&#39;, &#39;datasets/housing/my_train_12.csv&#39;, &#39;datasets/housing/my_train_13.csv&#39;, &#39;datasets/housing/my_train_14.csv&#39;, &#39;datasets/housing/my_train_15.csv&#39;, &#39;datasets/housing/my_train_16.csv&#39;, &#39;datasets/housing/my_train_17.csv&#39;, &#39;datasets/housing/my_train_18.csv&#39;, &#39;datasets/housing/my_train_19.csv&#39;] . 분할된 CSV파일 저장 경로 리스트 입니다. . filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed = 42) for filepath in filepath_dataset: print(filepath) . tf.Tensor(b&#39;datasets/housing/my_train_15.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_08.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_03.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_01.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_10.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_05.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_19.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_16.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_02.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_09.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_00.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_07.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_12.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_04.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_17.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_11.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_14.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_18.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_06.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_13.csv&#39;, shape=(), dtype=string) . list_files 함수는 파일 경로를 섞은 데이터셋을 반환합니다. . n_readers = 5 dataset = filepath_dataset.interleave( lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # 파일의 첫번째 줄은 열이름이므로 skip함. cycle_length = n_readers ) for line in dataset.take(5): print(line.numpy()) . b&#39;4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504&#39; b&#39;8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159&#39; b&#39;3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598&#39; b&#39;3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526&#39; b&#39;3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625&#39; . interleave 함수는 데이터셋(파일경로) 내 다섯 개에서 데이터를 읽는 데이터셋을 만듭니다. . 이 때 함수 내 lambda 함수를 이용해 새로운 데이터셋을 만들 것입니다. . 인터러브 데이터셋을 반복 구문에 적용하면 다섯 개의 TextLineDataset을 순회하고, 모든 데이터셋의 아이템이 소진될때까지 한 줄씩 읽습니다. . 출력된 값은 바이트 스트링입니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; &#54616;&#44592; . [0.] * 8 + [tf.constant([], dtype = tf.float32)] . [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, &lt;tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)&gt;] . n_inputs = X_train.shape[-1] def preprocess(line): defs = [0.] * n_inputs + [tf.constant([], dtype = tf.float32)] # 디폴트값 텐서 형태로 만들기 fields = tf.io.decode_csv(line, record_defaults=defs) # 스칼라 텐서의 리스트를 반환 x = tf.stack(fields[:-1]) # 마지막 전까지 y = tf.stack(fields[-1:]) return (x-X_mean) / X_std, y preprocess(b&#39;4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504&#39;) . (&lt;tf.Tensor: shape=(8,), dtype=float32, numpy= array([ 0.39593136, 0.74167496, -0.16415128, -0.40340805, -0.6199179 , -0.18355484, -1.4084505 , 1.2565969 ], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.504], dtype=float32)&gt;) . 전처리 하는 함수를 만들었습니다. . &#45936;&#51060;&#53552; &#51201;&#51116;, &#51204;&#52376;&#47532; &#54632;&#49688; &#47564;&#46308;&#44592; . def csv_reader_dataset(filepaths, repeat = 1, n_readers = 5, n_read_threads = None, shuffle_buffer_size = 10000, n_parse_threads = 5, batch_size = 32): dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat) # 파일 경로를 섞은 데이터셋 반환 dataset = dataset.interleave( # 한번에 n_readers 파일 수만큼 한 줄씩 번갈아 읽음. lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # 첫줄은 열이름이므로 제외. cycle_length = n_readers, num_parallel_calls = n_read_threads # 여러파일에서 병렬로 읽고싶을때 사용. ) dataset = dataset.shuffle(shuffle_buffer_size) # 데이터를 셔플합니다. 버퍼는 shuffle_buffer_size만큼. dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads) # preprocess 함수를 데이터 내 적용. return dataset.batch(batch_size).prefetch(1) # batch_size만큼 배치 함수를 이용해 데이터를 묶습니다. # 프리페치는 속도를 향상시킵니다. . 데이터를 적재하고 전처리 하는 부분까지 한번에 하는 함수를 구현했습니다. . tf.random.set_seed(42) train_set = csv_reader_dataset(train_filepaths, batch_size = 3) for X_batch, y_batch in train_set.take(2): print(&#39;X = &#39;, X_batch) print(&#39;y =&#39;, y_batch) . X = tf.Tensor( [[ 0.5804519 -0.20762321 0.05616303 -0.15191229 0.01343246 0.00604472 1.2525111 -1.3671792 ] [ 5.818099 1.8491895 1.1784915 0.28173092 -1.2496178 -0.3571987 0.7231292 -1.0023477 ] [-0.9253566 0.5834586 -0.7807257 -0.28213993 -0.36530012 0.27389365 -0.76194876 0.72684526]], shape=(3, 8), dtype=float32) y = tf.Tensor( [[1.752] [1.313] [1.535]], shape=(3, 1), dtype=float32) X = tf.Tensor( [[-0.8324941 0.6625668 -0.20741376 -0.18699841 -0.14536144 0.09635526 0.9807942 -0.67250353] [-0.62183803 0.5834586 -0.19862501 -0.3500319 -1.1437552 -0.3363751 1.107282 -0.8674123 ] [ 0.8683102 0.02970133 0.3427381 -0.29872298 0.7124906 0.28026953 -0.72915536 0.86178064]], shape=(3, 8), dtype=float32) y = tf.Tensor( [[0.919] [1.028] [2.182]], shape=(3, 1), dtype=float32) . 직접 구현한 함수를 통해 여러개로 나누어진 csv의 경로를 랜덤하게 배치 사이즈만큼 적재하고 전처리한 모습입니다. . train_set = csv_reader_dataset(train_filepaths, repeat = None) valid_set = csv_reader_dataset(valid_filepaths) test_set = csv_reader_dataset(test_filepaths) model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=X_train.shape[1:]), keras.layers.Dense(1), ]) model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(learning_rate=1e-3)) . csv 경로를 이용해 데이터셋을 적재하고 전처리 한 뒤 케라스로 간단한 딥러닝 모델을 구축합니다. . batch_size = 32 model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set) # steps_per_epoch 은 한 에포크당 몇번 진행하는지. 입력값이 train_set이기 때문에 확실히 모름. . Epoch 1/10 362/362 [==============================] - 1s 3ms/step - loss: 2.3908 - val_loss: 1.7373 Epoch 2/10 362/362 [==============================] - 1s 3ms/step - loss: 0.8812 - val_loss: 0.7719 Epoch 3/10 362/362 [==============================] - 1s 2ms/step - loss: 0.7535 - val_loss: 0.7118 Epoch 4/10 362/362 [==============================] - 1s 2ms/step - loss: 0.7236 - val_loss: 1.0734 Epoch 5/10 362/362 [==============================] - 1s 2ms/step - loss: 0.6724 - val_loss: 0.6422 Epoch 6/10 362/362 [==============================] - 1s 3ms/step - loss: 0.6682 - val_loss: 0.6402 Epoch 7/10 362/362 [==============================] - 1s 3ms/step - loss: 0.6197 - val_loss: 0.8125 Epoch 8/10 362/362 [==============================] - 1s 3ms/step - loss: 0.6181 - val_loss: 0.5822 Epoch 9/10 362/362 [==============================] - 1s 3ms/step - loss: 0.5769 - val_loss: 1.0522 Epoch 10/10 362/362 [==============================] - 1s 3ms/step - loss: 0.5596 - val_loss: 0.6446 . &lt;keras.callbacks.History at 0x7fa4918802d0&gt; . 잘 진행되는 모습입니다. . &#45712;&#45184;&#51216; . 데이터 API를 이용해 텐서플로에서 데이터 적재하는 방법을 간단히 알아봤습니다. . 실제로 대용량 데이터를 저장하고 효율적으로 읽기 위해선 TFRecord를 활용한다고 하는데 이후에 추가로 공부해보겠습니다. . 딥러닝에 대해 지금까지 가볍게 공부하고 있는데 솔직히 막연하다는 생각이 듭니다. 실전 경험이 없어서일까요. . 이와 별개로 개인적으로 스스로에게 휴식을 주고자 합니다. 몸상태가 좋지 못하고, 학기종강 이후 쉼을 못준 것 같네요. . 아마 당분간은 주 1회 SSUDA 스터디 코드만 올라갈 것 같습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/14/handssu4.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/14/handssu4.html",
            "date": " • Jan 14, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "[SSUDA] 텐서플로 사용하기",
            "content": ". &#45336;&#54028;&#51060;&#52376;&#47100; &#53584;&#49436;&#54540;&#47196; &#49324;&#50857;&#54616;&#44592; . import numpy as np import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.4&quot; # 버전이 2.4보다 커야합니다. np.random.seed(42) tf.random.set_seed(42) . 넘파이와 텐서플로 패키지를 설치합니다. . tf.constant([[1., 2., 3.], [4., 5., 6.]]) . &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy= array([[1., 2., 3.], [4., 5., 6.]], dtype=float32)&gt; . constant 함수로 텐서를 생성했습니다. 넘파이 행렬과 유사합니다. . 유의할 점은 텐서는 변경이 불가능한 객체입니다. . tf.constant(42) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt; . 스칼라 값도 입력이 가능합니다. . t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) t.shape . TensorShape([2, 3]) . t.dtype . tf.float32 . 넘파이 행렬과 비슷하게 크기(shape)와 데이터 타입(dtype)을 가집니다. . t[:, 1:] . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[2., 3.], [5., 6.]], dtype=float32)&gt; . t[:, 1, tf.newaxis] . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[2.], [5.]], dtype=float32)&gt; . tf.square(t) . &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy= array([[ 1., 4., 9.], [16., 25., 36.]], dtype=float32)&gt; . t @ tf.transpose(t) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[14., 32.], [32., 77.]], dtype=float32)&gt; . 텐서 데이터 타입은 넘파이와 마찬가지로 여러가지 연산이 가능합니다. . a = np.array([2., 4., 5.]) tf.constant(a) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])&gt; . t.numpy() . array([[1., 2., 3.], [4., 5., 6.]], dtype=float32) . 텐서롤 넘파이로, 넘파이를 텐서로 쉽게 변형이 가능합니다. . 다만 넘파이는 64비트 기반 텐서는 32비트 기반이기 때문에 넘파이 배열로 텐서를 만들때는 dtype = tf.float32로 해야합니다. . v = tf.Variable([[1.,2.,3.], [4., 5., 6.]]) v.assign(2 * v) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[ 2., 4., 6.], [ 8., 10., 12.]], dtype=float32)&gt; . v[0, 1].assign(42) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[ 2., 42., 6.], [ 8., 10., 12.]], dtype=float32)&gt; . tf.Tensor은 앞서 말한데로 변경이 불가능한 객체입니다. 이 객체만으로는 지속적으로 업데이트되는 신경망 가중치등을 사용할 수 없습니다. . tf.Variable은 이 경우에 필요합니다. assign 함수를 이용해 변수 값을 바꿀 수 있습니다. . &#51452;&#53469; &#45936;&#51060;&#53552;&#47196; &#49324;&#50857;&#51088; &#51221;&#51032; &#49552;&#49892; &#54632;&#49688; &#51201;&#50857;&#54644;&#48372;&#44592; . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split( housing.data, housing.target.reshape(-1, 1), random_state=42) X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, random_state=42) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_valid_scaled = scaler.transform(X_valid) X_test_scaled = scaler.transform(X_test) . 주택 가격 데이터를 입력해 트레인, 벨리드, 테스트 데이터로 분할하고 스케일링 해줍니다. . input_shape = X_train.shape[1:] model = keras.models.Sequential([ keras.layers.Dense(30, activation = &#39;selu&#39;, kernel_initializer= &#39;lecun_normal&#39;, # LeCun 정규분포에서 값을 추출해 가중치 초기값을 설정합니다. input_shape = input_shape), keras.layers.Dense(1) ]) . 간단한 딥러닝 모델을 구축합니다. 이때 활성화 함수로 자기정규화를 일으키는 selu을 사용합니다. . def huber_fn(y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) &lt; 1 # 작은 오차라면 squared_loss = tf.square(error) / 2 # 오차 제곱의 절반값 linear_loss = tf.abs(error) - 0.5 # 큰 오차라면 오차의 절대값 - 0.5 # where 함수는 조건에 따른 벡터 출력에 최적화 되어있습니다. # 참일때는 2번 입력값을 거짓일때는 3번입력값을 출력한 것을 묶어 벡터로 출력합니다. return tf.where(is_small_error, squared_loss, linear_loss) . 손실함수를 직접 구현합니다. 평균 제곱 오차는 큰 오차에 너무 과한 벌칙이 있고 절대값 오차는 이상치에 너무 관대합니다. . 그러므로 앞서 말한 단점을 보안한 후버 손실 함수를 직접 구현하였습니다. . model.compile(loss = huber_fn, optimizer = &#39;nadam&#39;, metrics = &#39;mae&#39;) model.fit(X_train_scaled, y_train, epochs = 2, validation_data=(X_valid_scaled, y_valid)) . Epoch 1/2 363/363 [==============================] - 2s 2ms/step - loss: 0.6235 - mae: 0.9953 - val_loss: 0.2862 - val_mae: 0.5866 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2197 - mae: 0.5177 - val_loss: 0.2382 - val_mae: 0.5281 . &lt;keras.callbacks.History at 0x7f4ed1f30110&gt; . 앞서 정의한 손실함수를 사용해 간단한 모델을 적합시켰습니다. . model.save(&#39;my_model_with_first&#39;) model = keras.models.load_model(&#39;my_model_with_first&#39;, custom_objects = {&#39;huber_fn&#39; : huber_fn}) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . INFO:tensorflow:Assets written to: my_model_with_first/assets Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2054 - mae: 0.4981 - val_loss: 0.2253 - val_mae: 0.5090 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.1993 - mae: 0.4891 - val_loss: 0.2154 - val_mae: 0.5019 . &lt;keras.callbacks.History at 0x7f4ed197b690&gt; . 모델을 저장하고 다시 불러왔습니다. 앞선 모델과 꽤 유사하게 정상적으로 잘 작동합니다. . &#49324;&#50857;&#51088; &#51221;&#51032; &#49552;&#49892; &#54632;&#49688;&#47484; &#53364;&#47000;&#49828;&#47196; &#51201;&#50857; &#54644;&#48372;&#44592; . class HuberLoss(keras.losses.Loss): # 케라스 loss 클래스 상속 def __init__(self, threshold = 1.0, **kwargs): self.threshold = threshold super().__init__(**kwargs) def call(self, y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) &lt; self.threshold squared_loss = tf.square(error) / 2 linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2/ 2 return tf.where(is_small_error, squared_loss, linear_loss) def get_config(self): # 이 함수를 구현하여 threshold 값을 모델 로드시에도 유지할 수 있습니다. base_config = super().get_config() return {**base_config, &#39;threshold&#39; : self.threshold} . 손실함수를 함수로 구현한다면 함수 내 매개변수가 있을 때 그 값은 모델 로드시 날아갑니다. . 이를 방지하려면 클래스로 손실함수를 구현해야합니다. 케라스 loss 클래스를 상속하고 get_config 함수를 구현하면 됩니다. . model.compile(loss = HuberLoss(2.), optimizer=&#39;nadam&#39;, metrics = &#39;mae&#39;) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2217 - mae: 0.4879 - val_loss: 0.2560 - val_mae: 0.4909 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2176 - mae: 0.4832 - val_loss: 0.2389 - val_mae: 0.4881 . &lt;keras.callbacks.History at 0x7f4ed1986990&gt; . model.save(&#39;my_model_with_second&#39;) model = keras.models.load_model(&#39;my_model_with_second&#39;, custom_objects = {&#39;HuberLoss&#39; : HuberLoss}) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . INFO:tensorflow:Assets written to: my_model_with_second/assets Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2145 - mae: 0.4790 - val_loss: 0.2345 - val_mae: 0.4753 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2117 - mae: 0.4753 - val_loss: 0.2183 - val_mae: 0.4731 . &lt;keras.callbacks.History at 0x7f4ece428dd0&gt; . 모델을 처음 만들었을때와 저장 후 로드한 모델을 새로 사용했을 때 비슷한 결과를 보이는 것을 알 수 있습니다. . 이는 매개변수가 잘 저장됬다는 것을 보여줍니다. (threshold = 2) . &#54876;&#49457;&#54868; &#54632;&#49688;, &#52488;&#44592;&#54868;, &#44508;&#51228;, &#51228;&#54620;&#51012; &#52964;&#49828;&#53552;&#47560;&#51060;&#51669;&#54616;&#44592; . keras.backend.clear_session() # 환경을 초기화 해줍니다. np.random.seed(42) tf.random.set_seed(42) def my_softplus(z): return tf.math.log(tf.exp(z) + 1.0) def my_glorot_initializer(shape, dtype = tf.float32): stddev = tf.sqrt(2. / (shape[0] + shape[1])) return tf.random.normal(shape, stddev = stddev, dtype = dtype) def my_l1_regularizer(weights): return tf.reduce_sum(tf.abs(0.01 * weights)) def my_positive_weights(weights): return tf.where(weights &lt; 0., tf.zeros_like(weights), weights) . 활성화 함수, 가중치 초기값 부여방식, 규제, 제한을 이미 텐서플로 내 존재하지만 직접 구현해보았습니다. . model = keras.models.Sequential([ keras.layers.Dense(30, activation = &#39;selu&#39;, kernel_initializer=&#39;lecun_normal&#39;, input_shape = input_shape), keras.layers.Dense(1, activation= my_softplus, kernel_regularizer = my_l1_regularizer, kernel_constraint = my_positive_weights, kernel_initializer = my_glorot_initializer), ]) model.compile(loss = &#39;mse&#39;, optimizer = &#39;nadam&#39;, metrics = [&#39;mae&#39;]) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 1.5542 - mae: 0.8962 - val_loss: 1.4154 - val_mae: 0.5607 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5943 - mae: 0.5256 - val_loss: 1.4399 - val_mae: 0.5137 . &lt;keras.callbacks.History at 0x7f4ed1607c10&gt; . 이를 적용시킨 모델을 만들었습니다. . model.save(&#39;my_model_with_third&#39;) model = keras.models.load_model(&#39;my_model_with_third&#39;, custom_objects={ &quot;my_l1_regularizer&quot;: my_l1_regularizer, &quot;my_positive_weights&quot;: my_positive_weights, &quot;my_glorot_initializer&quot;: my_glorot_initializer, &quot;my_softplus&quot;: my_softplus, }) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . INFO:tensorflow:Assets written to: my_model_with_third/assets Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5253 - mae: 0.4974 - val_loss: 1.4448 - val_mae: 0.4931 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5007 - mae: 0.4874 - val_loss: 1.1597 - val_mae: 0.4913 . &lt;keras.callbacks.History at 0x7f4ed01fda50&gt; . 잘 작동합니다. . &#49324;&#50857;&#51088; &#51221;&#51032; &#52789; . class MyDense(keras.layers.Layer): def __init__(self, units, activation = None, **kwargs): super().__init__(**kwargs) self.units = units self.activation = keras.activations.get(activation) def build(self, batch_input_shape): # 층이 처음 사용될 때 호출되는 함수 self.kernel = self.add_weight( name = &#39;kernel&#39;, shape = [batch_input_shape[-1], self.units], initializer = &#39;glorot_normal&#39; ) self.bias = self.add_weight( name = &#39;bias&#39;, shape = [self.units], initializer = &#39;zeros&#39; ) super().build(batch_input_shape) def call(self, X): return self.activation(X @ self.kernel + self.bias) def compute_output_shape(self, batch_input_shape): return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units]) def get_config(self): # 환경설정 저장 base_config = super().get_config() return {**base_config, &#39;units&#39;: self.units, &#39;activation&#39; : keras.activations.serialize(self.activation)} . 층을 직접 구현해보았습니다. 이 사용자 정의 층은 보통의 층과 동일하게 사용할 수 있습니다. . model = keras.models.Sequential([ MyDense(30, activation = &#39;relu&#39;, input_shape = input_shape), MyDense(1) ]) model.compile(loss = &#39;mse&#39;, optimizer = &#39;nadam&#39;) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) model.evaluate(X_test_scaled, y_test) . Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 1.2326 - val_loss: 1.3653 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5817 - val_loss: 0.7383 162/162 [==============================] - 0s 1ms/step - loss: 0.4993 . 0.4993399679660797 . 사용자 정의 층을 사용해 모델을 만들어봤어요. . &#54632;&#49688;&#54805; API . X_train_A, X_train_B = X_train_scaled[:, :5], X_train_scaled[:, 2:] # 0~4까지 첫번째 입력, 2~7까지 두번째 입력 X_valid_A, X_valid_B = X_valid_scaled[:, :5], X_valid_scaled[:, 2:] X_test_A, X_test_B = X_test_scaled[:, :5], X_test_scaled[:, 2:] X_new_A, X_new_B = X_test_A[:3], X_test_B[:3] # 입력층이 2개인 모델 구축 input_A = keras.layers.Input(shape = [5], name = &#39;wide_input&#39;) input_B = keras.layers.Input(shape = [6], name = &#39;depp_input&#39;) hidden1 = keras.layers.Dense(30, activation = &#39;relu&#39;)(input_B) hidden2 = keras.layers.Dense(30, activation = &#39;relu&#39;)(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) output = keras.layers.Dense(1, name = &#39;output&#39;)(concat) model = keras.Model(inputs = [input_A, input_B], outputs = [output]) . 입력층이 2개인 모델을 만들기 위해 데이터를 먼저 분할합니다. 그 다음 입력층을 두 개 받습니다. . 입력층_B에서 은닉층 2개를 통과한 뒤 입력층_A와 층 연결을 하고 아웃풋을 출력하는 모델을 만들었습니다. . model.compile(loss = &#39;mse&#39;, optimizer = keras.optimizers.SGD(lr = 1e-3)) history = model.fit((X_train_A, X_train_B), y_train, epochs = 20, validation_data = ((X_valid_A, X_valid_B), y_valid)) mse_test = model.evaluate((X_test_A, X_test_B), y_test) y_pred = model.predict((X_new_A, X_new_B)) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(SGD, self).__init__(name, **kwargs) . Epoch 1/20 363/363 [==============================] - 1s 2ms/step - loss: 2.1094 - val_loss: 1.0289 Epoch 2/20 363/363 [==============================] - 1s 2ms/step - loss: 0.7849 - val_loss: 0.6881 Epoch 3/20 363/363 [==============================] - 1s 2ms/step - loss: 0.6517 - val_loss: 0.6039 Epoch 4/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5965 - val_loss: 0.5446 Epoch 5/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5595 - val_loss: 0.5129 Epoch 6/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5311 - val_loss: 0.4873 Epoch 7/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5084 - val_loss: 0.4673 Epoch 8/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4898 - val_loss: 0.4499 Epoch 9/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4748 - val_loss: 0.4373 Epoch 10/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4641 - val_loss: 0.4269 Epoch 11/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4561 - val_loss: 0.4202 Epoch 12/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4496 - val_loss: 0.4141 Epoch 13/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4443 - val_loss: 0.4102 Epoch 14/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4398 - val_loss: 0.4063 Epoch 15/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4362 - val_loss: 0.4027 Epoch 16/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4332 - val_loss: 0.4012 Epoch 17/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4306 - val_loss: 0.3977 Epoch 18/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4282 - val_loss: 0.3951 Epoch 19/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4256 - val_loss: 0.3950 Epoch 20/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4240 - val_loss: 0.3968 162/162 [==============================] - 0s 1ms/step - loss: 0.4183 . 모델을 적합시키고 출력시켰습니다. 성능은 비슷한 것 같아요. . 비슷한 매커니즘으로 출력을 여러개 하는 것 또한 가능합니다. 이 때 손실함수는 각각 필요하며, 어느 출력물에 가중치를 늘리는것도 가능합니다. . &#49436;&#48652;&#53364;&#47000;&#49905; API&#47196; &#46041;&#51201; &#47784;&#45944; &#47564;&#46308;&#44592; . class WideAndDeepModel(keras.models.Model): def __init__(self, units = 30, activation = &#39;relu&#39;, **kwargs): super().__init__(**kwargs) self.hidden1 = keras.layers.Dense(units, activation = activation) self.hidden2 = keras.layers.Dense(units, activation = activation) self.main_output = keras.layers.Dense(1) self.aux_output = keras.layers.Dense(1) def call(self, inputs): input_A, input_B = inputs hidden1 = self.hidden1(input_B) hidden2 = self.hidden2(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) main_output = self.main_output(concat) aux_output = self.aux_output(hidden2) return main_output, aux_output model = WideAndDeepModel(30, activation = &#39;relu&#39;) . 앞서 만든 함수형 API는 정적이기 때문에 반복문/조건문 등 여러 동적인 구조를 필요로 하는 경우 사용이 힘듭니다. . 이를 극복하기 위해 서브클레싱 API 모델을 만들었습니다. 이전과 동일하나 출력층이 2개인 구조입니다. . 생성자에 층 구성을 하고 call 함수로 정방향 계산을 만들었습니다. 이때 input 클래스의 객체는 만들 필요가 없습니다. . model.compile(loss = &#39;mse&#39;, loss_weights= [0.9, 0.1], optimizer = keras.optimizers.SGD(learning_rate=1e-3)) history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10, validation_data = ((X_valid_A, X_valid_B), (y_valid, y_valid))) total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test)) y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B)) . Epoch 1/10 363/363 [==============================] - 2s 3ms/step - loss: 2.2525 - output_1_loss: 2.0189 - output_2_loss: 4.3545 - val_loss: 1.6827 - val_output_1_loss: 1.4017 - val_output_2_loss: 4.2116 Epoch 2/10 363/363 [==============================] - 1s 2ms/step - loss: 1.0489 - output_1_loss: 0.8583 - output_2_loss: 2.7644 - val_loss: 1.0083 - val_output_1_loss: 0.8151 - val_output_2_loss: 2.7478 Epoch 3/10 363/363 [==============================] - 1s 2ms/step - loss: 0.8499 - output_1_loss: 0.7183 - output_2_loss: 2.0346 - val_loss: 0.8146 - val_output_1_loss: 0.6664 - val_output_2_loss: 2.1485 Epoch 4/10 363/363 [==============================] - 1s 2ms/step - loss: 0.7706 - output_1_loss: 0.6679 - output_2_loss: 1.6955 - val_loss: 0.7338 - val_output_1_loss: 0.6259 - val_output_2_loss: 1.7048 Epoch 5/10 363/363 [==============================] - 1s 2ms/step - loss: 0.7236 - output_1_loss: 0.6343 - output_2_loss: 1.5267 - val_loss: 0.6865 - val_output_1_loss: 0.5929 - val_output_2_loss: 1.5288 Epoch 6/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6894 - output_1_loss: 0.6060 - output_2_loss: 1.4392 - val_loss: 0.6652 - val_output_1_loss: 0.5730 - val_output_2_loss: 1.4948 Epoch 7/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6610 - output_1_loss: 0.5809 - output_2_loss: 1.3827 - val_loss: 0.6579 - val_output_1_loss: 0.5670 - val_output_2_loss: 1.4753 Epoch 8/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6364 - output_1_loss: 0.5584 - output_2_loss: 1.3385 - val_loss: 0.6042 - val_output_1_loss: 0.5226 - val_output_2_loss: 1.3384 Epoch 9/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6117 - output_1_loss: 0.5351 - output_2_loss: 1.3005 - val_loss: 0.5785 - val_output_1_loss: 0.4940 - val_output_2_loss: 1.3390 Epoch 10/10 363/363 [==============================] - 1s 2ms/step - loss: 0.5894 - output_1_loss: 0.5144 - output_2_loss: 1.2646 - val_loss: 0.5881 - val_output_1_loss: 0.5014 - val_output_2_loss: 1.3680 162/162 [==============================] - 0s 1ms/step - loss: 0.5706 - output_1_loss: 0.4966 - output_2_loss: 1.2357 . 서브클래싱 API로 만든 모델입니다. 잘 실행됩니다. . &#49324;&#50857;&#51088; &#51221;&#51032; &#47784;&#45944; . . 다음과 같은 모델을 직접 정의하려고 합니다. . class ResidualBlock(keras.layers.Layer): def __init__(self, n_layers, n_neurons, **kwargs): super().__init__(**kwargs) # 히든층을 만들어줍니다. n_layers 개수만큼 층을 쌓습니다. self.hidden = [keras.layers.Dense(n_neurons, activation = &#39;elu&#39;, kernel_initializer = &#39;he_normal&#39;) for _ in range(n_layers)] def call(self, inputs): # 활성화 함수를 쓸때 쓰는 함수 # 인풋을 받으면 계속 히든 층에 레이어에 투입시킵니다. Z = inputs for layer in self.hidden: Z = layer(Z) return inputs + Z . 레이어의 수를 입력받아 그만큼 히든층을 만드는 작업을 해줍니다. 이때 잔차 블록은 출력에 입력을 더합니다. . class ResidualRegressor(keras.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self.hidden1 = keras.layers.Dense(30, activation = &#39;elu&#39;, kernel_initializer = &#39;he_normal&#39;) self.block1 = ResidualBlock(2, 30) self.block2 = ResidualBlock(2, 30) self.out = keras.layers.Dense(output_dim) def call(self, inputs): Z = self.hidden1(inputs) for _ in range(1 + 3): Z = self.block1(Z) Z = self.block2(Z) return self.out(Z) . 생성자에서 층을 만들고 call 메서드에서 이를 사용합니다. 이 모델은 다른 일반 모델처럼 사용도 가능합니다. . model = ResidualRegressor(1) model.compile(loss = &#39;mse&#39;, optimizer = &#39;nadam&#39;) history = model.fit(X_train_scaled, y_train, epochs = 5) score = model.evaluate(X_test_scaled, y_test) . Epoch 1/5 363/363 [==============================] - 2s 2ms/step - loss: 7.8337 Epoch 2/5 363/363 [==============================] - 1s 2ms/step - loss: 1.1220 Epoch 3/5 363/363 [==============================] - 1s 2ms/step - loss: 1.3495 Epoch 4/5 363/363 [==============================] - 1s 2ms/step - loss: 0.9622 Epoch 5/5 363/363 [==============================] - 1s 2ms/step - loss: 0.5799 162/162 [==============================] - 0s 1ms/step - loss: 0.5701 . 방금 정의한 모델을 적용시킨 결과입니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/12/handssu3.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/12/handssu3.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "[SSUDA] 심층 신경망 훈련하기",
            "content": ". import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . &#44592;&#52488; &#54876;&#49457;&#54868; &#54632;&#49688; . x = np.arange(-10, 10, 0.1) y = 1 / (1 + np.exp(-x)) plt.plot(x,y) plt.axhline(0.5,linewidth=0.5) plt.show() . &lt;matplotlib.lines.Line2D at 0x7fe0c8707a50&gt; . 시그모이드 함수 형태입니다. 입력값에 따라 값을 0~1로 변환해 주기 때문에 출력층에 사용하기 좋은 함수입니다. . 단조 증가함수이며 비선형함수라는 점은 활성화 함수로 사용하는데 유리합니다. . 하지만 최대 기울기는 0.25로 기울기가 상대적으로 평탄해 역방향 계산시 신경망 학습이 늦어지는 단점이 있습니다. . x = np.arange(-10, 10, 0.1) y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 하이퍼탄젠트 함수입니다. 시그모이드 함수와 전반적으로 비슷한 형태이고, 출력값이 -1~1로 차이가 있습니다. . 단조 증가함수이며 비선형함수라는 점이 시그모이드 함수와 비슷한 특징을 가집니다. . 그리고 기울기의 최댓값이 1이고 값의 평균이 0으로, 시그모이드 함수보다 큰 장점이 있습니다. . 하지만 입력값의 절대값이 2를 넘어가는 순간 출력값이 수렴하는 모습을 볼 수 있습니다. (시그모이드 또한 조금 늦게 수렴하지만 비슷합니다.) . 이 경우 기울기가 0에 가까워지게 되고 그레이언트 소실 문제가 생깁니다. . 쉽게 말해서 기울기가 O에 가깝기 때문에 학습이 진행이 안되는 현상이 발생합니다. 이를 방지하기 위해 RELU함수를 사용합니다. . x = np.arange(-10, 10, 0.1) y = np.where(x &gt; 0, x, 0) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 렐루 함수는 입력값이 0보다 크면 그대로, 0보다 작으면 0을 출력하는 함수입니다. . 입력값에 따라 on/off 해준다는 관점으로 볼 수 있고, 기울기도 1로 잘 유지가 됩니다. . Gradient Vanishing 문제(layer가 늘어날때 값이 사라지는 현상)가 해결되기 때문에, 가장 기본적인 활성화 함수로 사용합니다. . 하지만 0을 기준으로 명확하게 성질이 갈린다는 것은 단점입니다. 이를 극복하기 위해 leaky RELU 함수를 사용합니다. . &#47120;&#47336; &#54632;&#49688; &#48320;&#54805; . x = np.arange(-10, 5, 0.1) y = np.where(x &gt; 0, x, 0.01 * x) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 리키 렐루 함수는 0이상의 입력값일때 렐루함수와 같으며 0이하의 입력값일때 입력을 미세하게 유지하는 함수입니다. . 0이하의 입력값에 0.01을 곱하는 대신 다른 방식의 값을 준 함수들도 계속 소계하겠습니다. . 다만 연산비용이 상대적으로 크기 때문에 실제로 일반 렐루함수 또한 많이 사용합니다. . a = 1 # 조정가능 x = np.arange(-10, 10, 0.1) y = np.where(x &gt; 0, x, a*(np.exp(x) - 1)) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 활성화 함수로 많이 사용하는 EUL 함수 입니다. 0 이하 입력값에서 비선형함수를 쓴 것이 특징입니다. . 0 이하의 입력값에서 죽은 뉴런을 만들지 않는 것이 장점이고, 비선형함수가 섞여있기에 성능도 좋습니다. . 하지만 지수함수의 계산이기 때문에 속도가 확실히 느려진다는 단점이 있습니다. . 다음으로 SELU 함수입니다. ELU함수의 변종으로 자기 정규화가 일어나는것이 특징입니다. . 자기 정규화란 입력특성이 표준화되어 있을때 각 층의 출력이 평균 0과 표준편차 1를 유지하는 경향을 말합니다. . 이는 매우 좋은 성질로, 그레이디언트 소실, 폭주 문제를 막아주는 효과가 있습니다. . 자기 정규화를 일으키는 조건을 만족시킨다면 SELU가 성능이 우수하고, 조건을 만족하기 힘들때 ELU를 많이 사용합니다. . &#49324;&#51204; &#54617;&#49845; . 큰 규모의 DNN을 처음부터 새로 훈련하는 것 보다 비슷한 유형의 신경망을 찾은 이후에 그 신경망의 상위 은닉층을 재사용 하는 방법도 있습니다. . 이렇게 할 경우 훈련 속도는 당연히 크게 높아지고, 훈련 데이터도 적게 써도 좋은 성능을 낼 수 있습니다. . &#50741;&#54000;&#47560;&#51060;&#51200; . . 손실함수를 작게 하기 위해 최적의 파라미터를 찾는 방법들입니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : SGD . Stochastic Gradient Descent(확률적 경사 하강법) 가장 기본이 되는 옵티마이저 입니다. . 값을 하나씩 대입하여 가장 최적의 파라미터를 찾는 방법입니다. 이때 최적의 파라미터는 손실함수를 작게 만드는 값이죠. . 일반적으로 일부 데이터(미니 배치)를 이용해서 기울기를 구한 뒤 최적의 파라미터를 찾습니다. . 하지만 최적의 값을 찾아가기 위한 방향설정이 뒤죽박죽하고, 최적의 러닝레이트(학습률)을 찾기 힘들기 때문에 대체제가 등장합니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : Momentum . 파라미터 업데이트시 지역 최솟값에 빠질수가 있습니다. SGD는 이런 문제를 겪을 가능성이 높죠. . 이를 해결하기 위해 이전 기울기도 파라미터 업데이트 하는 계산에 포함시킵니다. 물론 오래될수록 작은 비율로 포함합니다. . 이런 방식의 옵티마이저를 Momentum 라고 합니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : Adagrad . SGD는 모든 파라미터에 같은 러닝레이트(학습률)을 적용합니다. 하지만 수렴지역으로 빨리 수렴하기 위해선 다른방법이 필요한데요. . 파라미터마다 많이 업데이트 된 파라미터가 있을 것이고 적게 업데이트된 파라미터도 있을 것입니다. . 왜냐하면 특정 입력값이 0이 많이 나올경우 기울기가 0이기 때문에 파라미터 업데이트가 되지 않습니다. . 그러면 그 입력값은 업데이트가 많이 안됬기 때문에 다른 입력값보다 상대적으로 최적값과 차이가 큽니다. 다른 입력값보다 학습률이 커야합니다. . 이런 이유로 각 파라미터의 업데이트 빈도 수에 따라 러닝레이트(학습률)을 다르게 줍니다. 빈도가 높을수록 학습률을 낮게 설정합니다. . 이런 방식의 옵티마이저를 Adagrad 라고 합니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : RMSprop . RMSprop은 Adagrad 에서 학습률이 너무 작아지는 문제를 해결하기 위해 나온 옵티마이저 입니다. . 기존 Adagrad에서는 항들을 그냥 더하지만 RMSprop에서는 지수평균으로 더해집니다. . 즉 시간이 지난 배치일수록 영향력이 줄어드는 것을 알 수 있습니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : Adam . Adam은 Momentum과 RMSprop의 장점을 결합한 옵티마이저로 현재 가장 많이 사용되는 옵티마이저 중 하나입니다. . 방향을 중심으로 한 Momentum 과 보폭을 중심으로 한 RMSprop이 합쳐저 보폭도, 방향도 적절하게 조절한 옵티마이저 입니다. . 잘 모르겠다 싶으면 Adam을 사용하면 됩니다. . &#44508;&#51228; . 우선 l1과 l2 규제가 있습니다. l1규제는 파라미터 절대값을, l2규제는 파라미터 제곱값을 손실함수에 포함한 모형입니다. . l1은 라쏘, l2는 릿지 회귀와 유사합니다. 이는 딥러닝 모델이 과적합되는것을 막아줍니다. . 또 드롭아웃 규제가 있습니다. 이는 간단한데, 매 훈련 스텝에서 각 뉴런은 드롭아웃될 확률을 가집니다. . 이 방식은 이웃한 뉴런에 맞추어 적응하기 보다 자기 자신이 유용한 방식으로 학습합니다. 즉 몇 개의 입력 뉴런에만 지나치게 의존하지 않죠. . &#45712;&#45184;&#51216; . 오늘은 딥러닝을 하기 위해 알아야하는, 튜닝 가능한 것들에 대해 살펴봤습니다. . 이름만 알고 지나갔었던 것이 많았는데 조금 지루하긴 하지만 스스로 확실히 다지고 갔기 때문에 유익했던 것 같아요. . 실력은 아직 많이 부족하지만 빨리 딥러닝 관련해서 실습도 하고 싶습니다. . &#52280;&#44256; . 활성화 함수부분 . https://wooono.tistory.com/209 . https://hwk0702.github.io/ml/dl/deep%20learning/2020/07/09/activation_function/ . 옵티마이저 부분 . https://dbstndi6316.tistory.com/297 . https://seamless.tistory.com/38 . https://hiddenbeginner.github.io/deeplearning/2019/09/22/optimization_algorithms_in_deep_learning.html#Adagrad .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/05/handssu2.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/05/handssu2.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "[SSUDA] 케라스로 다층퍼셉트론 구현하기",
            "content": ". &#45936;&#51060;&#53552; &#45796;&#50868;&#47196;&#46300; . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . from tensorflow import keras fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data() . 케라스 내 대표적인 예제 데이터인 fashion_mnist 데이터를 사용했습니다. . X_train_full.shape . (60000, 28, 28) . X_test.shape . (10000, 28, 28) . 데이터는 총 7만개이고, 28행 28열 값 입니다. . X_train_full.dtype . dtype(&#39;uint8&#39;) . 데이터 내 0부터 255까지 픽셀값이 들어가고, 70000 28 28개로 데이터 양이 꽤 많습니다. . 그러므로 메모리를 절약하기 위해 uint8 자료형을 사용합니다. . plt.imshow(X_train_full[0], cmap = &#39;binary&#39;) plt.axis(&#39;off&#39;) plt.show() . 첫번째 샘플은 부츠인것 같군요. . Sequential Model &#44396;&#52629; . model = keras.models.Sequential() # Flatten은 입력데이터의 shape을 일렬로 변경하는 클래스입니다.(reshape(-1,1)과 유사) model.add(keras.layers.Flatten(input_shape=[28,28])) # Dence는 이전 뉴런과 완전 연결된 밀집뉴런층을 의미합니다. 뉴런 수와 활성화 함수를 정의합니다. model.add(keras.layers.Dense(300, activation = &#39;relu&#39;)) model.add(keras.layers.Dense(100, activation = &#39;relu&#39;)) model.add(keras.layers.Dense(10, activation = &#39;softmax&#39;)) . 딥러닝 모델을 케라스를 통해서 구축합니다. . 분류기 모델이기 때문에 마지막 활성화 함수를 소프트멕스로 해서 0~1 사이로 값을 출력하게 합니다. . model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_1 (Flatten) (None, 784) 0 dense_3 (Dense) (None, 300) 235500 dense_4 (Dense) (None, 100) 30100 dense_5 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________ . 모델의 정보를 summary 함수를 통해서 출력했습니다. . model.layers[1].get_weights() . [array([[-0.02904556, 0.01724293, 0.00127908, ..., -0.03015002, 0.02537476, 0.02381387], [-0.01983445, 0.04740578, -0.07353676, ..., -0.03010103, -0.05431781, 0.02538292], [-0.04257569, 0.00067744, -0.01834078, ..., -0.06939676, -0.04211871, 0.05452839], ..., [ 0.05041559, 0.01164866, -0.01733586, ..., -0.06081748, 0.03213695, 0.01312949], [ 0.0624944 , -0.04938972, -0.03898798, ..., -0.01868138, -0.01819434, 0.04299803], [ 0.06238632, -0.05809005, 0.03214301, ..., 0.06427555, 0.02793135, -0.03146121]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] . 각 층의 있는 파라미터 값을 get_weights 함수를 이용해서 출력할 수 있습니다. . &#47784;&#45944; &#54617;&#49845;&#54616;&#44592; . model.compile(loss = &#39;sparse_categorical_crossentropy&#39;, # 다중 분류 손실 함수. Y가 원핫인코딩이 아닐때 optimizer = &#39;sgd&#39;, # 확률적 경사 하강법 사용. metrics = [&#39;accuracy&#39;]) # 학습을 진행할 때 마다 출력할 평가방식. . 모델을 컴파일합니다. 이때 손실함수, 옵티마이저, 테스트 셋으로 측청하는 것을 입력해줍니다. . X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0 y_valid, y_train = y_train_full[:5000], y_train_full[5000:] X_test = X_test/255.0 . 이후 출처를 밝히겠지만 인터넷 내 블로그 코드를 보고 따라했는데요. 이 부분이 중간에 생략되어있어서 많은 혼란이 있었습니다. . 왜냐하면 딥러닝에서 가장 중요한 것은 X 데이터의 스케일링입니다. 한 개의 데이터당 28 * 28 = 786 개의 값이 입력됩니다. . 이 786개의 값이 스케일이 물론 같지만, 0~255 값을 가지게 되면 밑에 코드의 학습이 전혀 되지 않습니다.(정확도가 계속 0.1에 머무름) . 반드시! 255로 나누어줘서 X 값의 범위를 0~1로 만들어야합니다. 긴 시간 시행착오를 겪었지만 피와 살이 되는 경험이였고 잊지 않을 것 같아요. . # X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=34) history = model.fit(X_train, y_train, epochs = 30, # 에포크는 학습반복수 validation_data = (X_valid, y_valid)) . Epoch 1/30 1719/1719 [==============================] - 14s 8ms/step - loss: 0.7189 - accuracy: 0.7660 - val_loss: 0.5232 - val_accuracy: 0.8210 Epoch 2/30 1719/1719 [==============================] - 12s 7ms/step - loss: 0.4830 - accuracy: 0.8316 - val_loss: 0.4391 - val_accuracy: 0.8532 Epoch 3/30 1719/1719 [==============================] - 12s 7ms/step - loss: 0.4382 - accuracy: 0.8471 - val_loss: 0.4311 - val_accuracy: 0.8524 Epoch 4/30 1719/1719 [==============================] - 11s 6ms/step - loss: 0.4107 - accuracy: 0.8562 - val_loss: 0.4112 - val_accuracy: 0.8538 Epoch 5/30 1719/1719 [==============================] - 11s 6ms/step - loss: 0.3917 - accuracy: 0.8620 - val_loss: 0.3939 - val_accuracy: 0.8576 Epoch 6/30 1719/1719 [==============================] - 11s 7ms/step - loss: 0.3749 - accuracy: 0.8676 - val_loss: 0.3780 - val_accuracy: 0.8642 Epoch 7/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3615 - accuracy: 0.8722 - val_loss: 0.3638 - val_accuracy: 0.8702 Epoch 8/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3502 - accuracy: 0.8766 - val_loss: 0.3508 - val_accuracy: 0.8750 Epoch 9/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3404 - accuracy: 0.8789 - val_loss: 0.3416 - val_accuracy: 0.8782 Epoch 10/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3309 - accuracy: 0.8826 - val_loss: 0.3549 - val_accuracy: 0.8754 Epoch 11/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3223 - accuracy: 0.8858 - val_loss: 0.3391 - val_accuracy: 0.8788 Epoch 12/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3145 - accuracy: 0.8877 - val_loss: 0.3287 - val_accuracy: 0.8810 Epoch 13/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.3073 - accuracy: 0.8912 - val_loss: 0.3309 - val_accuracy: 0.8818 Epoch 14/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.2999 - accuracy: 0.8929 - val_loss: 0.3256 - val_accuracy: 0.8840 Epoch 15/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2933 - accuracy: 0.8942 - val_loss: 0.3213 - val_accuracy: 0.8864 Epoch 16/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2877 - accuracy: 0.8968 - val_loss: 0.3121 - val_accuracy: 0.8884 Epoch 17/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2820 - accuracy: 0.8977 - val_loss: 0.3247 - val_accuracy: 0.8810 Epoch 18/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.2765 - accuracy: 0.9009 - val_loss: 0.3157 - val_accuracy: 0.8874 Epoch 19/30 1719/1719 [==============================] - 11s 7ms/step - loss: 0.2710 - accuracy: 0.9027 - val_loss: 0.3118 - val_accuracy: 0.8868 Epoch 20/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2660 - accuracy: 0.9040 - val_loss: 0.3133 - val_accuracy: 0.8876 Epoch 21/30 1719/1719 [==============================] - 12s 7ms/step - loss: 0.2614 - accuracy: 0.9063 - val_loss: 0.3317 - val_accuracy: 0.8800 Epoch 22/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2565 - accuracy: 0.9075 - val_loss: 0.3060 - val_accuracy: 0.8890 Epoch 23/30 1719/1719 [==============================] - 9s 5ms/step - loss: 0.2524 - accuracy: 0.9091 - val_loss: 0.3061 - val_accuracy: 0.8886 Epoch 24/30 1719/1719 [==============================] - 9s 5ms/step - loss: 0.2482 - accuracy: 0.9097 - val_loss: 0.2999 - val_accuracy: 0.8904 Epoch 25/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2441 - accuracy: 0.9123 - val_loss: 0.2960 - val_accuracy: 0.8912 Epoch 26/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.2397 - accuracy: 0.9134 - val_loss: 0.2991 - val_accuracy: 0.8922 Epoch 27/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2358 - accuracy: 0.9148 - val_loss: 0.3040 - val_accuracy: 0.8914 Epoch 28/30 1719/1719 [==============================] - 8s 5ms/step - loss: 0.2317 - accuracy: 0.9157 - val_loss: 0.2910 - val_accuracy: 0.8932 Epoch 29/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2281 - accuracy: 0.9179 - val_loss: 0.3280 - val_accuracy: 0.8824 Epoch 30/30 1719/1719 [==============================] - 8s 4ms/step - loss: 0.2244 - accuracy: 0.9203 - val_loss: 0.2989 - val_accuracy: 0.8900 . 테스트와 트레인 셋으로 나눈뒤 모델을 학습합니다. 에포크가 지날때 마다 반드시 개선되지는 않습니다. . &#54617;&#49845; &#49884;&#44033;&#54868;&#54616;&#44592; . import pandas as pd pd.DataFrame(history.history).plot(figsize = (8,5)) plt.grid(True) plt.gca().set_ylim(0, 1) plt.show() . 학습데이터는 에포크가 지날때마다 계속 개선이 되는것으로 보이나 val 데이터는 개선이 되었다가 감소하는걸 반복합니다. . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552;&#47196; &#54217;&#44032;&#50752; &#50696;&#52769;&#54616;&#44592; . model.evaluate(X_test, y_test) . 313/313 [==============================] - 1s 2ms/step - loss: 0.3289 - accuracy: 0.8798 . [0.32892856001853943, 0.879800021648407] . 테스트 데이터로 모델을 평가했습니다. 과적합 되지 않고 비슷한 성능을 보입니다. . X_new = X_test[:3] y_proba = model.predict(X_new) y_proba.round(2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99], [0. , 0. , 0.99, 0. , 0.01, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32) . predict 함수를 이용하면 범주마다의 확률을 예측해줍니다. . y_proba.argmax(axis=-1) . array([9, 2, 1]) . 이전에는 predict_classes로 범주를 뽑아낼 수 있었으나 케라스 버전 업그레이드 이후 이 함수가 사라졌습니다. . argmax 함수를 통해 가장 큰 확률을 가진 범주를 추출했습니다. . &#54924;&#44480; &#45936;&#51060;&#53552; &#45796;&#50868;&#47196;&#46300; . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state = 42) X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_valid = scaler.transform(X_valid) X_test = scaler.transform(X_test) . california_housing 데이터를 이용해 주택가격을 예측하는 모델을 케라스를 이용해 딥러닝으로 구현해보겠습니다. . 우선 딥러닝 모델 적용시 가장 중요한 스케일링을 진행합니다. 이때 데이터 리키지가 일어나는것 항상 조심해야겠죠. . &#54924;&#44480; &#47784;&#45944; &#54617;&#49845;&#54616;&#44592; . model = keras.models.Sequential([ keras.layers.Dense(30, activation = &#39;relu&#39;, input_shape = X_train.shape[1:]), keras.layers.Dense(1) ]) model.compile(loss = &#39;mean_squared_error&#39;, optimizer = keras.optimizers.SGD(lr = 1e-3)) history = model.fit(X_train, y_train, epochs = 20, validation_data =(X_valid, y_valid)) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(SGD, self).__init__(name, **kwargs) . Epoch 1/20 363/363 [==============================] - 2s 4ms/step - loss: 2.0042 - val_loss: 1.5977 Epoch 2/20 363/363 [==============================] - 1s 3ms/step - loss: 0.7294 - val_loss: 0.6821 Epoch 3/20 363/363 [==============================] - 1s 3ms/step - loss: 0.6309 - val_loss: 0.5893 Epoch 4/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5921 - val_loss: 0.5399 Epoch 5/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5632 - val_loss: 0.5151 Epoch 6/20 363/363 [==============================] - 1s 3ms/step - loss: 0.5404 - val_loss: 0.4985 Epoch 7/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5214 - val_loss: 0.4943 Epoch 8/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5060 - val_loss: 0.4857 Epoch 9/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4932 - val_loss: 0.4814 Epoch 10/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4827 - val_loss: 0.4671 Epoch 11/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4734 - val_loss: 0.4758 Epoch 12/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4657 - val_loss: 0.4704 Epoch 13/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4588 - val_loss: 0.4573 Epoch 14/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4529 - val_loss: 0.4710 Epoch 15/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4478 - val_loss: 0.4527 Epoch 16/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4430 - val_loss: 0.4503 Epoch 17/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4386 - val_loss: 0.4426 Epoch 18/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4349 - val_loss: 0.4592 Epoch 19/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4313 - val_loss: 0.4514 Epoch 20/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4281 - val_loss: 0.4557 . 우선 회귀모델이니 마지막 층에 소프트맥스 같은 활성화함수는 필요하지 않습니다. 또 손실함수는 MSE를 사용하였네요. . &#54924;&#44480; &#47784;&#45944; &#54217;&#44032;&#50752; &#50696;&#52769; . mse_test = model.evaluate(X_test, y_test) . 162/162 [==============================] - 0s 1ms/step - loss: 0.4182 . 테스트 데이터로 모델을 평가했는데, 로스값이 더 좋은 모습입니다. . X_new = X_test[:3] model.predict(X_new) . array([[0.54969954], [1.6468697 ], [3.592553 ]], dtype=float32) . 모델을 이용해 새 데이터를 예측했습니다. . plt.plot(pd.DataFrame(history.history)) plt.grid(True) plt.gca().set_ylim(0,1) plt.show() . 에포크가 진행될수록 트레인 데이터의 mse는 감소합니다. 하지만 valid 데이터는 일정수준이상이 되면 진동합니다. . &#49888;&#44221;&#47581; &#54616;&#51060;&#54140;&#54028;&#46972;&#48120;&#53552; &#53916;&#45789;&#54616;&#44592; . 머신러닝의 다른 모델들과 다르게 신경망은 매우 많은 하이퍼 파라미터가 있습니다. . 예시로 배치 사이즈, 옵티마이저, 학습률, 은닉층의 수, 활성화함수, loss, 드롭아웃 등 많습니다. . 우선 배치 사이즈의 경우 크면 메모리 문제가 많이 생깁니다. 또 훈련 초기 종종 불안정하기 훈련되기도 합니다. . 만약 너무 작으면 학습시간이 오래걸리고 노이즈도 커지는데요. gpu를 사용하기 때문에 2의 제곱수를 많이 사용합니다.(주로 32) . 이 부분은 의견이 많이 갈리기도 하는 부분이에요. . 다음은 옵티마이저 입니다. 간단하게 사진으로 대체했는데 사실 정확히 이해하기 힘듭니다. 따로 시간내서 공부해야합니다. . 일반적으로 Adam이 성능이 좋다고 합니다. . . 학습률은 디폴트가 0.01으로 크면 발산하고 작으면 지역 최저점에 머물 확률이 높아요. 옵티마이저 내부옵션으로도 사용됩니다. . 활성화함수는 은닉층에서 주로 Relu를 주로 사용합니다. 모델에 따라 tanh등을 쓰기도 합니다. . 은닉층은 많아지면 학습시간 문제, 과적합 문제가 많이 생깁니다. . &#45712;&#45184;&#51216; . 이번엔 텐서플로우 내 케라스라는 패키지를 이용해 딥러닝을 간단히 맛보았습니다. . 케라스는 고수준 패키지로 쉽게 사용할 수 있으나 세밀한 작업은 하지 못하는데요. . 간단한 예제 데이터를 통해 딥러닝이라는 것을 해봤습니다. 오늘 느낀것은 공부할 것이 참 많겠다 생각이 들었어요. . 계속 발전하는 분야라서 인터넷 글마다 정보도 다르고 한데, 저도 발전되는 그 곳에 함께하고 싶네요. . 더 열의를 다지는 시간이였던것 같아요. . &#52280;&#44256; . 케라스 예제: https://beoks.tistory.com/56?category=854491 . 하이퍼파라미터 부분 : https://velog.io/@qksekf/%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9DETF-Keras-Tuner#6-activation-fuction%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98 .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/karas/classifier/regression/2022/01/02/handssu1.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/karas/classifier/regression/2022/01/02/handssu1.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "[처음 시작하는 딥러닝] 2. 밑바닥부터 만들어보는 딥러닝",
            "content": ". &#49888;&#44221;&#47581;&#51032; &#44396;&#49457;&#50836;&#49548;: &#50672;&#49328; . def assert_same_shape(array, array_grad): assert array.shape == array_grad.shape return None . class Operation(object): def __init__(self): pass def forward(self, input_): # input_은 ndarray self.input_ = input_ self.output = self._output() return self.output def backward(self, output_grad): # output_grad은 ndarray assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) assert_same_shape(self.input_, self.input_grad) return self.input_grad # 서브 클래스에서 오버라이딩 하기 위해 있는 부분 # 이 클래스에서는 추상 메소드만 선언함 def _output(self): raise NotImplementedError() def _input_grad(self, output_grad): raise NotImplementedError() . 상속을 해주기 위한 간단한 연산 클래스를 만들었습니다. . forward 부분은 input_을 받고 아웃풋을 내는 함수를 호출합니다. . backward 부분은 output_grad를 받고 _input_grad 함수를 호출합니다. . 여기서 assert_same_shape은 입력값이 정상인지 확인하는 역할을 합니다. . class ParamOperation(Operation): def __init__(self, param): super().__init__() # 상속 받은 클레스의 생성자를 실행해줘야함 self.param = param # 파라미터도 입력받음. def backward(self, output_grad): assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) self.param_grad = self._param_grad(output_grad) assert_same_shape(self.input_, self.input_grad) assert_same_shape(self.param, self.param_grad) return self.input_grad def _param_grad(self, output_grad): raise NotImplementedError() . 파라미터가 있는 연산를 정의하기 위해 제일 기본적인 연산 클래스를 상속받아 만들었습니다. . backward 함수는 _input_grad, _param_grad(출력물에 대한 입력값/파라미터 기울기) 함수를 호출해 값을 구합니다. . 이전 backward와 달라진점은 _param_grad 부분이 추가했다는 것입니다. . 이 클래스를 상속하는 클래스는 _output, _input_grad, _pram_grad 함수를 정의해야합니다. . import numpy as np # 신경망의 가중치 행렬 곱 연산, 순방향/역방향 모두 제공. class WeightMultiply(ParamOperation): def __init__(self, W): super().__init__(W) # Operation 클래스 내 forward 함수에서 내부 호출 당하는 함수, 순방향 출력 def _output(self): return np.dot(self.input_, self.param) # forward 함수 내 self.input_은 정해줌. # Operation 클래스 내 backward 함수에서 내부 호출 당하는 함수, 역방향 출력 # 입력의 대한 기울기, 파라미터에 대한 기울기를 두 함수로 만듬. def _input_grad(self, output_grad): # 입력에 대한 기울기 출력. return np.dot(output_grad, np.transpose(self.param, (1,0))) def _param_grad(self, output_grad): return np.dot(np.transpose(self.input_, (1,0)), output_grad) . ParamOperation를 상속해 신경망의 가중치 행렬 곱 연산을 하는 WeightMultiply를 만들었습니다. . class BiasAdd(ParamOperation): def __init__(self, B): assert B.shape[0] == 1 # 상속받은 클래스(ParamOperation)의 생성자(param) 값에 B를 넣어줌. super().__init__(B) # forward 호출하는 내부함수. def _output(self): return self.input_ + self.param # backward에서 호출하는 내부함수(output_grad 이미 입력받음), 입력에 대한 기울기 def _input_grad(self, output_grad): return np.ones_like(self.input_) * output_grad # 파라미터에 대한 기울기 def _param_grad(self, output_grad): param_grad = np.ones_like(self.param) * output_grad return np.sum(param_grad, axis = 0).reshape(1, param_grad.shape[1]) . 편향을 더하는 연산도 마찬가지로 ParamOperation를 상속하여 만들었습니다. . class Sigmoid(Operation): def __init__(self): super().__init__() def _output(self): return 1.0/(1.0 + np.exp(-1.0 * self.input_)) # 입력에 대한 기울기 계산 def _input_grad(self, output_grad): sigmoid_backward = self.output * (1.0 - self.output) input_grad = sigmoid_backward * output_grad return input_grad . 시그모이드 연산은 파라미터가 없기 때문에 Operation 클래스를 상속했습니다. . class Linear(Operation): def __init__(self): super().__init__() def _output(self): return self.input_ def _input_grad(self, output_grad): return output_grad . 입력을 받은 대로 출력해주는 Linear 클래스 입니다. . &#49888;&#44221;&#47581;&#51032; &#44396;&#49457;&#50836;&#49548;: &#52789; . class Layer(object): # 뉴런의 개수는 층의 너비에 해당 def __init__(self, neurons): # neurons : 층의 너비 self.neurons = neurons self.first = True self.params = [] self.param_grads = [] self.operations = [] # 층을 구현하는 메서드 def _setup_layer(self, num_in): raise NotImplementedError() # 입력값을 연산에 순서대로 통과시켜 순방향 계산을 함. def forward(self, input_): if self.first: # 처음 층을 만드는 것이면, _setup_layer 함수 실행. self._setup_layer(input_) self.first = False self.input_ = input_ for operation in self.operations: # 여러개의 operations 들의 합 input_ = operation.forward(input_) self.output = input_ return self.output # output_grad를 각 연산에 역순으로 통과시켜 역방향 계산을 함 def backward(self, output_grad): assert_same_shape(self.output, output_grad) for operation in reversed(self.operations): output_grad = operation.backward(output_grad) input_grad = output_grad self._param_grads() return input_grad # 각 operation 객체에서 _param_grad 값을 꺼냄 def _param_grads(self): self.param_grads = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): # 서브클래스에 속하는가? self.param_grads.append(operation.param_grad) # 각 operationn 객체에서 _params 값을 꺼냄 def _params(self): self.params = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): self.params.append(operation.param) . 층을 정의합니다. 이때 Operation 객체의 리스트를 operations 속성에 담고 있습니다. . class Dence(Layer): def __init__(self, neurons, activation): super().__init__(neurons) self.activation = activation # 밀집층의 연산 정의 def _setup_layer(self, input_): if self.seed: np.random.seed(self.seed) self.params = [] # 가중치 self.params.append(np.random.randn(input_.shape[1], self.neurons)) #편향 self.params.append(np.random.randn(1, self.neurons)) self.operations = [WeightMultiply(self.params[0]), # 신경망의 가중치 행렬곱 연산 BiasAdd(self.params[1]), self.activation] return None . layer 클래스에 _setup_layer 함수를 추가로 구현하기 위해 Dence(밀집층) 클래스를 생성했습니다. . 랜덤 시드를 받아서 파라미터 초기값에 랜덤값을 넣어줍니다. . 여기서 operatings을 정의하는데 층을 하나 만들기 위해서 여러 연산 클래스를 사용합니다. . &#49888;&#44221;&#47581;&#51032; &#44396;&#49457;&#50836;&#49548;: &#49552;&#49892;&#54632;&#49688; . class Loss(object): def __init__(self): pass # 실제 손실값을 계산하는 함수 def forward(self, prediction, target): assert_same_shape(prediction, target) self.prediction = prediction self.target = target loss_value = self._output() return loss_value # 손실함수의 입력값에 대해 손실의 기울기를 계산. def backward(self): self.input_grad = self._input_grad() assert_same_shape(self.prediction, self.input_grad) return self.input_grad def _output(slef): raise NotImplementedError() def _input_grad(slef): raise NotImplementedError() . 손실함수를 구성하는 Loss 클래스 입니다. 타겟값과 예측값을 가지고 _output 함수를 돌려 loss를 구합니다. . backward 함수는 입력 값에 따른 손실의 기울기를 계산해줍니다. . class MeanSquaredError(Loss): def __init__(self): super().__init__() # 평균 제곱오차 손실함수 def _output(self): loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0] return loss # 예측값에 대한 평균제곱오차 손실의 기울기를 계산 def _input_grad(self): return 2.0 * (self.prediction - self.target) / self.prediction.shape[0] . Loss 클래스를 상속받아 MeanSquaredError 클래스를 만들었는데요. _output, _input_grad 두 함수의 수식을 구체적으로 구현했습니다. . &#46373;&#47084;&#45789;&#51032; &#44396;&#49457;&#50836;&#49548;: &#45684;&#47088; . class NeuralNetwork(object): def __init__(self, layers, loss, seed = 1): self.layers = layers # 신경망의 층 정의, layers 클래스를 받음.(리스트로 받을수도) self.loss = loss # loss 클래스를 받음. self.seed = seed if seed: for layer in self.layers: setattr(layer, &#39;seed&#39;, self.seed) # layer.seed = self.seed와 동일 # 데이터를 각 층에 순서대로 통과시킴 def forward(self, x_batch): # x_batch는 ndarray. x_out = x_batch for layer in self.layers: x_out = layer.forward(x_out) return x_out # 데이터를 각 층에 역순으로 통과시킴 def backward(self, loss_grad): grad = loss_grad for layer in reversed(self.layers): grad = layer.backward(grad) return None def train_batch(self, x_batch, y_batch): # 순방향 계산 수행. predictions = self.forward(x_batch) # 손실값 계산 loss = self.loss.forward(predictions, y_batch) # 역방향 계산 수행 self.backward(self.loss.backward()) return loss # 신경망의 파라미터 값을 받음 def params(self): for layer in self.layers: yield from layer.params # 리스트에 있는 요소를 한개씩 밖으로 전달. # 신경망의 각 파라미터에 대한 손실값의 기울기를 받음. def param_grads(self): for layer in self.layers: yield from layer.param_grads . neural_network = NeuralNetwork( layers = [Dence(neurons = 13, activation = Sigmoid()), Dence(neurons = 1, activation = Linear())], loss = MeanSquaredError(), #learning_rata = 0.01 ) . 딥러닝 구현을 위해 NeuralNetwork 클래스를 구현했습니다. 앞서 구현한 층, 손실함수를 이용했습니다. . 우선 layers로 층 클래스를 리스트로 받습니다. 층 클래스는 또 뉴런 개수와 활성화함수인 연산 클래스를 입력해야합니다. . 또 loss에는 손실함수 클래스를 넣어주면 됩니다. . forward 부분에서는 입력된 x값을 여러 층에 차레대로 넣습니다. 층에서는 또 차레대로 연산을 해서 prediction을 출력합니다. . 그 후 loss 클래스를 이용해서 손실값을 계산합니다. 다음으로 손실의 기울기를 backward 함수에 넣습니다. . backward 함수에서는 손실의 기울기를 여러 층에 앞선 차레와 반대 순서로 넣습니다. . 이렇게 나온 backward 함수의 최종 값은 input의 기울기 입니다. 이 값을 통해 loss를 줄여가는게 학습에 방향이겠죠. . &#46373;&#47084;&#45789;&#51032; &#44396;&#49457;&#50836;&#49548;: &#50741;&#54000;&#47560;&#51060;&#51200; . class Optimizer(object): def __init__(self, lr = 0.01): self.lr = lr def step(self): pass . 옵티마이저의 간단한 추상클래스입니다. lr은 학습률을 의미합니다. . class SGD(Optimizer): def __init__(self, lr = 0.01): super().__init__(lr) def step(self): for (param, param_grad) in zip(self.net.params(), self.net.param_grads()): # 뉴런에 있는 파라미터들을 꺼내오는 함수를 씀. param -= self.lr * param_grad # 이게 과연 층이나 연산 클래스 내 param까지 영향을 끼칠까?? . 옵티마이저에서 step 부분을 SGD(확률적 경사 하강법)을 이용해서 구성한 모습입니다. . 구체적으로 학습률 * (loss값에 영향을 주는 param_grad값)으로 param 값을 업데이트 해나가는 방식입니다. . &#46373;&#47084;&#45789;&#51032; &#44396;&#49457;&#50836;&#49548;: Trainer . def permute_data(X, y): perm = np.random.permutation(X.shape[0]) # 크기만큼 데이터를 셔플해줌 return X[perm], y[perm] class Trainer(object): def __init__(self, net, optim): # net은 NeuralNetwork, optim은 Optimizer self.net = net self.optim = optim setattr(self.optim, &#39;net&#39;, self.net) def generate_batches(self, X, y, size = 32): # 배치 사이즈로 데이터를 쪼개는 함수. assert X.shape[0] == y.shape[0] N = X.shape[0] for ii in range(0, N, size): X_batch, y_batch = X[ii:ii+size], y[ii:ii+size] # 배치만큼 잘라서 yield X_batch, y_batch # 지속적으로 내보냄 def fit(self, X_train, y_train, X_test, y_test, epochs = 100, eval_every = 10, batch_size = 32, seed = 1, restart = True): # eval_every 주기로 테스트 데이터를 사용해 예측성능 추정 np.random.seed(seed) if restart: for layer in self.net.layers: # 뉴런 내 모든 층은 layer.first = True # 층을 초기화하라. for e in range(epochs): X_train, y_train = permute_data(X_train, y_train) # 데이터 셔플 # 데이터가 배치 크기만큼 쪼개짐. batch_generator = self.generate_batches(X_train, y_train, batch_size) for ii, (X_batch, y_batch) in enumerate(batch_generator): # enumerate는 인덱스를 함께 출력해줌. self.net.train_batch(X_batch, y_batch) # 학습. self.optim.step() # 학습 후 나온 파라미터를 업데이트 해줌. if (e+1) % eval_every ==0: test_preds = self.net.forward(X_test) loss = self.net.loss.forward(test_preds, y_test) print(f&#39;{e+1}에폭에서 검증 데이터에 대한 손실값: {loss:.3f}&#39;) . 뉴런과 옵티마이저 클래스를 사용하는 트레이너 클래스입니다. . fit 함수로 데이터를 입력받아 데이터를 배치 단위로 쪼갠뒤 배치 데이터를 적용시켜 loss와 파라미터 기울기를 구합니다. . 그 후 옵티마이저 내 step 함수를 사용해 파라미터를 파라미터 기울기를 사용해서 업데이트 해줍니다. . &#50696;&#51228; &#51088;&#47308; &#50629;&#47196;&#46300; . from sklearn.datasets import load_boston boston = load_boston() data = boston.data target = boston.target features = boston.feature_names . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2. The Boston housing prices dataset has an ethical problem. You can refer to the documentation of this function for further details. The scikit-learn maintainers therefore strongly discourage the use of this dataset unless the purpose of the code is to study and educate about ethical issues in data science and machine learning. In this special case, you can fetch the dataset from the original source:: import pandas as pd import numpy as np data_url = &#34;http://lib.stat.cmu.edu/datasets/boston&#34; raw_df = pd.read_csv(data_url, sep=&#34; s+&#34;, skiprows=22, header=None) data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]) target = raw_df.values[1::2, 2] Alternative datasets include the California housing dataset (i.e. :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing dataset. You can load the datasets as follows:: from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() for the California housing dataset and:: from sklearn.datasets import fetch_openml housing = fetch_openml(name=&#34;house_prices&#34;, as_frame=True) for the Ames housing dataset. warnings.warn(msg, category=FutureWarning) . from sklearn.preprocessing import StandardScaler s = StandardScaler() data = s.fit_transform(data) . def to_2d_np(a,type = &#39;col&#39;): &#39;&#39;&#39; 1차원 텐서를 2차원으로 변환 &#39;&#39;&#39; assert a.ndim == 1, &quot;입력된 텐서는 1차원이어야 함&quot; if type == &quot;col&quot;: return a.reshape(-1, 1) elif type == &quot;row&quot;: return a.reshape(1, -1) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718) # 목푯값을 2차원 배열로 변환 y_train, y_test = to_2d_np(y_train), to_2d_np(y_test) . &#50696;&#49884; . optimizer = SGD(lr = 0.01) trainer = Trainer(neural_network, optimizer) trainer.fit(X_train, y_train, X_test, y_test, epochs = 50) . 10에폭에서 검증 데이터에 대한 손실값: 32.121 20에폭에서 검증 데이터에 대한 손실값: 26.972 30에폭에서 검증 데이터에 대한 손실값: 20.426 40에폭에서 검증 데이터에 대한 손실값: 18.131 50에폭에서 검증 데이터에 대한 손실값: 16.930 . &#45712;&#45184;&#51216; . 딥러닝 관련해서 저번에 신경망을 간단하게 구현을 했었습니다. . 오늘은 은닉층이 더 복잡해지기 때문에 일반화에 용이한 클래스로 딥러닝을 구현했습니다. . 자바로 객체지향프로그래밍을 조금 안 상태에서 학습을 해도 파이썬 문법하고 다른 측면이 있어서 학습이 다소 힘들긴 했습니다. . 처음 연산/층 클래스를 구현할 때는 이게 무슨 코드인지 이해가 안되고 재미도 없었는데 뉴런 부분을 구현할 때 전반적으로 책에서 정리를 해줘서 그 때 전반적인 감을 잡았던 것 같아요. . 코드도 일부 누락되어 있어 깃허브 찾아보면서 매꾸는 등 어려운 과정이 참 많았지만 하길 잘 한것 같습니다. . 딥러닝이란 무엇인가 정말 피부로 체감을 할 수 있었습니다. 그만큼 하나하나 천천히 이해하는데 시간이 오래걸리긴 했지만요. . 맨날 나오는 신경망, 뉴런, 옵티마이저, 트레이너 등등 단어의 의미를 이전보다 훨씬 직관적으로 이해를 잘 할 수 있었던 시간인것 같습니다. . 과정이 다소 복잡하기 때문에 주기적으로 복습을 하며 더 딥러닝과 가까워질 생각입니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/deep%20learning/matrix/math/class/2021/12/29/FirstDeep2.html",
            "relUrl": "/book/jupyter/deep%20learning/matrix/math/class/2021/12/29/FirstDeep2.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "[시계열분석] 2. ARIMA",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . install.packages(&quot;TTR&quot;) install.packages(&quot;forecast&quot;) install.packages(&quot;tseries&quot;) library(TTR) library(forecast) library(tseries) data &lt;- scan(&quot;http://robjhyndman.com/tsdldata/data/nybirths.dat&quot;) birth &lt;- ts(data, frequency = 12, start = c(1946, 1)) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) also installing the dependencies ‘xts’, ‘zoo’ Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) also installing the dependencies ‘quadprog’, ‘quantmod’, ‘fracdiff’, ‘lmtest’, ‘timeDate’, ‘tseries’, ‘urca’, ‘RcppArmadillo’ Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) Registered S3 method overwritten by &#39;quantmod&#39;: method from as.zoo.data.frame zoo . 필요한 패키지들을 설치하고, &#39;뉴욕 월별 출생자수&#39; 데이터를 입력받습니다. . ts 함수를 쓰면 보다 편리하게 원 자료를 시계열 자료로 변환이 가능합니다. . data . &lt;ol class=list-inline&gt;26.663 | 23.598 | 26.931 | 24.74 | 25.806 | 24.364 | 24.477 | 23.901 | 23.175 | 23.227 | 21.672 | 21.87 | 21.439 | 21.089 | 23.709 | 21.669 | 21.752 | 20.761 | 23.479 | 23.824 | 23.105 | 23.11 | 21.759 | 22.073 | 21.937 | 20.035 | 23.59 | 21.672 | 22.222 | 22.123 | 23.95 | 23.504 | 22.238 | 23.142 | 21.059 | 21.573 | 21.548 | 20 | 22.424 | 20.615 | 21.761 | 22.874 | 24.104 | 23.748 | 23.262 | 22.907 | 21.519 | 22.025 | 22.604 | 20.894 | 24.677 | 23.673 | 25.32 | 23.583 | 24.671 | 24.454 | 24.122 | 24.252 | 22.084 | 22.991 | 23.287 | 23.049 | 25.076 | 24.037 | 24.43 | 24.667 | 26.451 | 25.618 | 25.014 | 25.11 | 22.964 | 23.981 | 23.798 | 22.27 | 24.775 | 22.646 | 23.988 | 24.737 | 26.276 | 25.816 | 25.21 | 25.199 | 23.162 | 24.707 | 24.364 | 22.644 | 25.565 | 24.062 | 25.431 | 24.635 | 27.009 | 26.606 | 26.268 | 26.462 | 25.246 | 25.18 | 24.657 | 23.304 | 26.982 | 26.199 | 27.21 | 26.122 | 26.706 | 26.878 | 26.152 | 26.379 | 24.712 | 25.688 | 24.99 | 24.239 | 26.721 | 23.475 | 24.767 | 26.219 | 28.361 | 28.599 | 27.914 | 27.784 | 25.693 | 26.881 | 26.217 | 24.218 | 27.914 | 26.975 | 28.527 | 27.139 | 28.982 | 28.169 | 28.056 | 29.136 | 26.291 | 26.987 | 26.589 | 24.848 | 27.543 | 26.896 | 28.878 | 27.39 | 28.065 | 28.141 | 29.048 | 28.484 | 26.634 | 27.735 | 27.132 | 24.924 | 28.963 | 26.589 | 27.931 | 28.009 | 29.229 | 28.759 | 28.405 | 27.945 | 25.912 | 26.619 | 26.076 | 25.286 | 27.66 | 25.951 | 26.398 | 25.565 | 28.865 | 30 | 29.261 | 29.012 | 26.992 | 27.897 | &lt;/ol&gt; birth . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 194626.663 | 23.598 | 26.931 | 24.740 | 25.806 | 24.364 | 24.477 | 23.901 | 23.175 | 23.227 | 21.672 | 21.870 | . 194721.439 | 21.089 | 23.709 | 21.669 | 21.752 | 20.761 | 23.479 | 23.824 | 23.105 | 23.110 | 21.759 | 22.073 | . 194821.937 | 20.035 | 23.590 | 21.672 | 22.222 | 22.123 | 23.950 | 23.504 | 22.238 | 23.142 | 21.059 | 21.573 | . 194921.548 | 20.000 | 22.424 | 20.615 | 21.761 | 22.874 | 24.104 | 23.748 | 23.262 | 22.907 | 21.519 | 22.025 | . 195022.604 | 20.894 | 24.677 | 23.673 | 25.320 | 23.583 | 24.671 | 24.454 | 24.122 | 24.252 | 22.084 | 22.991 | . 195123.287 | 23.049 | 25.076 | 24.037 | 24.430 | 24.667 | 26.451 | 25.618 | 25.014 | 25.110 | 22.964 | 23.981 | . 195223.798 | 22.270 | 24.775 | 22.646 | 23.988 | 24.737 | 26.276 | 25.816 | 25.210 | 25.199 | 23.162 | 24.707 | . 195324.364 | 22.644 | 25.565 | 24.062 | 25.431 | 24.635 | 27.009 | 26.606 | 26.268 | 26.462 | 25.246 | 25.180 | . 195424.657 | 23.304 | 26.982 | 26.199 | 27.210 | 26.122 | 26.706 | 26.878 | 26.152 | 26.379 | 24.712 | 25.688 | . 195524.990 | 24.239 | 26.721 | 23.475 | 24.767 | 26.219 | 28.361 | 28.599 | 27.914 | 27.784 | 25.693 | 26.881 | . 195626.217 | 24.218 | 27.914 | 26.975 | 28.527 | 27.139 | 28.982 | 28.169 | 28.056 | 29.136 | 26.291 | 26.987 | . 195726.589 | 24.848 | 27.543 | 26.896 | 28.878 | 27.390 | 28.065 | 28.141 | 29.048 | 28.484 | 26.634 | 27.735 | . 195827.132 | 24.924 | 28.963 | 26.589 | 27.931 | 28.009 | 29.229 | 28.759 | 28.405 | 27.945 | 25.912 | 26.619 | . 195926.076 | 25.286 | 27.660 | 25.951 | 26.398 | 25.565 | 28.865 | 30.000 | 29.261 | 29.012 | 26.992 | 27.897 | . class(birth) . &#39;ts&#39; &#45936;&#51060;&#53552; &#44288;&#52272; . plot(birth) . 전반적인 추세도 조금 있는 것 같고, 계절성이 있는 것 같아요. . 평균과 분산이 시간에 따라 달라지는 것으로 관찰되는데 정상성을 만족하진 않는 자료인것 같습니다. . autoplot(decompose(birth)) . 시계열 자료를 원 데이터, 트렌드, 계절성, 그외 오차로 나눠주는 autoplot(decompose()) 함수 입니다. . 자료를 시각적으로 보기 매우 좋은 것 같아요. . acf(birth, lag.max=20) . acf 자기상관계수 입니다. 빠르게 감소하는 지점이 있다면 MA를 쓰는게 좋으나 그렇지 않아 보입니다. . pacf(birth, lag.max=20) . pacf 편자기상관계수 입니다. 빠르게 감소하는 지점이 있다면 AR을 쓰는 것이 적절합니다. . AR &#47784;&#45944; &#51201;&#54633; . fit &lt;- ar(birth, method = &#39;mle&#39;) fit . Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): “possible convergence problem: optim gave code = 1” Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): “possible convergence problem: optim gave code = 1” . Call: ar(x = birth, method = &#34;mle&#34;) Coefficients: 1 2 3 4 5 6 7 8 0.4043 0.2923 -0.0888 -0.0623 0.0544 -0.0443 -0.0171 -0.0640 9 10 11 12 0.1755 0.1134 -0.2982 0.5094 Order selected 12 sigma^2 estimated as 0.8879 . 자동으로 P값을 정해서 계산해줍니다. . est.1 &lt;- arima(birth, order = c(11,0,0), fixed = c(0,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA)) est.1 . Warning message in arima(birth, order = c(11, 0, 0), fixed = c(0, NA, NA, NA, NA, : “some AR parameters were fixed: setting transform.pars = FALSE” . Call: arima(x = birth, order = c(11, 0, 0), fixed = c(0, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)) Coefficients: ar1 ar2 ar3 ar4 ar5 ar6 ar7 ar8 ar9 0 0.5759 0.1475 -0.1288 0.0248 -0.0355 -0.0245 -0.1414 0.1476 s.e. 0 0.0773 0.0818 0.0880 0.0871 0.0894 0.0907 0.0897 0.0897 ar10 ar11 intercept 0.3941 -0.0112 25.6374 s.e. 0.0816 0.0800 1.2054 sigma^2 estimated as 1.319: log likelihood = -263.59, aic = 551.18 . order = (p,d,q) 이고 fixed는 0인경우 그 항의 계수를 사용하지 않고 NA인 경우 계수를 사용하겠다는 의미입니다. . acf(est$residuals) . ACF 값이 안정회된 모습입니다. . Box.test(est$residuals) . Box-Pierce test data: est$residuals X-squared = 0.30321, df = 1, p-value = 0.5819 . 통계적 검증도 할 수 있습니다. . plot(birth, type = &#39;l&#39;) lines(fitted(est), col = 2, lty = 1) . 예측한 값을 그래프로 표현했습니다. . fitted(est, h = 3) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1946 NA | NA | NA | 26.39963 | 24.84025 | 26.07144 | 25.12577 | 26.22498 | 24.46344 | 25.62999 | 24.50757 | 24.29708 | . 194725.62441 | 22.54370 | 25.18824 | 22.03637 | 24.05069 | 22.91059 | 23.23670 | 22.26126 | 21.60718 | 22.83138 | 22.08378 | 22.79384 | . 194822.55647 | 21.83367 | 23.23437 | 21.66766 | 21.93387 | 21.82316 | 23.70223 | 23.53885 | 23.31222 | 23.58903 | 22.09234 | 22.28697 | . 194922.71987 | 20.63233 | 23.14931 | 21.52470 | 22.57879 | 22.01042 | 23.20781 | 22.89247 | 22.59651 | 23.66815 | 21.93154 | 22.83969 | . 195022.21951 | 20.81767 | 22.75981 | 21.74831 | 22.83386 | 24.12212 | 25.20356 | 25.09516 | 23.69563 | 23.70509 | 22.50564 | 23.09822 | . 195123.88766 | 22.16144 | 24.98893 | 23.51125 | 25.64303 | 23.99667 | 25.15761 | 24.15729 | 24.38610 | 25.20321 | 23.47784 | 24.36874 | . 195224.38436 | 23.63832 | 24.98735 | 24.15418 | 24.32212 | 24.60992 | 25.34410 | 24.73596 | 24.71976 | 25.34774 | 23.78109 | 24.75321 | . 195324.32585 | 22.72202 | 24.97050 | 23.41424 | 24.66142 | 25.16567 | 26.33526 | 25.78175 | 24.92267 | 25.88477 | 24.23444 | 25.65019 | . 195425.37727 | 24.27083 | 25.75235 | 24.18496 | 25.68157 | 25.46286 | 27.66508 | 27.08732 | 26.73748 | 26.32181 | 25.21637 | 25.31379 | . 195525.37057 | 24.11308 | 26.99524 | 25.63835 | 26.85388 | 25.90547 | 25.44878 | 25.33983 | 25.80368 | 27.18613 | 26.03324 | 27.22084 | . 195626.26596 | 24.77874 | 26.70676 | 24.55815 | 25.67215 | 27.00288 | 29.10629 | 29.16034 | 27.72744 | 28.09431 | 25.79614 | 27.04030 | . 195727.25297 | 25.49749 | 27.86099 | 26.58632 | 27.96767 | 26.60047 | 28.58001 | 28.08253 | 27.79993 | 28.32824 | 26.17898 | 27.54414 | . 195826.78286 | 25.84530 | 28.03310 | 26.76667 | 27.88224 | 27.63906 | 28.01756 | 27.48962 | 28.56473 | 28.70882 | 26.94130 | 27.60250 | . 195927.18522 | 25.08607 | 27.93733 | 25.79046 | 27.72664 | 27.22245 | 28.27216 | 27.24954 | 26.55731 | 27.49123 | 26.86171 | 27.76910 | . fitted 함수를 가지고 모델로 만든 예측값을 사용했습니다. h를 3으로 제한했기 때문에 앞 3개 값은 나올 수가 없습니다. . MA &#47784;&#45944; &#51201;&#54633; . ma.est = arima(birth, order = c(0,0,9)) ma.est . Call: arima(x = birth, order = c(0, 0, 9)) Coefficients: ma1 ma2 ma3 ma4 ma5 ma6 ma7 ma8 ma9 0.6346 0.9636 0.5796 0.9329 0.6558 0.7272 0.5426 0.2053 0.4116 s.e. 0.0728 0.1019 0.1162 0.1137 0.1242 0.1304 0.1380 0.1186 0.0779 intercept 25.0739 s.e. 0.5602 sigma^2 estimated as 1.244: log likelihood = -261.17, aic = 544.33 . order = c(p, d, q) 에서 q값이 MA 개수를 결정합니다. . fitted(ma.est, h = 1) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 194625.95251 | 25.67143 | 25.40561 | 24.82399 | 26.19920 | 24.85453 | 24.84318 | 24.56051 | 23.66604 | 23.85843 | 22.47204 | 22.74461 | . 194721.84185 | 21.66282 | 21.80943 | 23.05167 | 23.45515 | 21.67424 | 22.22553 | 22.93592 | 24.34254 | 23.68077 | 22.79472 | 23.63520 | . 194821.75215 | 22.04347 | 21.34192 | 22.10192 | 23.88129 | 21.04898 | 23.39525 | 24.50027 | 23.74538 | 23.87659 | 22.05090 | 22.97825 | . 194921.21271 | 20.77855 | 21.63952 | 21.70288 | 22.21829 | 21.45152 | 24.03647 | 24.66171 | 23.90069 | 24.40842 | 22.92290 | 22.43632 | . 195021.67009 | 22.10285 | 22.36761 | 22.99342 | 25.27430 | 24.98947 | 25.48045 | 24.39333 | 25.02516 | 24.68061 | 22.67505 | 23.35984 | . 195122.19523 | 23.37591 | 23.54196 | 24.50552 | 25.98286 | 23.95424 | 25.24065 | 26.22603 | 25.54268 | 25.34561 | 24.44959 | 24.05728 | . 195223.30611 | 23.58397 | 23.01512 | 24.02431 | 23.98693 | 22.79686 | 26.13166 | 25.64112 | 26.16362 | 26.38574 | 24.02239 | 24.91265 | . 195323.25952 | 23.93089 | 23.87380 | 23.50638 | 25.78781 | 24.67518 | 25.76932 | 26.51528 | 27.00995 | 26.64015 | 25.28893 | 25.92493 | . 195424.85972 | 24.11851 | 23.56204 | 25.59806 | 27.34463 | 25.78523 | 27.48760 | 26.78627 | 26.53038 | 26.71510 | 24.66289 | 25.81467 | . 195524.39575 | 24.44927 | 25.02809 | 25.43336 | 25.62500 | 23.56793 | 26.44265 | 27.81548 | 28.05461 | 28.43752 | 27.09566 | 27.21027 | . 195625.36979 | 25.56654 | 24.94684 | 25.30917 | 27.68991 | 27.06423 | 28.63151 | 28.20405 | 28.76610 | 28.30218 | 26.83588 | 27.38308 | . 195725.26392 | 26.02184 | 25.01887 | 25.93747 | 28.28903 | 26.93810 | 29.01126 | 27.38416 | 27.39822 | 29.44724 | 26.91105 | 26.89012 | . 195826.88089 | 26.44548 | 25.57053 | 26.45435 | 28.26293 | 26.67500 | 28.00386 | 28.40625 | 29.06361 | 28.44631 | 26.25983 | 27.32193 | . 195925.22626 | 24.88989 | 25.60540 | 26.21734 | 26.92255 | 25.54339 | 26.56431 | 27.82080 | 29.60430 | 29.32033 | 28.02514 | 27.98380 | . fitted(ma.est, h = 10) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1946 NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | 25.07392 | 25.07392 | . 194725.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 194825.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 194925.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195025.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195125.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195225.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195325.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195425.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195525.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195625.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195725.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195825.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195925.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . h가 일정 수준 이상 커지면 MA 모델은 평균값으로 수렴합니다. . ARIMA &#47784;&#45944; &#51088;&#46041; &#51201;&#54633; . est &lt;- auto.arima(birth, stepwise = FALSE, max.p = 12, max.q = 10) est . Series: birth ARIMA(2,1,1)(1,1,1)[12] Coefficients: ar1 ar2 ma1 sar1 sma1 0.4349 -0.241 -0.4999 -0.2474 -0.8465 s.e. 0.1846 0.085 0.1854 0.0986 0.1004 sigma^2 estimated as 0.406: log likelihood=-157.78 AIC=327.56 AICc=328.12 BIC=345.82 . auto.arima 함수를 가지고 자동으로 arima 작업을 했습니다. . 이론적으로 p, q 값을 5 이하로 하는 것이 적당한데, 그에 맞는 결과입니다. . d값은 추세를 의미합니다. . fitted(est) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 194626.64761 | 23.59349 | 26.92523 | 24.73763 | 25.80315 | 24.36295 | 24.47600 | 23.90066 | 23.17539 | 23.22730 | 21.67376 | 21.88578 | . 194721.54343 | 19.36388 | 24.12830 | 21.13535 | 22.29703 | 20.42994 | 21.87548 | 23.07928 | 22.54888 | 22.71591 | 21.52740 | 22.16409 | . 194821.69823 | 19.93556 | 23.14138 | 21.42456 | 22.19807 | 21.02772 | 23.25265 | 23.36304 | 22.38182 | 22.31920 | 21.64650 | 21.36331 | . 194921.40590 | 19.95606 | 22.92007 | 20.36740 | 21.30675 | 20.88777 | 24.16941 | 23.37374 | 22.76478 | 23.29871 | 21.34524 | 22.00278 | . 195021.89446 | 20.86780 | 23.86333 | 22.58268 | 24.01855 | 24.13627 | 24.79698 | 24.52997 | 23.73809 | 24.46630 | 22.48823 | 22.58123 | . 195123.01638 | 21.62586 | 25.75329 | 22.76266 | 24.69739 | 23.99375 | 26.15594 | 26.02250 | 24.71870 | 25.22931 | 23.51161 | 23.49823 | . 195224.15874 | 22.04979 | 25.46094 | 23.01360 | 23.79163 | 23.55099 | 26.06118 | 25.66630 | 24.91792 | 25.29679 | 23.45490 | 23.78538 | . 195324.85942 | 22.82602 | 25.62198 | 24.11012 | 24.91516 | 24.91610 | 26.02685 | 26.60103 | 25.70685 | 26.28274 | 24.53334 | 25.73220 | . 195425.09205 | 23.41113 | 26.34662 | 25.32678 | 26.84850 | 26.69116 | 27.42612 | 26.53563 | 26.46959 | 26.25946 | 24.49761 | 25.56646 | . 195525.61437 | 23.52570 | 27.14373 | 24.89106 | 24.60156 | 24.94032 | 27.99554 | 27.51837 | 27.65685 | 27.69714 | 25.91558 | 26.36729 | . 195626.86700 | 24.66566 | 27.40157 | 26.69697 | 27.84578 | 27.73466 | 28.38714 | 28.71826 | 27.52005 | 28.28738 | 27.28773 | 26.76079 | . 195726.97638 | 25.41965 | 27.82162 | 25.86653 | 27.95865 | 28.52111 | 28.65298 | 28.17282 | 27.79869 | 29.06388 | 26.44817 | 27.42565 | . 195827.46425 | 25.68166 | 28.10134 | 27.39812 | 27.54637 | 27.78314 | 29.67634 | 28.82805 | 28.17901 | 28.78699 | 25.98107 | 26.86454 | . 195926.45649 | 24.74951 | 28.23975 | 26.15617 | 27.31077 | 26.00112 | 27.37088 | 28.74329 | 29.21407 | 28.90049 | 26.94265 | 27.87884 | . plot(birth, type = &#39;l&#39;) lines(fitted(est), col = 2, lty = 1) . 이전 모델보다 더 잘 적합된 모습입니다. . &#45712;&#45184;&#51216; . 간단한 ARIMA 모형에 대해서 공부했습니다. . AR와 MA가 무엇인지 부분적으로는 이해했는데 완벽히, 직관적으로, 누구에게 설명할 정도로 이해한 것 같진 않아서 조금 걱정입니다. . 요즘 딥러닝으로 시계열 자료를 많이 예측합니다만 소규모 데이터 셋은 과적합 방지를 위해, 또 설명력을 가지기 위해 ARIMA 모델도 사용합니다. . 시계열분석에 기초인 ARIMA 모델을 배워서 그동안 시계열 데이터를 보면 막막했던게 조금 사라진 것 같아요. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/timeseries/r/arima/2021/12/28/TimeSeries2.html",
            "relUrl": "/book/jupyter/timeseries/r/arima/2021/12/28/TimeSeries2.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "[Do it 자연어] 6. 질문에 답하는 웹 서비스 만들기",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.4 MB/s Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 4.8 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 16.6 MB/s Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 44.7 MB/s Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 44.4 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 45.4 MB/s Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 53.9 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 53.6 MB/s Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 25.7 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 550 kB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 51.6 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 44.2 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 6.2 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 51.0 MB/s Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 50.6 MB/s Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 51.1 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=a5637edc2a6d78b74eca383b627dad92eef464234aac303f8617646dbc1f4c65 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount= True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.qa import QATrainArguments args = QATrainArguments( pretrained_model_name= &#39;beomi/kcbert-base&#39;, # 프리트레인 모델 downstream_corpus_name=&#39;korquad-v1&#39;, # 입력 데이터 downstream_model_dir= &#39;/gdrive/My Drive/nlpbook/checkpoing-qa&#39;, # 중간저장위치 max_seq_length=128, # 입력문장 최대 길이(질문+지문) max_query_length=32, # 질문 최대 길이 doc_stride = 64, # 지문에서 몇개 토큰을 슬라이딩 해갈지? 결정 batch_size = 32 if torch.cuda.is_available() else 4, learning_rate=5e-5, epochs = 3, tpu_cores=0 if torch.cuda.is_available() else 8, seed = 7, ) . kcbert-base 모델로 프리트레인을 하였습니다. . LG CNS가 공개한 korqued 1.0 데이터를 활용해 모델을 구축하겠습니다. . 여기서 doc_stride은 만약 입력문장이 128보다 클 경우 지문 앞 부분을 입력값(64)만큼 제거하고 잘린 뒷부분을 가져와 붙입니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters QATrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_corpus_name=&#39;korquad-v1&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoing-qa&#39;, max_seq_length=128, doc_stride=64, max_query_length=32, threads=4, cpu_workers=2, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, fp16=False, tpu_cores=0, tqdm_enabled=True) INFO:ratsnlp:Training/evaluation parameters QATrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_corpus_name=&#39;korquad-v1&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoing-qa&#39;, max_seq_length=128, doc_stride=64, max_query_length=32, threads=4, cpu_workers=2, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, fp16=False, tpu_cores=0, tqdm_enabled=True) . set seed: 7 . 랜덤시드와 로거를 설정했습니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . nlpbook.download_downstream_dataset(args) . Downloading: 38.5MB [00:00, 71.7MB/s] Downloading: 3.88MB [00:00, 60.6MB/s] . downstream_corpus_name=&#39;korquad-v1&#39; 로 선언된 데이터를 내려받습니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . pretrained_model_name= &#39;beomi/kcbert-base&#39; 로 프리트레인을 해서 토크나이저를 생성합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.qa import KorQuADV1Corpus, QADataset corpus = KorQuADV1Corpus() # JSON 포멧의 KorQuAD 데이터를 읽어옴. train_dataset = QADataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39; ) . INFO:ratsnlp:Creating features from train dataset file at /root/Korpora/korquad-v1 INFO:ratsnlp:Creating features from train dataset file at /root/Korpora/korquad-v1 100%|██████████| 1420/1420 [00:00&lt;00:00, 4333.43it/s] convert squad examples to features: 100%|██████████| 57688/57688 [07:46&lt;00:00, 123.67it/s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:answer: 교 ##향 ##곡 INFO:ratsnlp:answer: 교 ##향 ##곡 INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=45, end_positions=47) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=45, end_positions=47) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=108, end_positions=110) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=108, end_positions=110) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=44, end_positions=46) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=44, end_positions=46) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/korquad-v1/cached_train_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 33.918 s] INFO:ratsnlp:Saving features into cached file /root/Korpora/korquad-v1/cached_train_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 33.918 s] . KorQuADV1Corpus 클레스로 KorQuAD 데이터를 QAExample 형식으로 읽어옵니다. . 여기서 QAExample은 질문, 지문, 정답, 정답의 시작 인덱스로 구성됩니다. . QADataset에서는 QAExample 형식으로 입력된 값들을 QAFeatures 형태로 가공해서 모델이 학습할 수 있도록 변환해줍니다. . 여기서 QAFeatures은 input_ids(입력 시퀀스), attention_mask(패딩 여부), token_type_ids(세그먼트 정보, 질문은 0/지문은 1), start(or end)_positions(정답 문장 위치) 총 5개로 구성됩니다. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . 데이터 로더를 구축했습니다. 일정 배치 크기만큼 뽑아서 모델을 학습시킬 수 있어요. . &#54217;&#44032; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = QADataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;val&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), collate_fn = nlpbook.data_collator, drop_last=False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Loading features from cached file /root/Korpora/korquad-v1/cached_val_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 4.527 s] INFO:ratsnlp:Loading features from cached file /root/Korpora/korquad-v1/cached_val_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 4.527 s] . 테스트용 데이터와 로더를 만들었습니다. . &#47784;&#45944; &#54617;&#49845; . from transformers import BertConfig, BertForQuestionAnswering pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, ) model = BertForQuestionAnswering.from_pretrained( args.pretrained_model_name, config = pretrained_model_config, ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForQuestionAnswering: [&#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;] - This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;qa_outputs.bias&#39;, &#39;qa_outputs.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . 윗 코드는 BERT 모델을 프리트레인 했고, 아랫 코드는 윗 코드로 만든 모델에 질의응답용 태스크 모듈을 덧붙힌 모델을 만들었습니다. . 지금 만든 모델은 빈 껍데기(초기 모델) 입니다. . from ratsnlp.nlpbook.qa import QATask task = QATask(model, args) trainer = nlpbook.get_trainer(args) . GPU available: True, used: True TPU available: False, using: 0 TPU cores . 옵티마이저와 러닝레이트를 정한 태스크를 만들어줍니다. . 그 후 트레이너를 정의하는데 모델에 세부 설정을 해줍니다.(GPU, 체크포인트 등) . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader, ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params 0 | model | BertForQuestionAnswering | 108 M 108 M Trainable params 0 Non-trainable params 108 M Total params 433.318 Total estimated model params size (MB) . &#51656;&#47928;&#50640; &#45813;&#54616;&#45716; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . from ratsnlp.nlpbook.ner import QADeployArguments args = QADeployArguments( pretrained_model_name= &#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, max_seq_length=128, max_query_length = 32, ) . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 인퍼런스 설정, 토크나이저 초기화를 합니다. . import torch from transformers import BertConfig, BertForQuestionAnswering fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, ) model = BertForQuestionAnswering(pretrained_model_config) . 체크포인트 로드, BERT 설정 로드, BERT 모델 초기화를 했습니다. . model.load_state_dict({k.replace(&#39;model.&#39;,&#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].ithes()}) model.eval() . 체크포인트를 주입하고 모델을 평가모드로 바꿉니다. . def inference_fn(question, context): if question and context: # 질문을 토큰화하고 인덱싱. max_query_length보다 길면 자르기 truncated_query = tokenizer.encode( question, add_special_tokens = False, truncation = True, max_length = args.max_query_length, ) # 앞서 처리한 질문을 지문과 함께 토큰화하고 인덱싱. max_seq_length보다 길면 자르기 inputs = tokenizer.encode_plus( text = truncated_query, text_pair = context, truncation = &#39;only_second&#39;, padding = &#39;max_length&#39;, max_length = args.max_seq_length, return_token_type_ids = True, ) with torch.no_grad(): outputs = model(**{k: torch.tensor([v]) for k, v in inputs.items()}) start_pred = outputs.start_logits.argmax(dim = -1).item() end_pred = outputs.end_logits.argmax(dim = -1).item() # 정답 시작부터 끝까지 토큰들을 이어붙여서 정답 만들기 pred_text = tokenizer.decode(inputs[&#39;input_ids&#39;][start_pred:end_pred+1]) else: pred_text = &#39;&#39; return{ &#39;question&#39;:question, &#39;context&#39;:context, &#39;answer&#39;:pred_text, } . 실제 값이 들어왔을때 처리해주는 함수입니다. . from ratsnlp.nlpbook.ner import get_web_service_app app = get_web_service_app(inference_fn) app.run() . 웹 서비스를 구현합니다. . (이쪽 개념을 잘 몰라서 실제 구현에는 실패했습니다.) .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/answer/2021/12/28/Do_natural_language6.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/answer/2021/12/28/Do_natural_language6.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "[DACON] 펭귄 몸무게 예측 경진대회",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import StandardScaler import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/Penguin/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 0 0 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 50.0 | 15.3 | 220 | MALE | 8.30515 | -25.19017 | 5550 | . 1 1 | Chinstrap penguin (Pygoscelis antarctica) | Dream | No | 49.5 | 19.0 | 200 | MALE | 9.63074 | -24.34684 | 3800 | . 2 2 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 45.1 | 14.4 | 210 | FEMALE | 8.51951 | -27.01854 | 4400 | . 3 3 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 44.5 | 14.7 | 214 | FEMALE | 8.20106 | -26.16524 | 4850 | . 4 4 | Gentoo penguin (Pygoscelis papua) | Biscoe | No | 49.6 | 16.0 | 225 | MALE | 8.38324 | -26.84272 | 5700 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 혹시 이 코드를 그대로 사용하신다면 path만 파일이 저장되어있는 주소로 바꾸시면 됩니다. . print(train.shape) print(test.shape) . (114, 11) (228, 10) . 우선 id포함 변수 개수가 10개이며 데이터 개수가 114, 228개로 다른 대회에 비해 적은 것을 알 수 있어요. . 복잡한 모델을 사용해 과적합이 되는 것을 많이 조심해야겠습니다. . 또 하나 특이한 점은 테스트 데이터가 약 2배 더 많네요. . &#44592;&#48376;&#48320;&#49688; &#49444;&#47749; . train[&#39;Species&#39;].value_counts() . Gentoo penguin (Pygoscelis papua) 48 Adelie Penguin (Pygoscelis adeliae) 41 Chinstrap penguin (Pygoscelis antarctica) 25 Name: Species, dtype: int64 . train[&#39;Island&#39;].value_counts() . Biscoe 57 Dream 44 Torgersen 13 Name: Island, dtype: int64 . train[&#39;Sex&#39;].value_counts() . MALE 56 FEMALE 55 Name: Sex, dtype: int64 . train.drop([&#39;id&#39;], axis = 1, inplace = True) test.drop([&#39;id&#39;], axis = 1, inplace = True) discrete_names = [&#39;Species&#39;, &#39;Island&#39;, &#39;Clutch Completion&#39;, &#39;Sex&#39;] continuous_names = [&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;, &#39;Flipper Length (mm)&#39;, &#39;Delta 15 N (o/oo)&#39;, &#39;Delta 13 C (o/oo)&#39;] . id : 데이터를 구분하기 위한 고유값, Species : 펭귄의 종(총 3개 종이 있음), Island : 샘플이 수집된 근처 섬(총 3개 섬이 있음) . Clutch Completion : 팽귄 둥지 알 개수가 2개인 경우 Yes, Culmen Length/Culmen Depth : 펭귄 부리의 가로/세로 길이 . Flipper Length : 펭귄 날개 길이, Sex : 펭귄 성별, Delta 15 N : 토양과 관련된 동위원소 비율, Delta 13 C : 먹이에 관련된 동위원소 비율 . id는 당연히 의미가 없으니 제외하고 Species/Island/Sex은 이산형 변수, 나머지 값은 연속형 변수 입니다. . &#44208;&#52769;&#52824; &#50668;&#48512; . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 114 entries, 0 to 113 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 Species 114 non-null object 1 Island 114 non-null object 2 Clutch Completion 114 non-null object 3 Culmen Length (mm) 114 non-null float64 4 Culmen Depth (mm) 114 non-null float64 5 Flipper Length (mm) 114 non-null int64 6 Sex 111 non-null object 7 Delta 15 N (o/oo) 111 non-null float64 8 Delta 13 C (o/oo) 111 non-null float64 9 Body Mass (g) 114 non-null int64 dtypes: float64(4), int64(2), object(4) memory usage: 9.0+ KB . train[train[&#39;Sex&#39;].isnull()] . Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 6 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 42.0 | 20.2 | 190 | NaN | 9.13362 | -25.09368 | 4250 | . 8 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 34.1 | 18.1 | 193 | NaN | NaN | NaN | 3475 | . 70 Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 46.2 | 14.4 | 214 | NaN | 8.24253 | -26.81540 | 4650 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train[train[&#39;Delta 15 N (o/oo)&#39;].isnull()] . Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 8 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 34.1 | 18.1 | 193 | NaN | NaN | NaN | 3475 | . 18 Adelie Penguin (Pygoscelis adeliae) | Dream | No | 39.8 | 19.1 | 184 | MALE | NaN | NaN | 4650 | . 109 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 36.6 | 17.8 | 185 | FEMALE | NaN | NaN | 3700 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 우선 변수들의 결측치 여부를 info 함수를 이용해서 알아봤습니다. . 3개 행에서 Sex와 2개의 Delta 변수가 결측치로 관측되었습니다. . 6,8,70 번 관측치가 Sex변수가 NULL 값이고 8, 18, 109번 관측치가 2개의 Delta 변수에 대해서 NULL 값이 나왔습니다. . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 228 entries, 0 to 227 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Species 228 non-null object 1 Island 228 non-null object 2 Clutch Completion 228 non-null object 3 Culmen Length (mm) 228 non-null float64 4 Culmen Depth (mm) 228 non-null float64 5 Flipper Length (mm) 228 non-null float64 6 Sex 222 non-null object 7 Delta 15 N (o/oo) 219 non-null float64 8 Delta 13 C (o/oo) 220 non-null float64 dtypes: float64(5), object(4) memory usage: 16.2+ KB . 테스트 데이터에서도 위에 언급한 3개의 변수들이 일부 NULL 값을 가진걸 확인 했습니다. . 소규모 데이터이기 때문에 조금 신중하게 접근할 필요가 있겠습니다. . &#50672;&#49549;&#54805; &#48320;&#49688; &#44288;&#52272; . train[&#39;Body Mass (g)&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe29d33c710&gt; . 반응변수인 몸무게의 히스토그램입니다. 이상치가 있어보이진 않고, 정규분포와 꽤 유사해보입니다. . plt.figure(figsize=(20,15)) plt.suptitle(&quot;Histogram&quot;, fontsize=40) for i in range(len(continuous_names)): plt.subplot(2,3,i+1) plt.title(continuous_names[i]) plt.hist(train[continuous_names[i]]) . (데이콘 운영자님이 작성하신 코드를 일부 참고했습니다.) . 연속형 변수의 히스토그램을 그렸을 때 이상치도 크게 없어보이고 나름(?) 정규성을 근접하게라도 따르는 것 같습니다. . 하지만 각 변수 마다 스케일이 다르기 때문에 표준화가 필요할 것 같아요. . plt.figure(figsize=(15,10)) ax = sns.heatmap(train.drop(discrete_names, axis = 1).corr(), annot=True) plt.show() . 모든 변수가 반응변수(Body Mass)와 꽤 높은 상관관계를 가지고 있습니다. 딱히 버릴 변수는 없을 것 같아요. . 또한 설명변수끼리도 상관관계가 어느정도 있는 것 같아요. 추후에 차원축소(PCA)를 적용할 수도 있겠습니다. . &#51060;&#49328;&#54805; &#48320;&#49688; &#44288;&#52272; . plt.figure(figsize=(20,15)) for i in range(len(discrete_names)): plt.subplot(2,2,i+1) plt.xlabel(discrete_names[i]) plt.ylabel(&#39;Body Mass (g)&#39;) sns.violinplot(x= train[discrete_names[i]], y= train[&#39;Body Mass (g)&#39;]) plt.tight_layout(rect=[0, 0.03, 1, 0.95]) plt.show() . (데이콘 운영자님의 코드를 참고했습니다.) . 각각의 이산형 변수로 구분한 반응변수를 그래프로 나타냈습니다. 쉽게 얘기해서 이산형 변수와 반응변수의 관계를 알 수 있습니다. . 네 변수 모두 유의미하게 반응변수에 영향을 끼친다고 볼 수 있습니다. 즉 버릴변수는 없어보입니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . train.fillna(train.mean(), inplace = True) test.fillna(train.mean(), inplace = True) train = pd.get_dummies(train) test = pd.get_dummies(test) print(train.shape) print(test.shape) . (114, 16) (228, 15) . 두 Delta 변수 결측값은 연속형이기 때문에 평균값으로 채우기로 했습니다. . 그리고 이산형 변수는 모두 get_dummies 함수를 사용해서 원-핫 인코딩 처리 했습니다. . 라벨인코딩에 경우 간단하나, 이 변수들은 순서형 자료가 아닌 범주형 자료이기 때문에 사용이 부적절하다고 생각했습니다. . 이 경우 Sex 변수 결측값은 Sex_men, Sex_women 어느 경우에도 속하지 않기 때문에 두 행 전부 0으로 처리됩니다. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() train_scaler = scaler.fit_transform(train[continuous_names]) train[continuous_names] = pd.DataFrame(data=train_scaler, columns=continuous_names) test_scaler = scaler.transform(test[continuous_names]) test[continuous_names] = pd.DataFrame(data=test_scaler, columns=continuous_names) train[continuous_names].head() . Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Delta 15 N (o/oo) Delta 13 C (o/oo) . 0 1.016685 | -0.887255 | 1.161653 | -0.775548 | 0.630951 | . 1 0.922318 | 1.027037 | -0.209242 | 1.601553 | 1.629486 | . 2 0.091884 | -1.352893 | 0.476205 | -0.391149 | -1.533908 | . 3 -0.021357 | -1.197680 | 0.750384 | -0.962206 | -0.523568 | . 4 0.941191 | -0.525091 | 1.504376 | -0.635514 | -1.325731 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 앞서 말한대로 연속형 변수에 경우 변수간 스케일을 맞춰주기 위해 StandardScaler를 사용했습니다. . StandardScaler는 평균 0, 분산 1이 되도록 값을 바꿔주기 때문에 변수간 스케일이 동등해집니다. . 여기서 저는 &#39;테스트 데이터(test.csv)를 학습에 사용&#39; 하지 않기 위해 트레인 데이터만 스케일링에 사용했는데요. . 테스트 데이터를 스케일링에 사용된다면 엄밀히 말해서 data leakage에 해당할 수도 있을거 같습니다. . 이 부분은 제가 문의게시판에 올려보겠습니다. . (윗 코드 결측치를 평균 대치 한 부분도 train 평균 값만을 사용했는데 같은 이치입니다.) . train_label = train[&#39;Body Mass (g)&#39;] train.drop([&#39;Body Mass (g)&#39;], axis = 1, inplace = True) . 모델 적합을 위해 반응변수를 따로 label로 빼줍니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score alphas = [0,0.1,1,10,100] for alpha in alphas: ridge = Ridge(alpha=alpha) neg_mse_scores = cross_val_score(ridge, train, train_label, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 339.4875 alpha 값 0.1 일때 평균 rmse : 337.4481 alpha 값 1 일때 평균 rmse : 330.4727 alpha 값 10 일때 평균 rmse : 329.2807 alpha 값 100 일때 평균 rmse : 405.4389 . 릿지 회귀 모델을 사용하였습니다. 교차검증을 통해 최적의 알파(하이퍼파라미터)를 찾는 과정입니다. . 릿지 회귀를 간단히 설명하면 일반 선형 회귀에 응용으로 회귀계수 크기를 패널티로 부여한 모델입니다. . 전반적으로 회귀계수 크기가 수축하는 효과를 보이며 과적합을 방지할 수 있습니다. . 다만 릿지 회귀는 입력 특성 스케일에 민감하기에 앞서 스케일링 작업을 해주었습니다. . ridge = Ridge(alpha = 10) ridge.fit(train, train_label) sample_submission[&#39;Body Mass (g)&#39;] = ridge.predict(test) sample_submission.to_csv(&#39;Penguin_final_1.csv&#39;,index=False) . 앞서 교차검증으로 나온 최적의 알파 값은 10이기 때문에 이를 대입해서 모델을 만듭니다. . 이 모델에 Public MSE 값은 309.92 정도가 나옵니다. . from sklearn.linear_model import Lasso alphas = [0,0.1,1,10,100] for alpha in alphas: lasso = Lasso(alpha=alpha) neg_mse_scores = cross_val_score(lasso, train, train_label, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 339.5967 alpha 값 0.1 일때 평균 rmse : 338.9466 alpha 값 1 일때 평균 rmse : 335.2101 alpha 값 10 일때 평균 rmse : 326.6552 alpha 값 100 일때 평균 rmse : 396.6299 . lasso = Lasso(alpha = 10) lasso.fit(train, train_label) sample_submission[&#39;Body Mass (g)&#39;] = lasso.predict(test) sample_submission.to_csv(&#39;Penguin_final_2.csv&#39;,index=False) . from sklearn.linear_model import ElasticNet alphas = [0,0.1,1,10,100] #alphas = [0.07,0.1,0.5,1,3] for alpha in alphas: elasticNet = ElasticNet(alpha=alpha) neg_mse_scores = cross_val_score(elasticNet, train, train_label, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 339.5967 alpha 값 0.1 일때 평균 rmse : 327.6324 alpha 값 1 일때 평균 rmse : 358.7528 alpha 값 10 일때 평균 rmse : 573.6431 alpha 값 100 일때 평균 rmse : 754.224 . elasticNet = ElasticNet(alpha = 0.1) elasticNet.fit(train, train_label) sample_submission[&#39;Body Mass (g)&#39;] = elasticNet.predict(test) sample_submission.to_csv(&#39;Penguin_final_3.csv&#39;,index=False) .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/eda/ridge/scale/regression/2021/12/27/dacon_penguin.html",
            "relUrl": "/dacon/jupyter/eda/ridge/scale/regression/2021/12/27/dacon_penguin.html",
            "date": " • Dec 27, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "[처음 시작하는 딥러닝] 1. 신경망 기초 실습",
            "content": ". &#45336;&#54028;&#51060; . import numpy as np a = [1,2,3] b = [4,5,6] print(&#39;a+b:&#39;, a+b) try: print(a*b) except TypeError: print(&#39;불가능&#39;) print() a = np.array([1,2,3]) b = np.array([4,5,6]) print(&#39;a+b&#39;, a+b) print(&#39;a*b&#39;, a*b) . a+b: [1, 2, 3, 4, 5, 6] 불가능 a+b [5 7 9] a*b [ 4 10 18] . R의 벡터 자료형 처럼 파이썬의 리스트 자료형은 더하기, 곱하기가 자유롭지 않습니다. . 이를 해결하기 위해 C언어 기반 넘파이 패키지를 활용합니다. . a = np.array([[1,2],[3,4]]) print(a) print(&#39;axis =0&#39;, a.sum(axis = 0)) print(&#39;axis =1&#39;, a.sum(axis = 1)) . [[1 2] [3 4]] axis =0 [4 6] axis =1 [3 7] . 간단한 행렬 연산입니다. axis = 0과 1의 차이점을 나타냅니다. . a = np.array([[1,2,3],[4,5,6]]) b = np.array([10,20,30]) print(&#39;a+b: n&#39;, a+b) . a+b: [[11 22 33] [14 25 36]] . R과 마찬가지로 재활용 규칙이 사용됩니다. . &#49888;&#44221;&#47581; &#44592;&#52488; &#54665;&#47148; &#50672;&#49328; . from typing import Callable def sigmoid(x): return 1 / (1 + np.exp(-x)) def deriv(func, input_, delta = 0.001): return (func(input_+delta) - func(input_-delta)) / (2 * delta) def matrix_function_forward_sum(X,W,sigma): assert X.shape[1] == W.shape[0] # assert은 아래 코드가 다음조건이 맞을때만 성립한다는 것을 알려줍니다. N = np.dot(X,W) S = sigma(N) L = np.sum(S) return L def matrix_function_backward_sum_1(X,W,sigma): assert X.shape[1] == W.shape[0] # assert은 아래 코드가 다음조건이 맞을때만 성립한다는 것을 알려줍니다. N = np.dot(X,W) S = sigma(N) # 입력받은 함수를 적용합니다. L = np.sum(S) dLdS = np.ones_like(S) # S배열 차원크기 유지, 값은 모두 1로 dSdN = deriv(sigma, N) dLdN = dLdS * dSdN dNdX = np.transpose(W, (1,0)) dLdX = np.dot(dLdN, dNdX) return dLdX . 행렬 미분입니다. X는 입력값 행렬, W는 가중치행렬 입니다. . np.random.seed(190204) X = np.random.randn(3,3) W = np.random.randn(3,2) print(&#39;X:&#39;) print(X) print(&#39;L:&#39;) print(round(matrix_function_forward_sum(X,W,sigmoid), 4)) print() print(&#39;dLdX:&#39;) print(matrix_function_backward_sum_1(X,W,sigmoid)) . X: [[-1.57752816 -0.6664228 0.63910406] [-0.56152218 0.73729959 -1.42307821] [-1.44348429 -0.39128029 0.1539322 ]] L: 2.3755 dLdX: [[ 0.2488887 -0.37478057 0.01121962] [ 0.12604152 -0.27807404 -0.13945837] [ 0.22992798 -0.36623443 -0.02252592]] . X는 입력하는 행렬이고 L은 출력값 dLdX은 X 행렬이 변할 때 L값의 변화량, 즉 미분값입니다. . 이 미분값은 d(sigma)/d(u) (XW) (W)^T으로 구할 수 있습니다. (손으로 증명 가능) . X1 = X.copy() X1[0,0] += 0.001 print(round((matrix_function_forward_sum(X1,W,sigmoid) - matrix_function_forward_sum(X,W,sigmoid))/0.001, 4)) . 0.2489 . X[0,0] 값을 0.001 증가시켰을때 L 값의 변화량을 구했습니다. dLdX [0,0] 값과 유사한 것을 알 수 있어요. . &#49440;&#54805;&#54924;&#44480; &#54665;&#47148; &#50672;&#49328; . def forward_linear_regression(X_batch, y_batch, weights): # X는 행렬, y는 벡터, weight는 딕셔너리형태(W + B) assert X_batch.shape[0] == y_batch.shape[0] # 데이터 크기가 같은가 assert X_batch.shape[1] == weights[&#39;W&#39;].shape[0] # 가중치 개수가 맞는가.(행렬 연산이 가능한가) assert weights[&#39;B&#39;].shape[0] == weights[&#39;B&#39;].shape[1] == 1 N = np.dot(X_batch, weights[&#39;W&#39;]) P = N + weights[&#39;B&#39;] loss = np.mean(np.power(y_batch-P, 2)) forward_info = {} forward_info[&#39;X&#39;] = X_batch forward_info[&#39;N&#39;] = N # XW forward_info[&#39;P&#39;] = P # XW + B forward_info[&#39;y&#39;] = y_batch return loss, forward_info . 입력값(X) 파라미터(가중치, W)가 주어졌을때 선형 회귀 값을 구하는 함수입니다. . 하지만 우리가 원하는건 파라미터(가중치)를 추정하는 일이죠. . L = mean(Sigma(y(i) - yhat(i))) 을 최소로 하는 파라미터를 미분을 이용해서 찾아봅시다. . def loss_gradients(forward_info, weights): batch_size = forward_info[&#39;X&#39;].shape[0] dLdP = -2 * (forward_info[&#39;y&#39;] - forward_info[&#39;P&#39;]) dPdN = np.ones_like(forward_info[&#39;N&#39;]) dPdB = np.ones_like(weights[&#39;B&#39;]) dLdN = dLdP * dPdN dNdW = np.transpose(forward_info[&#39;X&#39;], (1,0)) dLdW = np.dot(dNdW, dLdN) # X^T가 행렬 곱법칙 때문에 먼저나옴. dLdB = (dLdP * dPdB).sum(axis = 0) loss_gradients = {} loss_gradients[&#39;W&#39;] = dLdW loss_gradients[&#39;B&#39;] = dLdB return loss_gradients . 앞서 구한 신경망 기초 행렬 연산식을 이용해 선형회귀 가중치의 도함수를 구했습니다. . &#48145;&#48148;&#48149;&#48512;&#53552; &#47564;&#46300;&#45716; &#49888;&#44221;&#47581; . def forward_loss(X, y, weights): M1 = np.dot(X, weights[&#39;W1&#39;]) N1 = M1 + weights[&#39;B1&#39;] O1 = sigmoid(N1) M2 = np.dot(O1, weights[&#39;W2&#39;]) P = M2 + weights[&#39;B2&#39;] loss = np.mean(np.power(y - P, 2)) forward_info = {} forward_info[&#39;X1&#39;] = X1 # 입력값 forward_info[&#39;M1&#39;] = M1 # X * W(1), 여러개의 선형결합 결과들 forward_info[&#39;N1&#39;] = N1 # 상수항 더하기 forward_info[&#39;O1&#39;] = O1 # 시그모이드 함수 씌우기 forward_info[&#39;M2&#39;] = M2 # 시그모이드 함수 씨운 13개 값 다시 선형결합 forward_info[&#39;P&#39;] = P # 다시 상수항 더한 값 최종 Y_HAT으로 생각 forward_info[&#39;y&#39;] = y . 간단한 신경망 구조입니다. 선형결합을 행 개수만큼 하고 나온 여러개의 결과값에 시그모이드 함수를 씌우고 다시 선형결합해서 결과를 냅니다. . def loss_gradients(forward_info, weights): &#39;&#39;&#39; 신경망의 각 파라미터에 대한 손실의 편미분을 계산 &#39;&#39;&#39; dLdP = -(forward_info[&#39;y&#39;] - forward_info[&#39;P&#39;]) dPdM2 = np.ones_like(forward_info[&#39;M2&#39;]) # P = M2 + B2 dLdM2 = dLdP * dPdM2 dPdB2 = np.ones_like(weights[&#39;B2&#39;]) dLdB2 = (dLdP * dPdB2).sum(axis=0) dM2dW2 = np.transpose(forward_info[&#39;O1&#39;], (1, 0)) # O1 * W2 dLdW2 = np.dot(dM2dW2, dLdP) dM2dO1 = np.transpose(weights[&#39;W2&#39;], (1, 0)) # 이게 중요 dLdO1 = np.dot(dLdM2, dM2dO1) dO1dN1 = sigmoid(forward_info[&#39;N1&#39;]) * (1- sigmoid(forward_info[&#39;N1&#39;])) dLdN1 = dLdO1 * dO1dN1 dN1dB1 = np.ones_like(weights[&#39;B1&#39;]) dN1dM1 = np.ones_like(forward_info[&#39;M1&#39;]) dLdB1 = (dLdN1 * dN1dB1).sum(axis=0) dLdM1 = dLdN1 * dN1dM1 dM1dW1 = np.transpose(forward_info[&#39;X&#39;], (1, 0)) dLdW1 = np.dot(dM1dW1, dLdM1) loss_gradients = {} loss_gradients[&#39;W2&#39;] = dLdW2 loss_gradients[&#39;B2&#39;] = dLdB2.sum(axis=0) loss_gradients[&#39;W1&#39;] = dLdW1 loss_gradients[&#39;B1&#39;] = dLdB1.sum(axis=0) return loss_gradients . 조금 복잡하긴 하지만 직접 합성함수 미분을 해봤습니다. . &#45712;&#45184;&#51216; . 코드 실습은 많지 않지만 공부시간이 상당히 오래걸렸습니다. . 다변량 벡터나 행렬을 사용한 합성함수를 미분하는게 여간 힘든게 아니였어요. 수학적 지식, 머신러닝 경험이 조금 필요합니다. . 직관적으로 이해하기위해 많이 노력했습니다. . 그래도 딥러닝이 어떤것인지 토대를 배웠는데 정말 흥미로웠어요. . 그동안 겉핥기 식으로 대충 모형만 보고 패키지만 갔다가 썼는데 밑바닥부터 구현하니 딥러닝의 기초 구조를 알수 있어서 좋았습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/deep%20learning/foundation/matrix/math/2021/12/26/FirstDeep1.html",
            "relUrl": "/book/jupyter/deep%20learning/foundation/matrix/math/2021/12/26/FirstDeep1.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "[Do it 자연어] 5. 개체명 인식하기 + 웹 실습",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 807 kB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 2.5 MB/s Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 9.7 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 11.1 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 36.0 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 45.3 MB/s Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 39.2 MB/s Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 19.7 MB/s Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 33.1 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 45.4 MB/s Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 501 kB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 41.0 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 4.5 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 44.3 MB/s Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 43.9 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 43.7 MB/s Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=8f50ce81c51e52b253729b577e873fce19ff0f756218e5f0555208db01a16494 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.ner import NERTrainArguments args = NERTrainArguments( pretrained_model_name = &#39;beomi/kcbert-base&#39;, downstream_corpus_name = &#39;ner&#39;, # 한국해양대학교 자연언어처리연구실 데이터 + 자체데이터 downstream_model_dir = &#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, # 모델 체크포인트 저장위치 batch_size = 32 if torch.cuda.is_available() else 4, learning_rate = 5e-5, max_seq_length = 64, epochs = 3, tpu_cores = 0 if torch.cuda.is_available() else 8, seed = 7, ) . 앞서 한것과 크게 다르지 않습니다. 프리 트레인 모델로 이준범님의 kcbert-base 모델을 사용했습니다. . ner, 한국해양대학교 데이터를 실습 데이터로 사용하겠습니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters NERTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;named-entity-recognition&#39;, downstream_corpus_name=&#39;ner&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) . set seed: 7 . 랜덤 시드를 고정했으며 로거를 설정했습니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . nlpbook.download_downstream_dataset(args) . Downloading: 100%|██████████| 17.9M/17.9M [00:00&lt;00:00, 33.8MB/s] Downloading: 100%|██████████| 1.13M/1.13M [00:00&lt;00:00, 27.2MB/s] . 말뭉치를 내려받습니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 토크나이저를 선언합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.ner import NERCorpus, NERDataset corpus = NERCorpus(args) train_dataset = NERDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39; ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/ner INFO:ratsnlp:loading train data... LOOKING AT /root/Korpora/ner/train.txt INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 이어 옆으로 움직여 김일성의 오른쪽에서 한 차례씩 두 번 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다. INFO:ratsnlp:target: 이어 옆으로 움직여 &lt;김일성:PER&gt;의 오른쪽에서 &lt;한 차례:NOH&gt;씩 &lt;두 번:NOH&gt; 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다. INFO:ratsnlp:tokens: [CLS] 이어 옆 ##으로 움직 ##여 김일성 ##의 오른 ##쪽에서 한 차례 ##씩 두 번 상 ##체를 굽 ##혀 조문 ##했 ##으며 이 ##윽 ##고 안 ##경을 벗고 손 ##수 ##건으로 눈 ##주 ##위를 닦 ##기도 했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O O O O B-PER O O O B-NOH I-NOH O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 10704, 2287, 7965, 10598, 4327, 10819, 4042, 11790, 17431, 3354, 16729, 4679, 917, 1530, 1801, 9678, 359, 4443, 23831, 4062, 9511, 2451, 5953, 4034, 2173, 19033, 19778, 1898, 4110, 29483, 721, 4043, 10327, 788, 8517, 9212, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 5, 4, 4, 4, 6, 16, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 제철과일리코타치즈샐러드는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다. INFO:ratsnlp:target: &lt;제철과일리코타치즈샐러드:POH&gt;는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다. INFO:ratsnlp:tokens: [CLS] 제 ##철 ##과 ##일 ##리 ##코 ##타 ##치 ##즈 ##샐 ##러 ##드는 직접 만든 쫀 ##쫀 ##한 치 ##즈 ##도 맛 ##있지만 , 영 ##귤 ##청 ##드 ##레 ##싱 ##이 상 ##큼 ##함을 더한 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] B-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2545, 4748, 4128, 4046, 4038, 4599, 4361, 4077, 4146, 7035, 4053, 8609, 9099, 8634, 2771, 6003, 4047, 2972, 4146, 4029, 1306, 25974, 15, 2282, 5376, 4190, 4273, 4306, 4097, 4017, 1801, 4582, 11091, 11554, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 정씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다. INFO:ratsnlp:target: &lt;정:PER&gt;씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다. INFO:ratsnlp:tokens: [CLS] 정 ##씨는 [UNK] 사고 예측 ##을 위한 빅 ##데이 ##터 ##나 전자 항 ##해 등 그동안 알지 못했 ##던 분야 ##에 대해 배울 수 있는 기회 ##였다 [UNK] 며 [UNK] 새로운 교육이 재 ##취업 ##에 많은 도움이 됐다 [UNK] 고 말했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2539, 10786, 1, 8472, 16843, 4027, 8717, 1665, 19545, 4025, 4136, 12116, 3370, 4032, 963, 8996, 10630, 22474, 4217, 16029, 4113, 9305, 17534, 1931, 8032, 8993, 9827, 1, 1363, 1, 10794, 21266, 2499, 21150, 4113, 8298, 11439, 14054, 1, 303, 19646, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: ―효진 역의 김환희(14)가 특히 인상적이었다. INFO:ratsnlp:target: ―&lt;효진:PER&gt; 역의 &lt;김환희:PER&gt;(&lt;14:NOH&gt;)가 특히 인상적이었다. INFO:ratsnlp:tokens: [CLS] [UNK] 효 ##진 역 ##의 김 ##환 ##희 ( 14 ) 가 특히 인상 ##적이 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O B-PER I-PER O O B-PER I-PER I-PER O B-NOH O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 1, 3476, 4153, 2270, 4042, 420, 4185, 4346, 11, 11524, 12, 197, 9250, 11662, 8805, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 5, 15, 4, 4, 5, 15, 15, 4, 6, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 전문가들은 미국 증시의 상승세가 유지되고 ‘트럼프노믹스’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다. INFO:ratsnlp:target: 전문가들은 &lt;미국:ORG&gt; 증시의 상승세가 유지되고 ‘&lt;트럼프노믹스:POH&gt;’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다. INFO:ratsnlp:tokens: [CLS] 전문가들 ##은 미국 증 ##시 ##의 상승 ##세가 유지 ##되고 [UNK] 트럼프 ##노 ##믹 ##스 [UNK] 에 대한 불 ##확 ##실 ##성이 걷 ##히면 국내 증 ##시도 박 ##스 ##권 탈출 ##을 시도 ##할 수 있다는 분석 ##을 내놓 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O B-ORG O O O O O O O O B-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 28067, 4057, 8057, 2680, 4039, 4042, 12360, 11279, 9846, 8593, 1, 8565, 4041, 5618, 4103, 1, 2255, 8014, 1616, 4277, 4353, 8361, 253, 15723, 8791, 2680, 15399, 1481, 4103, 4285, 12882, 4027, 13335, 4082, 1931, 9340, 14481, 4027, 11326, 8588, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/ner/cached_train_BertTokenizer_64_ner_named-entity-recognition [took 12.842 s] . NERCorpus가 넘겨준 데이터와 kcbert-base 데이터로 학습한 토크나이저를 사용해 4가지 출력값을 줍니다. . 마지막 출력값은 label_ids로 B-PER(시작-인명), I-PER(시작아님-인명) B-NOH(시작-기타수량표현) 등을 정수를 할당해 표현합니다. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . 학습 데이터 셋으로 로더를 구축했습니다. 배치크기만큼 인스턴스를 랜덤하게 뽑은 뒤 이를 합처 배치를 만듭니다. . &#54217;&#44032; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = NERDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;val&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), # 순서대로 추출 collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/ner INFO:ratsnlp:loading val data... LOOKING AT /root/Korpora/ner/val.txt INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다 INFO:ratsnlp:target: 결국 초연은 &lt;4년 반:DUR&gt;이 지난 후에 &lt;드레스덴:LOC&gt;에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다 INFO:ratsnlp:tokens: [CLS] 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O O O B-DUR I-DUR O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 14, 24, 4, 4, 4, 10, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다 INFO:ratsnlp:target: 한편 &lt;1840년:DAT&gt;부터 &lt;바그너:PER&gt;와 알고 지내던 &lt;리스트:PER&gt;가 잊혀져 있던 &lt;1악장:NOH&gt;을 부활시켜 &lt;1852년:DAT&gt;에 &lt;바이마르:LOC&gt;에서 연주했다 INFO:ratsnlp:tokens: [CLS] 한편 18 ##40 ##년 ##부터 바 ##그 ##너 ##와 알고 지내 ##던 리스트 ##가 잊혀 ##져 있던 1 ##악 ##장을 부활 ##시켜 18 ##5 ##2년 ##에 바이 ##마 ##르 ##에서 연 ##주 ##했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O B-DAT I-DAT I-DAT O B-PER I-PER I-PER O O O O B-PER O O O O B-NOH I-NOH I-NOH O O B-DAT I-DAT I-DAT O B-LOC I-LOC I-LOC O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 17332, 8601, 13422, 4482, 8042, 1480, 4313, 4538, 4196, 8297, 13683, 4217, 20899, 4009, 21343, 4413, 11759, 20, 4158, 8915, 13705, 8292, 8601, 4044, 13970, 4113, 25418, 4168, 4138, 7971, 2273, 4043, 8258, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 9, 19, 19, 4, 5, 15, 15, 4, 4, 4, 4, 5, 4, 4, 4, 4, 6, 16, 16, 4, 4, 9, 19, 19, 4, 10, 20, 20, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다 INFO:ratsnlp:target: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, &lt;제1바이올린:POH&gt;으로 더욱 명확하게 나타난다 INFO:ratsnlp:tokens: [CLS] 첫 부분 ##의 저 ##음 주제 ##는 주요 주제 ( 고 ##뇌 ##와 갈 ##망 동 ##기 , 청춘 ##의 사랑 동 ##기 ) 를 암 ##시하고 있으며 , 제1 ##바이 ##올린 ##으로 더욱 명확 ##하게 나타 ##난다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-POH I-POH I-POH O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2881, 11515, 4042, 2523, 4126, 14654, 4008, 16190, 14654, 11, 303, 4703, 4196, 204, 4227, 875, 4184, 15, 23061, 4042, 9004, 875, 4184, 12, 1265, 2183, 24730, 22886, 15, 12218, 18056, 22881, 7965, 10365, 14635, 8007, 10498, 8647, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 3편 (난키 방면 1편, 간사이 공항 방면 2편)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 4편 가까이의 편수가 운전되고 있다 INFO:ratsnlp:target: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 &lt;3편:NOH&gt; (&lt;난키:LOC&gt; 방면 &lt;1편:NOH&gt;, &lt;간사이 공항:LOC&gt; 방면 &lt;2편:NOH&gt;)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 &lt;4편:NOH&gt; 가까이의 편수가 운전되고 있다 INFO:ratsnlp:tokens: [CLS] 대 ##낮 시간 ##대 이후에 ##는 심 ##야 시간 ##대를 제외하고 ##는 시간 ##당 3 ##편 ( 난 ##키 방 ##면 1 ##편 , 간 ##사이 공항 방 ##면 2 ##편 ) 으로 비교 ##적 운행 편 ##수가 많아 승객 ##이 많은 시기에 ##는 임시 열 ##차도 운전 ##되기 때문에 시간 ##당 4 ##편 가까이 ##의 편 ##수가 운전 ##되고 있다 [SEP] [PAD] INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O B-NOH I-NOH O B-LOC I-LOC O O B-NOH I-NOH O B-LOC I-LOC I-LOC O O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O B-NOH I-NOH O O O O O O O [SEP] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 809, 4734, 8485, 4140, 22395, 4008, 2015, 4144, 8485, 10633, 23993, 4008, 8485, 4081, 22, 4393, 11, 591, 4379, 1497, 4063, 20, 4393, 15, 201, 13160, 13862, 1497, 4063, 21, 4393, 12, 10442, 8898, 4022, 23167, 3282, 8356, 9737, 25443, 4017, 8298, 14686, 4008, 18002, 2275, 21509, 9381, 17221, 8360, 8485, 4081, 23, 4393, 15238, 4042, 3282, 8356, 9381, 8593, 8120, 3, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 10, 20, 4, 4, 6, 16, 4, 10, 20, 20, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 1, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 덴노지 역 ~ 와카야마 역 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 한와 선 페이지에서는 생략되어 있다 INFO:ratsnlp:target: &lt;덴노지 역:LOC&gt; ~ &lt;와카야마 역:LOC&gt; 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 &lt;한와 선:ORG&gt; 페이지에서는 생략되어 있다 INFO:ratsnlp:tokens: [CLS] 덴 ##노 ##지 역 ~ 와 ##카 ##야 ##마 역 간에 ##서는 정 ##차 ##역 ##이 비교 ##적 적 ##기 때문에 시 ##판 ##되고 있는 시간 ##표 ##의 한 ##와 선 페 ##이지 ##에서는 생 ##략 ##되어 있다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 849, 4041, 4102, 2270, 95, 2320, 4024, 4144, 4168, 2270, 22590, 9666, 2539, 4495, 4119, 4017, 8898, 4022, 2524, 4184, 8360, 2002, 4448, 8593, 8032, 8485, 4302, 4042, 3354, 4196, 1846, 3272, 8067, 8652, 1821, 4873, 9079, 8120, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 10, 20, 20, 20, 4, 10, 20, 20, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 18, 18, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/ner/cached_val_BertTokenizer_64_ner_named-entity-recognition [took 0.824 s] . 입력받은 데이터를 토크나이저로 분리한 후 로더를 이용해 테스트용 배치를 만듭니다. . &#47784;&#45944; &#54617;&#49845; . from transformers import BertConfig, BertForTokenClassification pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = corpus.num_labels, ) # 프리트레인 BERT 모델 model = BertForTokenClassification.from_pretrained( args.pretrained_model_name, config = pretrained_model_config, ) # 프리트레인 모델 + 개체명 인식을 위한 태스크모듈 . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForTokenClassification: [&#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;] - This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForTokenClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . 윗 코드는 BERT 모델을 프리트레인 했고, 아랫 코드는 개체명 인식을 위한 태스크 모듈을 덧붙힌 모델입니다. . from ratsnlp.nlpbook.ner import NERTask task = NERTask(model, args) trainer = nlpbook.get_trainer(args) . /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Checkpoint directory /gdrive/My Drive/nlpbook/checkpoint-ner exists and is not empty. warnings.warn(*args, **kwargs) GPU available: True, used: True TPU available: False, using: 0 TPU cores . 앞서 만든 모델을 이용해 데이터를 학습시킵니다. NERTask는 아담 옵티마이저와 ExponentialLR 러닝레이트를 사용합니다. . 그 뒤 GPU/TPU 설정, 로그 및 체크포인트등을 하는 트레이너를 정의합니다. . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader, ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | model | BertForTokenClassification | 108 M -- 108 M Trainable params 0 Non-trainable params 108 M Total params 433.389 Total estimated model params size (MB) . 모델을 학습합니다. . &#44060;&#52404;&#47749; &#51064;&#49885; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . from ratsnlp.nlpbook.ner import NERDeployArguments args = NERDeployArguments( pretrained_model_name= &#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, max_seq_length=64, ) . downstream_model_checkpoint_fpath: /gdrive/My Drive/nlpbook/checkpoint-ner/epoch=1-val_loss=0.20.ckpt downstream_model_labelmap_fpath: /gdrive/My Drive/nlpbook/checkpoint-ner/label_map.txt . 인퍼러스 설정을 다시 해줍니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 토크나이저도 초기화해줍니다. . import torch from transformers import BertConfig, BertForTokenClassification fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = fine_tuned_model_ckpt[&#39;state_dict&#39;][&#39;model.classifier.bias&#39;].shape.numel(), ) model = BertForTokenClassification(pretrained_model_config) . 체크포인트 로드, BERT 설정 로드, BERT 모델 초기화를 했습니다. . model.load_state_dict({k.replace(&#39;model.&#39;, &#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].items()}) model.eval() . BertForTokenClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30000, 768, padding_idx=0) (position_embeddings): Embedding(300, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=25, bias=True) ) . 체크포인트를 주입하고 모델을 평가모드로 바꿉니다. . labels = [label.strip() for label in open(args.downstream_model_labelmap_fpath, &quot;r&quot;).readlines()] id_to_label = {} for idx, label in enumerate(labels): if &quot;PER&quot; in label: label = &quot;인명&quot; elif &quot;LOC&quot; in label: label = &quot;지명&quot; elif &quot;ORG&quot; in label: label = &quot;기관명&quot; elif &quot;DAT&quot; in label: label = &quot;날짜&quot; elif &quot;TIM&quot; in label: label = &quot;시간&quot; elif &quot;DUR&quot; in label: label = &quot;기간&quot; elif &quot;MNY&quot; in label: label = &quot;통화&quot; elif &quot;PNT&quot; in label: label = &quot;비율&quot; elif &quot;NOH&quot; in label: label = &quot;기타 수량표현&quot; elif &quot;POH&quot; in label: label = &quot;기타&quot; else: label = label id_to_label[idx] = label id_to_label . {0: &#39;[CLS]&#39;, 1: &#39;[SEP]&#39;, 2: &#39;[PAD]&#39;, 3: &#39;[MASK]&#39;, 4: &#39;O&#39;, 5: &#39;인명&#39;, 6: &#39;기타 수량표현&#39;, 7: &#39;기타&#39;, 8: &#39;기관명&#39;, 9: &#39;날짜&#39;, 10: &#39;지명&#39;, 11: &#39;통화&#39;, 12: &#39;비율&#39;, 13: &#39;시간&#39;, 14: &#39;기간&#39;, 15: &#39;인명&#39;, 16: &#39;기타 수량표현&#39;, 17: &#39;기타&#39;, 18: &#39;기관명&#39;, 19: &#39;날짜&#39;, 20: &#39;지명&#39;, 21: &#39;통화&#39;, 22: &#39;비율&#39;, 23: &#39;시간&#39;, 24: &#39;기간&#39;} . 정수 인덱스를 레이블에 매핑하는 사전을 만듭니다. . def inference_fn(sentence): inputs = tokenizer( [sentence], max_length = args.mex_seq_length, padding = &#39;max_length&#39;, truncation = True, ) with torch.no_grad(): outputs = model(**{k : torch.tenser(v) for k, v in inputs.items()}), probs = outputs.logits[0].softmax(dim = 1), # 로짓에 소프트맥스를 취해 토큰이 어떤 개체명에 들어가나 확률 구하기. top_probs, preds = torch.topk(probs, dim = 1, k = 1), # 가장 높은 확률 값과 그 개체 인덱스 구하기 tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0]), # 토큰화된 것을 다시 읽을 수 있는 문자(한국어)로 복원 predicted_tags = [id_to_label[pred.item()] for pred in preds] # 개체 인덱스를 전에 라벨링한 문자로 바꿔주기.(PER =&gt; 인명) result = [] for token, predicted_tag, top_prob in zip(tokens, predicted_tags, top_probs): if token not in [tokenizer.pad_token, tokenizer.cls_token,tokenizer.seq_token]: # 이프문은 CLS, SEP, PAD를 제외하는 역활을 함. token_result = { &#39;token&#39;: token, # 단어 &#39;predicted_tag&#39;: predicted_tag, # 개체명 &#39;top_prob&#39; : str(round(top_prob[0].item(), 4)), # 그 확률 } result.append(token_result) return{&#39;sentence&#39; : sentence, &#39;result&#39; : result,} . 실제 입력값이 들어왓을때 처리 과정에 대한 함수입니다. 코드마다 설명을 상세히 했습니다. . from ratsnlp.nlpbook.ner import get_web_service_app app = get_web_service_app(inference_fn) app.run() . * Serving Flask app &#34;ratsnlp.nlpbook.ner.deploy&#34; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off . * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) . * Running on http://3f00-35-202-140-61.ngrok.io * Traffic stats available on http://127.0.0.1:4040 . 127.0.0.1 - - [24/Dec/2021 14:57:07] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [24/Dec/2021 14:57:07] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - . 웹 서비스로 구현합니다. . (이부분은 앞코드와 마찬가지로 따로 더 공부해야 오류없이 구동될것 같아요. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/24/Do_natural_language5.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/24/Do_natural_language5.html",
            "date": " • Dec 24, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "[시계열분석] 1. 시계열의 탐색적 자료분석",
            "content": ". EuStockMarkets &#45936;&#51060;&#53552;&#50640; &#45824;&#54644; . head(EuStockMarkets) . A matrix: 6 × 4 of type dbl DAXSMICACFTSE . 1628.75 | 1678.1 | 1772.8 | 2443.6 | . 1613.63 | 1688.5 | 1750.5 | 2460.2 | . 1606.51 | 1678.6 | 1718.0 | 2448.2 | . 1621.04 | 1684.1 | 1708.1 | 2470.4 | . 1618.16 | 1686.6 | 1723.1 | 2484.7 | . 1610.61 | 1671.6 | 1714.3 | 2466.8 | . EuStockMarkets은 1991년부터 1998년까지 유럽 4대 주가지수 일일 종가를 나타냅니다. . 이 데이터는 형식화 및 샘플링이 잘 되어 있어 이상치 조정 없이 바로 EDA가 가능합니다. . plot(EuStockMarkets) . 각 값을 개별 그래프로 그렸습니다. . class(EuStockMarkets) . &lt;ol class=list-inline&gt;&#39;mts&#39; | &#39;ts&#39; | &#39;matrix&#39; | &lt;/ol&gt; 다양한 시계열을 동시에 다루는 mts 객체를 사용합니다. . (ts는 단일 시계열을 다루는 객체입니다.) . ts &#44061;&#52404;&#51032; &#54632;&#49688; . frequency(EuStockMarkets) . 260 데이터의 연간 빈도를 출력하는 frequency 함수 입니다. . start(EuStockMarkets) . &lt;ol class=list-inline&gt;1991 | 130 | &lt;/ol&gt; end(EuStockMarkets) . &lt;ol class=list-inline&gt;1998 | 169 | &lt;/ol&gt; 데이터의 처음과 마지막 시간을 알아내는 start, end 함수 입니다. . window(EuStockMarkets, start = 1997, end = 1998) . A Time Series: 261 × 4 DAXSMICACFTSE . 1997.0002844.09 | 3869.8 | 2289.6 | 4092.5 | . 1997.0042844.09 | 3869.8 | 2289.6 | 4092.5 | . 1997.0082844.09 | 3869.8 | 2303.8 | 4092.5 | . 1997.0122859.22 | 3922.2 | 2307.0 | 4091.0 | . 1997.0152880.07 | 3948.3 | 2318.6 | 4115.7 | . 1997.0192880.07 | 3942.2 | 2315.7 | 4118.5 | . 1997.0232880.07 | 3942.2 | 2315.7 | 4118.5 | . 1997.0272820.81 | 3942.2 | 2257.0 | 4057.4 | . 1997.0312863.26 | 3940.1 | 2282.8 | 4089.5 | . 1997.0352890.20 | 3923.8 | 2306.7 | 4106.5 | . 1997.0382876.34 | 3922.9 | 2301.7 | 4078.8 | . 1997.0422904.08 | 3944.9 | 2331.6 | 4087.5 | . 1997.0462936.69 | 3966.2 | 2349.1 | 4087.0 | . 1997.0502915.81 | 3947.4 | 2327.5 | 4056.6 | . 1997.0542956.78 | 3975.5 | 2361.3 | 4107.3 | . 1997.0582978.84 | 3983.6 | 2402.1 | 4168.2 | . 1997.0622976.56 | 3979.6 | 2388.0 | 4158.9 | . 1997.0652996.12 | 4007.1 | 2407.8 | 4197.5 | . 1997.0693006.87 | 4019.9 | 2425.1 | 4207.7 | . 1997.0732999.19 | 4009.5 | 2406.1 | 4194.0 | . 1997.0773000.66 | 4023.1 | 2409.9 | 4195.5 | . 1997.0813026.63 | 4115.4 | 2442.5 | 4219.1 | . 1997.0853037.28 | 4161.0 | 2461.3 | 4219.1 | . 1997.0882982.63 | 4125.5 | 2430.3 | 4218.8 | . 1997.0922992.55 | 4127.3 | 2435.2 | 4212.0 | . 1997.0963028.27 | 4182.3 | 2482.8 | 4237.4 | . 1997.1002997.95 | 4169.7 | 2465.0 | 4207.5 | . 1997.1043018.58 | 4209.1 | 2503.1 | 4228.4 | . 1997.1083037.70 | 4272.2 | 2516.6 | 4275.8 | . 1997.1123064.70 | 4282.8 | 2508.6 | 4257.8 | . 1997.1153067.48 | 4296.5 | 2503.1 | 4260.9 | . 1997.1193114.73 | 4305.5 | 2541.3 | 4281.5 | . 1997.1233124.78 | 4309.8 | 2558.4 | 4265.9 | . 1997.1273161.36 | 4357.9 | 2597.5 | 4307.8 | . 1997.1313185.72 | 4384.3 | 2595.4 | 4307.7 | . 1997.1353191.45 | 4408.4 | 2582.1 | 4304.3 | . 1997.1383211.01 | 4444.1 | 2599.3 | 4304.3 | . 1997.1423256.86 | 4436.3 | 2628.4 | 4327.1 | . 1997.1463249.17 | 4464.2 | 2627.4 | 4341.0 | . 1997.1503260.30 | 4514.6 | 2634.5 | 4337.8 | . 1997.1543230.83 | 4490.7 | 2617.5 | 4332.3 | . 1997.1583209.04 | 4525.5 | 2594.8 | 4357.4 | . 1997.1623197.09 | 4530.8 | 2575.2 | 4356.1 | . 1997.1653203.79 | 4522.5 | 2562.8 | 4336.8 | . 1997.1693180.63 | 4463.2 | 2567.9 | 4331.1 | . 1997.1733233.34 | 4503.9 | 2607.7 | 4344.7 | . 1997.1773245.02 | 4539.0 | 2602.2 | 4329.3 | . 1997.1813272.58 | 4519.7 | 2629.4 | 4339.2 | . 1997.1853261.04 | 4487.6 | 2607.8 | 4308.3 | . 1997.1883258.74 | 4460.1 | 2600.3 | 4307.1 | . 1997.1923345.09 | 4513.7 | 2651.7 | 4357.7 | . 1997.1963375.45 | 4547.1 | 2666.2 | 4360.1 | . 1997.2003396.55 | 4605.2 | 2698.9 | 4399.3 | . 1997.2043419.51 | 4638.9 | 2708.3 | 4420.3 | . 1997.2083426.77 | 4684.4 | 2709.2 | 4437.4 | . 1997.2123430.95 | 4677.1 | 2686.2 | 4444.3 | . 1997.2153382.40 | 4676.2 | 2641.7 | 4422.5 | . 1997.2193367.82 | 4609.9 | 2632.1 | 4397.7 | . 1997.2233404.29 | 4636.2 | 2645.6 | 4424.3 | . 1997.2273337.11 | 4556.5 | 2588.4 | 4373.3 | . 1997.2313289.59 | 4519.9 | 2574.0 | 4356.8 | . 1997.2353305.72 | 4535.1 | 2596.8 | 4332.2 | . 1997.2383247.03 | 4442.9 | 2553.7 | 4258.1 | . 1997.2423288.52 | 4491.3 | 2587.1 | 4254.8 | . 1997.2463302.57 | 4497.3 | 2579.3 | 4214.8 | . 1997.2503374.93 | 4558.6 | 2624.3 | 4270.7 | . 1997.2543439.22 | 4620.5 | 2648.7 | 4301.5 | . 1997.2583407.83 | 4659.2 | 2656.7 | 4312.9 | . 1997.2623407.83 | 4659.2 | 2656.7 | 4312.9 | . 1997.2653407.83 | 4659.2 | 2656.7 | 4312.9 | . 1997.2693281.46 | 4501.7 | 2581.8 | 4248.1 | . 1997.2733210.94 | 4488.7 | 2530.3 | 4236.6 | . 1997.2773212.82 | 4463.9 | 2514.5 | 4214.6 | . 1997.2813235.35 | 4471.5 | 2518.0 | 4236.6 | . 1997.2853342.77 | 4588.0 | 2572.3 | 4271.7 | . 1997.2883328.13 | 4582.6 | 2579.0 | 4269.3 | . 1997.2923364.76 | 4634.9 | 2617.6 | 4292.3 | . 1997.2963352.58 | 4626.6 | 2608.0 | 4313.2 | . 1997.3003319.24 | 4604.2 | 2574.6 | 4270.7 | . 1997.3043297.52 | 4586.3 | 2566.1 | 4251.7 | . 1997.3083369.26 | 4643.4 | 2620.6 | 4286.8 | . 1997.3123347.54 | 4625.6 | 2621.0 | 4294.6 | . 1997.3153361.80 | 4665.7 | 2615.2 | 4298.9 | . 1997.3193361.20 | 4699.1 | 2547.6 | 4310.5 | . 1997.3233328.41 | 4740.1 | 2522.7 | 4328.7 | . 1997.3273348.90 | 4752.3 | 2514.7 | 4346.1 | . 1997.3313366.87 | 4781.1 | 2533.6 | 4387.7 | . 1997.3353396.49 | 4836.1 | 2539.8 | 4388.5 | . 1997.3383357.57 | 4772.3 | 2536.3 | 4369.7 | . 1997.3423372.96 | 4793.3 | 2550.3 | 4389.7 | . 1997.3463425.86 | 4855.1 | 2602.9 | 4433.2 | . 1997.3503438.09 | 4897.6 | 2639.5 | 4436.0 | . 1997.3543438.09 | 4897.6 | 2639.5 | 4445.0 | . 1997.3583491.08 | 4953.5 | 2655.3 | 4455.6 | . 1997.3623565.69 | 5029.6 | 2672.8 | 4455.6 | . 1997.3653548.52 | 4988.4 | 2651.9 | 4519.3 | . 1997.3693537.45 | 5016.0 | 2643.3 | 4537.5 | . 1997.3733537.45 | 5016.0 | 2643.3 | 4580.4 | . 1997.3773533.21 | 5004.7 | 2633.9 | 4630.9 | . 1997.3813593.14 | 5042.5 | 2693.1 | 4669.6 | . 1997.3853559.29 | 5084.2 | 2719.6 | 4691.0 | . 1997.3883588.57 | 5134.3 | 2774.6 | 4686.9 | . 1997.3923564.85 | 5141.7 | 2776.0 | 4681.2 | . 1997.3963569.26 | 5157.5 | 2784.3 | 4693.9 | . 1997.4003569.26 | 5157.5 | 2784.3 | 4645.2 | . 1997.4043516.20 | 5081.0 | 2751.1 | 4607.5 | . 1997.4083600.40 | 5178.6 | 2786.4 | 4642.0 | . 1997.4123575.44 | 5176.4 | 2741.7 | 4651.8 | . 1997.4153621.72 | 5181.0 | 2762.9 | 4661.8 | . 1997.4193669.31 | 5196.7 | 2654.7 | 4661.8 | . 1997.4233665.43 | 5190.0 | 2680.3 | 4681.6 | . 1997.4273626.60 | 5133.1 | 2583.2 | 4677.5 | . 1997.4313635.38 | 5132.1 | 2579.2 | 4672.3 | . 1997.4353562.73 | 5041.6 | 2583.9 | 4621.3 | . 1997.4383596.40 | 5150.0 | 2601.5 | 4562.8 | . 1997.4423655.59 | 5207.2 | 2624.5 | 4557.8 | . 1997.4463651.59 | 5238.5 | 2635.4 | 4557.1 | . 1997.4503684.60 | 5251.2 | 2690.9 | 4576.2 | . 1997.4543700.53 | 5320.0 | 2719.3 | 4645.0 | . 1997.4583668.61 | 5368.8 | 2686.2 | 4686.7 | . 1997.4623671.16 | 5361.9 | 2664.2 | 4739.6 | . 1997.4653671.87 | 5308.6 | 2696.2 | 4724.8 | . 1997.4693737.16 | 5364.2 | 2760.3 | 4757.4 | . 1997.4733752.37 | 5384.6 | 2808.5 | 4783.1 | . 1997.4773750.02 | 5362.0 | 2795.9 | 4745.1 | . 1997.4813721.18 | 5345.9 | 2762.6 | 4682.2 | . 1997.4853730.56 | 5405.0 | 2751.7 | 4657.0 | . 1997.4883777.56 | 5510.3 | 2739.7 | 4653.7 | . 1997.4923788.54 | 5561.8 | 2757.1 | 4593.9 | . 1997.4963748.79 | 5587.8 | 2762.2 | 4575.8 | . 1997.5003761.07 | 5576.1 | 2784.8 | 4596.3 | . 1997.5043819.52 | 5662.4 | 2867.4 | 4640.0 | . 1997.5083820.16 | 5669.9 | 2893.6 | 4657.9 | . 1997.5123809.92 | 5700.3 | 2891.0 | 4640.3 | . 1997.5153766.89 | 5620.6 | 2858.3 | 4604.6 | . 1997.5193834.84 | 5654.8 | 2944.0 | 4728.3 | . 1997.5233867.53 | 5674.3 | 2909.5 | 4751.4 | . 1997.5273939.73 | 5804.9 | 2937.0 | 4831.7 | . 1997.5313946.73 | 5846.5 | 2934.5 | 4812.8 | . 1997.5354003.35 | 5947.0 | 2947.7 | 4810.7 | . 1997.5384030.10 | 6012.6 | 2929.8 | 4758.5 | . 1997.5424026.97 | 5977.1 | 2950.6 | 4762.4 | . 1997.5464000.65 | 5885.4 | 2929.1 | 4767.8 | . 1997.5504074.30 | 5801.5 | 2941.6 | 4799.5 | . 1997.5544142.19 | 5845.8 | 2941.6 | 4857.4 | . 1997.5584139.68 | 5844.7 | 2950.7 | 4899.3 | . 1997.5624223.69 | 5927.5 | 2988.0 | 4964.2 | . 1997.5654203.91 | 5868.3 | 2958.6 | 4949.0 | . 1997.5694131.94 | 5737.1 | 2876.7 | 4877.2 | . 1997.5734139.96 | 5620.5 | 2874.1 | 4805.7 | . 1997.5774297.64 | 5677.1 | 2921.1 | 4846.7 | . 1997.5814384.82 | 5869.9 | 3003.5 | 4874.5 | . 1997.5854320.52 | 5849.2 | 2973.5 | 4862.9 | . 1997.5884368.54 | 5847.0 | 3025.9 | 4851.5 | . 1997.5924400.30 | 5888.0 | 3022.2 | 4862.6 | . 1997.5964377.70 | 5842.1 | 3023.6 | 4876.6 | . 1997.6004458.66 | 5929.5 | 3069.3 | 4927.3 | . 1997.6044405.52 | 5898.2 | 3075.7 | 4907.5 | . 1997.6084336.98 | 5898.2 | 3049.5 | 4899.3 | . 1997.6124302.50 | 5771.0 | 2992.4 | 4895.7 | . 1997.6154325.86 | 5765.2 | 2984.1 | 4960.6 | . 1997.6194364.25 | 5812.1 | 3037.1 | 5026.2 | . 1997.6234428.08 | 5922.1 | 3056.3 | 5086.8 | . 1997.6274342.31 | 5864.8 | 2996.3 | 5031.3 | . 1997.6314333.15 | 5825.6 | 2983.4 | 5031.9 | . 1997.6354377.51 | 5808.4 | 2998.6 | 5075.8 | . 1997.6384237.06 | 5682.1 | 2924.0 | 5003.6 | . 1997.6424195.53 | 5579.5 | 2921.8 | 4991.3 | . 1997.6464077.59 | 5498.5 | 2921.8 | 4865.8 | . 1997.6504080.55 | 5405.6 | 2870.1 | 4835.0 | . 1997.6544190.45 | 5580.1 | 2936.2 | 4914.2 | . 1997.6584251.93 | 5690.1 | 2979.3 | 4958.4 | . 1997.6624204.81 | 5668.8 | 2957.2 | 4978.0 | . 1997.6654090.14 | 5475.8 | 2904.2 | 4901.1 | . 1997.6694076.75 | 5473.9 | 2898.6 | 4901.1 | . 1997.6733993.70 | 5363.3 | 2869.3 | 4886.3 | . 1997.6773992.03 | 5409.6 | 2871.7 | 4906.9 | . 1997.6813897.43 | 5217.3 | 2828.4 | 4845.4 | . 1997.6853919.79 | 5216.7 | 2770.5 | 4817.5 | . 1997.6884001.81 | 5271.5 | 2805.8 | 4870.2 | . 1997.6924127.28 | 5447.5 | 2921.2 | 4952.2 | . 1997.6964062.13 | 5478.6 | 2918.0 | 4976.9 | . 1997.7004093.43 | 5478.1 | 2927.0 | 4991.3 | . 1997.7044073.71 | 5532.9 | 2924.5 | 4994.2 | . 1997.7084131.26 | 5505.3 | 2940.9 | 4985.2 | . 1997.7124104.57 | 5445.1 | 2919.7 | 4950.5 | . 1997.7154028.00 | 5356.7 | 2874.6 | 4905.2 | . 1997.7193890.24 | 5280.8 | 2843.6 | 4854.8 | . 1997.7233796.61 | 5281.9 | 2834.1 | 4848.2 | . 1997.7273869.53 | 5321.7 | 2898.6 | 4902.9 | . 1997.7313995.69 | 5417.8 | 2940.6 | 4976.4 | . 1997.7353970.44 | 5550.4 | 2944.0 | 5013.1 | . 1997.7384004.04 | 5629.0 | 2978.4 | 5046.2 | . 1997.7423983.06 | 5611.0 | 2977.2 | 5023.8 | . 1997.7464096.85 | 5705.1 | 3017.5 | 5075.7 | . 1997.7504091.77 | 5730.4 | 2997.2 | 5027.5 | . 1997.7544150.95 | 5732.5 | 3023.7 | 5077.2 | . 1997.7584104.93 | 5667.1 | 3005.4 | 5065.5 | . 1997.7624135.09 | 5716.6 | 2985.6 | 5226.3 | . 1997.7654116.52 | 5691.8 | 2989.0 | 5220.3 | . 1997.7694154.89 | 5673.6 | 3008.3 | 5244.2 | . 1997.7734262.98 | 5754.7 | 3054.9 | 5317.1 | . 1997.7774266.17 | 5825.0 | 3052.1 | 5296.1 | . 1997.7814266.17 | 5929.0 | 3094.0 | 5330.8 | . 1997.7854326.35 | 5897.4 | 3078.0 | 5300.0 | . 1997.7884311.13 | 5846.9 | 3064.4 | 5305.6 | . 1997.7924267.40 | 5822.3 | 3024.1 | 5262.1 | . 1997.7964179.92 | 5732.2 | 2960.7 | 5217.8 | . 1997.8004164.62 | 5699.5 | 2955.1 | 5227.3 | . 1997.8044225.27 | 5792.8 | 3002.9 | 5300.1 | . 1997.8084215.23 | 5836.3 | 3002.5 | 5298.9 | . 1997.8124168.62 | 5815.9 | 2992.2 | 5263.7 | . 1997.8154149.92 | 5806.8 | 2992.9 | 5287.9 | . 1997.8194049.16 | 5751.6 | 2958.0 | 5271.1 | . 1997.8234069.25 | 5777.2 | 2946.7 | 5211.0 | . 1997.8274172.47 | 5862.9 | 2989.9 | 5225.9 | . 1997.8314124.86 | 5803.2 | 2958.1 | 5148.8 | . 1997.8353976.38 | 5651.8 | 2856.9 | 4991.5 | . 1997.8383981.44 | 5689.5 | 2849.0 | 4970.2 | . 1997.8423871.39 | 5533.5 | 2769.6 | 4840.7 | . 1997.8463645.69 | 5279.7 | 2651.3 | 4755.4 | . 1997.8503806.66 | 5479.0 | 2818.0 | 4871.8 | . 1997.8543748.88 | 5370.9 | 2739.5 | 4801.9 | . 1997.8583753.66 | 5467.2 | 2739.3 | 4842.3 | . 1997.8623847.73 | 5581.6 | 2788.0 | 4906.4 | . 1997.8653784.80 | 5538.2 | 2774.9 | 4897.4 | . 1997.8693841.39 | 5601.6 | 2822.4 | 4908.3 | . 1997.8733813.88 | 5557.4 | 2781.8 | 4863.8 | . 1997.8773715.38 | 5438.6 | 2707.1 | 4764.3 | . 1997.8813728.37 | 5459.7 | 2707.1 | 4806.8 | . 1997.8853734.79 | 5483.9 | 2707.1 | 4793.7 | . 1997.8883697.48 | 5434.0 | 2694.5 | 4720.4 | . 1997.8923701.94 | 5418.2 | 2700.7 | 4711.0 | . 1997.8963676.65 | 5437.0 | 2698.9 | 4741.8 | . 1997.9003816.71 | 5565.0 | 2773.0 | 4867.0 | . 1997.9043844.14 | 5574.2 | 2782.6 | 4845.4 | . 1997.9083876.90 | 5571.7 | 2790.6 | 4830.1 | . 1997.9123931.81 | 5650.4 | 2821.2 | 4908.4 | . 1997.9153941.91 | 5725.5 | 2861.7 | 4985.8 | . 1997.9193832.10 | 5645.7 | 2802.5 | 4898.6 | . 1997.9233850.14 | 5666.3 | 2786.3 | 4863.5 | . 1997.9273926.93 | 5738.3 | 2811.7 | 4891.2 | . 1997.9313961.97 | 5772.4 | 2829.0 | 4889.0 | . 1997.9353972.08 | 5775.9 | 2854.4 | 4831.8 | . 1997.9384125.92 | 5875.1 | 2918.5 | 4921.8 | . 1997.9424096.40 | 5919.9 | 2913.1 | 4977.6 | . 1997.9464074.55 | 5922.7 | 2902.4 | 4970.7 | . 1997.9504159.72 | 5969.5 | 2914.5 | 5082.3 | . 1997.9544191.81 | 6009.0 | 2910.1 | 5142.9 | . 1997.9584208.14 | 6095.3 | 2932.5 | 5187.4 | . 1997.9624187.13 | 6103.2 | 2959.4 | 5177.1 | . 1997.9654116.70 | 6056.6 | 2932.2 | 5130.7 | . 1997.9694016.70 | 6021.8 | 2828.5 | 5035.9 | . 1997.9734061.91 | 6018.7 | 2830.3 | 5045.2 | . 1997.9774029.08 | 5986.6 | 2838.3 | 5121.8 | . 1997.9814150.31 | 6092.7 | 2912.2 | 5203.4 | . 1997.9854154.57 | 6122.1 | 2893.3 | 5190.8 | . 1997.9884162.92 | 6115.1 | 2894.5 | 5168.3 | . 1997.9924055.35 | 5989.9 | 2822.9 | 5020.2 | . 1997.9964125.54 | 6049.3 | 2869.7 | 5018.2 | . 1998.0004132.79 | 6044.7 | 2858.1 | 5049.8 | . 데이터에서 시간의 한 부분 범위를 얻을 수 있는 window 함수 입니다. . &#55176;&#49828;&#53664;&#44536;&#47016; . hist(EuStockMarkets[,&#39;SMI&#39;], 30) . 데이터의 히스토그램입니다. 특별한 인사이트가 없습니다. . hist(diff(EuStockMarkets[,&#39;SMI&#39;], 30)) . 인접한 데이터간 차이의 히스토그램 입니다. 정규분포와 유사한 모양을 띕니다. . 주가지수가 상승추세를 보이므로 차분(diff)은 양수쪽으로 미세하게 치우쳐 있습니다. . &#49328;&#51216;&#46020; . plot(EuStockMarkets[,&#39;SMI&#39;], EuStockMarkets[,&#39;DAX&#39;]) . 시간에 따른 두 주식의 가치를 보여주는 산점도 입니다. 강한 양의 상관관계를 보이네요. . plot(diff(EuStockMarkets[,&#39;SMI&#39;]), diff(EuStockMarkets[,&#39;DAX&#39;])) . 두 주식의 일일가치 변동을 보여주는 산점도입니다. 위 그림과 다르게 상관관계가 강하지 않습니다. . plot(lag(diff(EuStockMarkets[,&#39;SMI&#39;]),1), diff(EuStockMarkets[,&#39;DAX&#39;])) . 두 주식의 일일가치 변동이 참상관관계라도 실제로 적용시키는덴 무리가 있습니다. . 주식의 미래 가격을 알고 싶은건데, 같은 미래에 다른 주식의 가격하고 상관관계가 있는것은 의미가 없습니다. . 그보다 한 시점 전 타 주식 가격과 미래 주식 가격간의 연관이 있어야 유의미합니다. . lag 함수를 이용해서 한 주가를 1시점 앞으로 당기고 산점도를 구했습니다. . 두 주가의 상관관계가 사라졌다는 것을 산점도를 보고 알 수 있습니다. . &#47204;&#47553; &#50952;&#46020; . x &lt;- rnorm(n = 100, mean = 0, sd = 10) + 1:100 mn &lt;- function(n) rep(1/n, n) plot(x, type = &#39;l&#39;, lwd = 1) lines(filter(x, mn(5)), col = 2, lwd = 3, lty = 2) lines(filter(x, mn(50)), col = 3, lwd = 3, lty = 3) . filter(x, mn(5)) . A Time Series: &lt;ol class=list-inline&gt;&lt;NA&gt; | &lt;NA&gt; | -1.77288105461343 | 1.31992176123301 | 1.73981260314922 | 8.40487565556649 | 5.3748710170368 | 10.1065230240359 | 12.7346567822176 | 13.5586688980719 | 12.6921098544628 | 13.8435662061596 | 13.9730144970645 | 9.35900906311078 | 13.758956613448 | 15.6427630176612 | 20.4817849516833 | 22.3086556019964 | 25.3660016459804 | 22.1569653798478 | 25.1374316930229 | 18.9960252654437 | 19.6145257811429 | 19.2015080642112 | 24.7274005432306 | 21.6801410945616 | 28.8134751920388 | 30.3806401765035 | 30.2485143964029 | 26.9003444995948 | 28.4214160008667 | 30.6441878792136 | 28.7817189385494 | 32.4909977045378 | 31.6538379235585 | 30.4489153481993 | 28.5446624272632 | 31.6681220929798 | 37.6314633176833 | 38.4760566106296 | 45.8738959745044 | 45.4223532971025 | 47.5731596337499 | 47.2963569945348 | 47.7022098006225 | 46.3430642214953 | 47.909777607114 | 47.0691900552067 | 42.8364536442156 | 49.3877148972977 | 47.9736397149515 | 53.4000401255189 | 54.6497226664114 | 60.2655379215343 | 55.4698041274768 | 58.9018820051029 | 58.2373643951854 | 56.2450939372977 | 53.7826512222036 | 56.4110597160915 | 58.4204570491475 | 59.7460187553621 | 63.6126175676387 | 62.3517723657575 | 69.1501339271387 | 68.8062407224709 | 70.0341269606641 | 73.2089209563275 | 76.1726286991414 | 73.6915749387865 | 76.9208647715888 | 75.1541185829742 | 76.9942252691702 | 76.4561897047353 | 77.6567117512646 | 76.6029212106872 | 83.9077431791539 | 78.3277983013414 | 83.3466522428901 | 85.7994713218054 | 85.9370853983274 | 82.4936728886608 | 82.9567421627858 | 81.4989001340583 | 81.3373987045914 | 83.8820192148659 | 84.8431049407526 | 90.0661772264516 | 93.8484169700886 | 94.3840506066453 | 96.8521833747149 | 96.1322902206609 | 94.8408075569562 | 94.8032051094727 | 100.759551321651 | 101.59154547457 | 102.648280935003 | 106.438834514587 | &lt;NA&gt; | &lt;NA&gt; | &lt;/ol&gt; filter 함수를 이용해 이동평균 값을 구해서 그래프를 그렸습니다. . install.packages(&#39;zoo&#39;) library(zoo) f1 &lt;- rollapply(zoo(x), 20, function(w) min(w), align = &#39;left&#39;, partial = TRUE) f2 &lt;- rollapply(zoo(x), 20, function(w) min(w), align = &#39;right&#39;, partial = TRUE) plot(x, lwd =1, type = &#39;l&#39;) lines(f1, col = 2, lwd = 3, lty = 2) lines(f2, col = 3, lwd = 3, lty = 3) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . fitter 함수는 일차 선형 변환에 기반하므로 일차결합하지 않은 함수는 사용을 할 수 없습니다. . 그래서 zoo 패키지의 rollapply 함수를 사용했는데요. 사용형태로 rollapply(데이터, 원도 크기, 적용함수, 함수 적용 방향) 입니다. . 이때 ts 객체는 균등한 간격의 시계열을 가정하나 zoo 객체는 타임스테프를 색인 속성으로 저장하기에 주기적인 시계열을 요구하지 않습니다. . plot(x, type = &#39;l&#39;, lwd = 1) lines(cummax(x), col = 2, lwd = 3, lty = 2) # 최댓값 lines(cumsum(x)/1:length(x), col = 3, lwd = 3, lty = 3) # 평균 . 확장 윈도로 cummax, cumsum 함수를 사용했습니다. . plot(x, lwd =1, type = &#39;l&#39;) lines(rollapply(zoo(x), seq_along(x), function(w) max(w), partial = TRUE, align = &#39;right&#39;), col = 2, lwd = 3, lty = 2) lines(rollapply(zoo(x), seq_along(x), function(w) mean(w), partial = TRUE, align = &#39;right&#39;), col = 3, lwd = 3, lty = 3) . 윗 그림과 동일한 그림을 출력합니다. 이번엔 rollapply 함수를 사용했죠. . &#51088;&#44592; &#49345;&#44288; . x &lt;- 1:100 y &lt;- sin(x * pi / 3) plot(y, type = &#39;b&#39;) acf(y) . acf란 자기상관계수로 위 그림에서 x축인 Lag 값은 떨어진 정도를 의미합니다. . 더 자세히 설명하면 자기상관계수란 cor(Yt, Y(t-k))를 의미하는데 이때 k가 Lag 입니다. . install.packages(&#39;data.table&#39;) library(data.table) cor(y, shift(y,1), use = &#39;pairwise.complete.obs&#39;) cor(y, shift(y,2), use = &#39;pairwise.complete.obs&#39;) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . 0.500153115912332 -0.503715197153103 자기 상관 계수를 구하기 위해 data.table 패키지 내 shift 함수를 사용했습니다. . y &lt;- sin(x * pi / 3) plot(y[1:30], type = &#39;b&#39;) pacf(y) . pacf은 편자기상관 함수로 cor(et, e(t-k))을 나타냅니다. 즉 설명된 부분을 제외한 잔차의 상관계수를 나타내죠. . 다시 말해 k-1까지 설명된 정보 이외에 k번째 이전 변수가 현재 변수에 설명되는 것이 있는지를 나타낸 값이라고 할 수 있습니다. . 사인함수와 같이 일정 주기가 있는 경우 Lag(k) 값이 커질 수록 PACF 값이 0에 수렴합니다. . 다시말해 ACF와 달리 PACF는 불필요한 중복관계를 제거하는 역할도 합니다. . y1 &lt;- sin(x * pi / 3) plot(y1, type = &#39;b&#39;) acf(y1) pacf(y1) . y2 &lt;- sin(x * pi / 10) plot(y2, type = &#39;b&#39;) acf(y2) pacf(y2) . ACF와 PACF를 비교한 그래프들입니다. . 정상 데이터의 ACF는 빠르게 0으로 수렴해야합니다. 하지만 주기함수에서 ACF는 그렇지 못한 모습이죠. . y3 &lt;- y1 + y2 plot(y3, type = &#39;b&#39;) acf(y3) pacf(y3) . 두 계열을 더한 데이터의 ACF와 PACF를 구했습니다. . ACF는 앞서 구한 두 ACF를 합한 것과 같습니다. 반면 PACF는 단순히 합친것은 아닙니다. . noise1 &lt;- rnorm(100, sd = 0.05) noise2 &lt;- rnorm(100, sd = 0.05) y1 &lt;- y1 + noise1 y2 &lt;- y2 + noise2 y &lt;- y1 + y2 plot(y1, type = &#39;b&#39;) acf(y1) pacf(y1) . plot(y2, type = &#39;b&#39;) acf(y2) pacf(y2) . plot(y, type = &#39;b&#39;) acf(y) pacf(y) . 노이즈를 조금 추가했는데, 노이즈가 없는 것과 비슷한 결과가 나옵니다. . &#49884;&#44033;&#54868; . AirPassengers . A Time Series: 12 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1949112 | 118 | 132 | 129 | 121 | 135 | 148 | 148 | 136 | 119 | 104 | 118 | . 1950115 | 126 | 141 | 135 | 125 | 149 | 170 | 170 | 158 | 133 | 114 | 140 | . 1951145 | 150 | 178 | 163 | 172 | 178 | 199 | 199 | 184 | 162 | 146 | 166 | . 1952171 | 180 | 193 | 181 | 183 | 218 | 230 | 242 | 209 | 191 | 172 | 194 | . 1953196 | 196 | 236 | 235 | 229 | 243 | 264 | 272 | 237 | 211 | 180 | 201 | . 1954204 | 188 | 235 | 227 | 234 | 264 | 302 | 293 | 259 | 229 | 203 | 229 | . 1955242 | 233 | 267 | 269 | 270 | 315 | 364 | 347 | 312 | 274 | 237 | 278 | . 1956284 | 277 | 317 | 313 | 318 | 374 | 413 | 405 | 355 | 306 | 271 | 306 | . 1957315 | 301 | 356 | 348 | 355 | 422 | 465 | 467 | 404 | 347 | 305 | 336 | . 1958340 | 318 | 362 | 348 | 363 | 435 | 491 | 505 | 404 | 359 | 310 | 337 | . 1959360 | 342 | 406 | 396 | 420 | 472 | 548 | 559 | 463 | 407 | 362 | 405 | . 1960417 | 391 | 419 | 461 | 472 | 535 | 622 | 606 | 508 | 461 | 390 | 432 | . colors &lt;- c(&#39;green&#39;, &#39;red&#39;, &#39;pink&#39;, &#39;blue&#39;,&#39;yellow&#39;, &#39;lightsalmon&#39;, &#39;black&#39;, &#39;grey&#39;, &#39;cyan&#39;, &#39;lightblue&#39;, &#39;maroon&#39;, &#39;purple&#39;) matplot(matrix(AirPassengers, nrow = 12, ncol = 12), type = &#39;l&#39;, col = colors, lty = 1, lwd = 2.5, xaxt = &#39;n&#39;) legend(&#39;topleft&#39;, legend = 1949:1960, col = colors) . AirPassengers 자료를 시각화 했습니다. . install.packages(&#39;forecast&#39;) library(forecast) seasonplot(AirPassengers) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) also installing the dependencies ‘xts’, ‘TTR’, ‘quadprog’, ‘quantmod’, ‘fracdiff’, ‘lmtest’, ‘timeDate’, ‘tseries’, ‘urca’, ‘RcppArmadillo’ Registered S3 method overwritten by &#39;quantmod&#39;: method from as.zoo.data.frame zoo . forecast 패키지 내 seasonplot 함수로 비슷한 그림을 그렸습니다. . matplot(t(matrix(AirPassengers, nrow = 12, ncol = 12)), type = &#39;l&#39;, col = colors, lty = 1, lwd = 2.5, xaxt = &#39;n&#39;) . 이 그래프는 앞 그래프와 다르게 연도 시계열을 나타냅니다. . monthplot(AirPassengers) . 달 별로 값을 시각화 해주는 monthplot 함수를 사용했습니다. . &#45712;&#45184;&#51216; . 시계열 데이터를 분석하기 전 탐색적으로 자료분석 하는 여러가지 시각에 대해 공부했습니다. . 그 중 롤링 윈도와 자기 상관 부분이 앞으로 시계열 공부하는데 중요할 것 같네요. . 앞으로 공부할 내용이 더욱 기대됩니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/timeseries/r/eda/2021/12/23/TimeSeries1.html",
            "relUrl": "/book/jupyter/timeseries/r/eda/2021/12/23/TimeSeries1.html",
            "date": " • Dec 23, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "[Do it 자연어] 4. 문장 쌍 분류하기 + 웹 실습",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (0.0.9999) Requirement already satisfied: Korpora&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0) Requirement already satisfied: flask-ngrok&gt;=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25) Requirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Requirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4) Requirement already satisfied: flask-cors&gt;=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10) Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Requirement already satisfied: future&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.18.2) Requirement already satisfied: PyYAML&lt;=5.4.1,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (5.4.1) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.3.0) Requirement already satisfied: fsspec[http]&gt;=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2021.11.1) Requirement already satisfied: torchmetrics&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.2) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Requirement already satisfied: huggingface-hub&gt;=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.2.1) Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.0.46) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.10.3) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.8.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Requirement already satisfied: xlrd&gt;=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora&gt;=0.2.0-&gt;ratsnlp) (2.0.1) Requirement already satisfied: dataclasses&gt;=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora&gt;=0.2.0-&gt;ratsnlp) (0.6) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.2.0) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (5.2.0) Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.2.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.0.2) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.13.0) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.7.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.classification import ClassificationTrainArguments args = ClassificationTrainArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name = &#39;pair-classification&#39;, # 문장 쌍 분류를 할 예정이므로 downstream_corpus_name=&#39;klue-nli&#39;, # 업스테이지 기업이 공게한 KLUE-NLI 데이터로 파인튜닝 downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, batch_size = 32 if torch.cuda.is_available() else 4, learning_rate=5e-5, max_seq_length=64, epochs = 5, tpu_cores=0 if torch.cuda.is_available() else 8, seed = 7, ) . kcbert-base 모델을 klue-nli 데이터로 파인튜닝 합니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;pair-classification&#39;, downstream_corpus_name=&#39;klue-nli&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) . set seed: 7 . 랜덤 시드와 로거를 설정합니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . nlpbook.download_downstream_dataset(args) . Downloading: 100%|██████████| 12.3M/12.3M [00:00&lt;00:00, 37.5MB/s] Downloading: 100%|██████████| 1.47M/1.47M [00:00&lt;00:00, 35.4MB/s] . 말뭉치를 내려받습니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 토크나이저를 구축합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.paircls import KlueNLICorpus from ratsnlp.nlpbook.classification import ClassificationDataset corpus = KlueNLICorpus() # json 파일형식의 KLUE-NLI 데이터를 문장(전제+가설)과 레이블(참 거짓 중립)으로 읽음 train_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39;, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/klue-nli INFO:ratsnlp:loading train data... LOOKING AT /root/Korpora/klue-nli/klue_nli_train.json INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 17.969 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다. INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다. INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다. INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다. INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다. INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 2.044 s] . train_dataset[0] . ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) . KlueNLICorpus에서 받은 문장들을 미리 설정한 토크나이저로 분리합니다. . 출력물은 input_ids, attention_mask, token_type_ids, label 총 4개가 나옵니다. . 이전과 같은 결과인데 짧게 설명하며 input_ids은 토큰 시퀀스를, attention_mask는 패딩 여부를 알려줍니다. . token_type_ids은 세그먼트 정보로 첫번째 문장은 0, 두번째 문장은 1, 나머지 패딩은 0을 줍니다. . label은 0일때 참, 1일때 거짓, 2일때 중립을 의미합니다. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), collate_fn = nlpbook.data_collator, # 뽑은 인스턴스를 배치로 바꿔줌(텐서 형태로) drop_last = False, num_workers = args.cpu_workers, ) . 학습 데이터 셋으로 로더를 구축했습니다. 배치크기만큼 인스턴스를 랜덤하게 뽑은 뒤 이를 합처 배치를 만듭니다. . &#54217;&#44032;&#50857; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;test&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), # 순서대로 추출 collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/klue-nli INFO:ratsnlp:loading test data... LOOKING AT /root/Korpora/klue-nli/klue_nli_dev.json INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 1.457 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기 불편함이 많았다. INFO:ratsnlp:tokens: [CLS] 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 만족 ##했다 . [SEP] 10명 ##이 함께 사용 ##하기 불편 ##함이 많았 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 14184, 8258, 17, 3, 21000, 4017, 9158, 9021, 8268, 10588, 11467, 14338, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 성인 10명이 함께 사용하기 불편함없이 없었다. INFO:ratsnlp:tokens: [CLS] 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 만족 ##했다 . [SEP] 성인 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 없었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 14184, 8258, 17, 3, 13246, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 12629, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기에 만족스러웠다. INFO:ratsnlp:tokens: [CLS] 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 만족 ##했다 . [SEP] 10명 ##이 함께 사용 ##하기에 만족 ##스러 ##웠다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: entailment INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 14184, 8258, 17, 3, 21000, 4017, 9158, 9021, 19956, 14184, 13378, 19996, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10층에 건물사람들만 이용하는 수영장과 썬베드들이 있구요. + 건물사람들은 수영장과 썬베드를 이용할 수 있습니다. INFO:ratsnlp:tokens: [CLS] 10 ##층 ##에 건물 ##사람들 ##만 이용하는 수영 ##장과 썬 ##베 ##드 ##들이 있 ##구요 . [SEP] 건물 ##사람들은 수영 ##장과 썬 ##베 ##드 ##를 이용할 수 있습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: entailment INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4491, 4113, 10828, 9390, 4049, 14502, 26770, 21758, 2060, 4155, 4273, 7967, 2469, 8875, 17, 3, 10828, 11249, 26770, 21758, 2060, 4155, 4273, 4180, 29861, 1931, 8982, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10층에 건물사람들만 이용하는 수영장과 썬베드들이 있구요. + 수영장과 썬베드는 9층에 있습니다. INFO:ratsnlp:tokens: [CLS] 10 ##층 ##에 건물 ##사람들 ##만 이용하는 수영 ##장과 썬 ##베 ##드 ##들이 있 ##구요 . [SEP] 수영 ##장과 썬 ##베 ##드는 9 ##층 ##에 있습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4491, 4113, 10828, 9390, 4049, 14502, 26770, 21758, 2060, 4155, 4273, 7967, 2469, 8875, 17, 3, 26770, 21758, 2060, 4155, 8609, 28, 4491, 4113, 8982, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.293 s] . 입력받은 데이터를 토크나이저로 분리한 후 로더를 이용해 테스트용 배치를 만듭니다. . &#47784;&#45944; &#54617;&#49845; . from transformers import BertConfig, BertForSequenceClassification pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = corpus.num_labels, ) model = BertForSequenceClassification.from_pretrained( args.pretrained_model_name, config = pretrained_model_config ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . pretrained_model_config은 프리트레인을 마친 BERT모델을 기록한 형태입니다.분류할 라벨이 3인것까지 정보를 줍니다. . model은 윗 모델에 문서 분류용 태스크 모듈을 덧붙인 모델입니다. . from ratsnlp.nlpbook.classification import ClassificationTask task = ClassificationTask(model, args) . 앞서 정의한 모델을 학습시킵니다. ClassificationTask 내에는 옵티마이저와 러닝 레이트 스케줄러가 있습니다. . trainer = nlpbook.get_trainer(args) trainer . /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Checkpoint directory /gdrive/My Drive/nlpbook/checkpoint-paircls exists and is not empty. warnings.warn(*args, **kwargs) GPU available: True, used: True TPU available: False, using: 0 TPU cores . &lt;pytorch_lightning.trainer.trainer.Trainer at 0x7f92663bb710&gt; . 트레이너를 정의했습니다. 트레이너는 GPU/TPU 설정, 로그 및 체크포인트 등 귀찮은 설정을 알아서 해줍니다. . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | model | BertForSequenceClassification | 108 M -- 108 M Trainable params 0 Non-trainable params 108 M Total params 435.683 Total estimated model params size (MB) . &#51204;&#51228;&#50752; &#44032;&#49444;&#51012; &#44160;&#51613;&#54616;&#45716; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . from ratsnlp.nlpbook.classification import ClassificationDeployArguments args = ClassificationDeployArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, ) . downstream_model_checkpoint_fpath: /gdrive/My Drive/nlpbook/checkpoint-paircls/epoch=1-val_loss=0.82-v1.ckpt . 인퍼런스 설정을 해줍니다. . import torch from transformers import BertConfig, BertForSequenceClassification fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) . 체크 포인트를 로드해줍니다. (이전에 만든 모델 업로드) . from transformers import BertConfig pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = 3 ) model = BertForSequenceClassification(pretrained_model_config) . BERT 설절을 로드하고 BERT 모델을 초기화합니다. . model.load_state_dict({k.replace(&#39;model.&#39;, &#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].items()}) model.eval() . BertForSequenceClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30000, 768, padding_idx=0) (position_embeddings): Embedding(300, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=3, bias=True) ) . 초기화한 BERT 모델에 체크 포인트를 주입합니다. 그 후 모델을 평가모드로 바꿉니다. . def inference_fn(premise, hypothesis): inputs = tokenizer( [(premise, hypothesis)], max_length = args.max_seq_length, padding = &#39;max_length&#39;, truncation = True # 잘린 값 처리 여부 ) with torch.no_grad(): outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()}) prob = outputs.logits.softmax(dim=1) entailment_prob = round(prob[0][0].item(), 2) contradiction_prob = round(prob[0][1].item(), 2) neutral_prob = round(prob[0][1].item(), 2) if torch.argmax(prob) == 0: pred = &#39;참&#39; elif torch.argmax(prob) == 1: pred = &#39;거짓&#39; else: pred = &#39;중립&#39; return { &#39;premise&#39; : premise, &#39;hypothesis&#39; : hypothesis, &#39;prediction&#39; : pred, &#39;entailment_data&#39;: f&quot;참 {entailment_prob}&quot;, &#39;contradiction_data&#39;: f&quot;거짓 {contradiction_prob}&quot;, &#39;neutral_data&#39;: f&quot;중립 {neutral_prob}&quot;, &#39;entailment_width&#39;: f&quot;{entailment_prob*100}%&quot;, &#39;contradiction_width&#39;: f&quot;{contradiction_prob*100}%&quot;, &#39;neutral_width&#39;: f&quot;{neutral_prob*100}%&quot;, } . 전제와 가설을 입력받아 각각 토큰화, 인덱싱을 수행한 것을 파이토치 텐서 자료형으로 변환한뒤 모델에 입력하는 함수를 만듭니다. . from ratsnlp.nlpbook.classification import get_web_service_app app = get_web_service_app(inference_fn) app.run() . * Serving Flask app &#34;ratsnlp.nlpbook.classification.deploy&#34; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off . * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) . * Running on http://8cea-35-225-219-137.ngrok.io * Traffic stats available on http://127.0.0.1:4040 . 127.0.0.1 - - [23/Dec/2021 12:52:08] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:52:09] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:52:28] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:52:28] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:54:25] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:54:25] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:54:44] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:54:44] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:55:19] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:55:19] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:57:54] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:57:54] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - . 패키지를 설치하여 주어진 모델을 웹에서 서비스하게 해줍니다. . 다만 오류가 뜨네요.. 이부분은 공부를 더 해야겠습니다. . &#45712;&#45184;&#51216; . 앞서 영화 감상평을 긍정, 부정으로 구분하는 모델, 이번에 전제와 가설이 일치하는지 여부를 판단하는 모델을 만들었습니다. . 자연어의 기초 이론을 배우고 적용해봤는데, 언어를 수리적인 인풋으로 바꿔서 이를 판단하는 모델을 만든다는 것 자체가 신기했습니다. . 다만, 제가 처음 자연어 처리를 떠올렸을때와 약간 다른 점은 사람이 개입할 여지가 조금 작다는 느낌이 들었습니다. . 그래도 현실에서 유용한 모델을 만들 수 있다는 점이 신기했고, 더욱 더 공부하고 싶은 생각이 듭니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/23/Do_natural_language4.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/23/Do_natural_language4.html",
            "date": " • Dec 23, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "[DACON] 심장 질환 예측 경진대회",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/heart/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 1 | 53 | 1 | 2 | 130 | 197 | 1 | 0 | 152 | 0 | 1.2 | 0 | 0 | 2 | 1 | . 1 2 | 52 | 1 | 3 | 152 | 298 | 1 | 1 | 178 | 0 | 1.2 | 1 | 0 | 3 | 1 | . 2 3 | 54 | 1 | 1 | 192 | 283 | 0 | 0 | 195 | 0 | 0.0 | 2 | 1 | 3 | 0 | . 3 4 | 45 | 0 | 0 | 138 | 236 | 0 | 0 | 152 | 1 | 0.2 | 1 | 0 | 2 | 1 | . 4 5 | 35 | 1 | 1 | 122 | 192 | 0 | 1 | 174 | 0 | 0.0 | 2 | 0 | 2 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#44592;&#48376; &#48320;&#49688; &#49444;&#47749; . sex : 성별(0은 여자, 1은 남자), cp : 가슴통증(0~3, 클수록 심한통증), trestbps : 휴식 중 혈압 . chol : 혈중 콜레스테롤, fbs : 공복 중 혈당(120이상시 1), restecg : 휴식 중 심전도 결과(0은 좌심실 비대, 1은 정상, 2는 ST-T파 이상) . thalach : 최대 심박수, exang : 활동으로 인한 협심증 여부(0은 정상, 1은 이상), oldpeak : 휴식 대비 운동으로 인한 ST 하강 . slope : 활동 ST 분절 피크의 기울기(0 하강, 1 보통, 2 상승), ca : 주요 혈관 수(0-3개, 4는 NULL값), thal : 지중해빈혈 여부(0 Null, 1 정상, 2~3 결함) . train[&#39;target&#39;].value_counts() . 1 83 0 68 Name: target, dtype: int64 . 반응변수는 target으로 심장질환 판단 여부를 나타냅니다. 1은 이상, 0은 정상입니다. . 테스트 데이터는 이상이 83개, 정상이 68개로 이상이 조금 더 많습니다. . &#48276;&#51452;&#54805; &#48320;&#49688;&#47484; &#49900;&#51109;&#51656;&#54872; &#50668;&#48512;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54616;&#44592; . import matplotlib.pyplot as plt import seaborn as sns train_0 = train[train[&#39;target&#39;]==0] train_1 = train[train[&#39;target&#39;]==1] def cat_plot(column): f, ax = plt.subplots(1, 2, figsize=(16, 6)) sns.countplot(x = column, data = train_0, ax = ax[0], order = train_0[column].value_counts().index) ax[0].tick_params(labelsize=12) ax[0].set_title(&#39;target = 0&#39;) ax[0].set_ylabel(&#39;count&#39;) ax[0].tick_params(rotation=50) sns.countplot(x = column, data = train_1, ax = ax[1], order = train_1[column].value_counts().index) ax[1].tick_params(labelsize=12) ax[1].set_title(&#39;target = 1&#39;) ax[1].set_ylabel(&#39;count&#39;) ax[1].tick_params(rotation=50) plt.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() cat_plot(&quot;sex&quot;) . test[&#39;sex&#39;].value_counts() . 1 104 0 48 Name: sex, dtype: int64 . 우선 전반적으로 sex가 1인 자료가 많습니다. 앞서 설명한대로 sex가 1인 자료는 남성입니다. 이는 테스트 자료도 유사합니다. . 왼쪽 그림은 심장병이 없는 데이터의 성별 별 개수, 오른쪽 그림은 심장병이 있는 데이터의 성별 별 개수 입니다. . 그래프로 보아 주어진 데이터 내 여성의 심장병 발생 확률이 높은 것을 보여줍니다. . cat_plot(&quot;cp&quot;) . 다음은 심장병이 있는 데이터와 없는 데이터를 가슴통증 유무 변수로 확인했습니다. . 0이 가슴통증이 없는 값인데, cp가 0인 데이터들은 대부분 심장병이 없습니다. . 나머지 변수들은(cp가 1~3) 모두 심장병이 있을 확률이 더 높습니다. . 특이한 점은 cp가 3인 경우 가슴통증이 더 심해서 심장병이 있을 확률이 제일 높을 것이라고 생각하는데 그렇지는 않습니다. . 오히려 cp가 2인 경우가 더 심장병이 있을 확률이 더 높습니다. . 즉 이 변수는 순서형으로 보면 안됩니다. . cat_plot(&quot;fbs&quot;) . 범주형 변수들을 계속 같은 패턴으로 분석할 것 입니다. . 그래프에서는 심장질환 유무를 판단하는데 fbs는 크게 유의미한 변수는 아닌 것 같습니다. . cat_plot(&quot;restecg&quot;) test[&#39;restecg&#39;].value_counts() . 1 77 0 72 2 3 Name: restecg, dtype: int64 . restecg, 휴식 중 심전도 변수입니다. 1이 정상 값이나 0값 대비 오히려 심장질환이 있을 확률이 높은 것을 알 수 있어요. . 이 변수는 처리하기 애매합니다. 또 2는 스몰 샘플이나 모두 심장질환이 없는데, 너무 스몰샘플이라 함부로 처리하면 안되겠습니다. . 그래서 저는 이 변수는 유의미 하지 않다고 판단, 제거하겠습니다. . cat_plot(&quot;exang&quot;) . exang, 활동으로 인한 협심증 여부를 판단하는 변수 입니다. 역시 0은 정상, 1은 이상으로 알고 있는데 이상합니다. . 0이 나왔을때가 심장 질환을 가질 확률이 높습니다. 잘 이해가 되진 않는데 차이가 눈에 띄게 유의미하니 이 변수는 사용해야겠습니다. . cat_plot(&quot;slope&quot;) . slope, 활동 ST 분절 피크의 기울기 변수입니다. 우선 0인 값은 절대적 개수도 적고 심장 질환이 있든 없든 분포가 비슷합니다. . 차이가 나는 것은 1과 2인데 1은 심장병이 없을 확률이, 2는 심장병이 있을 확률이 높아집니다. . cat_plot(&quot;ca&quot;) test[&#39;ca&#39;].value_counts() . 0 80 1 34 2 23 3 10 4 5 Name: ca, dtype: int64 . ca, 확인된 주요 혈관 수 변수 입니다. 0이 절대적으로 많으며 2,3은 개수는 적으나 대부분 심장질환이 없습니다. . 그래프를 관찰해보면 2,3은 심장질환이 없다고 판단할 수 있는 좋은 변수 입니다. . 0은 약 70%가 심장질환이 있는 변수, 1은 대부분이 심장질환이 없는 변수 입니다. . 특이사항은 테스트 데이터에만 NULL값을 의미하는 4가 있는데 처리를 고민해야겠습니다. . 2와 3은 심장질환이 없을 확률이 대단히 높으므로 두 칼럼을 병합하겠습니다. . cat_plot(&quot;thal&quot;) test[&#39;thal&#39;].value_counts() . 2 82 3 59 1 10 0 1 Name: thal, dtype: int64 . thal, 지중해빈혈 여부 입니다. 우선 데이터 내 2번에 비율이 꽤 높습니다. . 2는 대부분 심장질환이 있는 변수, 3은 대부분 심장질환이 없는 변수 입니다. . 1은 정상을 의미하는 변수이나 심장질환을 판단하기 쉽지 않은 변수입니다. . 0은 NULL 값이므로 이 변수에선 판단을 보류한다는 의미에서 1과 합쳐주겠습니다. . &#50672;&#49549;&#54805; &#48320;&#49688;&#47484; &#49900;&#51109;&#51656;&#54872; &#50668;&#48512;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . def num_plot(column): fig, axes = plt.subplots(1, 2, figsize=(16, 6)) sns.distplot(train_0[column], ax = axes[0]) axes[0].tick_params(labelsize=12) axes[0].set_title(&#39;target = 0&#39;) axes[0].set_ylabel(&#39;count&#39;) sns.distplot(train_1[column], ax = axes[1]) axes[1].tick_params(labelsize=12) axes[1].set_title(&#39;target = 1&#39;) axes[1].set_ylabel(&#39;count&#39;) plt.subplots_adjust(wspace=0.3, hspace=0.3) num_plot(&quot;trestbps&quot;) [(train_0[&#39;trestbps&#39;]).mean(), (train_1[&#39;trestbps&#39;]).mean()] . [134.4558823529412, 130.04819277108433] . trestbps, 휴식 중 혈압 변수 입니다. 사실 두 집단 간 유의미한 차이가 있는 것 같진 않아요. . num_plot(&quot;chol&quot;) [(train_0[&#39;chol&#39;]).mean(), (train_1[&#39;chol&#39;]).mean()] . [242.23529411764707, 246.40963855421685] . chol, 콜레스테롤 변수 입니다. 두 분포가 유의미하게 차이있진 않아요. . num_plot(&quot;thalach&quot;) [(train_0[&#39;thalach&#39;]).mean(), (train_1[&#39;thalach&#39;]).mean()] . [141.19117647058823, 158.36144578313252] . thalach, 최대 심박수 변수 입니다. 확실히 thalach 값이 크면 심장질환일 확률이 늘어나는 것 같아요. . num_plot(&quot;oldpeak&quot;) [(train_0[&#39;oldpeak&#39;]).mean(), (train_1[&#39;oldpeak&#39;]).mean()] . [1.4808823529411763, 0.563855421686747] . oldpeak, 운동으로 인한 ST 하강 변수 입니다. 이 변수의 값이 크면 심장질환이 아닐 확률이 높아집니다. . &#45936;&#51060;&#53080; &#48288;&#51060;&#49828;&#46972;&#51064;&#50640; &#51080;&#45716; &#50672;&#49549;&#54805; &#48320;&#49688; EDA . fig, axes = plt.subplots(5, 3, figsize=(25, 20)) fig.suptitle(&#39;feature distributions per quality&#39;, fontsize= 40) for ax, col in zip(axes.flat, train.columns[1:-1]): sns.violinplot(x= &#39;target&#39;, y= col, ax=ax, data=train) ax.set_title(col, fontsize=20) plt.tight_layout(rect=[0, 0.03, 1, 0.95]) plt.show() . 다음 코드를 참고했습니다. . https://dacon.io/competitions/official/235848/codeshare/3832?page=1&amp;dtype=recent . 한눈에 변수들을 살펴볼 수 있어서 좋은 것 같아요. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . train[&#39;thal&#39;][train[&#39;thal&#39;] == 0] = 1 test[&#39;thal&#39;][test[&#39;thal&#39;] == 0] = 1 train_label = train[&#39;target&#39;] train.drop([&#39;trestbps&#39;,&#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;target&#39;], axis = 1, inplace= True) test.drop([&#39;trestbps&#39;,&#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;], axis = 1, inplace= True) . 앞서 EDA 한 정보를 바탕으로 trestbps, chol, fbs, restecg 변수를 모델에서 제외했습니다. . test2 = (test[test[&#39;ca&#39;] == 4]).drop([&#39;ca&#39;], axis = 1) test2id = test2[&#39;id&#39;] . 또 ca가 4인 값은 트레인 데이터에서 없는 NULL 값입니다. . 따라서 이 값을 가진 테스트 데이터는 ca변수가 없는 별개의 모델에서 학습하도록 값을 조정해줍니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;target&#39;] = rf.predict(test) # ca가 4인 데이터는 cp를 제외한 모델에서 생성된 결과를 사용하기로 한다. rf2 = RandomForestClassifier(random_state = 0, n_estimators = 100) rf2.fit(train.drop([&#39;ca&#39;], axis = 1),train_label) pred2 = rf2.predict(test2) k = 0 for i in test2id: sample_submission[&#39;target&#39;][sample_submission[&#39;id&#39;] == i] = pred2[k] k += 1 sample_submission.to_csv(&#39;heart_final_3.csv&#39;,index=False) . 랜덤포레스트로 모델을 만들었습니다. . from xgboost import XGBClassifier xgb = XGBClassifier() xgb.fit(train,train_label) sample_submission[&#39;target&#39;] = xgb.predict(test) # ca가 4인 데이터는 cp를 제외한 모델에서 생성된 결과를 사용하기로 한다. xgb2 = XGBClassifier() xgb2.fit(train.drop([&#39;ca&#39;], axis = 1),train_label) pred2 = xgb2.predict(test2) k = 0 for i in test2id: sample_submission[&#39;target&#39;][sample_submission[&#39;id&#39;] == i] = pred2[k] k += 1 sample_submission.to_csv(&#39;heart_final_4.csv&#39;,index=False) . [14:00:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [14:00:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . xgb 모델을 만들었습니다. .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/eda/heart/xgboost/randomforest/classifier/2021/12/22/dacon_heart.html",
            "relUrl": "/dacon/jupyter/eda/heart/xgboost/randomforest/classifier/2021/12/22/dacon_heart.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "[Do it 자연어] 3. 문서에 꼬리표 달기 + 웹 실습",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.4 MB/s Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 10.8 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 32.2 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 5.3 MB/s Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 35.1 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 42.8 MB/s Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 46.9 MB/s Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 45.5 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 504 kB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 31.9 MB/s Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 39.9 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 46.4 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 6.1 MB/s Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 46.4 MB/s Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 48.1 MB/s Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 36.6 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=1e7f96578c0a204626c5e0ce7352ca4a2a7b4da34d341e1960377950a7555852 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.classification import ClassificationTrainArguments args = ClassificationTrainArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, # 프리트레인 마친 언어모델의 이름 downstream_corpus_name=&#39;nsmc&#39;, # 다운스트림 데이터 이름 downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-doccls&#39;, # 파인튜닝된 모델의 체크포인트가 저장될 위치. batch_size = 32 if torch.cuda.is_available() else 4, #배치 크기. learning_rate=5e-5, max_seq_length = 128, # 토큰 기준 입력 문장 최대 길이 epochs = 1, # 학습 데이터 3번 반복 tpu_cores=0 if torch.cuda.is_available() else 8, seed = 7, ) . kcbert-base 모델을 NSMC 데이터로 파인튜닝 합니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) . set seed: 7 . 랜덤 시드를 설정합니다. . nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;document-classification&#39;, downstream_corpus_name=&#39;nsmc&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-doccls&#39;, max_seq_length=128, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) . 각종 로그를 출력하는 로거를 설정합니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . from Korpora import Korpora Korpora.fetch( corpus_name=args.downstream_corpus_name, # 다운스트림 데이터 이름 root_dir = args.downstream_corpus_root_dir, # 다운스트림 데이터를 내려받을 위치. force_download=True, ) . [nsmc] download ratings_train.txt: 14.6MB [00:00, 69.8MB/s] [nsmc] download ratings_test.txt: 4.90MB [00:00, 38.8MB/s] . 네이버 영화리뷰 데이터(NSMC)를 내려받습니다. . Korpora를 이용해 말뭉치를 특정장소에 저장합니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False ) . 토큰화를 수행하는 토크나이저를 선언합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset corpus = NsmcCorpus() # csv 파일형식의 NSMC 데이터를 문장과 레이블(긍/부정)으로 읽어 들임. train_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39;, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/nsmc INFO:ratsnlp:loading train data... LOOKING AT /root/Korpora/nsmc/ratings_train.txt INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 45.759 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 아 더빙.. 진짜 짜증나네요 목소리 INFO:ratsnlp:tokens: [CLS] 아 더 ##빙 . . 진짜 짜증나네 ##요 목소리 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 INFO:ratsnlp:tokens: [CLS] 흠 . . . 포 ##스터 ##보고 초딩 ##영화 ##줄 . . . . 오버 ##연기 ##조차 가볍 ##지 않 ##구나 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 1 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 3521, 17, 17, 17, 3294, 13069, 8190, 10635, 13796, 4006, 17, 17, 17, 17, 17613, 19625, 9790, 17775, 4102, 2175, 8030, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 너무재밓었다그래서보는것을추천한다 INFO:ratsnlp:tokens: [CLS] 너무 ##재 ##밓 ##었다 ##그래 ##서 ##보는 ##것을 ##추 ##천 ##한다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8069, 4089, 7847, 8217, 9791, 4072, 9136, 8750, 4142, 4244, 8008, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 INFO:ratsnlp:tokens: [CLS] 교도소 이야기 ##구먼 . . 솔직히 재미 ##는 없다 . . 평 ##점 조정 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 12164, 9089, 9828, 17, 17, 8876, 10827, 4008, 8131, 17, 17, 3288, 4213, 16612, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 INFO:ratsnlp:tokens: [CLS] 사이 ##몬 ##페 ##그 ##의 익 ##살 ##스런 연기 ##가 돋 ##보 ##였던 영화 ! 스파이 ##더 ##맨 ##에서 늙어 ##보이 ##기만 했던 커 ##스 ##틴 던 ##스트 ##가 너무나도 이뻐 ##보 ##였다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 1 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8538, 4880, 4335, 4313, 4042, 2452, 4471, 10670, 11219, 4009, 870, 4010, 13043, 9376, 5, 24034, 4356, 4617, 7971, 22878, 11980, 9235, 10129, 3010, 4103, 4713, 834, 8795, 4009, 22110, 23997, 4010, 9827, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/nsmc/cached_train_BertTokenizer_128_nsmc_document-classification [took 23.543 s] . 학습 데이터 셋을 구축했습니다. . 여기서 ClassificationDataset은 NsmcCorpus가 넘겨준 문장과 레이블을 각각 tokenizer을 활용해 가공합니다. . 이때 가공된 형태는 ClassificationFeatures 자료형으로 4가지 정보가 있습니다. . 첫번째로 input_ids로 토큰 시퀀스를 의미합니다. 두번째는 attention_mask로 패딩 토큰(0)을 구분합니다. . 세번째는 token_type_ids로 세그먼트 정보, 네번째는 label로 레이블 정보 입니다. . train_dataset[0] . ClassificationFeatures(input_ids=[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) . train_dataset[100] . ClassificationFeatures(input_ids=[2, 2005, 4024, 4017, 1293, 4599, 4775, 4042, 2478, 4075, 4196, 15, 1463, 4207, 4196, 8080, 4024, 10314, 11219, 4180, 12610, 10579, 832, 4140, 11414, 9827, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) . ClassificationFeatures 자료형을 개별 자료마다 확인했습니다. 4가지 정보가 있는걸 확인 할 수 있어요. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), # batch_size만큼 비복원 랜덤 추출. collate_fn = nlpbook.data_collator, # 뽑은 인스턴스를 배치로 만드는 역할. # 인스턴스들을 앞서 말한 4가지 정보별로 취합 후 파이토치가 요구하는 자료형인 텐서 형태로 바꿈. drop_last = False, num_workers = args.cpu_workers, ) . 학습 데이터 셋으로부터 로더를 구축했습니다. . 배치 크기만큼 인스턴스를 뽑아 이를 합처서 배치를 만듭니다. . &#54217;&#44032;&#50857; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode=&#39;test&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/nsmc INFO:ratsnlp:loading test data... LOOKING AT /root/Korpora/nsmc/ratings_test.txt INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 29.230 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 굳 ㅋ INFO:ratsnlp:tokens: [CLS] 굳 ㅋ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 1 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 352, 192, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: GDNTOPCLASSINTHECLUB INFO:ratsnlp:tokens: [CLS] G ##D ##N ##TO ##P ##C ##L ##A ##S ##S ##I ##N ##T ##H ##E ##C ##L ##U ##B [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 40, 4452, 4581, 25144, 4579, 4881, 4450, 4580, 4985, 4985, 4506, 4581, 4850, 5121, 4451, 4881, 4450, 5167, 4756, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아 INFO:ratsnlp:tokens: [CLS] 뭐야 이 평 ##점 ##들은 . . . . 나쁘 ##진 않지만 10 ##점 짜리 ##는 더더욱 아니잖아 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 10691, 2451, 3288, 4213, 7977, 17, 17, 17, 17, 10476, 4153, 15426, 8240, 4213, 21394, 4008, 15616, 13439, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 지루하지는 않은데 완전 막장임... 돈주고 보기에는.... INFO:ratsnlp:tokens: [CLS] 지 ##루 ##하지는 않은데 완전 막장 ##임 . . . 돈주고 보기에 ##는 . . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2688, 4532, 16036, 20879, 8357, 15971, 4252, 17, 17, 17, 13900, 25253, 4008, 17, 17, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠?? INFO:ratsnlp:tokens: [CLS] 3 ##D ##만 아니었 ##어도 별 다섯 개 줬 ##을텐데 . . 왜 3 ##D ##로 나와서 제 심 ##기를 불편 ##하게 하죠 ? ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 22, 4452, 4049, 18851, 8194, 1558, 23887, 220, 2648, 9243, 17, 17, 2332, 22, 4452, 4091, 10045, 2545, 2015, 8313, 10588, 8007, 18566, 32, 32, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/nsmc/cached_test_BertTokenizer_128_nsmc_document-classification [took 16.069 s] . 평가용 데이터는 인스턴스를 랜덤하게 뽑을 필요가 없습니다. . 그러므로 SequentialSampler을 사용해 인스턴스를 순서대로 추출합니다. . &#47784;&#45944; &#54617;&#49845; . args.pretrained_model_name . &#39;beomi/kcbert-base&#39; . from transformers import BertConfig, BertForSequenceClassification pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, # 프리트레인을 마친 BERT인 beomi/kcbert-base를 사용한다. num_labels = corpus.num_labels, # 긍정/부정을 분류하므로 2이다. ) model = BertForSequenceClassification.from_pretrained( args.pretrained_model_name, config = pretrained_model_config, ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.bias&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . BertForSequenceClassification 모델은 프리트레인을 마친 BERT 모델 위에 문서 분류용 태스크 모듈이 덧붙여진 형태입니다. . from ratsnlp.nlpbook.classification import ClassificationTask task = ClassificationTask(model, args) . ClassificationTask에는 옵티마이저(모델 파라미터를 업데이트 할때 최적의 방향/보폭 설정하는 도구), learning rata 스케줄러가 정의됩니다. . trainer = nlpbook.get_trainer(args) trainer . GPU available: True, used: True TPU available: False, using: 0 TPU cores . &lt;pytorch_lightning.trainer.trainer.Trainer at 0x7f5d99e99310&gt; . 트레이너를 정의했습니다. 트레이너는 GPU/TPU 설정, 로그 및 체크포인트 등 귀찮은 설정을 알아서 해줍니다. . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader, ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | model | BertForSequenceClassification | 108 M -- 108 M Trainable params 0 Non-trainable params 108 M Total params 435.680 Total estimated model params size (MB) /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown... warnings.warn(*args, **kwargs) . 1 에포크당 약 2시간?이 걸린다.. . &#50689;&#54868; &#47532;&#48624; &#44048;&#49457;&#48516;&#49437; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . # from google.colab import drive # drive.mount(&#39;/gdrive&#39;, force_remount=True) # from transformers import BertConfig, BertForSequenceClassification # from transformers import BertTokenizer # tokenizer = BertTokenizer.from_pretrained( # args.pretrained_model_name, # do_lower_case = False # ) . from ratsnlp.nlpbook.classification import ClassificationTrainArguments args = ClassificationTrainArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-doccls&#39;, max_seq_length = 128, ) . 인퍼런스 설정을 해줍니다. 프리트레인 모델은 앞선 분석에서 쓴 모델과 같은 모델을 사용해야합니다. . import torch fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = 2 ) model = BertForSequenceClassification.from_pretrained(pretrained_model_config) . AttributeError Traceback (most recent call last) &lt;ipython-input-29-6567a7cee6f5&gt; in &lt;module&gt;() 1 import torch 2 fine_tuned_model_ckpt = torch.load( -&gt; 3 args.downstream_model_checkpoint_fpath, 4 map_location = torch.device(&#39;cpu&#39;), 5 ) AttributeError: &#39;ClassificationTrainArguments&#39; object has no attribute &#39;downstream_model_checkpoint_fpath&#39; . 앞서 파인튜닝한 모델의 체크포인트를 읽어들입니다. . 그 뒤 BERT 모델을 초기화 해줍니다. . model.load_state_dict({k.replace(&#39;model.&#39;,&#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].items()}) model.eval() . 초기화한 BERT 모델에 체크포인트를 주입합니다. . 그 이후 모델을 평가모드로 전환합니다. 드롭아웃 등 학습 때만 사용하는 기능을 무력화 시키기 위해서 입니다. . (체크포인트란 모델이 사용하는 모든 매개변수에 대한 정확한 값을 기록하는 것) . from numpy.core.numeric import outer def inference_fn(sentence): inputs = tokenizer( [sentence], max_length = args.max_seq_length, padding = &#39;max_length&#39;, truncation = True, ) with torch.no_grad(): outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()}) # items : 키와 값 둘다 얻는 함수. prob = outputs.logits.softmax(dim = 1) # 로짓에 소프트맥스 취하기 positive_prob = round(prob[0][1].item(), 4) negative_prob = round(prob[0][0].item(), 4) pred = &#39;긍정&#39; if torch.argmax(prob) == 1 else &#39;부정&#39; return { &#39;sentence&#39; : sentence, &#39;prediction&#39; : pred, &#39;positive_data&#39;: f&quot;긍정 {positive_prob}&quot;, &#39;negative_data&#39;: f&quot;부정 {negative_prob}&quot;, &#39;positive_width&#39;: f&quot;{positive_prob * 100}%&quot;, &#39;negative_width&#39;: f&quot;{negative_prob * 100}%&quot;, } . 문장을 입력받으면 모델을 사용해 긍정/부정을 판단해주는 함수를 만듭니다. . 모델은 로짓값을 출력하므로 소프트맥스를 이용해 확률 값으로 나타냅니다. . with torch.no_grad()은 오차 역전파에 사용하는 계산량을 줄여서 처리 속도를 높입니다. . from ratsnlp.nlpbook.classification import get_web_service_app app = get_web_service_app(inference_fn) app.run() . 플라스크라는 파이썬 라이브러리의 도움을 받아 웹 서비스를 할 수 있습니다. . (딥러닝 모델에서 약 6시간.. 너무 오래 걸려서 오류 그대로 업로드 합니다.) .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/sentiment/web/classifier/2021/12/22/Do_natural_language3.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/sentiment/web/classifier/2021/12/22/Do_natural_language3.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "[Do it 자연어] 2. 숫자 세계로 떠난 자연어",
            "content": ". &#49472;&#54532; &#50612;&#53584;&#49440; &#46041;&#51089;&#50896;&#47532; . import torch x = torch.tensor([ [1.0, 0.0, 1.0, 0.0], [0.0, 2.0, 0.0, 2.0], [1.0, 1.0, 1.0, 1.0], ]) w_query = torch.tensor([ [1.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 1.0], ]) w_key = torch.tensor([ [0.0, 0.0, 1.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 1.0, 0.0], ]) w_value = torch.tensor([ [0.0, 2.0, 0.0], [0.0, 3.0, 0.0], [1.0, 0.0, 3.0], [1.0, 1.0, 0.0], ]) . 변수를 정의합니다. . keys = torch.matmul(x, w_key) querys = torch.matmul(x, w_query) values = torch.matmul(x, w_value) . 쿼리, 키, 벨류를 만듭니다. . attn_scores = torch.matmul(querys, keys.T) attn_scores . tensor([[ 2., 4., 4.], [ 4., 16., 12.], [ 4., 12., 10.]]) . 어텐션 스코어를 만듭니다. . import numpy as np from torch.nn.functional import softmax key_dim_sqrt = np.sqrt(keys.shape[-1]) attn_probs = softmax(attn_scores / key_dim_sqrt, dim = -1) attn_probs . tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01], [8.9045e-04, 9.0884e-01, 9.0267e-02], [7.4449e-03, 7.5471e-01, 2.3785e-01]]) . 소프트맥스 확률값을 만듭니다. . weighted_values = torch.matmul(attn_probs, values) weighted_values . tensor([[1.8639, 6.3194, 1.7042], [1.9991, 7.8141, 0.2735], [1.9926, 7.4796, 0.7359]]) . 소프트맥스 확률과 밸류를 가중합 하였습니다. 셀프 어텐션에 최종 출력값 입니다. . 셀프 어텐션은 가중치 행렬(w_..) 3개를 학습 대상으로 생각하고 태스크를 잘 수행하는 방향으로 업데이트 됩니다. . &#54588;&#46300;&#54252;&#50892;&#46300; &#45684;&#47092; &#45348;&#53944;&#50892;&#53356; &#44228;&#49328; &#50696;&#49884; . import torch x = torch.tensor([2,1]) w1 = torch.tensor([[3,2,-4],[2,-3,1]]) b1 = 1 w2 = torch.tensor([[-1,1],[1,2],[3,1]]) b2 = -1 . 변수를 입력합니다. . h_preact = torch.matmul(x, w1) + b1 h = torch.nn.functional.relu(h_preact) y = torch.matmul(h, w2) + b2 y . tensor([-8, 12]) . 결과 값 입니다. 여기서 w1, w2, b1, b2가 학습 대상이 됩니다. . 학습 대상은 태스크를 잘 수행하는 방향으로 업데이트 됩니다. . m = torch.nn.Dropout(p = 0.2) input = torch.randn(1,10) output = m(input) print(input) print(output) . tensor([[ 0.3678, 1.6664, 1.6091, 0.1445, 0.9486, 2.0537, 2.4539, 1.4420, 0.9701, -0.2877]]) tensor([[0.0000, 2.0830, 2.0113, 0.1806, 1.1858, 2.5672, 0.0000, 1.8025, 0.0000, -0.0000]]) . 간단한 드롭다웃 예제입니다. p 확률 만큼 뉴련을 0으로 대치해 계산에서 제외합니다. . &#47928;&#51109;&#51012; &#48289;&#53552;&#47196; &#48320;&#54872;&#54616;&#44592; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.6 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 15.9 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 6.4 MB/s Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 37.5 MB/s Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 56.2 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 59.2 MB/s Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 52.0 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 64.2 MB/s Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 44.7 MB/s Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 671 kB/s Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 48.7 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 46.2 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 6.6 MB/s Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 69.4 MB/s Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 57.9 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 71.0 MB/s Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=3cd467807e542363544f5ce4ef26f212f5b2dd8812ec646fa206e9b85b0c7b48 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( &#39;beomi/kcbert-base&#39;, do_lower_case = False, ) . BERT(kcbert-base) 모델이 쓰는 토크나이저를 선언합니다. . from transformers import BertConfig, BertModel pretrained_model_config = BertConfig.from_pretrained( &#39;beomi/kcbert-base&#39; ) model = BertModel.from_pretrained( &#39;beomi/kcbert-base&#39;, config = pretrained_model_config, ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;] - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). . BERT(kcbert-base) 모델을 읽어들입니다. . pretrained_model_config . BertConfig { &#34;_name_or_path&#34;: &#34;beomi/kcbert-base&#34;, &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;classifier_dropout&#34;: null, &#34;directionality&#34;: &#34;bidi&#34;, &#34;gradient_checkpointing&#34;: false, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 300, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;pooler_fc_size&#34;: 768, &#34;pooler_num_attention_heads&#34;: 12, &#34;pooler_num_fc_layers&#34;: 3, &#34;pooler_size_per_head&#34;: 128, &#34;pooler_type&#34;: &#34;first_token_transform&#34;, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.10.0&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 30000 } . pretrained_model_config은 BERT 모델을 프리트레인 할때 설정했던 내용이 있습니다. . 블록 수는 12개, 헤드 수는 12개, 어휘 집합 크기는 3만개 입니다. . sentences = [&#39;안녕하세요&#39;, &#39;하이!&#39;] features = tokenizer( sentences, max_length = 10, padding = &#39;max_length&#39;, truncation = True, ) features . {&#39;input_ids&#39;: [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 5, 3, 0, 0, 0, 0, 0, 0]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]} . BERT 모델의 입력값을 만듭니다. 앞서 배운 BERT 모델과 같이 3개의 변수가 나옵니다. . features = {k : torch.tensor(v) for k, v in features.items()} . 피처를 파이토치에 넣기 위해선 자료형이 텐서(tensor)이여야 하기 때문에 자료형을 변경했습니다. . outputs = model(**features) outputs . BaseModelOutputWithPoolingAndCrossAttentions([(&#39;last_hidden_state&#39;, tensor([[[-0.6969, -0.8248, 1.7512, ..., -0.3732, 0.7399, 1.1907], [-1.4803, -0.4398, 0.9444, ..., -0.7405, -0.0211, 1.3064], [-1.4299, -0.5033, -0.2069, ..., 0.1285, -0.2611, 1.6057], ..., [-1.4406, 0.3431, 1.4043, ..., -0.0565, 0.8450, -0.2170], [-1.3625, -0.2404, 1.1757, ..., 0.8876, -0.1054, 0.0734], [-1.4244, 0.1518, 1.2920, ..., 0.0245, 0.7572, 0.0080]], [[ 0.9371, -1.4749, 1.7351, ..., -0.3426, 0.8050, 0.4031], [ 1.6095, -1.7269, 2.7936, ..., 0.3100, -0.4787, -1.2491], [ 0.4861, -0.4569, 0.5712, ..., -0.1769, 1.1253, -0.2756], ..., [ 1.2362, -0.6181, 2.0906, ..., 1.3677, 0.8132, -0.2742], [ 0.5409, -0.9652, 1.6237, ..., 1.2395, 0.9185, 0.1782], [ 1.9001, -0.5859, 3.0156, ..., 1.4967, 0.1924, -0.4448]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)), (&#39;pooler_output&#39;, tensor([[-0.1594, 0.0547, 0.1101, ..., 0.2684, 0.1596, -0.9828], [-0.9221, 0.2969, -0.0110, ..., 0.4291, 0.0311, -0.9955]], grad_fn=&lt;TanhBackward0&gt;))]) . BERT 모델에 features를 적용했습니다. 두 개의 출력물 last_hidden_state, pooler_output이 나옵니다. . 전자를 단어수준 임베딩, 후자를 문장수준 임베딩이라고 부릅니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/self-attention/tokenizer/2021/12/21/Do_natural_language2.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/self-attention/tokenizer/2021/12/21/Do_natural_language2.html",
            "date": " • Dec 21, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "[Do it 자연어] 1. 문장을 작은 단위로 쪼개기",
            "content": ". &#54056;&#53412;&#51648; &#45796;&#50868;/&#44396;&#44544; &#50672;&#46041; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.4 MB/s Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 8.2 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 48.6 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 4.9 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 31.4 MB/s Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 55.5 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 43.3 MB/s Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 47.3 MB/s Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 13.9 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 451 kB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 58.6 MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 13.2 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 5.1 MB/s Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 57.5 MB/s Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 51.6 MB/s Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 55.2 MB/s Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=47a5461d5402fcdc2dcef6ae30181e838b5332b2f517b7570f63edff2f2ce53f Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from Korpora import Korpora nsmc = Korpora.load(&#39;nsmc&#39;, force_download=True) . Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을 손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다. 말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다. 해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고, 해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다. # Description Author : e9t@github Repository : https://github.com/e9t/nsmc References : www.lucypark.kr/docs/2015-pyconkr/#39 Naver sentiment movie corpus v1.0 This is a movie review dataset in the Korean language. Reviews were scraped from Naver Movies. The dataset construction is based on the method noted in [Large movie review dataset][^1] from Maas et al., 2011. [^1]: http://ai.stanford.edu/~amaas/data/sentiment/ # License CC0 1.0 Universal (CC0 1.0) Public Domain Dedication Details in https://creativecommons.org/publicdomain/zero/1.0/ . [nsmc] download ratings_train.txt: 14.6MB [00:00, 61.5MB/s] [nsmc] download ratings_test.txt: 4.90MB [00:00, 33.0MB/s] . NSMC는 네이버 영화 리뷰자료 입니다. . import os def write_lines(path, lines): with open(path, &#39;w&#39;, encoding = &#39;utf-8&#39;) as f: for line in lines: f.write(f&#39;{line} n&#39;) write_lines(&#39;/root/train.txt&#39;, nsmc.train.get_all_texts()) write_lines(&#39;/root/test.txt&#39;, nsmc.test.get_all_texts()) . 영화 리뷰들을 순수 텍스트 형태로 저장했습니다. . !head /root/train.txt . 아 더빙.. 진짜 짜증나네요 목소리 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 너무재밓었다그래서보는것을추천한다 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움. 원작의 긴장감을 제대로 살려내지못했다. 별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네 액션이 없는데도 재미 있는 몇안되는 영화 왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나? . 리뷰 앞 내용입니다. . GPT &#53664;&#53356;&#45208;&#51060;&#51200; &#44396;&#52629; . import os os.makedirs(&#39;/gdrive/My Drive/nlpbook/bbpe&#39;, exist_ok= True) . 저장용 디렉터리를 만들었습니다. . from tokenizers import ByteLevelBPETokenizer bytebpe_tokenizer = ByteLevelBPETokenizer() bytebpe_tokenizer.train( files=[&quot;/root/train.txt&quot;, &quot;/root/test.txt&quot;], # 학습 말뭉치를 리스트 형태로 vocab_size=10000, # 어휘 집합 크기 조절 special_tokens=[&quot;[PAD]&quot;] # 특수 토큰 추가 ) bytebpe_tokenizer.save_model(&quot;/gdrive/My Drive/nlpbook/bbpe&quot;) . [&#39;/gdrive/My Drive/nlpbook/bbpe/vocab.json&#39;, &#39;/gdrive/My Drive/nlpbook/bbpe/merges.txt&#39;] . 코드가 실행되면 vocab.json, merges.txt가 생성됩니다. . 전자는 바이트 수준 BPE 어휘집합, 후자는 바이그램 쌍과 병합우선순위가 있습니다. . !cat /gdrive/My Drive/nlpbook/bbpe/vocab.json . {&#34;[PAD]&#34;:0,&#34;!&#34;:1,&#34; &#34;&#34;:2,&#34;#&#34;:3,&#34;$&#34;:4,&#34;%&#34;:5,&#34;&amp;&#34;:6,&#34;&#39;&#34;:7,&#34;(&#34;:8,&#34;)&#34;:9,&#34;*&#34;:10,&#34;+&#34;:11,&#34;,&#34;:12,&#34;-&#34;:13,&#34;.&#34;:14,&#34;/&#34;:15,&#34;0&#34;:16,&#34;1&#34;:17,&#34;2&#34;:18,&#34;3&#34;:19,&#34;4&#34;:20,&#34;5&#34;:21,&#34;6&#34;:22,&#34;7&#34;:23,&#34;8&#34;:24,&#34;9&#34;:25,&#34;:&#34;:26,&#34;;&#34;:27,&#34;&lt;&#34;:28,&#34;=&#34;:29,&#34;&gt;&#34;:30,&#34;?&#34;:31,&#34;@&#34;:32,&#34;A&#34;:33,&#34;B&#34;:34,&#34;C&#34;:35,&#34;D&#34;:36,&#34;E&#34;:37,&#34;F&#34;:38,&#34;G&#34;:39,&#34;H&#34;:40,&#34;I&#34;:41,&#34;J&#34;:42,&#34;K&#34;:43,&#34;L&#34;:44,&#34;M&#34;:45,&#34;N&#34;:46,&#34;O&#34;:47,&#34;P&#34;:48,&#34;Q&#34;:49,&#34;R&#34;:50,&#34;S&#34;:51,&#34;T&#34;:52,&#34;U&#34;:53,&#34;V&#34;:54,&#34;W&#34;:55,&#34;X&#34;:56,&#34;Y&#34;:57,&#34;Z&#34;:58,&#34;[&#34;:59,&#34; &#34;:60,&#34;]&#34;:61,&#34;^&#34;:62,&#34;_&#34;:63,&#34;`&#34;:64,&#34;a&#34;:65,&#34;b&#34;:66,&#34;c&#34;:67,&#34;d&#34;:68,&#34;e&#34;:69,&#34;f&#34;:70,&#34;g&#34;:71,&#34;h&#34;:72,&#34;i&#34;:73,&#34;j&#34;:74,&#34;k&#34;:75,&#34;l&#34;:76,&#34;m&#34;:77,&#34;n&#34;:78,&#34;o&#34;:79,&#34;p&#34;:80,&#34;q&#34;:81,&#34;r&#34;:82,&#34;s&#34;:83,&#34;t&#34;:84,&#34;u&#34;:85,&#34;v&#34;:86,&#34;w&#34;:87,&#34;x&#34;:88,&#34;y&#34;:89,&#34;z&#34;:90,&#34;{&#34;:91,&#34;|&#34;:92,&#34;}&#34;:93,&#34;~&#34;:94,&#34;¡&#34;:95,&#34;¢&#34;:96,&#34;£&#34;:97,&#34;¤&#34;:98,&#34;¥&#34;:99,&#34;¦&#34;:100,&#34;§&#34;:101,&#34;¨&#34;:102,&#34;©&#34;:103,&#34;ª&#34;:104,&#34;«&#34;:105,&#34;¬&#34;:106,&#34;®&#34;:107,&#34;¯&#34;:108,&#34;°&#34;:109,&#34;±&#34;:110,&#34;²&#34;:111,&#34;³&#34;:112,&#34;´&#34;:113,&#34;µ&#34;:114,&#34;¶&#34;:115,&#34;·&#34;:116,&#34;¸&#34;:117,&#34;¹&#34;:118,&#34;º&#34;:119,&#34;»&#34;:120,&#34;¼&#34;:121,&#34;½&#34;:122,&#34;¾&#34;:123,&#34;¿&#34;:124,&#34;À&#34;:125,&#34;Á&#34;:126,&#34;Â&#34;:127,&#34;Ã&#34;:128,&#34;Ä&#34;:129,&#34;Å&#34;:130,&#34;Æ&#34;:131,&#34;Ç&#34;:132,&#34;È&#34;:133,&#34;É&#34;:134,&#34;Ê&#34;:135,&#34;Ë&#34;:136,&#34;Ì&#34;:137,&#34;Í&#34;:138,&#34;Î&#34;:139,&#34;Ï&#34;:140,&#34;Ð&#34;:141,&#34;Ñ&#34;:142,&#34;Ò&#34;:143,&#34;Ó&#34;:144,&#34;Ô&#34;:145,&#34;Õ&#34;:146,&#34;Ö&#34;:147,&#34;×&#34;:148,&#34;Ø&#34;:149,&#34;Ù&#34;:150,&#34;Ú&#34;:151,&#34;Û&#34;:152,&#34;Ü&#34;:153,&#34;Ý&#34;:154,&#34;Þ&#34;:155,&#34;ß&#34;:156,&#34;à&#34;:157,&#34;á&#34;:158,&#34;â&#34;:159,&#34;ã&#34;:160,&#34;ä&#34;:161,&#34;å&#34;:162,&#34;æ&#34;:163,&#34;ç&#34;:164,&#34;è&#34;:165,&#34;é&#34;:166,&#34;ê&#34;:167,&#34;ë&#34;:168,&#34;ì&#34;:169,&#34;í&#34;:170,&#34;î&#34;:171,&#34;ï&#34;:172,&#34;ð&#34;:173,&#34;ñ&#34;:174,&#34;ò&#34;:175,&#34;ó&#34;:176,&#34;ô&#34;:177,&#34;õ&#34;:178,&#34;ö&#34;:179,&#34;÷&#34;:180,&#34;ø&#34;:181,&#34;ù&#34;:182,&#34;ú&#34;:183,&#34;û&#34;:184,&#34;ü&#34;:185,&#34;ý&#34;:186,&#34;þ&#34;:187,&#34;ÿ&#34;:188,&#34;Ā&#34;:189,&#34;ā&#34;:190,&#34;Ă&#34;:191,&#34;ă&#34;:192,&#34;Ą&#34;:193,&#34;ą&#34;:194,&#34;Ć&#34;:195,&#34;ć&#34;:196,&#34;Ĉ&#34;:197,&#34;ĉ&#34;:198,&#34;Ċ&#34;:199,&#34;ċ&#34;:200,&#34;Č&#34;:201,&#34;č&#34;:202,&#34;Ď&#34;:203,&#34;ď&#34;:204,&#34;Đ&#34;:205,&#34;đ&#34;:206,&#34;Ē&#34;:207,&#34;ē&#34;:208,&#34;Ĕ&#34;:209,&#34;ĕ&#34;:210,&#34;Ė&#34;:211,&#34;ė&#34;:212,&#34;Ę&#34;:213,&#34;ę&#34;:214,&#34;Ě&#34;:215,&#34;ě&#34;:216,&#34;Ĝ&#34;:217,&#34;ĝ&#34;:218,&#34;Ğ&#34;:219,&#34;ğ&#34;:220,&#34;Ġ&#34;:221,&#34;ġ&#34;:222,&#34;Ģ&#34;:223,&#34;ģ&#34;:224,&#34;Ĥ&#34;:225,&#34;ĥ&#34;:226,&#34;Ħ&#34;:227,&#34;ħ&#34;:228,&#34;Ĩ&#34;:229,&#34;ĩ&#34;:230,&#34;Ī&#34;:231,&#34;ī&#34;:232,&#34;Ĭ&#34;:233,&#34;ĭ&#34;:234,&#34;Į&#34;:235,&#34;į&#34;:236,&#34;İ&#34;:237,&#34;ı&#34;:238,&#34;Ĳ&#34;:239,&#34;ĳ&#34;:240,&#34;Ĵ&#34;:241,&#34;ĵ&#34;:242,&#34;Ķ&#34;:243,&#34;ķ&#34;:244,&#34;ĸ&#34;:245,&#34;Ĺ&#34;:246,&#34;ĺ&#34;:247,&#34;Ļ&#34;:248,&#34;ļ&#34;:249,&#34;Ľ&#34;:250,&#34;ľ&#34;:251,&#34;Ŀ&#34;:252,&#34;ŀ&#34;:253,&#34;Ł&#34;:254,&#34;ł&#34;:255,&#34;Ń&#34;:256,&#34;Ġì&#34;:257,&#34;Ġë&#34;:258,&#34;ìĿ&#34;:259,&#34;ëĭ&#34;:260,&#34;íķ&#34;:261,&#34;ê°&#34;:262,&#34;..&#34;:263,&#34;ìĿ´&#34;:264,&#34;ëĭ¤&#34;:265,&#34;ëĬ&#34;:266,&#34;ìĹ&#34;:267,&#34;ê³&#34;:268,&#34;ì§&#34;:269,&#34;ëĬĶ&#34;:270,&#34;ìŀ&#34;:271,&#34;ë§&#34;:272,&#34;íĻ&#34;:273,&#34;ê³ł&#34;:274,&#34;ìł&#34;:275,&#34;íĻĶ&#34;:276,&#34;ĺģ&#34;:277,&#34;Ġê&#34;:278,&#34;ëı&#34;:279,&#34;ìķ&#34;:280,&#34;ãħ&#34;:281,&#34;ĺģíĻĶ&#34;:282,&#34;ìļ&#34;:283,&#34;ì§Ģ&#34;:284,&#34;íķĺ&#34;:285,&#34;ê°Ģ&#34;:286,&#34;ëĤ&#34;:287,&#34;ê²&#34;:288,&#34;ìĦ&#34;:289,&#34;Ġìŀ&#34;:290,&#34;¬ë&#34;:291,&#34;ê¸&#34;:292,&#34;Ġìķ&#34;:293,&#34;ëıĦ&#34;:294,&#34;Ġí&#34;:295,&#34;ëĵ&#34;:296,&#34;ë¦&#34;:297,&#34;ìĹĲ&#34;:298,&#34;ĠìĿ&#34;:299,&#34;íķľ&#34;:300,&#34;ĠìĺģíĻĶ&#34;:301,&#34;Īë&#34;:302,&#34;³´&#34;:303,&#34;ìĭ&#34;:304,&#34;ĸ´&#34;:305,&#34;ìĿĺ&#34;:306,&#34;ê¸°&#34;:307,&#34;ãħĭ&#34;:308,&#34;ĠìĹ&#34;:309,&#34;ìĿĢ&#34;:310,&#34;ë¡&#34;:311,&#34;ëį&#34;:312,&#34;ìĿĦ&#34;:313,&#34;Ŀ¼&#34;:314,&#34;ëĤĺ&#34;:315,&#34;ê²Į&#34;:316,&#34;ĠìĿ´&#34;:317,&#34;ìĦľ&#34;:318,&#34;Ġë§&#34;:319,&#34;ìļĶ&#34;:320,&#34;ìĬ&#34;:321,&#34;ìĸ´&#34;:322,&#34;ë¡ľ&#34;:323,&#34;ĠëĤ&#34;:324,&#34;ë§Į&#34;:325,&#34;ëĿ¼&#34;:326,&#34;ë¦¬&#34;:327,&#34;Ġìł&#34;:328,&#34;·¸&#34;:329,&#34;ëĭĪ&#34;:330,&#34;ëĵ¤&#34;:331,&#34;ë¥&#34;:332,&#34;ê±&#34;:333,&#34;ìķĦ&#34;:334,&#34;ëł&#34;:335,&#34;...&#34;:336,&#34;ë©&#34;:337,&#34;¬´&#34;:338,&#34;ìľ&#34;:339,&#34;Ġê°&#34;:340,&#34;ìĿ¸&#34;:341,&#34;ãħĭãħĭ&#34;:342,&#34;¯¸&#34;:343,&#34;ëį°&#34;:344,&#34;Ġì§&#34;:345,&#34;ëĦ&#34;:346,&#34;ĠìķĦ&#34;:347,&#34;ĮĢ&#34;:348,&#34;ëŁ&#34;:349,&#34;ìĺ&#34;:350,&#34;êµ&#34;:351,&#34;íķ´&#34;:352,&#34;Ġë³´&#34;:353,&#34;ë©´&#34;:354,&#34;ìĥ&#34;:355,&#34;ìĺģíĻĶ&#34;:356,&#34;Ġìĭ&#34;:357,&#34;Ġê·¸&#34;:358,&#34;ê¹&#34;:359,&#34;ë°&#34;:360,&#34;Ġëª&#34;:361,&#34;ìłĲ&#34;:362,&#34;ìĭľ&#34;:363,&#34;Īĺ&#34;:364,&#34;Ġëĭ&#34;:365,&#34;ë³´&#34;:366,&#34;ìĿĮ&#34;:367,&#34;ìĬ¤&#34;:368,&#34;£¼&#34;:369,&#34;ëŀ&#34;:370,&#34;Ġë°&#34;:371,&#34;ìľ¼&#34;:372,&#34;ëĦ¤&#34;:373,&#34;Ġìŀ¬ë&#34;:374,&#34;ë¥¼&#34;:375,&#34;ë§Ĳ&#34;:376,&#34;Ġì¢&#34;:377,&#34;!!&#34;:378,&#34;ë¶&#34;:379,&#34;ì¤&#34;:380,&#34;ĠìĤ&#34;:381,&#34;ê±°&#34;:382,&#34;ìĤ&#34;:383,&#34;ìĻ&#34;:384,&#34;ëĮĢ&#34;:385,&#34;ëĭĪëĭ¤&#34;:386,&#34;Īë¬´&#34;:387,&#34;ìŀĲ&#34;:388,&#34;ëĬĶëį°&#34;:389,&#34;ìĽ&#34;:390,&#34;Ġë³&#34;:391,&#34;ìłķ&#34;:392,&#34;ĠëĦ&#34;:393,&#34;ë§Ī&#34;:394,&#34;ê¹Į&#34;:395,&#34;ì²&#34;:396,&#34;ìĹĨ&#34;:397,&#34;ĠìĹĨ&#34;:398,&#34;ìĹĪ&#34;:399,&#34;ĠëĤĺ&#34;:400,&#34;Ġíķĺ&#34;:401,&#34;ìļ°&#34;:402,&#34;Ġë´&#34;:403,&#34;ì¹&#34;:404,&#34;ìķ¼&#34;:405,&#34;Ġì¢ĭ&#34;:406,&#34;ì£¼&#34;:407,&#34;ì§Ħ&#34;:408,&#34;Ġëĭ¤&#34;:409,&#34;ìĪĺ&#34;:410,&#34;íĸ&#34;:411,&#34;ë³&#34;:412,&#34;ë²&#34;:413,&#34;ìłģ&#34;:414,&#34;µľ&#34;:415,&#34;ìŀ¥&#34;:416,&#34;ìŀĪ&#34;:417,&#34;ìŀĳ&#34;:418,&#34;ìłĦ&#34;:419,&#34;ìĥģ&#34;:420,&#34;ëª&#34;:421,&#34;....&#34;:422,&#34;Ġìĥ&#34;:423,&#34;Ġìłķ&#34;:424,&#34;ì§ľ&#34;:425,&#34;ìĨ&#34;:426,&#34;°Į&#34;:427,&#34;ìľ¼ë¡ľ&#34;:428,&#34;Ġê²&#34;:429,&#34;ĠìŀĪ&#34;:430,&#34;ì§Ģë§Į&#34;:431,&#34;íĺ&#34;:432,&#34;ê°Ħ&#34;:433,&#34;ĠìĹ°&#34;:434,&#34;íķĺê³ł&#34;:435,&#34;ĠìĻ&#34;:436,&#34;¬ëŀ&#34;:437,&#34;ê³¼&#34;:438,&#34;Ĳëı&#34;:439,&#34;ìĺ¤&#34;:440,&#34;ĠìĬ&#34;:441,&#34;Ġëĵ&#34;:442,&#34;ëĤ´&#34;:443,&#34;Ġê¸&#34;:444,&#34;ıī&#34;:445,&#34;ãħł&#34;:446,&#34;ĠëĦĪë¬´&#34;:447,&#34;ëŁ°&#34;:448,&#34;ëħ&#34;:449,&#34;Ġìĸ´&#34;:450,&#34;Ġìĺ&#34;:451,&#34;Ġë§Į&#34;:452,&#34;íĥ&#34;:453,&#34;Ġìŀ¬ë¯¸&#34;:454,&#34;Ġì§Ģ&#34;:455,&#34;¹Ħ&#34;:456,&#34;ëĶ&#34;:457,&#34;ê·¸&#34;:458,&#34;ì°&#34;:459,&#34;íŀ&#34;:460,&#34;ëĥ&#34;:461,&#34;ìĹĲìĦľ&#34;:462,&#34;ĠëĤ´&#34;:463,&#34;ëĦ¤ìļĶ&#34;:464,&#34;ê±´&#34;:465,&#34;Ĳĺ&#34;:466,&#34;Ġíķľ&#34;:467,&#34;ëĵľ&#34;:468,&#34;Ġìĭľ&#34;:469,&#34;íĨ&#34;:470,&#34;Ġë¶&#34;:471,&#34;ìķĺ&#34;:472,&#34;íķł&#34;:473,&#34;ĠìĦ&#34;:474,&#34;ķĮ&#34;:475,&#34;ì¡&#34;:476,&#34;ìŀ¬ë&#34;:477,&#34;ìĹ°&#34;:478,&#34;Ġì§Ħ&#34;:479,&#34;Ġë´¤&#34;:480,&#34;ë£&#34;:481,&#34;Ġê°Ģ&#34;:482,&#34;ìļ´&#34;:483,&#34;ĠìĬ¤&#34;:484,&#34;ê³µ&#34;:485,&#34;Ġìµľ&#34;:486,&#34;ë´&#34;:487,&#34;ìĦ±&#34;:488,&#34;ìĻĢ&#34;:489,&#34;Ġëı&#34;:490,&#34;ë¯¸&#34;:491,&#34;Ġìļ&#34;:492,&#34;ìĹ¬&#34;:493,&#34;ê°ģ&#34;:494,&#34;ìĬµ&#34;:495,&#34;Ġì°&#34;:496,&#34;ê²ĥ&#34;:497,&#34; &#34; &#34;&#34;:498,&#34;íŀĪ&#34;:499,&#34;Ġëį&#34;:500,&#34;ìłľ&#34;:501,&#34;Ġ1&#34;:502,&#34;íĶ&#34;:503,&#34;ì¹ĺ&#34;:504,&#34;ì¶&#34;:505,&#34;ìĸ&#34;:506,&#34;ìļ©&#34;:507,&#34;Ġê¸°&#34;:508,&#34;íķĺëĬĶ&#34;:509,&#34;ĠëĮĢ&#34;:510,&#34;ĠìĨ&#34;:511,&#34;ë¶Ģ&#34;:512,&#34;ëł¤&#34;:513,&#34;ìĿ¼&#34;:514,&#34;ĠìĹ°ê¸°&#34;:515,&#34;íĨł&#34;:516,&#34;ëŀĺ&#34;:517,&#34;Ġê°Ĳëı&#34;:518,&#34;íĸĪ&#34;:519,&#34;íĮ&#34;:520,&#34;Ġìŀ¬ë°Į&#34;:521,&#34;ıīìłĲ&#34;:522,&#34;ë¬&#34;:523,&#34;ĠìĪĺ&#34;:524,&#34;Ħ°&#34;:525,&#34;êµ¬&#34;:526,&#34;Ġëª¨&#34;:527,&#34;ì¦&#34;:528,&#34;íķ¨&#34;:529,&#34;ë£¨&#34;:530,&#34;ìĤ¬&#34;:531,&#34;ìĸ´ìļĶ&#34;:532,&#34;Ġìłķë§Ĳ&#34;:533,&#34;ĠìłĦ&#34;:534,&#34;ĠìĤ¬ëŀ&#34;:535,&#34;Ŀê°ģ&#34;:536,&#34;êµŃ&#34;:537,&#34;Ġìľ&#34;:538,&#34;ãħİ&#34;:539,&#34;Ġì¤&#34;:540,&#34;ìĭł&#34;:541,&#34;íĦ°&#34;:542,&#34;Ġìŀĺ&#34;:543,&#34;ì¢&#34;:544,&#34;ì¤ĳ&#34;:545,&#34;ĠìķĬ&#34;:546,&#34;Ġë¬´&#34;:547,&#34;ë¶Ħ&#34;:548,&#34;íĬ&#34;:549,&#34;ëŁ¬&#34;:550,&#34;ìħ&#34;:551,&#34;ãħĭãħĭãħĭãħĭ&#34;:552,&#34;ê²ł&#34;:553,&#34;ìĬµëĭĪëĭ¤&#34;:554,&#34;ìĿ´ëĭ¤&#34;:555,&#34;íİ&#34;:556,&#34;ĠëįĶ&#34;:557,&#34;ìĦ¸&#34;:558,&#34;ĠìķĪ&#34;:559,&#34;íķĺëĭ¤&#34;:560,&#34;ĠëĬ&#34;:561,&#34;Ġì¡&#34;:562,&#34;ëłĪ&#34;:563,&#34;Ġê±&#34;:564,&#34;Ġì£¼&#34;:565,&#34;ê°Ļ&#34;:566,&#34;°ìļ°&#34;:567,&#34;ë¥´&#34;:568,&#34;ĵ°&#34;:569,&#34;ëķĮ&#34;:570,&#34;ĠìĽ&#34;:571,&#34;ìĨĮ&#34;:572,&#34;ê°ľ&#34;:573,&#34;~~&#34;:574,&#34;ĠëŃ&#34;:575,&#34;ìŀĦ&#34;:576,&#34;íĨłë¦¬&#34;:577,&#34;Ġëĵľ&#34;:578,&#34;ĠìĥĿê°ģ&#34;:579,&#34;ìĥĿ&#34;:580,&#34;Ġì§Ħì§ľ&#34;:581,&#34;ãħ¡&#34;:582,&#34;ê°Ĳ&#34;:583,&#34;Ġë§Ī&#34;:584,&#34;ëł¥&#34;:585,&#34;ëĵł&#34;:586,&#34;ëįĺ&#34;:587,&#34;ĠìĿ¸&#34;:588,&#34;ìŀħ&#34;:589,&#34;ìĭ¤&#34;:590,&#34;Ġê°Ļ&#34;:591,&#34;Ġìµľê³ł&#34;:592,&#34;íģ&#34;:593,&#34;Ġì²&#34;:594,&#34;Ġë§Ĳ&#34;:595,&#34;Ġêµ&#34;:596,&#34;Ġëª»&#34;:597,&#34;ê·&#34;:598,&#34;ëĤľ&#34;:599,&#34;ëĵ¯&#34;:600,&#34;ëĿ¼ë§Ī&#34;:601,&#34;ëĵ¤ìĿ´&#34;:602,&#34;ë¬´&#34;:603,&#34;Ġê°ľ&#34;:604,&#34;ĠìĹ¬&#34;:605,&#34;ëħĦ&#34;:606,&#34;ìķħ&#34;:607,&#34;íĴ&#34;:608,&#34;ãħłãħł&#34;:609,&#34;ĠìŀĲ&#34;:610,&#34;ëĶĶ&#34;:611,&#34;Ġìłľ&#34;:612,&#34;ĠëĬĲ&#34;:613,&#34;Ġëģ&#34;:614,&#34;ê¹Įì§Ģ&#34;:615,&#34;ê¸Ī&#34;:616,&#34;ë¦Ħ&#34;:617,&#34;ĠìĻľ&#34;:618,&#34;ëĥ¥&#34;:619,&#34;íİ¸&#34;:620,&#34;Ġê´&#34;:621,&#34;íĥĢ&#34;:622,&#34;íķĺê²Į&#34;:623,&#34;ë¹Ħ&#34;:624,&#34;ë³¸&#34;:625,&#34;Ġë³¸&#34;:626,&#34;Ġì¶&#34;:627,&#34;ëłĩ&#34;:628,&#34;ĳĲ&#34;:629,&#34;ìĺĢ&#34;:630,&#34;Ġìĸ&#34;:631,&#34;ĠìķĮ&#34;:632,&#34;ĠëĤ¨&#34;:633,&#34;íı&#34;:634,&#34;Ġíĺ&#34;:635,&#34;ìĿ´ëĿ¼&#34;:636,&#34;íĬ¸&#34;:637,&#34;Ġê²ĥ&#34;:638,&#34;ì¤Ģ&#34;:639,&#34;Ġìĺ¤&#34;:640,&#34;ĠíıīìłĲ&#34;:641,&#34;ë³´ëĭ¤&#34;:642,&#34;ìĹĪëĭ¤&#34;:643,&#34;Ġê·&#34;:644,&#34;½Ķ&#34;:645,&#34;Ġë³´ê³ł&#34;:646,&#34;ìħĺ&#34;:647,&#34;Ġë³¼&#34;:648,&#34;ĠìĿ´ëŁ°&#34;:649,&#34;ìľł&#34;:650,&#34;ê±¸&#34;:651,&#34;;;&#34;:652,&#34;ìłĢ&#34;:653,&#34;ìłķë§Ĳ&#34;:654,&#34;ĠìĨĮ&#34;:655,&#34;ë§ī&#34;:656,&#34;Ġë©&#34;:657,&#34;ëįĶ&#34;:658,&#34;ê´&#34;:659,&#34;??&#34;:660,&#34;ì²´&#34;:661,&#34;ë¹&#34;:662,&#34;ì§Ħì§ľ&#34;:663,&#34;ìĶ&#34;:664,&#34;Ġë¹&#34;:665,&#34;Ġë¹Ħ&#34;:666,&#34;ëģ&#34;:667,&#34;ĽĦ&#34;:668,&#34;ìĭ¬&#34;:669,&#34;Ġê°ĲëıĻ&#34;:670,&#34;Ġê¹&#34;:671,&#34;Ġë§İ&#34;:672,&#34;ĠíĿ&#34;:673,&#34;ĠìķĦëĭ&#34;:674,&#34;ĠìĬ¤íĨłë¦¬&#34;:675,&#34;ìŀ¬ë¯¸&#34;:676,&#34;ëłĪê¸°&#34;:677,&#34;íĴĪ&#34;:678,&#34;íķ´ìĦľ&#34;:679,&#34;ìķĪ&#34;:680,&#34;ìĽĲ&#34;:681,&#34;Ġë¯¸&#34;:682,&#34;ĠëĶ&#34;:683,&#34;ĠëĪ&#34;:684,&#34;Ġìŀĳ&#34;:685,&#34;ë²Ħ&#34;:686,&#34;ĵ°ëłĪê¸°&#34;:687,&#34;ìĪ&#34;:688,&#34;Ġìľł&#34;:689,&#34;Ġìŀ¥&#34;:690,&#34;Ĳľ&#34;:691,&#34;¡ľ&#34;:692,&#34;Ġë²&#34;:693,&#34;ëĦĪë¬´&#34;:694,&#34;ĠìĤ¬ëŀĮ&#34;:695,&#34;ëĥĲ&#34;:696,&#34;Ġë¶Ģ&#34;:697,&#34;ë©´ìĦľ&#34;:698,&#34;Ġë§Įëĵ¤&#34;:699,&#34;ĠìĿ¼&#34;:700,&#34;Ġíĸ&#34;:701,&#34;ë¨&#34;:702,&#34;ëĭ¨&#34;:703,&#34;ëĲĺ&#34;:704,&#34;ì¢ĭ&#34;:705,&#34;Ġãħĭãħĭ&#34;:706,&#34;ìµľ&#34;:707,&#34;ê³Ħ&#34;:708,&#34;Īëį&#34;:709,&#34;Ġì§Ģë£¨&#34;:710,&#34;ë¬¸&#34;:711,&#34;ë²Ī&#34;:712,&#34;ĠëĵľëĿ¼ë§Ī&#34;:713,&#34;ĠìķĪë&#34;:714,&#34;ìłģìĿ¸&#34;:715,&#34;ĠìĤ¬&#34;:716,&#34;Ġì¤ĳ&#34;:717,&#34;ëªħ&#34;:718,&#34;ìĦł&#34;:719,&#34;íĭ&#34;:720,&#34;Ġê³µ&#34;:721,&#34;ëłĩê²Į&#34;:722,&#34;ëĭ¤ëĬĶ&#34;:723,&#34;Ħë¡ľ&#34;:724,&#34;ĠëģĿ&#34;:725,&#34;ìĺģ&#34;:726,&#34;ãħľ&#34;:727,&#34;Ġë°°ìļ°&#34;:728,&#34;ì§ģ&#34;:729,&#34;ìĸµ&#34;:730,&#34;ì¶ľ&#34;:731,&#34;ëĭ¹&#34;:732,&#34;ĠëĤ´ìļ©&#34;:733,&#34;ë¦¬ê³ł&#34;:734,&#34;ë¦°&#34;:735,&#34;ë§Ŀ&#34;:736,&#34;ë¦¬ë&#34;:737,&#34;ìĽĮ&#34;:738,&#34;£½&#34;:739,&#34;ëŀĳ&#34;:740,&#34;ĠëĲĺ&#34;:741,&#34;Ġì¡°&#34;:742,&#34;íļ&#34;:743,&#34;ëıĻ&#34;:744,&#34;ë¯&#34;:745,&#34;Ġìļ°&#34;:746,&#34;Ġì¢Ģ&#34;:747,&#34;Ġíķ&#34;:748,&#34;Ġíķ´&#34;:749,&#34;,,&#34;:750,&#34;ĦìłĦ&#34;:751,&#34;Ġ10&#34;:752,&#34;ê¹Ŀ&#34;:753,&#34;ì¡°&#34;:754,&#34;^^&#34;:755,&#34;ĠëŃĲ&#34;:756,&#34;ĠëĨ&#34;:757,&#34;ë³´ê³ł&#34;:758,&#34;Ġìķł&#34;:759,&#34;íĤ&#34;:760,&#34;ãħİãħİ&#34;:761,&#34;ë´¤&#34;:762,&#34;ìŀ¬&#34;:763,&#34;¡ìħĺ&#34;:764,&#34;ì§Ģë§ī&#34;:765,&#34;Ġê°Ĳëıħ&#34;:766,&#34;Ġ2&#34;:767,&#34;ê°Ĳëı&#34;:768,&#34;ë¬¼&#34;:769,&#34;Ġìŀ¬ë¯¸ìŀĪ&#34;:770,&#34;ë¥¸&#34;:771,&#34;Ġë´Ĳ&#34;:772,&#34;Ġìĭ¶&#34;:773,&#34;Ġê°Ĳ&#34;:774,&#34;ëĨ&#34;:775,&#34;ìľ¼ë©´&#34;:776,&#34;ĠìĺģíĻĶë¥¼&#34;:777,&#34;ìŀ¬ë°Į&#34;:778,&#34;Ġì¹&#34;:779,&#34;ĠíĮ&#34;:780,&#34;ìĤ¬ëŀ&#34;:781,&#34;ê¸´&#34;:782,&#34;ëª¨&#34;:783,&#34;ë¦´&#34;:784,&#34;ìķł&#34;:785,&#34;ĠìĤ¬ëŀĳ&#34;:786,&#34;ĠìłĢ&#34;:787,&#34;íĺĦ&#34;:788,&#34;ìĨį&#34;:789,&#34;ĠíĹ&#34;:790,&#34;íĿ&#34;:791,&#34;íı¬&#34;:792,&#34;Ġëªħ&#34;:793,&#34;Ġê³ł&#34;:794,&#34;Ġëĺ&#34;:795,&#34;Ġë°ĺ&#34;:796,&#34;Ġì¢ĭìķĦ&#34;:797,&#34;Īëįĺ&#34;:798,&#34;ĠìĽĥ&#34;:799,&#34;ëĳĲ&#34;:800,&#34;Ġê±°&#34;:801,&#34;Ġìŀ¬ë¯¸ìĹĨ&#34;:802,&#34;Ġëħ&#34;:803,&#34;ìĹŃ&#34;:804,&#34;Ġì°¸&#34;:805,&#34;ì¤Ħ&#34;:806,&#34;ë°Ķ&#34;:807,&#34;Ġë³´ëĬĶ&#34;:808,&#34;ì²ĺ&#34;:809,&#34;Ġê·¸ëĥ¥&#34;:810,&#34;Ġë´¤ëĬĶëį°&#34;:811,&#34;¬¼&#34;:812,&#34;íķĺì§Ģ&#34;:813,&#34;ĠìĹŃ&#34;:814,&#34;ì¡±&#34;:815,&#34;íħ&#34;:816,&#34;Ġë°Ķ&#34;:817,&#34;Ġìĺģ&#34;:818,&#34;Ġìĥģ&#34;:819,&#34;ëĸ&#34;:820,&#34;ìľĦ&#34;:821,&#34;ëĵ¤ìĿĺ&#34;:822,&#34;ê»&#34;:823,&#34;Ġìĵ°ëłĪê¸°&#34;:824,&#34;ë°ķ&#34;:825,&#34;ĠìĹĨëĬĶ&#34;:826,&#34;íĶĦ&#34;:827,&#34;ãĦ&#34;:828,&#34;OO&#34;:829,&#34;ĠìŀĳíĴĪ&#34;:830,&#34;ëĤ¨&#34;:831,&#34;Ġëĭ¤ìĭľ&#34;:832,&#34;¥¼&#34;:833,&#34;ëĭĺ&#34;:834,&#34;ëħ¸&#34;:835,&#34;ìĿ¸ê³µ&#34;:836,&#34;ì§ĢëĬĶ&#34;:837,&#34;Ġë§¤&#34;:838,&#34;ëŁ½&#34;:839,&#34;ìŀħëĭĪëĭ¤&#34;:840,&#34;íĹ&#34;:841,&#34;ìļ¸&#34;:842,&#34;ì²Ń&#34;:843,&#34;Ġê²°&#34;:844,&#34;ìĭĿ&#34;:845,&#34;ĠìĺģíĻĶëĬĶ&#34;:846,&#34;ìĭ¶&#34;:847,&#34;íıīìłĲ&#34;:848,&#34;ĠëĬĲëĤ&#34;:849,&#34;Ġìĭ¤&#34;:850,&#34;ë§Īë&#34;:851,&#34;ëŃ&#34;:852,&#34;Ħ¤&#34;:853,&#34;ë°ĺ&#34;:854,&#34;!!!&#34;:855,&#34;¬ë¦&#34;:856,&#34;ìŀ¼&#34;:857,&#34;ĠíĻ&#34;:858,&#34;ê²½&#34;:859,&#34;ĠìŀĪëĬĶ&#34;:860,&#34;ì§Ī&#34;:861,&#34;Ġì¢ĭìĿĢ&#34;:862,&#34;©ëĭĪëĭ¤&#34;:863,&#34;Ġì¢ĭìķĺ&#34;:864,&#34;ê´Ģ&#34;:865,&#34;ëĭ¤ê³ł&#34;:866,&#34;ìī&#34;:867,&#34;Ġìĭľê°Ħ&#34;:868,&#34;ê¸¸&#34;:869,&#34;ëĿ¼ê³ł&#34;:870,&#34;ìĹĶ&#34;:871,&#34;ĠìĤ´&#34;:872,&#34;êµ°&#34;:873,&#34;Ġìĭł&#34;:874,&#34;ìĹ°ê¸°&#34;:875,&#34;ìĦ¤&#34;:876,&#34;ìķ¼ê¸°&#34;:877,&#34;ĠìĺģíĻĶê°Ģ&#34;:878,&#34;ëĭ¤ê°Ģ&#34;:879,&#34;ë°ľ&#34;:880,&#34;ĠíķĺëĤĺ&#34;:881,&#34;ìĬ¨&#34;:882,&#34;ºĲ&#34;:883,&#34;íļĮ&#34;:884,&#34;ìĹĨëĬĶ&#34;:885,&#34;Ġíĥ&#34;:886,&#34;ê°ĻìĿĢ&#34;:887,&#34;Ġì´&#34;:888,&#34;ìĸ´ìĦľ&#34;:889,&#34;ĠëķĮ&#34;:890,&#34;ì¶Ķ&#34;:891,&#34;Ġë¨&#34;:892,&#34;âĻ&#34;:893,&#34;ëŀĢ&#34;:894,&#34;ë´Ĳ&#34;:895,&#34;ĳľ&#34;:896,&#34;ĠìĹĨëĭ¤&#34;:897,&#34;Ġì²ĺ&#34;:898,&#34;ìĦ¸ìļĶ&#34;:899,&#34;ĠìĺĪ&#34;:900,&#34;ìĿ´ëŁ°&#34;:901,&#34;Ġíķł&#34;:902,&#34;ìłģìĿ´&#34;:903,&#34;ìµľê³ł&#34;:904,&#34;ìģ&#34;:905,&#34;Ġëħ¸&#34;:906,&#34;ĠíķĺëĬĶ&#34;:907,&#34;ìĿ¸ëį°&#34;:908,&#34;ëĲľ&#34;:909,&#34;ĠëĤĺìĺ¤&#34;:910,&#34;ëĭµ&#34;:911,&#34;!!!!&#34;:912,&#34;ĠëĦĺ&#34;:913,&#34;ĠìķĦëĭĪ&#34;:914,&#34;ì¦Ī&#34;:915,&#34;Ġì£½&#34;:916,&#34;ì¦Ŀ&#34;:917,&#34;ĠíĶ&#34;:918,&#34;ľì°&#34;:919,&#34;ĠìĿĺ&#34;:920,&#34;ìĹĲê²Į&#34;:921,&#34;ìĺĪ&#34;:922,&#34;Ġìķ¡ìħĺ&#34;:923,&#34;ëĵ¤ìĿĢ&#34;:924,&#34;ë¯¼&#34;:925,&#34;ìĽĢ&#34;:926,&#34;ãħ¡ãħ¡&#34;:927,&#34;ì½Ķ&#34;:928,&#34;Ġê¼&#34;:929,&#34;Ġì½Ķ&#34;:930,&#34;ìķĬ&#34;:931,&#34;ê·¹&#34;:932,&#34;Ġëª¨ë¥´&#34;:933,&#34;Ġíı&#34;:934,&#34;êµĲ&#34;:935,&#34;Ġëª°&#34;:936,&#34;ĠìķĦê¹Ŀ&#34;:937,&#34;Ġì¶Ķ&#34;:938,&#34;ìł¸&#34;:939,&#34;ìłģìľ¼ë¡ľ&#34;:940,&#34;ìĿ´ëĤĺ&#34;:941,&#34;ìĺ¨&#34;:942,&#34;ì¼&#34;:943,&#34;ĠíĽĦ&#34;:944,&#34;Ġ3&#34;:945,&#34;Ġê¸°ëĮĢ&#34;:946,&#34;ì²ľ&#34;:947,&#34;ĮìĿ´&#34;:948,&#34;ê²łëĭ¤&#34;:949,&#34;íĮĲ&#34;:950,&#34;Ġìµľê³łìĿĺ&#34;:951,&#34;ìŀĪëĬĶ&#34;:952,&#34;ê²¨&#34;:953,&#34;Ġíŀ&#34;:954,&#34;íķĻ&#34;:955,&#34;»Ķ&#34;:956,&#34;ë¶ĢíĦ°&#34;:957,&#34;íĸī&#34;:958,&#34;Ġëĸ&#34;:959,&#34;ë©°&#34;:960,&#34;ìĶ¨&#34;:961,&#34;ì´&#34;:962,&#34;ĠìĦ±&#34;:963,&#34;ľì°®&#34;:964,&#34;ëĮĢë¡ľ&#34;:965,&#34;ĠìĿ´íķ´&#34;:966,&#34;ĠëĺĲ&#34;:967,&#34;ê¸ī&#34;:968,&#34;íķľëĭ¤&#34;:969,&#34;ì°¨&#34;:970,&#34;ĪëĦ¤&#34;:971,&#34;ëĤł&#34;:972,&#34;ë¡Ŀ&#34;:973,&#34;íĭ°&#34;:974,&#34;íĸĪëĭ¤&#34;:975,&#34;ëĬĲ&#34;:976,&#34;ĠíĺĦ&#34;:977,&#34;ìĭľê°Ħ&#34;:978,&#34;íĽĦ&#34;:979,&#34;ĠíĬ&#34;:980,&#34;ĠìĹ°ì¶ľ&#34;:981,&#34;ĠíĸĪ&#34;:982,&#34;Ġëĵ¤&#34;:983,&#34;Ġë§Īì§Ģë§ī&#34;:984,&#34;Ġë¶Ī&#34;:985,&#34;ë°°ìļ°&#34;:986,&#34;ĠìĿ´ëłĩê²Į&#34;:987,&#34;ìŀĶ&#34;:988,&#34;Ġë¶Ħ&#34;:989,&#34;Ġë©ĭ&#34;:990,&#34;ì£&#34;:991,&#34;Ġì²ĺìĿĮ&#34;:992,&#34;Ġê´Ģ&#34;:993,&#34;ĠìĽĲ&#34;:994,&#34;Ġì§ľ&#34;:995,&#34;ĠìĿ´ìķ¼ê¸°&#34;:996,&#34;Ġê·¹&#34;:997,&#34;ìłĪ&#34;:998,&#34;ìłķëıĦ&#34;:999,&#34;íķ©ëĭĪëĭ¤&#34;:1000,&#34;íĺ¸&#34;:1001,&#34;íĶ¼&#34;:1002,&#34;Ġê¸°ìĸµ&#34;:1003,&#34;ĠìľĦ&#34;:1004,&#34;ĠëĶ°&#34;:1005,&#34;ì§ĢëıĦ&#34;:1006,&#34;ĠíĽ&#34;:1007,&#34;Ġíģ&#34;:1008,&#34;ì£¼ëĬĶ&#34;:1009,&#34;ê·¸ëĥ¥&#34;:1010,&#34;ì¹ľ&#34;:1011,&#34;ĠëıĦ&#34;:1012,&#34;ĠìĿ´ê±´&#34;:1013,&#34;ìĬ´&#34;:1014,&#34; &#34; &#34; &#34; &#34;&#34;:1015,&#34;ë§¨&#34;:1016,&#34;ìĤ´&#34;:1017,&#34;Ġìŀ¥ë©´&#34;:1018,&#34;ë¸&#34;:1019,&#34;âĻ¥&#34;:1020,&#34;íĤ¤&#34;:1021,&#34;ë¡ł&#34;:1022,&#34;Ġìµľìķħ&#34;:1023,&#34;Ġê°ķ&#34;:1024,&#34;íĺĢ&#34;:1025,&#34;Ġë§İìĿ´&#34;:1026,&#34;ë¥ĺ&#34;:1027,&#34;ë¬¸ìĹĲ&#34;:1028,&#34;ĠìĦ¸&#34;:1029,&#34;Ġíıī&#34;:1030,&#34;ĠíķľêµŃ&#34;:1031,&#34;Ġê·¸ë¦¬ê³ł&#34;:1032,&#34;Ġë§Įëĵł&#34;:1033,&#34;ëĤĺëĬĶ&#34;:1034,&#34;Ġë¬&#34;:1035,&#34;ãħĭãħĭãħĭ&#34;:1036,&#34;Ġëªħìŀĳ&#34;:1037,&#34;ĠìĪ&#34;:1038,&#34;ĠëĳĲ&#34;:1039,&#34;ĪëĿ¼&#34;:1040,&#34;ĠíĻĶ&#34;:1041,&#34;ĠëıĻ&#34;:1042,&#34;ĠìĻĦìłĦ&#34;:1043,&#34;ìĿ´ê±°&#34;:1044,&#34;ë¦¬ëĬĶ&#34;:1045,&#34;ë³¼&#34;:1046,&#34;ëĿ¼ëĬĶ&#34;:1047,&#34;ê¸Ģ&#34;:1048,&#34;Ġìŀ¬ë°Įê²Į&#34;:1049,&#34;ĠìķĦê¹Į&#34;:1050,&#34;ëŁ¼&#34;:1051,&#34;ë¨¸&#34;:1052,&#34;ì¢Ģ&#34;:1053,&#34;ëª»&#34;:1054,&#34;ê°Ĳëıħ&#34;:1055,&#34;ìĹĨëĭ¤&#34;:1056,&#34;ĠëĤľ&#34;:1057,&#34;Ġë³Ħë¡ľ&#34;:1058,&#34;ĦìļĶ&#34;:1059,&#34;Ġê·¸ëŁ°&#34;:1060,&#34;ìķĺëĭ¤&#34;:1061,&#34;ë°©&#34;:1062,&#34;ĠìķĦìī&#34;:1063,&#34;ĠìºĲ&#34;:1064,&#34;ìķĮ&#34;:1065,&#34;Ġë³´ìĹ¬&#34;:1066,&#34;ë³´ëĬĶ&#34;:1067,&#34;ĶìĿ´&#34;:1068,&#34;00&#34;:1069,&#34;ë§¤&#34;:1070,&#34;Īë¬¼&#34;:1071,&#34;ë³Ħ&#34;:1072,&#34;Ġê´ľì°®&#34;:1073,&#34;ĠìĿ´ê±°&#34;:1074,&#34;ê²°&#34;:1075,&#34;Ġë°°&#34;:1076,&#34;ĠìķĦëĭĮ&#34;:1077,&#34;¿Ĳ&#34;:1078,&#34;ëł¸&#34;:1079,&#34;ëĪ&#34;:1080,&#34;Ġíİ&#34;:1081,&#34;ê²ĥëıĦ&#34;:1082,&#34;Ġë¡ľ&#34;:1083,&#34;ëŃĲ&#34;:1084,&#34;íķĺëĤĺ&#34;:1085,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:1086,&#34;Ġë³Ħ&#34;:1087,&#34;ĠëıĪ&#34;:1088,&#34;ìĤ¬ëŀĮ&#34;:1089,&#34;ë¦Ń&#34;:1090,&#34;íĺķ&#34;:1091,&#34;ì©&#34;:1092,&#34;ë§ĮìĹĲ&#34;:1093,&#34;ìĥī&#34;:1094,&#34;ĠëĤ´ê°Ģ&#34;:1095,&#34;ì³&#34;:1096,&#34;ëģ¼&#34;:1097,&#34;ë¨¹&#34;:1098,&#34;ìĻľ&#34;:1099,&#34;ìŀĺ&#34;:1100,&#34;Ġìŀ¼&#34;:1101,&#34;ãĦ·&#34;:1102,&#34;ìĿĦê¹Į&#34;:1103,&#34;ê°ĢëĬĶ&#34;:1104,&#34;Ġê°ĻìĿĢ&#34;:1105,&#34;ëħĢ&#34;:1106,&#34;ìķĦìĦľ&#34;:1107,&#34;Ġìĸ´ëĸ&#34;:1108,&#34;ĠëŃĶ&#34;:1109,&#34;ĠìĹĨê³ł&#34;:1110,&#34;íħĮ&#34;:1111,&#34;Ġë°ľ&#34;:1112,&#34;ìĽł&#34;:1113,&#34;ê·¼&#34;:1114,&#34;Ġëĭ¹&#34;:1115,&#34;°ĭ&#34;:1116,&#34;ĠìķĦë¦Ħ&#34;:1117,&#34;ìĻĶ&#34;:1118,&#34;ê²©&#34;:1119,&#34;ë³µ&#34;:1120,&#34;Ġê·¸ëŀĺ&#34;:1121,&#34;ìĹĨìĿ´&#34;:1122,&#34;ê¹Ģ&#34;:1123,&#34;ë¦¼&#34;:1124,&#34;ìĪł&#34;:1125,&#34;ëª©&#34;:1126,&#34;........&#34;:1127,&#34;íĽ&#34;:1128,&#34;¬ë¦¬&#34;:1129,&#34;ìĹĪëĬĶëį°&#34;:1130,&#34;Ġì§Ģê¸Ī&#34;:1131,&#34;Ġê¹Ģ&#34;:1132,&#34;Ġìļ¸&#34;:1133,&#34;ĠíĬ¹&#34;:1134,&#34;ëĲ&#34;:1135,&#34;ìĿ¸ê°Ģ&#34;:1136,&#34;ìĬ¤íĨłë¦¬&#34;:1137,&#34;ĠìĹ¬ìŀĲ&#34;:1138,&#34;íķĺê¸°&#34;:1139,&#34;Ġíĳľ&#34;:1140,&#34;Ġê¼Ń&#34;:1141,&#34;ì½&#34;:1142,&#34;ĠëĬĲëĤĮ&#34;:1143,&#34;ëľ&#34;:1144,&#34;ìĽĥ&#34;:1145,&#34;Ġë§ī&#34;:1146,&#34;Ġíŀĺ&#34;:1147,&#34;ëĭĪë&#34;:1148,&#34;ëĤ´ìļ©&#34;:1149,&#34;Ġìĭ¬&#34;:1150,&#34;ãħī&#34;:1151,&#34;ĠìķĦê¹Ŀëĭ¤&#34;:1152,&#34;ë´ī&#34;:1153,&#34;ì§±&#34;:1154,&#34;ĠìĹĦ&#34;:1155,&#34;ê¼&#34;:1156,&#34;ìĹĩ&#34;:1157,&#34;íķĺë©´&#34;:1158,&#34;ãħľãħľ&#34;:1159,&#34;ìĺĢëĭ¤&#34;:1160,&#34;Ġë§ĪìĿĮ&#34;:1161,&#34;Ġãħłãħł&#34;:1162,&#34;ëĵľëĬĶ&#34;:1163,&#34;ĠíĹĪ&#34;:1164,&#34;Ġì¤Ħ&#34;:1165,&#34;ìĺģíĻĶëĬĶ&#34;:1166,&#34; &#34; &#34; &#34;&#34;:1167,&#34;ëĭĪê¹Į&#34;:1168,&#34;¬ëĿ¼&#34;:1169,&#34;ëĵľëĿ¼ë§Ī&#34;:1170,&#34;ìĪľ&#34;:1171,&#34;ëĤ´ê°Ģ&#34;:1172,&#34;Ġê·Ģ&#34;:1173,&#34;ê°ķ&#34;:1174,&#34;ì¹´&#34;:1175,&#34;ĠíĨ&#34;:1176,&#34;êµ¬ëĤĺ&#34;:1177,&#34;ĠìĿĮ&#34;:1178,&#34;íĮĮ&#34;:1179,&#34;ë§ĲìĿ´&#34;:1180,&#34;ì§ĳ&#34;:1181,&#34;ìĺģíĻĶê°Ģ&#34;:1182,&#34;ìĿ´ê²Į&#34;:1183,&#34;ĠìĦ¤&#34;:1184,&#34;Ġì£¼ìĿ¸ê³µ&#34;:1185,&#34;Ġë§Ŀ&#34;:1186,&#34;íķĺì§Ģë§Į&#34;:1187,&#34;Ġë³´ë©´&#34;:1188,&#34;ĠìĿ´ìĥģ&#34;:1189,&#34;Ġêµ¬&#34;:1190,&#34;ĠíıīìłĲìĿ´&#34;:1191,&#34;ĠìĹĲ&#34;:1192,&#34;Ġì°¨&#34;:1193,&#34;ĠëĲ&#34;:1194,&#34;ìķĦìļĶ&#34;:1195,&#34;Ġëĭ¨&#34;:1196,&#34;ĠìłĲ&#34;:1197,&#34;Ġë³´ê¸°&#34;:1198,&#34;ìĿ´ê³ł&#34;:1199,&#34;ĠìĺģíĻĶëĭ¤&#34;:1200,&#34;Ķì§ģ&#34;:1201,&#34;ê°ĲëıĻ&#34;:1202,&#34;ìĺģíĻĶë¥¼&#34;:1203,&#34;ëĵ±&#34;:1204,&#34;ë¹ł&#34;:1205,&#34;ë¯¸ëĶĶ&#34;:1206,&#34;ĠìĬ¬&#34;:1207,&#34;Ġì¡¸&#34;:1208,&#34;íģ¬&#34;:1209,&#34;ì»&#34;:1210,&#34;íĥľ&#34;:1211,&#34;Ġë¯&#34;:1212,&#34;ĠëĤ®&#34;:1213,&#34;ìĿ¸ì§Ģ&#34;:1214,&#34;Ġë§¤ëł¥&#34;:1215,&#34;ìĬ¤ëŁ½&#34;:1216,&#34;ĠìļĶ&#34;:1217,&#34;10&#34;:1218,&#34;Ġë§ŀ&#34;:1219,&#34;ĠíĺĦìĭ¤&#34;:1220,&#34;Ġê°Ģìŀ¥&#34;:1221,&#34;ëĬĶëĭ¤&#34;:1222,&#34;ëĬĶì§Ģ&#34;:1223,&#34;Ġê±¸&#34;:1224,&#34;ĠëĨĴ&#34;:1225,&#34;ìķĪë&#34;:1226,&#34;ì°¸&#34;:1227,&#34;į¼&#34;:1228,&#34;ĠìĨį&#34;:1229,&#34;íıī&#34;:1230,&#34;ìĥĿê°ģ&#34;:1231,&#34;ì¼Ģ&#34;:1232,&#34;ìĸ¸&#34;:1233,&#34;ĠìķĦìĿ´&#34;:1234,&#34;ìĸĳ&#34;:1235,&#34;Ġëª°ìŀħ&#34;:1236,&#34;ìĿ´ê±´&#34;:1237,&#34;ëĶ°&#34;:1238,&#34;Ġ.&#34;:1239,&#34;Ġë¹ł&#34;:1240,&#34;Ġë³¼ë§Į&#34;:1241,&#34;ìģĺ&#34;:1242,&#34;ì§Ģë£¨&#34;:1243,&#34;ë¦ŃíĦ°&#34;:1244,&#34;ĠìŀĪëĭ¤&#34;:1245,&#34;ĠìłķëıĦ&#34;:1246,&#34;ĠíĿ¥&#34;:1247,&#34;Ġì§ľì¦Ŀ&#34;:1248,&#34;íĮħ&#34;:1249,&#34;Ġë¦&#34;:1250,&#34;Ġ..&#34;:1251,&#34;Ġëĭ¤ë¥¸&#34;:1252,&#34;ì£¼ìĿ¸ê³µ&#34;:1253,&#34;Ġë¬´ìĬ¨&#34;:1254,&#34;ĠìĦł&#34;:1255,&#34;Ġìĸ¼&#34;:1256,&#34;ìĻĦìłĦ&#34;:1257,&#34;Ġë§Įëĵ¤ìĸ´&#34;:1258,&#34;ĠìŀĶ&#34;:1259,&#34;ìĶ¬&#34;:1260,&#34;Ġê³Ħ&#34;:1261,&#34;ìĹĪëįĺ&#34;:1262,&#34;Ġãħĭãħĭãħĭ&#34;:1263,&#34;ì£ł&#34;:1264,&#34;ìĹĲëĬĶ&#34;:1265,&#34;ĠëĮĢíķľ&#34;:1266,&#34;¨ìĸ´&#34;:1267,&#34;Ġìľłì¹ĺ&#34;:1268,&#34;Ġì¡´&#34;:1269,&#34;ĠëĪĪë¬¼&#34;:1270,&#34;ãħĩ&#34;:1271,&#34;ë°°&#34;:1272,&#34;ë°Ľ&#34;:1273,&#34;ìĬ¤ëŁ¬&#34;:1274,&#34;Ġì¶©&#34;:1275,&#34;ì§Ģê³ł&#34;:1276,&#34;ì§ĢìķĬ&#34;:1277,&#34;ĠìĿ´ê²Į&#34;:1278,&#34;ëĤĺìĦľ&#34;:1279,&#34;ìĤ¬ë&#34;:1280,&#34;Ġê³¼&#34;:1281,&#34;ëĶ©&#34;:1282,&#34;ĠìķĪëĲĺ&#34;:1283,&#34;Ġìłģ&#34;:1284,&#34;ĠìĹŃìĭľ&#34;:1285,&#34;ĠíķĦìļĶ&#34;:1286,&#34;ĠìĦľ&#34;:1287,&#34;ëĬ¥&#34;:1288,&#34;Ġãħİãħİ&#34;:1289,&#34;ìĤ°&#34;:1290,&#34;ĠìĥĿ&#34;:1291,&#34;ì²ĺëŁ¼&#34;:1292,&#34;Ġìļ°ë¦¬&#34;:1293,&#34;Ġëĵ¯&#34;:1294,&#34;ĠìłĦê°ľ&#34;:1295,&#34;ì±&#34;:1296,&#34;Ķì§ģíŀĪ&#34;:1297,&#34;Ġë°ĺìłĦ&#34;:1298,&#34;ëł¤ê³ł&#34;:1299,&#34;ìĵ°ëłĪê¸°&#34;:1300,&#34;ëĤĺìĺ¤&#34;:1301,&#34;íĽĪ&#34;:1302,&#34;Ġãħĭ&#34;:1303,&#34;ĠëĪĦ&#34;:1304,&#34;Ġê±´&#34;:1305,&#34;íĪ&#34;:1306,&#34;ë©Ķ&#34;:1307,&#34;ì¡Į&#34;:1308,&#34;ĠëĪĪ&#34;:1309,&#34;Ġê³µíı¬&#34;:1310,&#34;ĠëĤ¨ìŀĲ&#34;:1311,&#34;ëŀľ&#34;:1312,&#34;ĠìĬ¤ë¦´&#34;:1313,&#34;¾Į&#34;:1314,&#34;ì¦ĺ&#34;:1315,&#34;Ġë°©&#34;:1316,&#34;Ġì§ĳ&#34;:1317,&#34;ìŀ¬ë¯¸ìŀĪ&#34;:1318,&#34;ì³Ĳ&#34;:1319,&#34;ë§ģ&#34;:1320,&#34;ìĹĪìĿĮ&#34;:1321,&#34;Ġë°ķ&#34;:1322,&#34;ëĭ¤ëĭĪ&#34;:1323,&#34;ĠìĿ¸ìĥĿ&#34;:1324,&#34;Ġíı¬&#34;:1325,&#34;êµ¬ë&#34;:1326,&#34;êµ¿&#34;:1327,&#34;ĸìĹĲ&#34;:1328,&#34;ìŀ¡&#34;:1329,&#34;Ġì¦&#34;:1330,&#34;ĠëıĮ&#34;:1331,&#34;íģ¼&#34;:1332,&#34;Ġë»Ķ&#34;:1333,&#34;íĶĪ&#34;:1334,&#34;Ġì°¾&#34;:1335,&#34;ĠìķĦë¬´&#34;:1336,&#34;Ġíķĺì§Ģë§Į&#34;:1337,&#34;ĠìĿ¼ë³¸&#34;:1338,&#34;ĠìĪĺì¤Ģ&#34;:1339,&#34;ëĬĶê±°&#34;:1340,&#34;íĨµ&#34;:1341,&#34;ê»ĺ&#34;:1342,&#34;ë¶Ī&#34;:1343,&#34;ë§Īì§Ģë§ī&#34;:1344,&#34;ëĭ¤ìļ´&#34;:1345,&#34;íĥĦ&#34;:1346,&#34;ìĦĿ&#34;:1347,&#34;Ġê¸´&#34;:1348,&#34;Ġê°ĢìĬ´&#34;:1349,&#34;Ġë§İìĿĢ&#34;:1350,&#34;ìĤ¬ëŀĳ&#34;:1351,&#34;ìŁ&#34;:1352,&#34;Ġíķĺê³ł&#34;:1353,&#34;¸ìłľ&#34;:1354,&#34;ì¾Į&#34;:1355,&#34;ëĭ¬&#34;:1356,&#34;Ġãħ¡ãħ¡&#34;:1357,&#34;Ġë°Ľ&#34;:1358,&#34;ì¤ĳìĹĲ&#34;:1359,&#34;ĠëģĿê¹Įì§Ģ&#34;:1360,&#34;Ġìĸµ&#34;:1361,&#34;Īë°&#34;:1362,&#34;Ġì±&#34;:1363,&#34;ëĵ¤ìĿĦ&#34;:1364,&#34;ìĬ¤íĦ°&#34;:1365,&#34;į¨&#34;:1366,&#34;»ê²Į&#34;:1367,&#34;ë¦½&#34;:1368,&#34;Ġì§±&#34;:1369,&#34;ëĤĺëĿ¼&#34;:1370,&#34;ĠíĴ&#34;:1371,&#34;ĠìķĦëĭĪëĿ¼&#34;:1372,&#34;ĠìĿ¸ê°Ħ&#34;:1373,&#34;Ġìĺģìĥģ&#34;:1374,&#34;ĠOO&#34;:1375,&#34;ìĸ¼&#34;:1376,&#34;ìĿ´ì§Ģ&#34;:1377,&#34;Ĵ¤&#34;:1378,&#34;ê»´&#34;:1379,&#34;ë¦¬ì¦Ī&#34;:1380,&#34;ëįĶëĿ¼&#34;:1381,&#34;Ġíĭ&#34;:1382,&#34;Ġë´ĲëıĦ&#34;:1383,&#34;íĿ¬&#34;:1384,&#34;ĠëĲľ&#34;:1385,&#34;ê¸°ëıĦ&#34;:1386,&#34;Ġíĸī&#34;:1387,&#34;º¼&#34;:1388,&#34;ìĨ¡&#34;:1389,&#34;íĸĪëįĺ&#34;:1390,&#34;Ġëª¨ëĳĲ&#34;:1391,&#34;Ġì°į&#34;:1392,&#34;Ġê·¹ìŀ¥&#34;:1393,&#34;íĻ©&#34;:1394,&#34;ìŀ¥ë©´&#34;:1395,&#34;ĠìĪľ&#34;:1396,&#34;ì£½&#34;:1397,&#34;.....&#34;:1398,&#34;Ġíİ¸&#34;:1399,&#34;ĠíĶ¼&#34;:1400,&#34;ìĹŃìĭľ&#34;:1401,&#34;ëĲĺëĬĶ&#34;:1402,&#34;Ġìļķ&#34;:1403,&#34;ìłĳ&#34;:1404,&#34;Ġì¶Ķì²ľ&#34;:1405,&#34;ì£¼ê³ł&#34;:1406,&#34;ł¤&#34;:1407,&#34;Ġëª¨ëĵł&#34;:1408,&#34;ëĦĺ&#34;:1409,&#34;ê¶&#34;:1410,&#34;ë§İ&#34;:1411,&#34;Ġë´¤ëĭ¤&#34;:1412,&#34;ìŁģ&#34;:1413,&#34;ìĬ¤íĬ¸&#34;:1414,&#34;Ġê·¼&#34;:1415,&#34;ìĿĦëķĮ&#34;:1416,&#34;Ġìŀ¬&#34;:1417,&#34;ìķĦìĿ´&#34;:1418,&#34;Ġê°Ļëĭ¤&#34;:1419,&#34;ëĬĺ&#34;:1420,&#34;ĠìĿĮìķħ&#34;:1421,&#34;Ġìĭ¤ë§Ŀ&#34;:1422,&#34;Ġë¨¸&#34;:1423,&#34;ĪëĦ¤ìļĶ&#34;:1424,&#34;ë§Įëĵ¤&#34;:1425,&#34;ì¢ħ&#34;:1426,&#34;ë¿Ĳ&#34;:1427,&#34;Ġìĵ°&#34;:1428,&#34;ë¦¬ê°Ģ&#34;:1429,&#34;ìĹĲëıĦ&#34;:1430,&#34;ĠìĻ¸&#34;:1431,&#34;Ġëªĩ&#34;:1432,&#34;ĪëĮĢ&#34;:1433,&#34;ìĿ´ìĥģ&#34;:1434,&#34;ìĿ´ëŀĳ&#34;:1435,&#34;ê±´ì§Ģ&#34;:1436,&#34;ëĨĵ&#34;:1437,&#34;ìĹĨê³ł&#34;:1438,&#34;ĠìĻĦ&#34;:1439,&#34;ìĦŃ&#34;:1440,&#34;Ġê²Į&#34;:1441,&#34;Ġ&#39;&#34;:1442,&#34;ĠìķĦì§ģ&#34;:1443,&#34;ëĺ&#34;:1444,&#34;ĠíĤ&#34;:1445,&#34;ìĪĺë¡Ŀ&#34;:1446,&#34;ĠëĤĺìĺ¨&#34;:1447,&#34;ĠìłĦíĺĢ&#34;:1448,&#34;íĸĪëĬĶëį°&#34;:1449,&#34;ĵ¤&#34;:1450,&#34;ĠìĹ´&#34;:1451,&#34;Ġë¨¹&#34;:1452,&#34;ë´ĲëıĦ&#34;:1453,&#34;ìĻ¸&#34;:1454,&#34;Ġëª¨ìĬµ&#34;:1455,&#34;ĠìĹ°ê¸°ëł¥&#34;:1456,&#34;Ġíķľë²Ī&#34;:1457,&#34;Ġìĸµì§Ģ&#34;:1458,&#34;Ġì¤Ģ&#34;:1459,&#34;ëľ»&#34;:1460,&#34;Ġ4&#34;:1461,&#34;íĬ¹&#34;:1462,&#34;ìłĲëıĦ&#34;:1463,&#34;Ġê²½&#34;:1464,&#34;Ġ...&#34;:1465,&#34;ĠìºĲë¦ŃíĦ°&#34;:1466,&#34;ìŀ¥ê°Ĳ&#34;:1467,&#34;ìĿ´ëĿ¼ëĬĶ&#34;:1468,&#34;ëĵ¤ëıĦ&#34;:1469,&#34;Ŀ½&#34;:1470,&#34;ìĿ´ëĬĶ&#34;:1471,&#34;ĠëĤ´ëĤ´&#34;:1472,&#34;Ġê²°ë§Ĳ&#34;:1473,&#34;íį¼&#34;:1474,&#34;ëıħ&#34;:1475,&#34;ê¸°ëĬĶ&#34;:1476,&#34;ìĬ¹&#34;:1477,&#34;Ġì¢ĭëĭ¤&#34;:1478,&#34;ĠíĳľíĺĦ&#34;:1479,&#34;ëĮĢì²´&#34;:1480,&#34;Ġê³µê°Ĳ&#34;:1481,&#34;Ġë³´ì§Ģ&#34;:1482,&#34;ìĪĺê°Ģ&#34;:1483,&#34;ĠëĨĢ&#34;:1484,&#34;Ġì¹ĺ&#34;:1485,&#34;ĠìķĬê³ł&#34;:1486,&#34;Ġê³ĦìĨį&#34;:1487,&#34;©ĶìĿ´&#34;:1488,&#34;íĥĢìŀĦ&#34;:1489,&#34;Ġê°Ħ&#34;:1490,&#34;Ġíķ¨&#34;:1491,&#34;ëıĪ&#34;:1492,&#34;Ġì¶ľ&#34;:1493,&#34;ë§ŀ&#34;:1494,&#34;ê¸°ëĮĢ&#34;:1495,&#34;ìĸ´ìķ¼&#34;:1496,&#34;íķĺëĦ¤ìļĶ&#34;:1497,&#34;ìĹĪìĸ´ìļĶ&#34;:1498,&#34;Ġ8&#34;:1499,&#34;ĠìĹ¬ìļ´&#34;:1500,&#34;ëŁ¬ë&#34;:1501,&#34;Ġì½Ķë¯¸ëĶĶ&#34;:1502,&#34;ì¢ĭìĿĢ&#34;:1503,&#34;íĳľ&#34;:1504,&#34;ĠìĿ´ìłľ&#34;:1505,&#34;Ġë§ĲìĿ´&#34;:1506,&#34;ë¸Į&#34;:1507,&#34;ľ´&#34;:1508,&#34;ì¡´&#34;:1509,&#34;Ġ9&#34;:1510,&#34;íķĺëĦ¤&#34;:1511,&#34;ĠìĺģíĻĶìĹĲ&#34;:1512,&#34;ê¾&#34;:1513,&#34;ëĤ´ëĤ´&#34;:1514,&#34;ëķ&#34;:1515,&#34;ëĿ¼ëıĦ&#34;:1516,&#34;Ġëĵ±&#34;:1517,&#34;íĻĺ&#34;:1518,&#34;ìłĦìĹĲ&#34;:1519,&#34;Ġë²Ħ&#34;:1520,&#34;ĠëĮĢëĭ¨&#34;:1521,&#34;ëķĮë¬¸ìĹĲ&#34;:1522,&#34;Ġê¶&#34;:1523,&#34;Ġì»&#34;:1524,&#34;ĠìĹĦì²Ń&#34;:1525,&#34;íĹĪ&#34;:1526,&#34;ĠëĶ±&#34;:1527,&#34;ĠëĤĺìĺ¤ëĬĶ&#34;:1528,&#34;ê·¸ëŀĺ&#34;:1529,&#34;Ġëĭµ&#34;:1530,&#34;ëģĿ&#34;:1531,&#34;Ġ5&#34;:1532,&#34;ëĭ´&#34;:1533,&#34;ë»Ķ&#34;:1534,&#34;ê°Ŀ&#34;:1535,&#34;Ġì¦Ĳ&#34;:1536,&#34;ĠëĿ¼&#34;:1537,&#34;ĠìķĦëĭĪëĭ¤&#34;:1538,&#34;Ġëĸ¨ìĸ´&#34;:1539,&#34;ĠêµĲ&#34;:1540,&#34;ëĿ¼ë¦¬&#34;:1541,&#34;ĠìĹī&#34;:1542,&#34;ĠìķĬìĿĢ&#34;:1543,&#34;~~~&#34;:1544,&#34;ĠëĬĲê»´&#34;:1545,&#34;ìĿ´ê°Ģ&#34;:1546,&#34;ëŀĮ&#34;:1547,&#34;ë²Ķ&#34;:1548,&#34;íķľíħĮ&#34;:1549,&#34;ê¸°ìĹĲ&#34;:1550,&#34;Ġê°Ī&#34;:1551,&#34;Ł¬&#34;:1552,&#34;ìĸ´ëıĦ&#34;:1553,&#34;Ġ7&#34;:1554,&#34;ìķ¡ìħĺ&#34;:1555,&#34;ë¦¬ìĺ¤&#34;:1556,&#34;Ġê·¸ëłĩ&#34;:1557,&#34;ìķ½&#34;:1558,&#34;ë§Įíķľ&#34;:1559,&#34;Ġë¯¿&#34;:1560,&#34;ĠìĺģíĻĶìĿĺ&#34;:1561,&#34;ì´Ī&#34;:1562,&#34;ìłĲìĿ´&#34;:1563,&#34;Ġìŀħ&#34;:1564,&#34;Ġíĺ¸&#34;:1565,&#34;ëĭ¤ë©´&#34;:1566,&#34;íķ´ìļĶ&#34;:1567,&#34;ĠìķĦìī½&#34;:1568,&#34;íķĺë©´ìĦľ&#34;:1569,&#34;Ġìķ½&#34;:1570,&#34;ìĦ±ìĿ´&#34;:1571,&#34;Ġë§ĮëĵľëĬĶ&#34;:1572,&#34;ĠìĹ°ê¸°ê°Ģ&#34;:1573,&#34;ëĿ¼ë§Īë&#34;:1574,&#34;ìĵ°&#34;:1575,&#34;ì²ĺìĿĮ&#34;:1576,&#34;ë¥Ń&#34;:1577,&#34;ìĿ´ëĿ¼ê³ł&#34;:1578,&#34;ëĭ¤ìĭľ&#34;:1579,&#34;ìĹĦ&#34;:1580,&#34;ĠìŀĲì²´&#34;:1581,&#34;ëĿ½&#34;:1582,&#34;Ġê°ľë´ī&#34;:1583,&#34;ãĦ·ãĦ·&#34;:1584,&#34;ë³ĳ&#34;:1585,&#34;íģĲ&#34;:1586,&#34;ìŀĩ&#34;:1587,&#34;íĸ¥&#34;:1588,&#34;ĠìĨĲ&#34;:1589,&#34;ìĹħ&#34;:1590,&#34;Ġê¿&#34;:1591,&#34;ĶĶ&#34;:1592,&#34;łĪ&#34;:1593,&#34;ë§Įíģ¼&#34;:1594,&#34;ĠëŃĶê°Ģ&#34;:1595,&#34;ìµľê³łìĿĺ&#34;:1596,&#34;ĠíĮĮ&#34;:1597,&#34;Ġìĸ´ëĸ»ê²Į&#34;:1598,&#34;Ġëĭ´&#34;:1599,&#34;Ġëĸł&#34;:1600,&#34;Ġê¸°ë¶Ħ&#34;:1601,&#34;ĠìĽĲìŀĳ&#34;:1602,&#34;ĠíĿ¥ë¯¸&#34;:1603,&#34;ĠìĻĢ&#34;:1604,&#34;Ġë´Ĳìķ¼&#34;:1605,&#34;ìľ¼ëĤĺ&#34;:1606,&#34;ĠìķĦìī¬&#34;:1607,&#34;ê°Ļëĭ¤&#34;:1608,&#34;Ġìĭ¸&#34;:1609,&#34;Ġì¹ľ&#34;:1610,&#34;Įë¥Ń&#34;:1611,&#34;ë´¤ëĬĶëį°&#34;:1612,&#34;ìķĦëĭĪ&#34;:1613,&#34;ëįĺëį°&#34;:1614,&#34;Ġë¬¼&#34;:1615,&#34;ìĿ¸ëĵ¯&#34;:1616,&#34;Īë°ĺ&#34;:1617,&#34;Īë¡ľ&#34;:1618,&#34;ëĶ´&#34;:1619,&#34;ëĤ¬&#34;:1620,&#34;Ġìŀ¬ë¯¸ìŀĪê²Į&#34;:1621,&#34;ĳĺ&#34;:1622,&#34;ê¸°ê°Ģ&#34;:1623,&#34;ĠëĤĺìĻĶ&#34;:1624,&#34;ìŀĪëĭ¤&#34;:1625,&#34;ëĬĶê²Į&#34;:1626,&#34;íĺ¼&#34;:1627,&#34;Ġãħĭãħĭãħĭãħĭ&#34;:1628,&#34;ĠìµľìķħìĿĺ&#34;:1629,&#34;Ġê°Ģì¡±&#34;:1630,&#34;íķľêµŃ&#34;:1631,&#34;ì°½&#34;:1632,&#34;ìĿ´ëŀĢ&#34;:1633,&#34;Ġê³&#34;:1634,&#34;ĠìķĦì£¼&#34;:1635,&#34;Ġë¶Ģì¡±&#34;:1636,&#34;ìŀ¬ë°ĭ&#34;:1637,&#34;Ġ-&#34;:1638,&#34;Ġê·¸ëŀĺëıĦ&#34;:1639,&#34;ĠìĹ°ê¸°ëıĦ&#34;:1640,&#34;Ġìķħ&#34;:1641,&#34;ĠìłľìĿ¼&#34;:1642,&#34;Ġëıħ&#34;:1643,&#34;ëħĦëĮĢ&#34;:1644,&#34;ĠìķĦëĭĪê³ł&#34;:1645,&#34;Ġë¸&#34;:1646,&#34;ĠìĤ¶&#34;:1647,&#34;ëĤľëĭ¤&#34;:1648,&#34;ĠìķĦê¹Įìļ´&#34;:1649,&#34;ìĹĨìĿĮ&#34;:1650,&#34;ë³Ħë¡ľ&#34;:1651,&#34;ìģľ&#34;:1652,&#34;ìºĲ&#34;:1653,&#34;ì§Ģê¸Ī&#34;:1654,&#34;ĠìķĬëĬĶ&#34;:1655,&#34;ĠëĮĢë°ķ&#34;:1656,&#34;ë³´ë©´&#34;:1657,&#34;ĠìłľëĮĢë¡ľ&#34;:1658,&#34;Ġê·¼ëį°&#34;:1659,&#34;ĠìĹ¬ë&#34;:1660,&#34;ì¹ĺëĬĶ&#34;:1661,&#34;©´&#34;:1662,&#34;ê¿&#34;:1663,&#34;ìį¨&#34;:1664,&#34;ãħīãħī&#34;:1665,&#34;ì§Ĳ&#34;:1666,&#34;íķ´ëıĦ&#34;:1667,&#34;Įį&#34;:1668,&#34;Ġê½&#34;:1669,&#34;Ġìŀł&#34;:1670,&#34;Ġìĸ´ëĶĶ&#34;:1671,&#34;Ġãħ&#34;:1672,&#34;ĠìŀĬ&#34;:1673,&#34;ĠíĥĢ&#34;:1674,&#34;ĠìĶ&#34;:1675,&#34;íĮ¨&#34;:1676,&#34;ë¹Ļ&#34;:1677,&#34;Ġë³´ëĭ¤&#34;:1678,&#34;ĠëķĮë¬¸ìĹĲ&#34;:1679,&#34;Ġìŀ¬ë°ĭ&#34;:1680,&#34;ìĹ¬ìŀĲ&#34;:1681,&#34;íıīìłĲìĿ´&#34;:1682,&#34;ĠìĨĮìŀ¬&#34;:1683,&#34;Ġëª©&#34;:1684,&#34;êº¼&#34;:1685,&#34;ìĹ¬ë&#34;:1686,&#34;ìĺ¬&#34;:1687,&#34;íķłìĪĺ&#34;:1688,&#34;ĠìĿ´ìľł&#34;:1689,&#34;íĪ¬&#34;:1690,&#34;·¨&#34;:1691,&#34;ìĨĶì§ģíŀĪ&#34;:1692,&#34;Ġì¡°ê¸Ī&#34;:1693,&#34;ì¸&#34;:1694,&#34;ìĻķ&#34;:1695,&#34;Ġë©Ķ&#34;:1696,&#34;ĠëĤł&#34;:1697,&#34;ìĹĲìļĶ&#34;:1698,&#34;Ġì§Ī&#34;:1699,&#34;íģ´&#34;:1700,&#34;ëĤĺê³ł&#34;:1701,&#34;ëĤĺëıĦ&#34;:1702,&#34;ìµľìķħ&#34;:1703,&#34;ĠìĺģíĻĶëĿ¼ê³ł&#34;:1704,&#34;ë£¡&#34;:1705,&#34;ĠëĤ´ìļ©ìĿ´&#34;:1706,&#34;ĠëĲł&#34;:1707,&#34;ìĭ¶ëĭ¤&#34;:1708,&#34;ë²½&#34;:1709,&#34;ê°Ī&#34;:1710,&#34;ë¦¬ë¥¼&#34;:1711,&#34;ì§Ħëĭ¤&#34;:1712,&#34;~~~~&#34;:1713,&#34;Ġìĭ¶ìĿĢ&#34;:1714,&#34;ĵ¸&#34;:1715,&#34;ìĿ´ëłĩê²Į&#34;:1716,&#34;Ġë³´ê²Į&#34;:1717,&#34;ìķĺëĬĶëį°&#34;:1718,&#34;ë§Īëĭ¤&#34;:1719,&#34;Ġì¹´&#34;:1720,&#34;ëĤĺë¦¬ìĺ¤&#34;:1721,&#34;¬ë§ģ&#34;:1722,&#34;Ġë²ł&#34;:1723,&#34;Ĳëĭ¤&#34;:1724,&#34;ëĪĦ&#34;:1725,&#34;ìĿ´ëĦ¤&#34;:1726,&#34;ê¸°ë¥¼&#34;:1727,&#34;ĠëĤĺìĻĢ&#34;:1728,&#34;ìłģìĿ´ê³ł&#34;:1729,&#34;Ġëª¨ë¥´ê²ł&#34;:1730,&#34;íķĺëĬĶëį°&#34;:1731,&#34;êµ°ìļĶ&#34;:1732,&#34;ê¶Į&#34;:1733,&#34;ëıĮ&#34;:1734,&#34;ìĬ¤íĮħ&#34;:1735,&#34;ĠëĬĲëģ¼&#34;:1736,&#34;Ġë°°ìļ°ëĵ¤&#34;:1737,&#34;Ġê¸´ìŀ¥ê°Ĳ&#34;:1738,&#34;ê¾¸&#34;:1739,&#34;ìĿ´ìķ¼&#34;:1740,&#34;Ġìķŀ&#34;:1741,&#34;ëĤĺìļĶ&#34;:1742,&#34;ëŀľë§ĮìĹĲ&#34;:1743,&#34;ĠìĤ´ìķĦ&#34;:1744,&#34;Ġìŀ¬ë°Įëĭ¤&#34;:1745,&#34;ê³¤&#34;:1746,&#34;ìłĲìĿĢ&#34;:1747,&#34;ĠìĺģíĻĶìŀħëĭĪëĭ¤&#34;:1748,&#34;Ġíĺķ&#34;:1749,&#34;ìĬ¤ëŁ¬ìļ´&#34;:1750,&#34;ĠíĨµ&#34;:1751,&#34;ê²ģ&#34;:1752,&#34;ìĺģíĻĶìĿĺ&#34;:1753,&#34;ë°ĸìĹĲ&#34;:1754,&#34;ĠíĬ¹íŀĪ&#34;:1755,&#34;ĠìĺģíĻĶë¡ľ&#34;:1756,&#34;ë¸Ķ&#34;:1757,&#34;êµ´&#34;:1758,&#34;Ġìĭ¶ëĭ¤&#34;:1759,&#34;ĠìĿĺë¯¸&#34;:1760,&#34;ĠìĺģíĻĶìĺĢ&#34;:1761,&#34;ì¤ĺ&#34;:1762,&#34;ìĹĪìĬµëĭĪëĭ¤&#34;:1763,&#34;Ġìłķìĭł&#34;:1764,&#34;ìľ¼ë¡ľëıĦ&#34;:1765,&#34;Ġêµ¿&#34;:1766,&#34;Ġë§īìŀ¥&#34;:1767,&#34;Ľ°&#34;:1768,&#34;ìłķìĿ´&#34;:1769,&#34;ĠëĮĢìĤ¬&#34;:1770,&#34;Ġì£¼ëĬĶ&#34;:1771,&#34;Ġìĭ«&#34;:1772,&#34;Ġìĭľìŀĳ&#34;:1773,&#34;ìļ±&#34;:1774,&#34;ĠìłĪëĮĢ&#34;:1775,&#34;Ġì¢ĭê³ł&#34;:1776,&#34;Ġìłľìŀĳ&#34;:1777,&#34;âĻ¥âĻ¥&#34;:1778,&#34;ìķĦëĭ&#34;:1779,&#34;ê·Ģ&#34;:1780,&#34;ĠëĤ´ìļ©ëıĦ&#34;:1781,&#34;Ġëħ¸ëŀĺ&#34;:1782,&#34;Ġëį°&#34;:1783,&#34;ĠíĽĮë¥Ń&#34;:1784,&#34;Ġì²ľ&#34;:1785,&#34;Ġìĸ´ì©&#34;:1786,&#34;Ġìķ¼&#34;:1787,&#34;Ġê°ģ&#34;:1788,&#34;ëĦĪ&#34;:1789,&#34;ĠëĴ¤&#34;:1790,&#34;ĠëĲĺëĬĶ&#34;:1791,&#34;ĠìķĦë¦Ħëĭ¤ìļ´&#34;:1792,&#34;ëįķ&#34;:1793,&#34;ëł¨&#34;:1794,&#34;íķ´ìķ¼&#34;:1795,&#34;ĠëĤ¨ëĬĶ&#34;:1796,&#34;Ġ^^&#34;:1797,&#34;ĠíĽ¨&#34;:1798,&#34;ĠìĽĥê¸°&#34;:1799,&#34;íĬ¼&#34;:1800,&#34;ë¯¹&#34;:1801,&#34;ì§Ģê°Ģ&#34;:1802,&#34;ì§¸&#34;:1803,&#34;Ġë¶Ģë¶Ħ&#34;:1804,&#34;ëĨĪ&#34;:1805,&#34;ëĭĪë©ĶìĿ´&#34;:1806,&#34;Ġì¶ľìĹ°&#34;:1807,&#34;ĠìĪĺìŀĳ&#34;:1808,&#34;Ġì´Ī&#34;:1809,&#34;ĥĲ&#34;:1810,&#34;Ġë¦¬&#34;:1811,&#34;ëĤ¸&#34;:1812,&#34;ì¢ĭìķĦ&#34;:1813,&#34;ìĿ¸ìłģìľ¼ë¡ľ&#34;:1814,&#34;ĪëĶ&#34;:1815,&#34;ì¤Ģëĭ¤&#34;:1816,&#34;ëĭĪë©ĶìĿ´ìħĺ&#34;:1817,&#34;íķ©&#34;:1818,&#34;Ġëľ&#34;:1819,&#34;ìĭ¶ìĿĢ&#34;:1820,&#34;íľ´&#34;:1821,&#34;ëĤĺê²Į&#34;:1822,&#34;Ġë³µ&#34;:1823,&#34;ĠìĹĨìĿĮ&#34;:1824,&#34;???&#34;:1825,&#34;ë§ĪëĤĺ&#34;:1826,&#34;Ġì¤ĳê°Ħ&#34;:1827,&#34;ìĿ¸ìĿ´&#34;:1828,&#34;ë²ķ&#34;:1829,&#34;Ġãħł&#34;:1830,&#34;ìĬ¬&#34;:1831,&#34;ĠëłĪ&#34;:1832,&#34;Ģìĸ´&#34;:1833,&#34;ìĽĲìŀĳ&#34;:1834,&#34;íķľëį°&#34;:1835,&#34;Ġë´¤ìĬµëĭĪëĭ¤&#34;:1836,&#34;Ġë¹¼&#34;:1837,&#34;ĠëĶ°ëľ»&#34;:1838,&#34;ëŃĶ&#34;:1839,&#34;ì¦Į&#34;:1840,&#34;;;;&#34;:1841,&#34;_-&#34;:1842,&#34;ì¯&#34;:1843,&#34;Ġë´¤ëįĺ&#34;:1844,&#34;Ġë¡ľë§¨&#34;:1845,&#34;´Ĳ&#34;:1846,&#34;ìĮį&#34;:1847,&#34;ë°±&#34;:1848,&#34;ëĵĿ&#34;:1849,&#34;Ġìĭľë¦¬ì¦Ī&#34;:1850,&#34;íļ¨&#34;:1851,&#34;Ġ20&#34;:1852,&#34;ì§ĵ&#34;:1853,&#34;ìĿ¸ìĥĿ&#34;:1854,&#34;ìĦ±ëıĦ&#34;:1855,&#34;ê²ĥìĿ´&#34;:1856,&#34;ì¹ĺê³ł&#34;:1857,&#34;íħĲ&#34;:1858,&#34;ìĥĪ&#34;:1859,&#34;Ġë¬´ìĦľ&#34;:1860,&#34;Ġì¢ĭìķĺëĭ¤&#34;:1861,&#34;ìŀĳíĴĪ&#34;:1862,&#34;Ġë¬´ìĹĩ&#34;:1863,&#34;ê´ĳ&#34;:1864,&#34;ĠëĶĶ&#34;:1865,&#34;ìķĦê¹Ŀ&#34;:1866,&#34;Ġìĸ´ìĦ¤&#34;:1867,&#34;ĠìķĮë°Ķ&#34;:1868,&#34;ëĭ¥&#34;:1869,&#34;íķľê±°&#34;:1870,&#34;Ġì¢ĭìķĦíķĺëĬĶ&#34;:1871,&#34;íĹĺ&#34;:1872,&#34;ĠìĤ¼&#34;:1873,&#34;Ġìŀ¬ë¯¸ëıĦ&#34;:1874,&#34;Ġíĸīë³µ&#34;:1875,&#34;Ġë³´ëĭ¤ê°Ģ&#34;:1876,&#34;ì§Ŀ&#34;:1877,&#34;ìŀĸ&#34;:1878,&#34;ìĿ¼ë³¸&#34;:1879,&#34;ĠêµŃ&#34;:1880,&#34;oo&#34;:1881,&#34;ë©ĭ&#34;:1882,&#34;Ġìŀ¬ë¯¸ìĹĨëĭ¤&#34;:1883,&#34;ĠìĿ´íķ´ê°Ģ&#34;:1884,&#34;ĠìĤ°&#34;:1885,&#34;íĶĮ&#34;:1886,&#34;ìĹ´&#34;:1887,&#34;Ġê¹Ĭ&#34;:1888,&#34;ê²¬&#34;:1889,&#34;ĠìĿ´ê±¸&#34;:1890,&#34;ĠìĥĿê°ģìĿ´&#34;:1891,&#34;¬ë§ģíĥĢìŀĦ&#34;:1892,&#34;ì¶©&#34;:1893,&#34;ĠíĮĲ&#34;:1894,&#34;ëĶĶìĺ¤&#34;:1895,&#34;ìłĲìĿĦ&#34;:1896,&#34;Ġì§Ħìĭ¬&#34;:1897,&#34;ìķĦë¬´&#34;:1898,&#34;Ġë³Ģ&#34;:1899,&#34;ìĿ´ëĦ¤ìļĶ&#34;:1900,&#34;ìķĦê¹Į&#34;:1901,&#34;ë¶Ģë¶Ħ&#34;:1902,&#34;Ġìĭľê°ĦìĿ´&#34;:1903,&#34;ìļķ&#34;:1904,&#34;êµ¬ë§Į&#34;:1905,&#34;ĠíĻį&#34;:1906,&#34;,,,&#34;:1907,&#34;ĠìĿ¸ìĥģ&#34;:1908,&#34;ĠìĬ¤íĨłë¦¬ëıĦ&#34;:1909,&#34;ë¦¬ì§Ģ&#34;:1910,&#34;ĠëŃĺ&#34;:1911,&#34;ìĤ¼&#34;:1912,&#34;Ġìĸ´ìĥī&#34;:1913,&#34;Ġë¬´ìĦŃ&#34;:1914,&#34;ĠìłĲìĪĺ&#34;:1915,&#34;ĠìĹĨìĿ´&#34;:1916,&#34;ë§Ľ&#34;:1917,&#34;ëıĻìķĪ&#34;:1918,&#34;ìĹ½&#34;:1919,&#34;Ġíŀĺëĵ¤&#34;:1920,&#34;ìłĲëĮĢ&#34;:1921,&#34;Ġìŀĳê°Ģ&#34;:1922,&#34;Ġëĵ¤ìĸ´&#34;:1923,&#34;ĠìķĦì§ģëıĦ&#34;:1924,&#34;ĽëĤł&#34;:1925,&#34;ë²ł&#34;:1926,&#34;ĠìĬ¤íĨłë¦¬ê°Ģ&#34;:1927,&#34;Ġì¼&#34;:1928,&#34;ëŁ½ê²Į&#34;:1929,&#34;Ġëª»íķľ&#34;:1930,&#34;Ġíķ¨ê»ĺ&#34;:1931,&#34;ĠìĿ´ìĺģíĻĶ&#34;:1932,&#34;ìļ°ë¦¬&#34;:1933,&#34;Ġìłľëª©&#34;:1934,&#34;ĠìķłëĭĪ&#34;:1935,&#34;ëģĶ&#34;:1936,&#34;ëĤ¨ìŀĲ&#34;:1937,&#34;Ġê°ĳ&#34;:1938,&#34;ì¶ĺ&#34;:1939,&#34;ìĿĦëĵ¯&#34;:1940,&#34;Ġìŀ¡&#34;:1941,&#34;Ġê·¸ëŁ¬&#34;:1942,&#34;ĠìĹ¬ìļ´ìĿ´&#34;:1943,&#34;ĠìķĪë³´&#34;:1944,&#34;ĠíĻķ&#34;:1945,&#34;Ł¬ë&#34;:1946,&#34;ĠëĦ¤&#34;:1947,&#34;ĠìĤ¬ìĭ¤&#34;:1948,&#34;Ġëĳĺ&#34;:1949,&#34;Ġê¸¸&#34;:1950,&#34;Ġìĭľì²Ń&#34;:1951,&#34;Ġê¶ģ&#34;:1952,&#34;ĠìĨĮë¦Ħ&#34;:1953,&#34;ì¹¨&#34;:1954,&#34;Ġëĭµëĭµ&#34;:1955,&#34;ëħ¸ìŀ¼&#34;:1956,&#34;ĠìŀĲìĭł&#34;:1957,&#34;ëĵ¤ìĹĲê²Į&#34;:1958,&#34;Ġë³´ë©´ìĦľ&#34;:1959,&#34;Ġê°ĲëıħìĿĺ&#34;:1960,&#34;ìħ¨&#34;:1961,&#34;Ġìī&#34;:1962,&#34;Ġë¬¸ìłľ&#34;:1963,&#34;~!&#34;:1964,&#34;Ńë¹Ħ&#34;:1965,&#34;Ġì°¾ìķĦ&#34;:1966,&#34;ì¿&#34;:1967,&#34;Ġë¿Ĳ&#34;:1968,&#34;ĠëĽ°&#34;:1969,&#34;ìĿ´ì§Ģë§Į&#34;:1970,&#34;ëĺĲ&#34;:1971,&#34;ĠìĭłìĦł&#34;:1972,&#34;ìĿĮìķħ&#34;:1973,&#34;ìĽĶ&#34;:1974,&#34;íĺľ&#34;:1975,&#34;Ġë©ĭì§Ħ&#34;:1976,&#34;ìĿ´ìķ¼ê¸°&#34;:1977,&#34;Ġê¹¨&#34;:1978,&#34;Ġíĭ°&#34;:1979,&#34;ê±°ë¦¬&#34;:1980,&#34;ê±´ê°Ģ&#34;:1981,&#34;Ġíķĺì§Ģ&#34;:1982,&#34;Ġìĸ´ìĿ´&#34;:1983,&#34;Ġê´ľì°®ìĿĢ&#34;:1984,&#34;ê²¼&#34;:1985,&#34;ìŀĲê¸°&#34;:1986,&#34;ĠëĤĺë¦Ħ&#34;:1987,&#34;ëįĶëĭĪ&#34;:1988,&#34;ĪëįĶ&#34;:1989,&#34;Ġ6&#34;:1990,&#34;íķĺìĭľ&#34;:1991,&#34;Ġê°ĲëıħìĿ´&#34;:1992,&#34;Īë³´&#34;:1993,&#34;Ġë¹¨&#34;:1994,&#34;ìĬ·&#34;:1995,&#34;Ġë¶ĦìľĦ&#34;:1996,&#34;ĠíĹĪìłĳ&#34;:1997,&#34;ĠìĬ¤ë¦´ëŁ¬&#34;:1998,&#34;ĠìĹĶ&#34;:1999,&#34;ĠëĤļ&#34;:2000,&#34;ëĿ¼ìĿ´&#34;:2001,&#34;ìĨĮë¦¬&#34;:2002,&#34;ëĭ¤ë¥¸&#34;:2003,&#34;Īëģ&#34;:2004,&#34;êµ¬ìļĶ&#34;:2005,&#34;Ġê´Ģê°Ŀ&#34;:2006,&#34;ê²Ł&#34;:2007,&#34;ì£¼ìĿĺ&#34;:2008,&#34;Ġìļ©&#34;:2009,&#34;ëĤĺë§Ī&#34;:2010,&#34;ĠìĦ¸ìĥģ&#34;:2011,&#34;ĠC&#34;:2012,&#34;ì£Ħ&#34;:2013,&#34;Ġë³´ìĦ¸ìļĶ&#34;:2014,&#34;Ġë¯¸êµŃ&#34;:2015,&#34;ĠìķĪíĥĢ&#34;:2016,&#34;ĠìŀĶìŀĶ&#34;:2017,&#34;Ġìŀ¼ìŀĪ&#34;:2018,&#34;ëª°&#34;:2019,&#34;âĻ¡&#34;:2020,&#34;Ġíļ&#34;:2021,&#34;Ġ &#34; &#34; &#34; &#34;&#34;:2022,&#34;ëĳ&#34;:2023,&#34;Ġìŀ¬ë¯¸ê°Ģ&#34;:2024,&#34;Ġë¦¬ë&#34;:2025,&#34;ĠìĭľëĤĺë¦¬ìĺ¤&#34;:2026,&#34;Ġíģ°&#34;:2027,&#34;ìľ¤&#34;:2028,&#34;ì¤ĳê°Ħ&#34;:2029,&#34;Ġë§ĺ&#34;:2030,&#34;ë§Įëĵł&#34;:2031,&#34;Ġì¡¸ìŀĳ&#34;:2032,&#34;Īë²&#34;:2033,&#34;ĠíĿĲ&#34;:2034,&#34;ĠíĻ©&#34;:2035,&#34;ì»¤&#34;:2036,&#34;íķľìĺģíĻĶ&#34;:2037,&#34;íķĺë©°&#34;:2038,&#34;íķ¨ìĿĦ&#34;:2039,&#34;Ġ0&#34;:2040,&#34;ĠìļĶì¦ĺ&#34;:2041,&#34;ìłĦê°ľ&#34;:2042,&#34;ì¡¸&#34;:2043,&#34;ĠìĹ°ê¸°ëĬĶ&#34;:2044,&#34;ë¹Ī&#34;:2045,&#34;ì±ħ&#34;:2046,&#34;ë§ĪëĿ¼&#34;:2047,&#34;Ġê¸Ģ&#34;:2048,&#34;íĥĿ&#34;:2049,&#34;ê°ĻìĿ´&#34;:2050,&#34;Ġì°¨ëĿ¼ë¦¬&#34;:2051,&#34;ĠìŀĪìĹĪ&#34;:2052,&#34;Ġë§ĮíĻĶ&#34;:2053,&#34;ì°¬&#34;:2054,&#34;Ġìĸ¼ë§ĪëĤĺ&#34;:2055,&#34;ĠëıĮìķĦ&#34;:2056,&#34;ĠëĤĺëĬĶ&#34;:2057,&#34;ëłĪìĿ´&#34;:2058,&#34;Ġë°°ê²½&#34;:2059,&#34;ĠìĬ¬íĶĦ&#34;:2060,&#34;Ġìĸ¸ìłľ&#34;:2061,&#34;Ġê°ķì¶Ķ&#34;:2062,&#34;Ġì²¨&#34;:2063,&#34;ë§ĪëĶĶ&#34;:2064,&#34;ĠíĻĺ&#34;:2065,&#34;ĠíĽĦíļĮ&#34;:2066,&#34;Ġì©&#34;:2067,&#34;ì²Ļ&#34;:2068,&#34;ĠëĬĲëĤĮìĿ´&#34;:2069,&#34;Ġì¼Ģ&#34;:2070,&#34;ê¸¸ëŀĺ&#34;:2071,&#34;ĠìĻĦë²½&#34;:2072,&#34;ĠëĤ«&#34;:2073,&#34;ë³´ëĭ¨&#34;:2074,&#34;Ġíĺ¼&#34;:2075,&#34;!!!!!!!!&#34;:2076,&#34;Ġìĸ´ëĸ¤&#34;:2077,&#34;ĠìķĦë¦Ħëĭµ&#34;:2078,&#34;ê°Ģì§Ģ&#34;:2079,&#34;ëĮĢíķľ&#34;:2080,&#34;Ġê·ĢìĹ¬&#34;:2081,&#34;st&#34;:2082,&#34;Ġëª¸&#34;:2083,&#34;íĺĦìĭ¤&#34;:2084,&#34;ì±Ħ&#34;:2085,&#34;íĭ±&#34;:2086,&#34;ëĭ¹íŀĪ&#34;:2087,&#34;ĠìĺģíĻĶëĿ¼&#34;:2088,&#34;ëŀĦ&#34;:2089,&#34;ĠìķĬìķĺ&#34;:2090,&#34;Ġìĸĳ&#34;:2091,&#34;ê¹Ĭ&#34;:2092,&#34;ë³´ëĭĪ&#34;:2093,&#34;ëªĩ&#34;:2094,&#34;íĸĪìĿĮ&#34;:2095,&#34;Ġê·¸ìłĢ&#34;:2096,&#34;ìĻĦ&#34;:2097,&#34;ĠìĤ¬ëŀĮìĿ´&#34;:2098,&#34;íķĻêµĲ&#34;:2099,&#34;Ġë»Ķíķľ&#34;:2100,&#34;Ġì£¼ê³ł&#34;:2101,&#34;ĠìķĮê³ł&#34;:2102,&#34;ĠìĭĿ&#34;:2103,&#34;Ġê²ĥìĿ´&#34;:2104,&#34;ì¤Į&#34;:2105,&#34;Ġëĭ¤íģĲ&#34;:2106,&#34;ãħłãħłãħłãħł&#34;:2107,&#34;ĠìķłëĭĪë©ĶìĿ´ìħĺ&#34;:2108,&#34;ĠíĨł&#34;:2109,&#34;Ġë²Ķ&#34;:2110,&#34;ëĬĲëĤ&#34;:2111,&#34;ìŀĪê³ł&#34;:2112,&#34;íĥĪ&#34;:2113,&#34;ìķĪëĲĺ&#34;:2114,&#34;íħĲëį°&#34;:2115,&#34;ĠìŀĪê³ł&#34;:2116,&#34;ĠëĵľëĿ¼ë§Īë&#34;:2117,&#34;ìĿ¸ì¤Ħ&#34;:2118,&#34;Ġìŀ¬ë°ĮëĬĶ&#34;:2119,&#34;Ġìĸĺ&#34;:2120,&#34;ìĿ´ë²Ħ&#34;:2121,&#34;ĠìĺģíĻĶëıĦ&#34;:2122,&#34;ìľłì¹ĺ&#34;:2123,&#34;íıŃ&#34;:2124,&#34;Ġì´Ŀ&#34;:2125,&#34;Ġì²Ń&#34;:2126,&#34;ìŀ¬ë¯¸ìĹĨ&#34;:2127,&#34;Ġì¶Ķìĸµ&#34;:2128,&#34;Ġãħİ&#34;:2129,&#34;ĠìĽĥìĿĮ&#34;:2130,&#34;ìĽħ&#34;:2131,&#34;Ġëª¨ë¥´ê²łëĭ¤&#34;:2132,&#34;ĠíĿ¬&#34;:2133,&#34;Ġê¸°ìĸµìĹĲ&#34;:2134,&#34;ìĭ¸&#34;:2135,&#34;Ġìŀ¥ëĤľ&#34;:2136,&#34;ĠìĭľëĮĢ&#34;:2137,&#34;ĠìłĦìŁģ&#34;:2138,&#34;Ġê°ĻìĿ´&#34;:2139,&#34;Ġê·¸ëłĩê²Į&#34;:2140,&#34;ìĭľê¸¸&#34;:2141,&#34;ëªħìŀĳ&#34;:2142,&#34;Ġê¶ģê¸Ī&#34;:2143,&#34;ê°ĳ&#34;:2144,&#34;ê±°ì§Ģ&#34;:2145,&#34;ì¼ľ&#34;:2146,&#34;íı¬ë&#34;:2147,&#34;ìĽłëĭ¤&#34;:2148,&#34;ëĵ¤ê³¼&#34;:2149,&#34;Ġì¶©ë¶Ħ&#34;:2150,&#34;ëŁ´&#34;:2151,&#34;Ġíķľëĭ¤&#34;:2152,&#34;ëł¤ëĬĶ&#34;:2153,&#34;ĠìĹĶëĶ©&#34;:2154,&#34;Ġê°Ĳìłķ&#34;:2155,&#34;ê´ĢìĹĲìĦľ&#34;:2156,&#34;Ġê²°êµŃ&#34;:2157,&#34;ì½©&#34;:2158,&#34;ìĿ´ê±¸&#34;:2159,&#34;ì°į&#34;:2160,&#34;ë¬´ìĬ¨&#34;:2161,&#34;20&#34;:2162,&#34;Ġë´¤ìĸ´ìļĶ&#34;:2163,&#34;ê²ĥê°Ļ&#34;:2164,&#34;Ġê¹Į&#34;:2165,&#34;ëĭĿ&#34;:2166,&#34;ìĬ¤íĥĢ&#34;:2167,&#34;ì°°&#34;:2168,&#34;ë´Ħ&#34;:2169,&#34;ë¥´ëĬĶ&#34;:2170,&#34;ëł¥ìĿ´&#34;:2171,&#34;ĠìķĪëĲĺëĬĶ&#34;:2172,&#34;Ģìŀ¼&#34;:2173,&#34;ĠíĿĺ&#34;:2174,&#34;Ġì§Ģë£¨íķľ&#34;:2175,&#34;ìłķíķľ&#34;:2176,&#34;Ġë¹łìł¸&#34;:2177,&#34;ĠìĬ¤íĥĢ&#34;:2178,&#34;Ġì§ĳì¤ĳ&#34;:2179,&#34;Ġë¬¸&#34;:2180,&#34;Ġê¿Ī&#34;:2181,&#34;ìĿ¸ê°Ħ&#34;:2182,&#34;ìķ¼ì§Ģ&#34;:2183,&#34;ë²Ħë¦°&#34;:2184,&#34;Ġì£¼ìĹ°&#34;:2185,&#34;íķ¨ìĿ´&#34;:2186,&#34;Ġê°ľìĹ°&#34;:2187,&#34;ĠëĶ°ëĿ¼&#34;:2188,&#34;Ġìĸ´ë¦°&#34;:2189,&#34;ìłľëª©&#34;:2190,&#34;Ġë°°ìļ°ëĵ¤ìĿĺ&#34;:2191,&#34;°°ìļ°&#34;:2192,&#34;Ġíħ&#34;:2193,&#34;ëŁŃ&#34;:2194,&#34;ë°Ģ&#34;:2195,&#34;Ġì¢ħ&#34;:2196,&#34;ĪëĶ©&#34;:2197,&#34;íİĺ&#34;:2198,&#34;ê´´&#34;:2199,&#34;īìŀ¥&#34;:2200,&#34;Ġì£&#34;:2201,&#34;ê°Ķ&#34;:2202,&#34;ìĦ±ìĿĦ&#34;:2203,&#34;ĠíıŃ&#34;:2204,&#34;ëł¸ëĭ¤&#34;:2205,&#34;ĠíķĺëĤĺëıĦ&#34;:2206,&#34;íĻľ&#34;:2207,&#34;ĠìķĶ&#34;:2208,&#34;ĠìĺģíĻĶì¤ĳ&#34;:2209,&#34;Ġì¶©ê²©&#34;:2210,&#34;ë³Ģ&#34;:2211,&#34;ìŀ¬ë°Įê²Į&#34;:2212,&#34;ëĲł&#34;:2213,&#34;ĠìĺģíĻĶëĦ¤ìļĶ&#34;:2214,&#34;ĠíķĻ&#34;:2215,&#34;ĠíĶĦ&#34;:2216,&#34;íĸĪì§Ģë§Į&#34;:2217,&#34;ĠíĪ&#34;:2218,&#34;ëģĦ&#34;:2219,&#34;ĠìĺĪìĪł&#34;:2220,&#34;ë¡Ń&#34;:2221,&#34;íİ¸ìĿ´&#34;:2222,&#34;ĠìľĦíķ´&#34;:2223,&#34;ĠB&#34;:2224,&#34;ĠëģĮ&#34;:2225,&#34;ëĨĵê³ł&#34;:2226,&#34;ìĪĺì¤Ģ&#34;:2227,&#34;Ġì½Ķë¯¹&#34;:2228,&#34;ëĮĢë°ķ&#34;:2229,&#34;ìŀĲì²´&#34;:2230,&#34;Ġt&#34;:2231,&#34;Ġãħľãħľ&#34;:2232,&#34;ĠìķĬëĬĶëĭ¤&#34;:2233,&#34;ìĺģìĥģ&#34;:2234,&#34;Ġì§Ħë¶Ģ&#34;:2235,&#34;Ġìŀ¬ë°Įìĸ´ìļĶ&#34;:2236,&#34;íķĺíķĺ&#34;:2237,&#34;ĠìŀĦ&#34;:2238,&#34;ĠíĦ°&#34;:2239,&#34;......&#34;:2240,&#34;ĠëĽ°ìĸ´&#34;:2241,&#34;ìĭľëĮĢ&#34;:2242,&#34;ĠìĬ¤íĨł&#34;:2243,&#34;Ġëıĭ&#34;:2244,&#34;????&#34;:2245,&#34;Įĵ&#34;:2246,&#34;ì¹ł&#34;:2247,&#34;ëĿ¼ë©´&#34;:2248,&#34;ĠìľĦíķľ&#34;:2249,&#34;ìĿ´ìĺģíĻĶ&#34;:2250,&#34;ê¸°ë§Į&#34;:2251,&#34;íķ´ì§ĢëĬĶ&#34;:2252,&#34;ìĪĺìŀĪ&#34;:2253,&#34;ê³µíı¬&#34;:2254,&#34;Ġë¯¼&#34;:2255,&#34;ĠìĿ´ìģĺ&#34;:2256,&#34;íŀĪë&#34;:2257,&#34;Ġì§Ģë£¨íķĺê³ł&#34;:2258,&#34;Ġì»¤&#34;:2259,&#34;ĠìĻľìĿ´&#34;:2260,&#34;ìĽĮìĦľ&#34;:2261,&#34;ëĲľëĭ¤&#34;:2262,&#34;ë¦¬ìĬ¤&#34;:2263,&#34;Ġíĥľ&#34;:2264,&#34;ĠìĹĨìĹĪëĭ¤&#34;:2265,&#34;ĠëĤĺëıĦ&#34;:2266,&#34;ìī¬&#34;:2267,&#34;ìķĦì£¼&#34;:2268,&#34;ë³´ëĭ¤ê°Ģ&#34;:2269,&#34;ĠíĮ¨&#34;:2270,&#34;ê°ĢìĦľ&#34;:2271,&#34;ìĿ´ëĿ¼ëıĦ&#34;:2272,&#34;Ġë¹ĦìĬ·&#34;:2273,&#34;ĵ´&#34;:2274,&#34;ëĭ¹ìĭľ&#34;:2275,&#34;Ġëª°ìŀħëıĦ&#34;:2276,&#34;ìħĶ&#34;:2277,&#34;ľë¡ľ&#34;:2278,&#34;íĻį&#34;:2279,&#34;ĠìĿ´ë¦Ħ&#34;:2280,&#34;ì²ł&#34;:2281,&#34;ĠìĿ½&#34;:2282,&#34;Ġì±ħ&#34;:2283,&#34;ìłł&#34;:2284,&#34;ê²ĥìĿĦ&#34;:2285,&#34;Ġë§Įëĵ¤ìĹĪ&#34;:2286,&#34;ê·¸ë¦¬ê³ł&#34;:2287,&#34;ĠìĨĶì§ģíŀĪ&#34;:2288,&#34;Ġë°±&#34;:2289,&#34;íĺ¹&#34;:2290,&#34;Ġê±į&#34;:2291,&#34;ĠíĥĦ&#34;:2292,&#34;Ġíģ´&#34;:2293,&#34;Ġ,&#34;:2294,&#34;ê·¸ëŁ°&#34;:2295,&#34;ëĵ¯íķľ&#34;:2296,&#34;ĠìĤ¬ëŀĮëĵ¤&#34;:2297,&#34;ìĺ´&#34;:2298,&#34;Ġì§Ģë£¨íķĺëĭ¤&#34;:2299,&#34;ĠìºĲìĬ¤íĮħ&#34;:2300,&#34;ë³´ê¸°&#34;:2301,&#34;ì½Ķë¯¸ëĶĶ&#34;:2302,&#34;íĸĩ&#34;:2303,&#34;Ġê´´&#34;:2304,&#34;ëĨĢ&#34;:2305,&#34;Ġì£¼ìłľ&#34;:2306,&#34;Ġìľłì¾Į&#34;:2307,&#34;ĠíĮ¬&#34;:2308,&#34;ĠíĽ¨ìĶ¬&#34;:2309,&#34;ìĿ´ìłľ&#34;:2310,&#34;Ġë¨¼&#34;:2311,&#34;ìĺ¬ë&#34;:2312,&#34;Ġê·¸ë§Į&#34;:2313,&#34;Ġìĸ´ëĬĲ&#34;:2314,&#34;Ġë¹ĦêµĲ&#34;:2315,&#34;ãĦ±&#34;:2316,&#34;Ġë¶ĪìĮį&#34;:2317,&#34;Ġìļ°ë¦¬ëĤĺëĿ¼&#34;:2318,&#34;ì°Į&#34;:2319,&#34;Ġ!&#34;:2320,&#34;ìĿ¸ìĿĺ&#34;:2321,&#34;ë°ĺìłĦ&#34;:2322,&#34;Ġìĸ¼êµ´&#34;:2323,&#34;Ġìĥģìĥģ&#34;:2324,&#34;ìĨĮìŀ¬&#34;:2325,&#34;Ġëĺĳ&#34;:2326,&#34;Ġê³µíı¬ìĺģíĻĶ&#34;:2327,&#34;ĠOOO&#34;:2328,&#34;ìĸ´ëĸ&#34;:2329,&#34;Ġë³ĳ&#34;:2330,&#34;ë³´ì§Ģ&#34;:2331,&#34;Ġë¶Īë&#34;:2332,&#34;ê²łì§Ģë§Į&#34;:2333,&#34;íĿ¥&#34;:2334,&#34;Ġê±¸ìŀĳ&#34;:2335,&#34;ĠíĶĦë¡ľ&#34;:2336,&#34;ĠëĪĪë¬¼ìĿ´&#34;:2337,&#34;ĠëįĶë¹Ļ&#34;:2338,&#34;Ġìĺ¤ëŀĺ&#34;:2339,&#34;ìŀĶìŀĶ&#34;:2340,&#34;ĠëıĦëĮĢì²´&#34;:2341,&#34;¬ëĭ¤&#34;:2342,&#34;ê¼Ń&#34;:2343,&#34;Ġìµľê³łëĭ¤&#34;:2344,&#34;ì§Ģì§Ģ&#34;:2345,&#34;Ġë§Īì§Ģë§īìĹĲ&#34;:2346,&#34;Ġì±Ħ&#34;:2347,&#34;Ġê²ĥìĿĦ&#34;:2348,&#34;Ġê°ĲëıĻëıĦ&#34;:2349,&#34;ìĭľíĤ¤&#34;:2350,&#34;ĠìĹ°ê¸°ë¥¼&#34;:2351,&#34;íĥĢì§Ģ&#34;:2352,&#34;ĠìķĮìķĺ&#34;:2353,&#34;Ġë²Ī&#34;:2354,&#34;ĠìĦ¤ìłķ&#34;:2355,&#34;íķľë²Ī&#34;:2356,&#34;Ġìĥī&#34;:2357,&#34;Ġë³´ìĹ¬ì£¼ëĬĶ&#34;:2358,&#34;ľìĹĲ&#34;:2359,&#34;ìľ¼ëĭĪ&#34;:2360,&#34;ìłķëıĦë¡ľ&#34;:2361,&#34;ìĿ´ìĹĪëĭ¤&#34;:2362,&#34;ë¥ł&#34;:2363,&#34;¬ë§ģíĥĢìŀĦìļ©&#34;:2364,&#34;ĠìŀĪìĿĦ&#34;:2365,&#34;Ġê¸°ìĸµìĿ´&#34;:2366,&#34;ìļĶì¦ĺ&#34;:2367,&#34;Ġë°ĸìĹĲ&#34;:2368,&#34;ĠìĹĨìĸ´&#34;:2369,&#34;ë£Į&#34;:2370,&#34;íĶ½&#34;:2371,&#34;ëĤĺìĺ¨&#34;:2372,&#34;ë´£&#34;:2373,&#34;ê²łì§Ģ&#34;:2374,&#34;ê·¹ìŀ¥&#34;:2375,&#34;Ġë§¤ìļ°&#34;:2376,&#34;ĠìĿ´ëĶ´&#34;:2377,&#34;Ġê°ĸ&#34;:2378,&#34;Ġê²ĥëıĦ&#34;:2379,&#34;ãħĩãħĩ&#34;:2380,&#34;ĠìĹīìĦ±&#34;:2381,&#34;ë§īìŀ¥&#34;:2382,&#34;ì¯¤&#34;:2383,&#34;ëĶ±&#34;:2384,&#34;ìĹ°ì¶ľ&#34;:2385,&#34;ĠëĬĲëĤĦ&#34;:2386,&#34;£¨&#34;:2387,&#34;ë»&#34;:2388,&#34;ìĬ¤ëŁ½ëĭ¤&#34;:2389,&#34;íķĦìļĶ&#34;:2390,&#34;ood&#34;:2391,&#34;ê°ĢìĬ´&#34;:2392,&#34;ë³´ë©´ìĦľ&#34;:2393,&#34;ìĬ¤ë¦´&#34;:2394,&#34;ì§Ħìĭ¬&#34;:2395,&#34;ìĹĶëĶ©&#34;:2396,&#34;ìĬ¤ê°Ģ&#34;:2397,&#34;ë§ĪìĿĮ&#34;:2398,&#34;ìĿ´ëĿ¼ë©´&#34;:2399,&#34;ĠìķĮê²Į&#34;:2400,&#34;Ġê°ĲëıĻìłģìĿ´&#34;:2401,&#34;ë¨¼&#34;:2402,&#34;íģ¬ë&#34;:2403,&#34;Ġì¢ĭìķĦìļĶ&#34;:2404,&#34;ĠìĪľìĪĺ&#34;:2405,&#34;ê½&#34;:2406,&#34;ëŀĲ&#34;:2407,&#34;ĠìĥģíĻ©&#34;:2408,&#34;ĩ¼&#34;:2409,&#34;ì§ķ&#34;:2410,&#34;ãħħ&#34;:2411,&#34;ìĭľìłĪ&#34;:2412,&#34;ë§Ĳê³ł&#34;:2413,&#34;Ġê´ĳ&#34;:2414,&#34;Ġë§Ľ&#34;:2415,&#34;ĠëĮĢíķ´&#34;:2416,&#34;Ġìŀ¬ë°ĮìĿĮ&#34;:2417,&#34;ê°ľëıĦ&#34;:2418,&#34;ëħĦëıĦ&#34;:2419,&#34;ë²Ħì§Ģ&#34;:2420,&#34;ĠëĨĵ&#34;:2421,&#34;ĠëĭĪ&#34;:2422,&#34;ë´¤ëĭ¤&#34;:2423,&#34;ë§İìĿĢ&#34;:2424,&#34;ì¸ł&#34;:2425,&#34;ĠìŀĶìĿ¸&#34;:2426,&#34;ĠìĺģíĻĶìĺĢëĭ¤&#34;:2427,&#34;ë¡łê°Ģ&#34;:2428,&#34;ìĸ´ë¦´&#34;:2429,&#34;ê¹Ĳ&#34;:2430,&#34;ê²Ĳ&#34;:2431,&#34;ìłģìĿ´ëĭ¤&#34;:2432,&#34;Ġë§ĲëıĦ&#34;:2433,&#34;Ġì§Ģë£¨íķ¨&#34;:2434,&#34;ĠíĽĦë°ĺ&#34;:2435,&#34;Ġê½¤&#34;:2436,&#34;´¤&#34;:2437,&#34;ìķĹ&#34;:2438,&#34;ì§ľì¦Ŀ&#34;:2439,&#34;Ġêµ°&#34;:2440,&#34;ë¹¨&#34;:2441,&#34;ĠìķĬëĭ¤&#34;:2442,&#34;íķĻìĥĿ&#34;:2443,&#34;ª½&#34;:2444,&#34;Ġëķ&#34;:2445,&#34;ìľ¼ë©´ìĦľ&#34;:2446,&#34;ìĺ¤ëŀľë§ĮìĹĲ&#34;:2447,&#34;íİ¸ìĿĢ&#34;:2448,&#34;ĠìĨĮìŀ¬ë&#34;:2449,&#34;ìĵ¸&#34;:2450,&#34;Ġì£Ħ&#34;:2451,&#34;ë³´ê²Į&#34;:2452,&#34;Ġëĭ¹ìĭľ&#34;:2453,&#34;Ġê·¹ìŀ¥ìĹĲìĦľ&#34;:2454,&#34;´ĲëıĦ&#34;:2455,&#34;ëı¼&#34;:2456,&#34;Ġê·¸ëĤĺë§Ī&#34;:2457,&#34;ìĹĪê³ł&#34;:2458,&#34;Ġê°ĻìķĦìļĶ&#34;:2459,&#34;íĤ¨&#34;:2460,&#34;Ġë¸Ķ&#34;:2461,&#34;Ġì¢ĭê²ł&#34;:2462,&#34;Ġë°Ķë¡ľ&#34;:2463,&#34;ëĪĪ&#34;:2464,&#34;TV&#34;:2465,&#34;ëŀį&#34;:2466,&#34;ê°ĲìĿ´&#34;:2467,&#34;Ġìłľë°ľ&#34;:2468,&#34;;;;;&#34;:2469,&#34;ê±°ê°Ļ&#34;:2470,&#34;ĠìĿ¸ë¬¼&#34;:2471,&#34;Ġê°ĪìĪĺë¡Ŀ&#34;:2472,&#34;Ġë³´ëĬĶëĤ´ëĤ´&#34;:2473,&#34;ì³¤&#34;:2474,&#34;ìķĦê¹Ŀëĭ¤&#34;:2475,&#34;ìŀĸìķĦ&#34;:2476,&#34;Ġë³´ìķĺ&#34;:2477,&#34;ĠìĹŃìĤ¬&#34;:2478,&#34;ê´ľì°®&#34;:2479,&#34;ëĴ¤&#34;:2480,&#34;ĠìĻł&#34;:2481,&#34;ê±´ëį°&#34;:2482,&#34;Ġì·¨&#34;:2483,&#34;Ġë³´ëĭĪ&#34;:2484,&#34;Ġë¯¸ì¹ľ&#34;:2485,&#34;Ġëĺ¥&#34;:2486,&#34;Ġê°ĲëıĻìĿ´&#34;:2487,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿ´&#34;:2488,&#34;er&#34;:2489,&#34;íķ´ëĿ¼&#34;:2490,&#34;ëħĦìĿ´&#34;:2491,&#34;ê³¡&#34;:2492,&#34;ìŀ¬ë°Įìĸ´ìļĶ&#34;:2493,&#34;Ġ~&#34;:2494,&#34;ìĨĲ&#34;:2495,&#34;ìŀ¬ë¯¸ìĹĨëĭ¤&#34;:2496,&#34;íĭĢ&#34;:2497,&#34;Ġo&#34;:2498,&#34;ìĪĺë¥¼&#34;:2499,&#34;ìĭłìĿ´&#34;:2500,&#34;Ġë¡ľë§¨ìĬ¤&#34;:2501,&#34;ê°Ģì¡±&#34;:2502,&#34;ĠìĿ´íĽĦ&#34;:2503,&#34;ì§ľë¦¬&#34;:2504,&#34;ìĹ°íŀĪ&#34;:2505,&#34;ĠìķĦëĭĺ&#34;:2506,&#34;ëª¨ë¥´&#34;:2507,&#34;ìŀĲê°Ģ&#34;:2508,&#34;īìŀ¥íŀĪ&#34;:2509,&#34;ĠìĺģíĻĶìŀĦ&#34;:2510,&#34;ëĵ¤ìĸ´&#34;:2511,&#34;ìŀ¬ë°Įëĭ¤&#34;:2512,&#34;ìķĮë°Ķ&#34;:2513,&#34;ëĤĺìĺ¤ëĬĶ&#34;:2514,&#34;Ġíķ´ìĦľ&#34;:2515,&#34;ê°Ģê³ł&#34;:2516,&#34;Ġë§ĺìĹĲ&#34;:2517,&#34;ìķĦëĭĮ&#34;:2518,&#34;íĦ&#34;:2519,&#34;ê±°ìķ¼&#34;:2520,&#34;ëł¤ìĦľ&#34;:2521,&#34;ĠíĸĪëĬĶëį°&#34;:2522,&#34;ìľ¨&#34;:2523,&#34;ê°ľë´ī&#34;:2524,&#34;Ġë§ĪìĿĮìĿ´&#34;:2525,&#34;ĵľ&#34;:2526,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:2527,&#34;Ġìķ½ê°Ħ&#34;:2528,&#34;Ġê°ĲìĤ¬&#34;:2529,&#34;ê·¸ëŀĺëıĦ&#34;:2530,&#34;ĠìĿĢ&#34;:2531,&#34;ì£¼ìĦ¸ìļĶ&#34;:2532,&#34;ĠëĦĪë¬´ëĤĺ&#34;:2533,&#34;ĠëĬĺ&#34;:2534,&#34;ìĶ©&#34;:2535,&#34;ëł¹&#34;:2536,&#34;êµ³&#34;:2537,&#34;íĴį&#34;:2538,&#34;íķĦ&#34;:2539,&#34;íķłëķĮ&#34;:2540,&#34;Ġìŀĺëª»&#34;:2541,&#34;ĠìķĪëĲ&#34;:2542,&#34;ì´Īë°ĺ&#34;:2543,&#34;Ġíıīê°Ģ&#34;:2544,&#34;ĠìĥĿê°ģìĿĦ&#34;:2545,&#34;Ġë²Į&#34;:2546,&#34;Ġëĭ¬&#34;:2547,&#34;ĠìłĦë¶Ģ&#34;:2548,&#34;Ġìĺ¤ê¸Ģ&#34;:2549,&#34;Ġìĺ¤ëĬĺ&#34;:2550,&#34;ì¡Įëĭ¤&#34;:2551,&#34;,.&#34;:2552,&#34;ìĹ¬ìĦľ&#34;:2553,&#34;Ġì¤ĳêµŃ&#34;:2554,&#34;Ġë°ľìĹ°ê¸°&#34;:2555,&#34;Ġì³&#34;:2556,&#34;ëĤ®&#34;:2557,&#34;Ġëĭ¤ìĿĮ&#34;:2558,&#34;ĠìĹ¬ê¸°&#34;:2559,&#34;ê²¹&#34;:2560,&#34;Ġê·¸ëĮĢë¡ľ&#34;:2561,&#34;Ġì²«&#34;:2562,&#34;Ġê·¸ëŀĺìĦľ&#34;:2563,&#34;ĠëĨĴìĿĢ&#34;:2564,&#34;ĠíĴį&#34;:2565,&#34;Ġë¸Į&#34;:2566,&#34;ëĬĲëĤĮ&#34;:2567,&#34;ëļ&#34;:2568,&#34;ê±°ëĤĺ&#34;:2569,&#34;ĠìŀĪìĿĦê¹Į&#34;:2570,&#34;Ġì²ł&#34;:2571,&#34;®¤&#34;:2572,&#34;ìĽĮìļĶ&#34;:2573,&#34;ĠìłķëıĦë¡ľ&#34;:2574,&#34;ìĪĻ&#34;:2575,&#34;ëĲ¨&#34;:2576,&#34;íĶĶ&#34;:2577,&#34;ë¦°ëĭ¤&#34;:2578,&#34;Ġì´Īë°ĺ&#34;:2579,&#34;Ġìĺģìĥģë¯¸&#34;:2580,&#34;Ġìĸ´ìļ¸&#34;:2581,&#34;ĠìķĦìī½ëĭ¤&#34;:2582,&#34;ĠíĪ¬&#34;:2583,&#34;íķĺìŀĲ&#34;:2584,&#34;ëħĲ&#34;:2585,&#34;Ġì§ĢëĤĺ&#34;:2586,&#34;ëŃĶê°Ģ&#34;:2587,&#34;Ġëĭ¤ë¥´&#34;:2588,&#34;ê°Ħëĭ¤&#34;:2589,&#34;ĠíĿĶ&#34;:2590,&#34;ë¨¸ëĭĪ&#34;:2591,&#34;Ġê°ĻìĿĢëį°&#34;:2592,&#34;Ńìĥģ&#34;:2593,&#34;ĠìĿ´ë¯¸&#34;:2594,&#34;ìķĦìķ¼&#34;:2595,&#34;ë§ĪìĦ¸ìļĶ&#34;:2596,&#34;ĠìŀĪì§Ģë§Į&#34;:2597,&#34;Ġê·ĢìĹ½&#34;:2598,&#34;ĠíĤ¤&#34;:2599,&#34;ê·¸ëŀ&#34;:2600,&#34;Ġë´ĲìĦľ&#34;:2601,&#34;Ĳë¦¬&#34;:2602,&#34;Ġë§Īë&#34;:2603,&#34;Ġê¸°ëĭ¤&#34;:2604,&#34;ê·¼ëį°&#34;:2605,&#34;ë¶Ħëĵ¤&#34;:2606,&#34;ìĭľê°ĦìĿ´&#34;:2607,&#34;ĠìĺģíĻĶìĿ¸ëį°&#34;:2608,&#34;ìĺģíĻĶìĹĲ&#34;:2609,&#34;ìĹĲìĦľëıĦ&#34;:2610,&#34;Ġìĸ¸&#34;:2611,&#34;ĠëĶ¸&#34;:2612,&#34;ĠCG&#34;:2613,&#34;ãħĤ&#34;:2614,&#34;ëĤŃë¹Ħ&#34;:2615,&#34;ëĿ¼ìĦľ&#34;:2616,&#34;ë³´ëĭ¤ëĬĶ&#34;:2617,&#34;ë´¤ìĬµëĭĪëĭ¤&#34;:2618,&#34;Ġëĵ±ìŀ¥&#34;:2619,&#34;ĠëĤĺìĻĢìĦľ&#34;:2620,&#34;-_-&#34;:2621,&#34;ëĤĺìĻĢ&#34;:2622,&#34;Ġìĺ¨&#34;:2623,&#34;Ġìŀ¥ë¥´&#34;:2624,&#34;ĠìķĪëĲľ&#34;:2625,&#34;ê°ĲëıħìĿ´&#34;:2626,&#34;ĠêµĲíĽĪ&#34;:2627,&#34;ĠëĮĢì²´&#34;:2628,&#34;ìķĶ&#34;:2629,&#34;Ġë´¤ì§Ģë§Į&#34;:2630,&#34;Įĵê¸Ģ&#34;:2631,&#34;ìĺ¥&#34;:2632,&#34;Ġíķĺëĭ¤&#34;:2633,&#34;ìŀĳê°Ģ&#34;:2634,&#34;Ġìŀ¬ë°Įê³ł&#34;:2635,&#34;Ġc&#34;:2636,&#34;ìĿĦìĪĺ&#34;:2637,&#34;ìĿĦíħĲëį°&#34;:2638,&#34;Ġì¢ĭìĿĢëį°&#34;:2639,&#34;ĠìķĦë¦Ħëĭ¤&#34;:2640,&#34;Ġãħīãħī&#34;:2641,&#34;ìĺģíĻĶëĭ¤&#34;:2642,&#34;ĠìĪ¨&#34;:2643,&#34;................&#34;:2644,&#34;¬ë¦¬ìĬ¤&#34;:2645,&#34;ìĤ¬ë¥¼&#34;:2646,&#34;ĠìłĦì²´&#34;:2647,&#34;Ġë³´ê³łìĭ¶&#34;:2648,&#34;ĠëĤ´ìļ©ìĿĢ&#34;:2649,&#34;ì·¨&#34;:2650,&#34;Ġìı&#34;:2651,&#34;ìĹī&#34;:2652,&#34;ìŀĪëĦ¤ìļĶ&#34;:2653,&#34;ìĹĪì§Ģë§Į&#34;:2654,&#34;Ġë³¼ìĪĺ&#34;:2655,&#34;Ġì¤Ģëĭ¤&#34;:2656,&#34;ĠìłĪ&#34;:2657,&#34;ìķĦë¦Ħ&#34;:2658,&#34;ìķĦì§ģ&#34;:2659,&#34;ĠìĤ¬íļĮ&#34;:2660,&#34;ãĦ´&#34;:2661,&#34;ë§ĪëĶĶë¡ľ&#34;:2662,&#34;Ġb&#34;:2663,&#34;ìķŀ&#34;:2664,&#34;ĠìĥĪë¡ľ&#34;:2665,&#34;Ġê°ĲëıĻìĿĦ&#34;:2666,&#34;íķĺëĥĲ&#34;:2667,&#34;ë©ĺ&#34;:2668,&#34;ìĽĥìĿĮ&#34;:2669,&#34;Ġì§ģ&#34;:2670,&#34;íĥķ&#34;:2671,&#34;Ġê°ľìĿ¸ìłģìľ¼ë¡ľ&#34;:2672,&#34;Ġë¹ĦëĶĶìĺ¤&#34;:2673,&#34;ìĿ´íķ´&#34;:2674,&#34;¬ë¦¬ë&#34;:2675,&#34;ì§Ģë¥¼&#34;:2676,&#34;ĪëĥĲ&#34;:2677,&#34;ê·ľ&#34;:2678,&#34;¸Į&#34;:2679,&#34;ìĺĢìĿĮ&#34;:2680,&#34;Ġë©į&#34;:2681,&#34;¥´&#34;:2682,&#34;âĺ&#34;:2683,&#34;ê³łëĬĶ&#34;:2684,&#34;Ġìŀĥ&#34;:2685,&#34;Ġìķķ&#34;:2686,&#34;ĠìĦ¹&#34;:2687,&#34;ë°°ìļ°ëĵ¤&#34;:2688,&#34;Ġì«&#34;:2689,&#34;Ġë´£&#34;:2690,&#34;ìĹĲìĦľëĬĶ&#34;:2691,&#34;ĠëįĶìļ±&#34;:2692,&#34;ĠíķľêµŃìĺģíĻĶ&#34;:2693,&#34;Ġì¡´ìŀ¬&#34;:2694,&#34;ìĭŃ&#34;:2695,&#34;Ġê±°ìĿĺ&#34;:2696,&#34;Ġêµ¬ìĦ±&#34;:2697,&#34;Ġ?&#34;:2698,&#34;ëŁī&#34;:2699,&#34;ëŀĳìĬ¤&#34;:2700,&#34;ĠìĻķ&#34;:2701,&#34;ëĭ¤ëĬĶê²Į&#34;:2702,&#34;Ġëĭ¤ìļ´&#34;:2703,&#34;ìĤ¬ê°Ģ&#34;:2704,&#34;Ġíĺ¼ìŀĲ&#34;:2705,&#34;¶Ģ&#34;:2706,&#34;ìĹĲìĦł&#34;:2707,&#34;ëĪĪë¬¼&#34;:2708,&#34;ĠS&#34;:2709,&#34;ë¡ľìļ´&#34;:2710,&#34;ìĭľë¦¬ì¦Ī&#34;:2711,&#34;Ġê°Ģì¹ĺ&#34;:2712,&#34;íĮ¬&#34;:2713,&#34;ìĭ¤ë§Ŀ&#34;:2714,&#34;Ġìŀ¬ë¯¸ìĹĨìĿĮ&#34;:2715,&#34;ì¶Ķì²ľ&#34;:2716,&#34;ìĺĪìļĶ&#34;:2717,&#34;ĠëĤ®ìĿĢ&#34;:2718,&#34;ĠëĪĦê°Ģ&#34;:2719,&#34;Ġíı¬ìĬ¤íĦ°&#34;:2720,&#34;ĩĮ&#34;:2721,&#34;ĠìŀĪìĸ´ìĦľ&#34;:2722,&#34;Ġìŀ¥ë©´ìĿ´&#34;:2723,&#34;Ġê°ĢìĬ´ìĿ´&#34;:2724,&#34;ìĿ´íĮħ&#34;:2725,&#34;íĭ´&#34;:2726,&#34;ëĨĶ&#34;:2727,&#34;ìĿ¼ìĿ´&#34;:2728,&#34;ĠíĬ¸&#34;:2729,&#34;ìĭľëĭ¤&#34;:2730,&#34;ìĪ¨&#34;:2731,&#34;ĠíĹĪë¬´&#34;:2732,&#34;Ġs&#34;:2733,&#34;ëĵ¬&#34;:2734,&#34;ìĤ¬íļĮ&#34;:2735,&#34;ĠìĹ¬ì£¼ìĿ¸ê³µ&#34;:2736,&#34;Ġì½&#34;:2737,&#34;ìĺ¤ëĬĺ&#34;:2738,&#34;Ġëĵ£&#34;:2739,&#34;ë§İìĿ´&#34;:2740,&#34;íĮĮìĿ´&#34;:2741,&#34;ë²Īì§¸&#34;:2742,&#34;ëıĦëĮĢì²´&#34;:2743,&#34;íķľê²Į&#34;:2744,&#34;ĠëĤĺìĺ¬&#34;:2745,&#34;Ġê°ĲëıĻìłģìĿ¸&#34;:2746,&#34;ĠìłĢëŁ°&#34;:2747,&#34;~!!&#34;:2748,&#34;ìĿ´íĬ¸&#34;:2749,&#34;Ġê·¸ë¦¬&#34;:2750,&#34;ĠìĥĿê°ģíķĺê²Į&#34;:2751,&#34;good&#34;:2752,&#34;ëĦĲ&#34;:2753,&#34;ìĨįìĹĲ&#34;:2754,&#34;Ġë§ĪìĿĮìĹĲ&#34;:2755,&#34;ĠìķĦë¬´ë¦¬&#34;:2756,&#34;ìĹĲê²Ĳ&#34;:2757,&#34;ĠìĿ´ê²ĥ&#34;:2758,&#34;ìĿ¸ëĭ¤&#34;:2759,&#34;ìĿ´ëĿ¼ëĭĪ&#34;:2760,&#34;ĠìĹĦë§Ī&#34;:2761,&#34;ĠBê¸ī&#34;:2762,&#34;Ġìĵ¸&#34;:2763,&#34;íĻķ&#34;:2764,&#34;Ġë°©ìĨ¡&#34;:2765,&#34;ĠìĿ´ìłķëıĦ&#34;:2766,&#34;ìĸ´ë¦°&#34;:2767,&#34;ì¢ĭëĭ¤&#34;:2768,&#34;ë©ĶëĶĶ&#34;:2769,&#34;ĠìŀĲì²´ê°Ģ&#34;:2770,&#34;ë§ĮíĻĶ&#34;:2771,&#34;ĠìŀĪìĸ´&#34;:2772,&#34;ê²°ë§Ĳ&#34;:2773,&#34;Ġëĭ¨ìĪľ&#34;:2774,&#34;ëłµ&#34;:2775,&#34;ĠìŀĪìĹĪëĭ¤&#34;:2776,&#34;ìķĺìĿĮ&#34;:2777,&#34;Ġë©ľë¡ľ&#34;:2778,&#34;Ġë´ĲëĿ¼&#34;:2779,&#34;Īëģ¼&#34;:2780,&#34;ìĹ¬ìļ´&#34;:2781,&#34;ìłĢíŀĪ&#34;:2782,&#34;,,,,&#34;:2783,&#34;ë³´ëĬĶëĤ´ëĤ´&#34;:2784,&#34;Ġìį¨&#34;:2785,&#34;ëĿ¼ëĭĪ&#34;:2786,&#34;ë¶ģ&#34;:2787,&#34;ĠìĦ¸ê³Ħ&#34;:2788,&#34;ìĿµ&#34;:2789,&#34;ê¸°ê³ł&#34;:2790,&#34;Ġë¹µ&#34;:2791,&#34;Ġìķłëĵ¤&#34;:2792,&#34;ĠìĺĪìģĺ&#34;:2793,&#34;Ġ30&#34;:2794,&#34;Ġãħ¡&#34;:2795,&#34;ê¸Īë&#34;:2796,&#34;Ġëĭ¤ëĵ¤&#34;:2797,&#34;Ġë´¤ìĿĮ&#34;:2798,&#34;Ġëª»íķĺëĬĶ&#34;:2799,&#34;ìĭ«&#34;:2800,&#34;ìĺģíĻĶì¤ĳ&#34;:2801,&#34;Ġë´Ħ&#34;:2802,&#34;ì½ľ&#34;:2803,&#34;ì¥&#34;:2804,&#34;ê¸°ìĸµ&#34;:2805,&#34;íĸĪìĸ´ìļĶ&#34;:2806,&#34;ëŁ¬ìļ´&#34;:2807,&#34;ëĤĺë¦Ħ&#34;:2808,&#34;Ġ100&#34;:2809,&#34;ĠìĺĪìĥģ&#34;:2810,&#34;ĠíĸĪëĭ¤&#34;:2811,&#34;ëĤ´ìļ©ìĿ´&#34;:2812,&#34;Ġê°ģë³¸&#34;:2813,&#34;íķĺëĭĪ&#34;:2814,&#34;ĠìŀĪëĬĶëį°&#34;:2815,&#34;ĠìĺĽëĤł&#34;:2816,&#34;Ġë§Įëĵľ&#34;:2817,&#34;ëħĦìłĦ&#34;:2818,&#34;Ġë¹Ľ&#34;:2819,&#34;ëĲĺê³ł&#34;:2820,&#34;Ġì¤ĳìļĶ&#34;:2821,&#34;íķĺì§ĢìķĬ&#34;:2822,&#34;¬ë¦°&#34;:2823,&#34;ĠíĻĶëł¤&#34;:2824,&#34;ì§Ģê²Į&#34;:2825,&#34;ìĸ´ê°Ģ&#34;:2826,&#34;Ġì°©&#34;:2827,&#34;ì°¨ëĿ¼ë¦¬&#34;:2828,&#34;Ġê·¸ëŁ°ì§Ģ&#34;:2829,&#34;ìĬ¤ëŁ½ê³ł&#34;:2830,&#34;Ġì¹ľêµ¬&#34;:2831,&#34;ë¦¬íĭ°&#34;:2832,&#34;ìĬ¤ìĿĺ&#34;:2833,&#34;ĠìĥĿê°ģíķ´&#34;:2834,&#34;Ġê°ľê·¸&#34;:2835,&#34;ĠëģĿëĤĺ&#34;:2836,&#34;ĠìŀĲìĹ°&#34;:2837,&#34;ìľłìĿĺ&#34;:2838,&#34;ĠìķĦëĭĲ&#34;:2839,&#34;·°&#34;:2840,&#34;Ġìŀ¬ë¯¸ìŀĪëĬĶ&#34;:2841,&#34;ĠìĤ¼ë¥ĺ&#34;:2842,&#34;Ġì³Ĳ&#34;:2843,&#34;ê³¨&#34;:2844,&#34;ëĵ¤ìķĦ&#34;:2845,&#34;ìļ°ëĵľ&#34;:2846,&#34;Ġê°Ģì§Ģê³ł&#34;:2847,&#34;ĠíıīìłĲìĿĦ&#34;:2848,&#34;ìĸµì§Ģ&#34;:2849,&#34;ĠìĬ¬íĶĪ&#34;:2850,&#34;Ġ(&#34;:2851,&#34;Ġíĺ¹&#34;:2852,&#34;Ġìĥģëĭ¹íŀĪ&#34;:2853,&#34;Ġëıĭë³´&#34;:2854,&#34;ëıĦë¡Ŀ&#34;:2855,&#34;ĠìĹĨìĸ´ìĦľ&#34;:2856,&#34;ĠìŀĲê¸°&#34;:2857,&#34;Ġê·¹ìŀ¥íĮĲ&#34;:2858,&#34;99&#34;:2859,&#34;ĠìĻĶ&#34;:2860,&#34;Ġë¶Īíİ¸&#34;:2861,&#34;Ġíĭ°ë¹Ħ&#34;:2862,&#34;ìĿ´ì½Ķ&#34;:2863,&#34;ìķĺëįĺ&#34;:2864,&#34;Ġíĸ¥&#34;:2865,&#34;Ġê²°ë§ĲìĿ´&#34;:2866,&#34;Ġìį&#34;:2867,&#34;¬ëį&#34;:2868,&#34;ì§Ħì§Ħ&#34;:2869,&#34;ìŀ¥ëĤľ&#34;:2870,&#34;ê°ľìĿ¸ìłģìľ¼ë¡ľ&#34;:2871,&#34;Ġë¹Ħíķ´&#34;:2872,&#34;Ġíķ´ëıĦ&#34;:2873,&#34;Ġë³´ìĹ¬ì£¼&#34;:2874,&#34;Ġ....&#34;:2875,&#34;íı¬ìĬ¤íĦ°&#34;:2876,&#34;Ġì¦Ĳê±°&#34;:2877,&#34;Ġìĸ´ìĦ¤íĶĪ&#34;:2878,&#34;ìĿĳ&#34;:2879,&#34;¬ëł¤&#34;:2880,&#34;ìĿ¸ë¬¼&#34;:2881,&#34;Ġìĵ°ëłĪê¸°ìĺģíĻĶ&#34;:2882,&#34;Ġì´ĪëĶ©&#34;:2883,&#34;Ġë¬ĺ&#34;:2884,&#34;ë§Īì§Ģë§īìĹĲ&#34;:2885,&#34;Ġê·¸ëŁ¬ëĤĺ&#34;:2886,&#34;ëıĭ&#34;:2887,&#34;íĸĪê³ł&#34;:2888,&#34;Ġêµ³&#34;:2889,&#34;Ġë²Ĺ&#34;:2890,&#34;ĠìĤ¬ëŀĮìĿĢ&#34;:2891,&#34;Ġíķ©ëĭĪëĭ¤&#34;:2892,&#34;ì¡°ê±´&#34;:2893,&#34;ĠíŀĪ&#34;:2894,&#34;ĠìķĦëĭĮê°Ģ&#34;:2895,&#34;in&#34;:2896,&#34;ë¯¼êµŃ&#34;:2897,&#34;Ġì¡´ëĤĺ&#34;:2898,&#34;ħëĭĪëĭ¤&#34;:2899,&#34;ìľ¼ë©°&#34;:2900,&#34;ìłĦìŁģ&#34;:2901,&#34;ëĭĺìĿĺ&#34;:2902,&#34;Ġìĭ¤íĻĶ&#34;:2903,&#34;ìĺģíĻĶë¡ľ&#34;:2904,&#34;Ġìĭ¤ìłľ&#34;:2905,&#34;Ġì¤ĳê°ĦìĹĲ&#34;:2906,&#34;íĺĳ&#34;:2907,&#34;ìĦ¸ìĥģ&#34;:2908,&#34;Ġìĺ¤ëŀľë§ĮìĹĲ&#34;:2909,&#34;ëĭ¹íķľ&#34;:2910,&#34;Ġìŀ¬ë¯¸ìĹĨëĬĶ&#34;:2911,&#34;Īë¡&#34;:2912,&#34;ëķĮë§Īëĭ¤&#34;:2913,&#34;Ġë°°ìļ°ëĵ¤ìĿ´&#34;:2914,&#34;íķĺì§ĢëıĦ&#34;:2915,&#34;Ġë³¼ë§Įíķľ&#34;:2916,&#34;ê±°ëĭ¤&#34;:2917,&#34;ëª¨ëĵł&#34;:2918,&#34;Ġëħ¸ìŀ¼&#34;:2919,&#34;ì¿ł&#34;:2920,&#34;Ġêº¼&#34;:2921,&#34;ìĸ´ëł¸&#34;:2922,&#34;ë¯¸êµŃ&#34;:2923,&#34;Ġê²ĥìĿĢ&#34;:2924,&#34;íĭ°ë¹Ħ&#34;:2925,&#34;ĠìķĦê¹ĮìĽĢ&#34;:2926,&#34;ĳ¸&#34;:2927,&#34;ìŀĪê²Į&#34;:2928,&#34;íŀĺ&#34;:2929,&#34;ìĿ¼ê¹Į&#34;:2930,&#34;ëįĺê°Ģ&#34;:2931,&#34;ë¹µ&#34;:2932,&#34;ë¦¿&#34;:2933,&#34;ĠìĤ¬ë&#34;:2934,&#34;ë´¤ìĸ´ìļĶ&#34;:2935,&#34;ë§Įìľ¼ë¡ľëıĦ&#34;:2936,&#34;ë¦¬ìĿĺ&#34;:2937,&#34;ë³¼ë§Į&#34;:2938,&#34;ëİ&#34;:2939,&#34;ìľ¡&#34;:2940,&#34;ê¹¨&#34;:2941,&#34;ëĵľëĿ¼ë§Īë&#34;:2942,&#34;ĠëĬ¥&#34;:2943,&#34;íŀĪëł¤&#34;:2944,&#34;ìķĦëıĦ&#34;:2945,&#34;ĠíĴĢìĸ´&#34;:2946,&#34;ë»Ķíķľ&#34;:2947,&#34;ĠìĺģíĻĶê´ĢìĹĲìĦľ&#34;:2948,&#34;íİ¸ìĿĺ&#34;:2949,&#34;ëķĲ&#34;:2950,&#34;ĠìĹ¬ë°°ìļ°&#34;:2951,&#34;ĠíĮĲíĥĢì§Ģ&#34;:2952,&#34;ê±į&#34;:2953,&#34;ĠìĹĨëĦ¤&#34;:2954,&#34;ìĹĪìľ¼ë©´&#34;:2955,&#34;ì°©&#34;:2956,&#34;ĠìĪĺê°Ģ&#34;:2957,&#34;ìº&#34;:2958,&#34;Ġêµīìŀ¥íŀĪ&#34;:2959,&#34;ì¡°ì°¨&#34;:2960,&#34;ìĸ´ì§ĢëĬĶ&#34;:2961,&#34;ëĤ´ëĬĶ&#34;:2962,&#34;ĠìĬ¤íĨłë¦¬ëĬĶ&#34;:2963,&#34;ĠíĿ¥íĸī&#34;:2964,&#34;ĠëıħíĬ¹&#34;:2965,&#34;ĠìķĪë³&#34;:2966,&#34;ìķĦë¬´ë¦¬&#34;:2967,&#34;¥ľ&#34;:2968,&#34;¬ìĺģ&#34;:2969,&#34;íķĺê¸´&#34;:2970,&#34;ë§ĮìĿĺ&#34;:2971,&#34;ëłĪë&#34;:2972,&#34;ê²ĥê°Ļëĭ¤&#34;:2973,&#34;ìłľê°Ģ&#34;:2974,&#34;Ġìĥģìĺģ&#34;:2975,&#34;ë¦½ëĭĪëĭ¤&#34;:2976,&#34;Ġê°ĳìŀĲê¸°&#34;:2977,&#34;ĠíĥĦíĥĦ&#34;:2978,&#34;Ġíķĺê²Į&#34;:2979,&#34;Ġìļ´&#34;:2980,&#34;ìłĢìĶ¨&#34;:2981,&#34;ĠìĦłíĥĿ&#34;:2982,&#34;Ġê¶Į&#34;:2983,&#34;ĠëĦ¤ìĿ´ë²Ħ&#34;:2984,&#34;ĠìĹĲë¡ľ&#34;:2985,&#34;Ġì§Ħíĸī&#34;:2986,&#34;Ġê°ĲìĦ±&#34;:2987,&#34;Ġìĸ´ì©Į&#34;:2988,&#34;Ġìĭľì²Ńë¥ł&#34;:2989,&#34;ĠíļĮ&#34;:2990,&#34;ë§ĮìĿ´&#34;:2991,&#34;ê²ĥìĿĢ&#34;:2992,&#34;ĠíķŃìĥģ&#34;:2993,&#34;ĠìĪľê°Ħ&#34;:2994,&#34;ë¿&#34;:2995,&#34;Ġìĭľì¦Į&#34;:2996,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿĢ&#34;:2997,&#34;Ġìŀ¬ë¯¸ìŀĪëĭ¤&#34;:2998,&#34;Ġì¶©ë¶ĦíŀĪ&#34;:2999,&#34;Īëĭ¤&#34;:3000,&#34;ìĸ´ëĤĺ&#34;:3001,&#34;Ġëĵł&#34;:3002,&#34;Ġìĸ´ë¥¸&#34;:3003,&#34;Ġê°ĢëĬĶ&#34;:3004,&#34;Ġë¹Į&#34;:3005,&#34;ìŀ¬ë¯¸ëıĦ&#34;:3006,&#34;ĠëĲľëĭ¤&#34;:3007,&#34;íķĺëĭ¤ê³ł&#34;:3008,&#34;Ġê·¸ëħĢ&#34;:3009,&#34;ì¤¬&#34;:3010,&#34;ĠëĦĪ&#34;:3011,&#34;ì²¨&#34;:3012,&#34;Ġë§Įì¡±&#34;:3013,&#34;ĠëĬĲê¼&#34;:3014,&#34;ĠíķĦ&#34;:3015,&#34;ëĭĺìĿ´&#34;:3016,&#34;ì½¤&#34;:3017,&#34;ĠíħĮ&#34;:3018,&#34;íķĺìĹ¬&#34;:3019,&#34;Ġê²ģ&#34;:3020,&#34;ĠìĭľìłĪ&#34;:3021,&#34;íĸĪìĬµëĭĪëĭ¤&#34;:3022,&#34;Ġì¢ĭê²łëĭ¤&#34;:3023,&#34;Ġì¢Ģë¹Ħ&#34;:3024,&#34;ĠìĹīë§Ŀ&#34;:3025,&#34;Ġìĸĺê¸°&#34;:3026,&#34;ê³³&#34;:3027,&#34;ìŀĪëĦ¤&#34;:3028,&#34;Ġë³´ëĬĶëį°&#34;:3029,&#34;ĠíĿĳ&#34;:3030,&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;&#34;:3031,&#34;ëĤ´ìļ©ëıĦ&#34;:3032,&#34;Ġë¶ĦìľĦê¸°&#34;:3033,&#34;ìĹĺ&#34;:3034,&#34;ëŀ¬&#34;:3035,&#34;ë³į&#34;:3036,&#34;Ġìī½&#34;:3037,&#34;.......&#34;:3038,&#34;ìĭ¤íŀĪ&#34;:3039,&#34;ĠìĿĺëıĦ&#34;:3040,&#34;Ġë¨¸ë¦¬&#34;:3041,&#34;ĠìĿ´ë²Ī&#34;:3042,&#34;Ġê¸ī&#34;:3043,&#34;íĶĦë¡ľ&#34;:3044,&#34;ĠíĹ¤&#34;:3045,&#34;ĠíĽĪ&#34;:3046,&#34;ê°ķì¶Ķ&#34;:3047,&#34;ëķľìĹĲ&#34;:3048,&#34;ìĹĦì²Ń&#34;:3049,&#34;Ġë¦¬ìĸ¼&#34;:3050,&#34;íĢ&#34;:3051,&#34;Ġìĵ´&#34;:3052,&#34;ìĿ´ëĥĲ&#34;:3053,&#34;ìļ°ëĬĶ&#34;:3054,&#34;Ġìŀ¬ë¯¸ëĬĶ&#34;:3055,&#34;Ġìŀĺë§Įëĵ¤&#34;:3056,&#34;ĠëĪĪë&#34;:3057,&#34;Ġíķ´ìķ¼&#34;:3058,&#34;//&#34;:3059,&#34;ì§Ģëĭ¤&#34;:3060,&#34;ëł¬&#34;:3061,&#34;ĠëĨĪ&#34;:3062,&#34;íĿĲ&#34;:3063,&#34;ì´Ŀ&#34;:3064,&#34;ĠíĽĮë¥Ńíķľ&#34;:3065,&#34;ìļ°ë¦¬ëĤĺëĿ¼&#34;:3066,&#34;ĠíĻĶë©´&#34;:3067,&#34;Ġë¯¿ê³ł&#34;:3068,&#34;Ġìĸ´ì©Ķ&#34;:3069,&#34;ĠìĥĪë¡ľìļ´&#34;:3070,&#34;Ġê°Ķ&#34;:3071,&#34;ìĭľê³ł&#34;:3072,&#34;Ġì¢ĭëĦ¤ìļĶ&#34;:3073,&#34;ĠìķĪíĥĢê¹Ŀ&#34;:3074,&#34;ìĿ´íķĺ&#34;:3075,&#34;ĠìĺģíĻĶìĿ¸&#34;:3076,&#34;Ġê·¸ê²Į&#34;:3077,&#34;ĠëĤĺìĿĺ&#34;:3078,&#34;Ġê²Ģ&#34;:3079,&#34;íķĺëĬĶê±°&#34;:3080,&#34;Ġëª¨ë¥´ê³ł&#34;:3081,&#34;ë°°ìļ°ëĵ¤ìĿĺ&#34;:3082,&#34;Ġë¹¨ë¦¬&#34;:3083,&#34;ĠM&#34;:3084,&#34;Ġìĸ´ëł¤&#34;:3085,&#34;ìłľë°ľ&#34;:3086,&#34;Ġìŀ¬ë°ĮëĦ¤ìļĶ&#34;:3087,&#34;ë³¸ëĭ¤&#34;:3088,&#34;ì¢ĭìķĦìļĶ&#34;:3089,&#34;ĠìĤ´ìĿ¸&#34;:3090,&#34;ìĺĽëĤł&#34;:3091,&#34;ìĺ¤ëĬĶ&#34;:3092,&#34;ìĦ¸ê³Ħ&#34;:3093,&#34;ìłĢëıĦ&#34;:3094,&#34;ĠìķĦëĭĪë&#34;:3095,&#34;Ġì°¸ê³ł&#34;:3096,&#34;ë´Ĳìķ¼&#34;:3097,&#34;ìĬ¤ëŁ°&#34;:3098,&#34;ĠìŀĪìĿĮ&#34;:3099,&#34;ê·ł&#34;:3100,&#34;ëħĦìĹĲ&#34;:3101,&#34;ĠëŃĲê°Ģ&#34;:3102,&#34;Ġíŀĺëĵł&#34;:3103,&#34;ëª°ìŀħ&#34;:3104,&#34;Ġì§Ŀ&#34;:3105,&#34;ìĺģíĻĶëıĦ&#34;:3106,&#34;ĠìĹ¬ìłĦ&#34;:3107,&#34;ĠëŃĲëĥĲ&#34;:3108,&#34;Ġë¬»&#34;:3109,&#34;íĺķìłģìĿ¸&#34;:3110,&#34;ĠìķĦìī¬ìļ´&#34;:3111,&#34;ĠìķĪíĥĢê¹Į&#34;:3112,&#34;ê·¸ëŀ¨&#34;:3113,&#34;ĠO&#34;:3114,&#34;ìĭľëĬĶ&#34;:3115,&#34;ë²Īë&#34;:3116,&#34;ê°Ħë§ĮìĹĲ&#34;:3117,&#34;Ġì§Ħìłķíķľ&#34;:3118,&#34;Ġë§ĪìĿ´&#34;:3119,&#34;ëĨĴ&#34;:3120,&#34;ê¿Ģìŀ¼&#34;:3121,&#34;ìµľìķħìĿĺ&#34;:3122,&#34;ìĩ¼&#34;:3123,&#34;ĳìĹĲ&#34;:3124,&#34;Ġë§¨&#34;:3125,&#34;ëĤ´ê³ł&#34;:3126,&#34;Ġìľ¼&#34;:3127,&#34;Ġê´ľ&#34;:3128,&#34;Ġê²ĥìĿ´ëĭ¤&#34;:3129,&#34;íĶĦëĭ¤&#34;:3130,&#34;Ġê·¸ëķĮ&#34;:3131,&#34;ê²łëĦ¤&#34;:3132,&#34;ĠëĮĢëĭ¨íķľ&#34;:3133,&#34;ĠëŁ¬&#34;:3134,&#34;ìĿ´ë©´&#34;:3135,&#34;ìĸ´ë¡ľ&#34;:3136,&#34;ìŀĲìĿĺ&#34;:3137,&#34;OOO&#34;:3138,&#34;Ġë©ĭìŀĪ&#34;:3139,&#34;Ġíģ¬&#34;:3140,&#34;Ġìĸ´ëĶĶìĦľ&#34;:3141,&#34;ìĹĪëĦ¤ìļĶ&#34;:3142,&#34;ĠìĨĮìĦ¤&#34;:3143,&#34;ëĲĺìĸ´&#34;:3144,&#34;Ħ¤ìļĶ&#34;:3145,&#34;ĠíĢ&#34;:3146,&#34;ĠìŀĪëĭ¤ëĬĶ&#34;:3147,&#34;ĠìĿ´ìĺģíĻĶë¥¼&#34;:3148,&#34;ĠìĬ¹&#34;:3149,&#34;Ġíķľìĭ¬&#34;:3150,&#34;íķ¨ê³¼&#34;:3151,&#34;ìłľìŀĳ&#34;:3152,&#34;Ġê°Ĳìĥģ&#34;:3153,&#34;ìĻł&#34;:3154,&#34;ê·¸ëŁ¬&#34;:3155,&#34;Ġë¨¹ë¨¹&#34;:3156,&#34;íĦ´&#34;:3157,&#34;ìĬ¤ë¥¼&#34;:3158,&#34;ĠëĦ£&#34;:3159,&#34;Ġë´¤ëĦ¤ìļĶ&#34;:3160,&#34;ĠìĨĮì¤ĳ&#34;:3161,&#34;ĠëŃĲì§Ģ&#34;:3162,&#34;ìĤ¬ëŀĮìĿ´&#34;:3163,&#34;ĠìĦ±ìļ°&#34;:3164,&#34;Ġëĭ¹ìĭł&#34;:3165,&#34;ìĬ¤íĨłë¦¬ê°Ģ&#34;:3166,&#34;íıīê°Ģ&#34;:3167,&#34;ĠíĤ¬ë§ģíĥĢìŀĦìļ©&#34;:3168,&#34;ĠìķĪëĲľëĭ¤&#34;:3169,&#34;¬ê¸Ī&#34;:3170,&#34;ĠìĹĨìĿĦ&#34;:3171,&#34;ĠìĹĨëĦ¤ìļĶ&#34;:3172,&#34;ë²Į&#34;:3173,&#34;ĠìĨ¡&#34;:3174,&#34;ĠëĲĺìĸ´&#34;:3175,&#34;ĠëĬĲëģ¼ê²Į&#34;:3176,&#34;Ġê°ľìĹ°ìĦ±&#34;:3177,&#34;Ġì°Į&#34;:3178,&#34;ìĿ´ë¸Ķ&#34;:3179,&#34;ê³łìĭ¶&#34;:3180,&#34;ëłĮ&#34;:3181,&#34;ê²ĥìĿ´ëĭ¤&#34;:3182,&#34;ĠìķĮëł¤&#34;:3183,&#34;Ġíıīë²Ķ&#34;:3184,&#34;ëľ¨&#34;:3185,&#34;ìłķìĭł&#34;:3186,&#34;ìħĪ&#34;:3187,&#34;ë¦¬ë¡ľ&#34;:3188,&#34;ìĽĢìĿ´&#34;:3189,&#34;Ġë§Ŀìŀĳ&#34;:3190,&#34;Ġë¬¼ë¡ł&#34;:3191,&#34;ìºĲë¦ŃíĦ°&#34;:3192,&#34;ĠãĦ·ãĦ·&#34;:3193,&#34;¬ëŀĳ&#34;:3194,&#34;Ġê°ĢëĬ¥&#34;:3195,&#34;ĠëŃĲìķ¼&#34;:3196,&#34;Ġíİ¸ì§ĳ&#34;:3197,&#34;Ġíľ´&#34;:3198,&#34;Ġë°Ģ&#34;:3199,&#34;ìĹĨìĸ´&#34;:3200,&#34;Ġíķĺë©´&#34;:3201,&#34;ìķłëĭĪ&#34;:3202,&#34;Ġê´ĢëŀĮ&#34;:3203,&#34;ëŀĻ&#34;:3204,&#34;Ġìľ¤&#34;:3205,&#34;ìĺĢëĬĶëį°&#34;:3206,&#34;ãĦ¹&#34;:3207,&#34;íĹĲ&#34;:3208,&#34;ĠìłĲìłĲ&#34;:3209,&#34;ĠìļĶìĨĮ&#34;:3210,&#34;Ġë§ĮìłĲ&#34;:3211,&#34;íŀĮ&#34;:3212,&#34;ĠìŀĲê·¹&#34;:3213,&#34;ëħ¸ëŀĺ&#34;:3214,&#34;Ġ-_-&#34;:3215,&#34;Ġìĺ¤ê¸Ģê±°&#34;:3216,&#34;ST&#34;:3217,&#34;ëĢ&#34;:3218,&#34;ìª½&#34;:3219,&#34;Ġëĥ&#34;:3220,&#34;ìŀĲëĵ¤&#34;:3221,&#34;Ġì¤ĺ&#34;:3222,&#34;ìĸ´ëĸ»ê²Į&#34;:3223,&#34;Ġm&#34;:3224,&#34;ìĦ¼&#34;:3225,&#34;íķĺëĬĶê²Į&#34;:3226,&#34;ãħĭãħĭãħĭãħĭãħĭ&#34;:3227,&#34;Ġë°ĶëĿ¼&#34;:3228,&#34;ìĺĪìĤ°&#34;:3229,&#34;ì»¬&#34;:3230,&#34;ª¨&#34;:3231,&#34;íĻĶê°Ģ&#34;:3232,&#34;Ġê¹Ķ&#34;:3233,&#34;Ġ200&#34;:3234,&#34;ĠìĤ¬ëŀĳìĿĦ&#34;:3235,&#34;Ġì£½ëĬĶ&#34;:3236,&#34;Ġë³ĦìłĲ&#34;:3237,&#34;Ġëĵ¯íķľ&#34;:3238,&#34;--&#34;:3239,&#34;12&#34;:3240,&#34;ê¸°ìĹĶ&#34;:3241,&#34;Ġë²ķ&#34;:3242,&#34;Ġë§Įëĵ¤ìĹĪëĭ¤&#34;:3243,&#34;ì©Į&#34;:3244,&#34;ĠëĤ«ëĭ¤&#34;:3245,&#34;Ġì¿&#34;:3246,&#34;Ġë½&#34;:3247,&#34;ìł¤&#34;:3248,&#34;ĠëĤĺìģľ&#34;:3249,&#34;Ġì¢ĭìĿĮ&#34;:3250,&#34;ĠìĥĪ&#34;:3251,&#34;Ġìŀ¬ë°ĮìĹĪëĭ¤&#34;:3252,&#34;ë¹¼&#34;:3253,&#34;ìĭ¬ìĿĦ&#34;:3254,&#34;Ġëª¨ë¥´ëĬĶ&#34;:3255,&#34;ĠìłĦê°ľê°Ģ&#34;:3256,&#34;ĠìĭĿìĥģ&#34;:3257,&#34;ìį&#34;:3258,&#34;ìĿ´ëĶ´&#34;:3259,&#34;íķľëĵ¯&#34;:3260,&#34;íİĳ&#34;:3261,&#34;ĠíıīìłĲìĿĢ&#34;:3262,&#34;ìķĪíķĺê³ł&#34;:3263,&#34;ĠìŀħëĭĪëĭ¤&#34;:3264,&#34;Ġë§¥&#34;:3265,&#34;ĠëģĿëĤ´&#34;:3266,&#34;Ġìŀ¬ë¯¸ìŀĪìĸ´ìļĶ&#34;:3267,&#34;ê°ĻìĿĢëį°&#34;:3268,&#34;ìµľê³łëĭ¤&#34;:3269,&#34;ĠëĲĲ&#34;:3270,&#34;ĠìĤ¬ëĿ¼&#34;:3271,&#34;ìĹĨëĦ¤&#34;:3272,&#34;ĠëĤĺìĿ´&#34;:3273,&#34;íĴĭ&#34;:3274,&#34;ĠìŀĲê¾¸&#34;:3275,&#34;ĠìĿ´ìķ¼ê¸°ë¥¼&#34;:3276,&#34;ĠíĿ¥ë¯¸ì§Ħì§Ħ&#34;:3277,&#34;ĠìĶ¨&#34;:3278,&#34;ìİ&#34;:3279,&#34;ìı&#34;:3280,&#34;ë³´ìĦ¸ìļĶ&#34;:3281,&#34;ĠìĹ¬ì£¼&#34;:3282,&#34;ëŀ«&#34;:3283,&#34;ĠìķĮìķĺëĭ¤&#34;:3284,&#34;íıĲ&#34;:3285,&#34;ĠíĭĢ&#34;:3286,&#34;Ġ!!&#34;:3287,&#34;Ġìĺ¤íŀĪëł¤&#34;:3288,&#34;ê»ı&#34;:3289,&#34;Ġê¼¬&#34;:3290,&#34;ìĽłëįĺ&#34;:3291,&#34;Ġë²Ķì£Ħ&#34;:3292,&#34;ĠìĨĮë¦¬&#34;:3293,&#34;Ġìĭ«ìĸ´&#34;:3294,&#34;ëŀĦê¹Į&#34;:3295,&#34;ìķĪëĲĺëĬĶ&#34;:3296,&#34;ĠëĮĵê¸Ģ&#34;:3297,&#34;ëĭ¿&#34;:3298,&#34;ëĤĺë©´&#34;:3299,&#34;ìķĦìķĦ&#34;:3300,&#34;ãħłãħłãħł&#34;:3301,&#34;ìĭ¬ìĿ´&#34;:3302,&#34;Ġë§Įëĵ¤ì§Ģ&#34;:3303,&#34;ìĭĿìĿ´&#34;:3304,&#34;Ġì¢ĭìķĺìĸ´ìļĶ&#34;:3305,&#34;ĠíĿĺëŁ¬&#34;:3306,&#34;âĢ&#34;:3307,&#34;ì°¾&#34;:3308,&#34;Ġì°½&#34;:3309,&#34;Ġë¯¸íĻĶ&#34;:3310,&#34;Ġãħħ&#34;:3311,&#34;ĠìĿ´ë¦¬&#34;:3312,&#34;ìĸ´ìļ¸&#34;:3313,&#34;ĠëĤĺëĿ¼&#34;:3314,&#34;Ġì§Ħì§Ģ&#34;:3315,&#34;íİ¸ìĿĦ&#34;:3316,&#34;ĠìĽĥê¸´&#34;:3317,&#34;ë©ĶëĿ¼&#34;:3318,&#34;Ġëª¨ìĬµìĿ´&#34;:3319,&#34;ìĹĪëĦ¤&#34;:3320,&#34;ĠìķĮìķĺëĬĶëį°&#34;:3321,&#34;ê¼¬&#34;:3322,&#34;êµ¿êµ¿&#34;:3323,&#34;ë³´ëĬĶëį°&#34;:3324,&#34;ĠìĺģìĽħ&#34;:3325,&#34;ë´ĲìĦľ&#34;:3326,&#34;ĠìĦ¤ëªħ&#34;:3327,&#34;ìºĲìĬ¤íĮħ&#34;:3328,&#34;Ġë£¨&#34;:3329,&#34;Ġëĭ¤íĸī&#34;:3330,&#34;ìĦ¸ê°Ģ&#34;:3331,&#34;ë²ĪìĿĦ&#34;:3332,&#34;Ġìŀ¥ë©´ìĿĢ&#34;:3333,&#34;Ġì£¼ìĿ¸ê³µìĿ´&#34;:3334,&#34;êµ¬ëĵ¤&#34;:3335,&#34;ì§Īë&#34;:3336,&#34;ĠìłĬ&#34;:3337,&#34;ìķĦë¨¹&#34;:3338,&#34;Ġê²¨&#34;:3339,&#34;ìĸĺ&#34;:3340,&#34;ìĨĮìĦ¤&#34;:3341,&#34;Ġë§Ĳê³ł&#34;:3342,&#34;ëĺ¥&#34;:3343,&#34;Ġë©įì²Ń&#34;:3344,&#34;Ġa&#34;:3345,&#34;Ġê»&#34;:3346,&#34;Ġê·¸ëŁ´&#34;:3347,&#34;ìĥģìĿĦ&#34;:3348,&#34;ìī½&#34;:3349,&#34;Ġìĭłê¸°&#34;:3350,&#34;ĠìĹĦì²ŃëĤľ&#34;:3351,&#34;ĠìķĦë¹ł&#34;:3352,&#34;ìłĲì¤Ģ&#34;:3353,&#34;ĠìĦ±ë£¡&#34;:3354,&#34;Ġë©ĶìĦ¸&#34;:3355,&#34;he&#34;:3356,&#34;Ġê¾&#34;:3357,&#34;ëª½&#34;:3358,&#34;Ġë¬´ì¡°ê±´&#34;:3359,&#34;Ġì¤ĳìĹĲ&#34;:3360,&#34;ìĹ°ê¸°ëıĦ&#34;:3361,&#34;ĠíĸīëıĻ&#34;:3362,&#34;ìĸ´ëĶĶ&#34;:3363,&#34;Ġì§§&#34;:3364,&#34;ĠìłĦê¸°&#34;:3365,&#34;íĶĦê³ł&#34;:3366,&#34;Ġë³´ìĹ¬ì¤Ģ&#34;:3367,&#34;ìĭ±&#34;:3368,&#34;ë§Įíķĺë©´&#34;:3369,&#34;ìĬ¤ë¡ľ&#34;:3370,&#34;ê·¸ìłĢ&#34;:3371,&#34;ì¤ĦìķĮ&#34;:3372,&#34;ì¹ľêµ¬&#34;:3373,&#34;ĠíĻĶìĿ´íĮħ&#34;:3374,&#34;Ġ90&#34;:3375,&#34;ĠìĺģíĻĶìĺĢìĬµëĭĪëĭ¤&#34;:3376,&#34;¶Ħ&#34;:3377,&#34;Ġì¥&#34;:3378,&#34;ê¸°ëĭ¤&#34;:3379,&#34;ìĭľì¦Į&#34;:3380,&#34;íĤ¹&#34;:3381,&#34;ĠìĹ°ì¶ľëł¥&#34;:3382,&#34;ê°ĲëıħìĿĺ&#34;:3383,&#34;ĠìĬ¤íĥĢìĿ¼&#34;:3384,&#34;ìŀł&#34;:3385,&#34;ìķĻ&#34;:3386,&#34;ìĸ´ìĿ´&#34;:3387,&#34;ë¦¬ìķĦ&#34;:3388,&#34;Ġê·¸ë¦¼&#34;:3389,&#34;ĠëĮĢìŀĳ&#34;:3390,&#34;Ġì£½ìĿĮ&#34;:3391,&#34;?)&#34;:3392,&#34;ë½&#34;:3393,&#34;íķĺê²ł&#34;:3394,&#34;ëĤĺìĿ´&#34;:3395,&#34;ë¦¬ê²Į&#34;:3396,&#34;íĮĶ&#34;:3397,&#34;ìĨĮíķľ&#34;:3398,&#34;Ġëģ¼&#34;:3399,&#34;Ĵ·&#34;:3400,&#34;Ġë®¤&#34;:3401,&#34;ìĿ¸ìĿĦ&#34;:3402,&#34;Ġê·¸ëĭ¥&#34;:3403,&#34;ĠíķĺëĬĶëį°&#34;:3404,&#34;Ġìĸ´ë¦´&#34;:3405,&#34;ĠìŀĳíĴĪìĿ´&#34;:3406,&#34;Ġëħ¸ëł¥&#34;:3407,&#34;ĠìŀĬíĺĢ&#34;:3408,&#34;200&#34;:3409,&#34;ìłķìĿĦ&#34;:3410,&#34;Ġê°ĻëĦ¤ìļĶ&#34;:3411,&#34;Ġë©´&#34;:3412,&#34;Ġìľłë¨¸&#34;:3413,&#34;ë¦¬ëĿ¼&#34;:3414,&#34;ìŀ¬ë°ĮìĿĮ&#34;:3415,&#34;Ġê´Ģê³Ħ&#34;:3416,&#34;ĠìŀĶìŀĶíķľ&#34;:3417,&#34;ĠíĿ¬ë§Ŀ&#34;:3418,&#34;ê¸Īë´ĲëıĦ&#34;:3419,&#34;ĠìĿ´ìĸ´&#34;:3420,&#34;Ġë§Īë¬´&#34;:3421,&#34;ìĿ¸ê±°&#34;:3422,&#34;Ġëįķ&#34;:3423,&#34;ìĤ¬ëĬĶ&#34;:3424,&#34;Ġíķ©&#34;:3425,&#34;Ġê²°íĺ¼&#34;:3426,&#34;Ġì¢ĭìķĺê³ł&#34;:3427,&#34;ĠìĺģíĻĶìĹĲìĦľ&#34;:3428,&#34;ë§Ĳë¡ľ&#34;:3429,&#34;ìŀĲëĬĶ&#34;:3430,&#34;ìŀĪìĸ´ìļĶ&#34;:3431,&#34;íĥģ&#34;:3432,&#34;Ġê°ĢëĵĿ&#34;:3433,&#34;ì¹ĺê²Į&#34;:3434,&#34;ĠíĹĲë¦¬&#34;:3435,&#34;ĠìĽĥê²¨&#34;:3436,&#34;ë°©ìĨ¡&#34;:3437,&#34;Ġë¬´ìĦľìļ´&#34;:3438,&#34;ĩ´&#34;:3439,&#34;Ġë§Įíģ¼&#34;:3440,&#34;ìĹ¬ì£¼&#34;:3441,&#34;ë¶ĦìĿ´&#34;:3442,&#34;Ġì°¸ìĭł&#34;:3443,&#34;ĠëıĪì£¼ê³ł&#34;:3444,&#34;ĠìķĦë¬´ê²ĥëıĦ&#34;:3445,&#34;©ĶìĿ´íģ¬&#34;:3446,&#34;ìĿ¸ìĿĢ&#34;:3447,&#34;Ġëª»ë³´&#34;:3448,&#34;ìĤ¬ëŀĮëĵ¤&#34;:3449,&#34;íķĺë©´ìĦľëıĦ&#34;:3450,&#34;Ġê·¸ëł¤&#34;:3451,&#34;ê·¸ëĿ¼&#34;:3452,&#34;Ġ19&#34;:3453,&#34;Ġì¢ĭìķĺìĬµëĭĪëĭ¤&#34;:3454,&#34;ĠëĺĲíķľ&#34;:3455,&#34;ĠíĬ¹ìľłìĿĺ&#34;:3456,&#34;ĠíĽĪíĽĪ&#34;:3457,&#34;íī&#34;:3458,&#34;íķľê±´&#34;:3459,&#34;ìĭľìŀĳ&#34;:3460,&#34;Ġìĺ¬&#34;:3461,&#34;ìĺĢìĬµëĭĪëĭ¤&#34;:3462,&#34;ĠìĬ¤íĨłë¦¬ìĹĲ&#34;:3463,&#34;ê¸´ìŀ¥ê°Ĳ&#34;:3464,&#34;ĠìķĦëĭĪì§Ģë§Į&#34;:3465,&#34;ìŀ¬ë¯¸ìŀĪê²Į&#34;:3466,&#34;Ġìĸµì§Ģë¡ľ&#34;:3467,&#34;Ġì¦Ĳê²ģ&#34;:3468,&#34;ê±¸ê¹Į&#34;:3469,&#34;ë´ĲëĿ¼&#34;:3470,&#34;Ġì½Ķë©ĶëĶĶ&#34;:3471,&#34;ì½ĺ&#34;:3472,&#34;ĠëĨĢëĿ¼&#34;:3473,&#34;ĠëĿ¼ëĬĶ&#34;:3474,&#34;ìķĦëĭĪê³ł&#34;:3475,&#34;Ġë¡ľë§¨íĭ±&#34;:3476,&#34;Ġë³ĳë§Ľ&#34;:3477,&#34;ĠTV&#34;:3478,&#34;ĠìĿ´ëĶ°&#34;:3479,&#34;íķ´ì£¼&#34;:3480,&#34;ìŀĲë¥¼&#34;:3481,&#34;ë¶ĦìĹĲ&#34;:3482,&#34;ëĦĪë¬´ëĦĪë¬´&#34;:3483,&#34;ĠìĻĦìĦ±ëıĦ&#34;:3484,&#34;Ġê·¸ëłĩê³ł&#34;:3485,&#34;ĠìĻĦë²½íķľ&#34;:3486,&#34;ëĮ&#34;:3487,&#34;ìĹ¼&#34;:3488,&#34;ìłĲì¤Ģëĭ¤&#34;:3489,&#34;ĠëĤĺìģĺ&#34;:3490,&#34;Ġì§Ħìĭ¤&#34;:3491,&#34;ë¥´ê³ł&#34;:3492,&#34;Ġìķ¡ìħĺëıĦ&#34;:3493,&#34;Ġëª¨ìĬµìĿĦ&#34;:3494,&#34;Ġê¿Ģìŀ¼&#34;:3495,&#34;²Ħ&#34;:3496,&#34;ì¦Īë&#34;:3497,&#34;ĠìķĬìĿĦ&#34;:3498,&#34;ĠìĥĿê°ģë³´ëĭ¤&#34;:3499,&#34;ìķłëĵ¤&#34;:3500,&#34;ëŃĲìķ¼&#34;:3501,&#34;âĺħ&#34;:3502,&#34;ĠìķĦëĭĲê¹Į&#34;:3503,&#34;~^^&#34;:3504,&#34;ìŀĪëĬĶëį°&#34;:3505,&#34;ĠìĥĿê°ģíķľëĭ¤&#34;:3506,&#34;Ġëħ¸ì¶ľ&#34;:3507,&#34;Ġê¸°ëĮĢíķĺê³ł&#34;:3508,&#34;ĠìĦ±ìŀ¥&#34;:3509,&#34;ëĵ±íķĻêµĲ&#34;:3510,&#34;Ġê¹ĬìĿĢ&#34;:3511,&#34;íĻĶë¥¼&#34;:3512,&#34;ëĵľê°Ģ&#34;:3513,&#34;ê°ĲìĿĦ&#34;:3514,&#34;ĠëıĦìłĢíŀĪ&#34;:3515,&#34;Ġë³µìĪĺ&#34;:3516,&#34;ë»Ĳ&#34;:3517,&#34;ëĳĺ&#34;:3518,&#34;ê³łíİ¸&#34;:3519,&#34;íķĺëįĺ&#34;:3520,&#34;ê³¼ëĬĶ&#34;:3521,&#34;Ġë¶ģ&#34;:3522,&#34;ĠìķĬìķĺëĭ¤&#34;:3523,&#34;ìĺĢëįĺ&#34;:3524,&#34;ãħİãħİãħİ&#34;:3525,&#34;Ġê°ĲëıħìĿĢ&#34;:3526,&#34;ìł¸ìĦľ&#34;:3527,&#34;ëĤ¬ëĭ¤&#34;:3528,&#34;ĠíĽĦë°ĺë¶Ģ&#34;:3529,&#34;ĠìĿ¸ìłķ&#34;:3530,&#34;Ġë§Ĳíķĺê³ł&#34;:3531,&#34;ĠíĮĶ&#34;:3532,&#34;Ġíıīë¡łê°Ģ&#34;:3533,&#34;Ġê·¸ëŀĺíĶ½&#34;:3534,&#34;Ġë¶Ģë¶ĦìĿ´&#34;:3535,&#34;ī´&#34;:3536,&#34;ìĿ´ìĹĲìļĶ&#34;:3537,&#34;ĠìĺģíĻĶì¤ĳìĹĲ&#34;:3538,&#34;ĠìķĦë²Ħì§Ģ&#34;:3539,&#34;íķĺëĬĶì§Ģ&#34;:3540,&#34;ëł¤ë©´&#34;:3541,&#34;Ġë³¸ëĭ¤&#34;:3542,&#34;ì¢ĭê³ł&#34;:3543,&#34;ĠíĻįì½©&#34;:3544,&#34;íķľëĭ¤ëĬĶ&#34;:3545,&#34;ëĤĺìĻĶ&#34;:3546,&#34;ĠëįĶëŁ½ê²Į&#34;:3547,&#34;ĠìķĪë§ŀ&#34;:3548,&#34;Ġëª»íķĺê³ł&#34;:3549,&#34;Ġìľłëªħ&#34;:3550,&#34;ìģ¨&#34;:3551,&#34;ĠëıĮëł¤&#34;:3552,&#34;Ġê³¨&#34;:3553,&#34;ĠëĶ°ëľ»íķľ&#34;:3554,&#34;Ġë¹ĦìĬ·íķľ&#34;:3555,&#34;ìķĦë¦Ħëĭ¤ìļ´&#34;:3556,&#34;ëĭ«&#34;:3557,&#34;ìĽ¨&#34;:3558,&#34;ìĹĪëĤĺ&#34;:3559,&#34;ĠìķĬìĿĮ&#34;:3560,&#34;Ġìłľê°Ģ&#34;:3561,&#34;Ġíİĺ&#34;:3562,&#34;ì§Ģë£¨íķĺê³ł&#34;:3563,&#34;Ġëľ¬ê¸Ī&#34;:3564,&#34;ì§Ģëª»&#34;:3565,&#34;¬ëĵ¤&#34;:3566,&#34;ĠìĺģíĻĶìĿ´ëĭ¤&#34;:3567,&#34;Ġìłĳ&#34;:3568,&#34;ìŀ¥ìĿ´&#34;:3569,&#34;ĠìķĪëĤĺìĺ¤&#34;:3570,&#34;ë¯¿&#34;:3571,&#34;Ġì´Į&#34;:3572,&#34;Ġë¶Ħëªħ&#34;:3573,&#34;Ġê³¼ê±°&#34;:3574,&#34;or&#34;:3575,&#34;ìłĪëĮĢ&#34;:3576,&#34;ì§ĢëĦ¤ìļĶ&#34;:3577,&#34;íķ´ì£¼ëĬĶ&#34;:3578,&#34;ëģĮ&#34;:3579,&#34;íĿĳ&#34;:3580,&#34;íĹ¤&#34;:3581,&#34;ĠìĹ°ì¶ľìĿ´&#34;:3582,&#34;ìĪľê°Ħ&#34;:3583,&#34;Ġë¨¼ìłĢ&#34;:3584,&#34;Ģë¡ľ&#34;:3585,&#34;ìĹ¿&#34;:3586,&#34;ë¶Īë&#34;:3587,&#34;ìĪĺìŀĪëĬĶ&#34;:3588,&#34;ìĿ¼ë¿Ĳ&#34;:3589,&#34;ĠìŀĺìĥĿ&#34;:3590,&#34;ë§¹&#34;:3591,&#34;ìĪĺëıĦ&#34;:3592,&#34;ĠìĺĪë&#34;:3593,&#34;ì¶ķ&#34;:3594,&#34;íĸĪìľ¼ë©´&#34;:3595,&#34;Ġê°ĻìĬµëĭĪëĭ¤&#34;:3596,&#34;ëįĶêµ°ìļĶ&#34;:3597,&#34;Ġíķ´ì£¼ëĬĶ&#34;:3598,&#34;Ġê±°ì§Ģ&#34;:3599,&#34;Ġë°Ķë³´&#34;:3600,&#34;ëĸ¨ìĸ´&#34;:3601,&#34;Ġì´¬ìĺģ&#34;:3602,&#34;ëĭµëĭµ&#34;:3603,&#34;ĠìĦľë¡ľ&#34;:3604,&#34;ê¸Īë³´&#34;:3605,&#34;ìŀĪìĿĮ&#34;:3606,&#34;ĠìĬĪ&#34;:3607,&#34;ĠìķĮê²ł&#34;:3608,&#34;Ġë¹Ī&#34;:3609,&#34;ëģĿê¹Įì§Ģ&#34;:3610,&#34;ĠìķĦëĭĪë©´&#34;:3611,&#34;ê°ĢìļĶ&#34;:3612,&#34;ëĵłëĭ¤&#34;:3613,&#34;ĠìĤ¬ê±´&#34;:3614,&#34;ĠëŃĶì§Ģ&#34;:3615,&#34;ĠëĤĺìĺ¨ëĭ¤&#34;:3616,&#34;ìĹĦë§Ī&#34;:3617,&#34;ĠSF&#34;:3618,&#34;ìŀī&#34;:3619,&#34;ìĥĪëģ¼&#34;:3620,&#34;íķłëĵ¯&#34;:3621,&#34;ìĦ±ìķł&#34;:3622,&#34;Ġë°°ìļ°ê°Ģ&#34;:3623,&#34;ìĹ°ê¸°ê°Ģ&#34;:3624,&#34;ìłĪíķľ&#34;:3625,&#34;ëĲĲ&#34;:3626,&#34;Ġãħľ&#34;:3627,&#34;..?&#34;:3628,&#34;ëĮĢìĤ¬&#34;:3629,&#34;íģ°&#34;:3630,&#34;ëįĶë¹Ļ&#34;:3631,&#34;ìĶĢ&#34;:3632,&#34;Ġê¹ľ&#34;:3633,&#34;ìĬ¤íĨłë¦¬ëıĦ&#34;:3634,&#34;ĠíĶ¼íķ´&#34;:3635,&#34;íĬ¹íŀĪ&#34;:3636,&#34;Īë²½&#34;:3637,&#34;ĠëŁ¬ë&#34;:3638,&#34;ìĿĺë¯¸&#34;:3639,&#34;ë°ĭ&#34;:3640,&#34;ìłĲì§ľë¦¬&#34;:3641,&#34;ĠìŀĪìĹĪëįĺ&#34;:3642,&#34;ìĺ¤ëŀĺ&#34;:3643,&#34;ëĤ´ìļĶ&#34;:3644,&#34;ëĶ¸&#34;:3645,&#34;ĠëĤ¨ìķĦ&#34;:3646,&#34;ì§ĢëıĦìķĬ&#34;:3647,&#34;ĠìĿ¸ê°ĦìĿĺ&#34;:3648,&#34;ëĨĵìĿĢ&#34;:3649,&#34;ĠìĤ¶ìĿĦ&#34;:3650,&#34;ĠíĶĦëŀĳìĬ¤&#34;:3651,&#34;ìĬ¤ë¦´ëŁ¬&#34;:3652,&#34;Ģëĭ¤&#34;:3653,&#34;ĠìĺģíĻĶë³´ëĭ¤&#34;:3654,&#34;ë§ĪìĿ´&#34;:3655,&#34;ì¶°&#34;:3656,&#34;Ġë°ĶëŀĮ&#34;:3657,&#34;ĠìĺģìĽĲ&#34;:3658,&#34;Ġì¢ĭìķĺìĿĮ&#34;:3659,&#34;ĠëĬĲê»´ì§ĢëĬĶ&#34;:3660,&#34;Ġë¬´ìĹĩë³´ëĭ¤&#34;:3661,&#34;ëĤĺìĺ¨ëĭ¤&#34;:3662,&#34;ìĸ´ìĦ¤&#34;:3663,&#34;ĠëĤŃë¹Ħ&#34;:3664,&#34;ë§Įìľ¼ë¡ľ&#34;:3665,&#34;ëĮĢëĭ¨&#34;:3666,&#34;ìķĺìĸ´ìļĶ&#34;:3667,&#34;ë¬ĺ&#34;:3668,&#34;ĠìłĢì§Ī&#34;:3669,&#34;Ġì¢ĭìĿĢìĺģíĻĶ&#34;:3670,&#34;Ġëª°ëŀĲ&#34;:3671,&#34;Ġíģ¬ë¦¬ìĬ¤&#34;:3672,&#34;Ġì§Ģê¸ĪëıĦ&#34;:3673,&#34;ĠìĿ´ìĥģíķľ&#34;:3674,&#34;Ġëĭ¤ìļ´ë°Ľ&#34;:3675,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:3676,&#34;íķ´ë³´&#34;:3677,&#34;ìĬ¤ëĬĶ&#34;:3678,&#34;ìĺ¤ê¸Ģ&#34;:3679,&#34;Ġì§ĢëĤľ&#34;:3680,&#34;ìĦ±ë£¡&#34;:3681,&#34;íķĺëĭ¤ëĬĶ&#34;:3682,&#34;ìĨĮëħĦ&#34;:3683,&#34;ë¦¬ëĵ¤&#34;:3684,&#34;íĹĮ&#34;:3685,&#34;ìĤ¬ëĵ¤&#34;:3686,&#34;Ġd&#34;:3687,&#34;ĠìĿ´ìĺģíĻĶëĬĶ&#34;:3688,&#34;Ġê·¸ìĿĺ&#34;:3689,&#34;Ġëĭ¤ê°Ģ&#34;:3690,&#34;ĠìłĦìĦ¤&#34;:3691,&#34;Ġê°Ĳëıħëĭĺ&#34;:3692,&#34;íĿł&#34;:3693,&#34;ìĸ¸ìłľ&#34;:3694,&#34;Ġì¦Ŀ&#34;:3695,&#34;Ġê²½ì°°&#34;:3696,&#34;Ġë¶Ģì¡±íķľ&#34;:3697,&#34;Ġì§Īì§Ī&#34;:3698,&#34;Ġì²ľìŀ¬&#34;:3699,&#34;ĠíĻĺìĥģ&#34;:3700,&#34;ëĭĿíĥĢìŀĦ&#34;:3701,&#34;ĠìĻľìĿ´ë¦¬&#34;:3702,&#34;ìĸ´ë¦´ëķĮ&#34;:3703,&#34;¹Ľ&#34;:3704,&#34;ëĭ¤íģĲ&#34;:3705,&#34;ĠëĤĺë¨¸&#34;:3706,&#34;ĠíĸĪëįĺ&#34;:3707,&#34;ëª¨ìĬµ&#34;:3708,&#34;Ġê·¸ëŁ°ëį°&#34;:3709,&#34;ìĬ¤ëŁ½ê²Į&#34;:3710,&#34;80&#34;:3711,&#34;ìĭľì¼ľ&#34;:3712,&#34;Ġìĸ»&#34;:3713,&#34;ĠìĦ±ê³µ&#34;:3714,&#34;Ġë§ĪìĿĮìĿĦ&#34;:3715,&#34;ĠìĿ¸ìĥģê¹Ĭ&#34;:3716,&#34;tv&#34;:3717,&#34;ľĺ&#34;:3718,&#34;ë¦¬ëĭ¤&#34;:3719,&#34;Ġì§ĵ&#34;:3720,&#34;ìĤ¶&#34;:3721,&#34;íı°&#34;:3722,&#34;Ġë¹Ħì¶Ķ&#34;:3723,&#34;ìĭ¬íŀĪ&#34;:3724,&#34;Ġë¯¸ìĨĮ&#34;:3725,&#34;ĠìĹŃëĮĢ&#34;:3726,&#34;Ġì¢ĭìķĺëĬĶëį°&#34;:3727,&#34;ìĽĢìĿĦ&#34;:3728,&#34;ĠíĸĪì§Ģë§Į&#34;:3729,&#34;ĠìĿ´ìķ¼ê¸°ê°Ģ&#34;:3730,&#34;êµ¬ëł¤&#34;:3731,&#34;Ġëŀ&#34;:3732,&#34;ìĦľëĬĶ&#34;:3733,&#34;ìĸ´ëĿ¼&#34;:3734,&#34;Ġìĸ´ëłµ&#34;:3735,&#34;ìłĢëŁŃ&#34;:3736,&#34;ê³ĦìĨį&#34;:3737,&#34;ĠìĻĢëĭ¿&#34;:3738,&#34;Ġ:&#34;:3739,&#34;íķĺìĦ¸ìļĶ&#34;:3740,&#34;ê¸°ë¡ľ&#34;:3741,&#34;ë¦¬ìĹĲ&#34;:3742,&#34;ìķĦëĿ¼&#34;:3743,&#34;ëĦ¤ìĹ¬&#34;:3744,&#34;ìĹĪìĿĦ&#34;:3745,&#34;Ġë¶Ģëª¨&#34;:3746,&#34;ëĵ±íķĻìĥĿ&#34;:3747,&#34;ĠìĹ¬ëŁ¬&#34;:3748,&#34;Ġëĵłëĭ¤&#34;:3749,&#34;íĺģ&#34;:3750,&#34;ëıĦëĬĶ&#34;:3751,&#34;ëĤĺê°Ģ&#34;:3752,&#34;ìĬĪ&#34;:3753,&#34;ìķĦê°Ģ&#34;:3754,&#34;ê¹Ķ&#34;:3755,&#34;Ġíķľê³Ħ&#34;:3756,&#34;ĠëĵľëĬĶ&#34;:3757,&#34;ë¹ĦëĶĶìĺ¤&#34;:3758,&#34;Ġì¡°ìĹ°&#34;:3759,&#34;ĠëĤĺìĺ¤ê³ł&#34;:3760,&#34;ĠëªħìŀĳìĿ´ëĭ¤&#34;:3761,&#34;ĠìķĦëĭĮëį°&#34;:3762,&#34;ĠíĹĪìĪł&#34;:3763,&#34;ìĻ¸ë¡ľ&#34;:3764,&#34;ĠíĶĦë¡ľê·¸ëŀ¨&#34;:3765,&#34;Ĳë§ģ&#34;:3766,&#34;ìĿ´ìľł&#34;:3767,&#34;ìķĺìĬµëĭĪëĭ¤&#34;:3768,&#34;Ġíķ´ì¤Ģ&#34;:3769,&#34;Ġìĭ¤íĮ¨&#34;:3770,&#34;ĠëĦĺìĸ´&#34;:3771,&#34;Ġì²ĺìĿĮìĿ´ëĭ¤&#34;:3772,&#34;ë°°ê²½&#34;:3773,&#34;©°&#34;:3774,&#34;ĠìĺģíĻĶì§Ģë§Į&#34;:3775,&#34;ëĮĢê°Ģ&#34;:3776,&#34;Ġê°ĻìĿĮ&#34;:3777,&#34;Ġì¶¤&#34;:3778,&#34;Ġë©Ģ&#34;:3779,&#34;ĠëıĪìĿ´&#34;:3780,&#34;ì§ĢìķĬëĬĶ&#34;:3781,&#34;ì¡´ëĤĺ&#34;:3782,&#34;Ġ;;&#34;:3783,&#34;ë¦¬ëıĦ&#34;:3784,&#34;ìĹ¬ì£¼ìĿ¸ê³µ&#34;:3785,&#34;ĠíĿł&#34;:3786,&#34;ë¦¬ë©´&#34;:3787,&#34;Ġë§ŀëĬĶ&#34;:3788,&#34;ê°ĪìĪĺë¡Ŀ&#34;:3789,&#34;ê·¹ìŀ¥ìĹĲìĦľ&#34;:3790,&#34;Ġost&#34;:3791,&#34;ĠëĴ·&#34;:3792,&#34;ìĿ´ìĿĺ&#34;:3793,&#34;ëĤĺìĺ´&#34;:3794,&#34;ìĦľìķ¼&#34;:3795,&#34;ëģĪ&#34;:3796,&#34;ĠëĵľëĿ¼ë§Īê°Ģ&#34;:3797,&#34;ëªħìĿ´&#34;:3798,&#34;ĠëıĻíĻĶ&#34;:3799,&#34;ìķĦì§ģëıĦ&#34;:3800,&#34;Ħë¦¬íĭ°&#34;:3801,&#34;ìĿ´ìĬ¤&#34;:3802,&#34;ìķĦëĤĺ&#34;:3803,&#34;ìŀ¬ë¯¸ìĹĨìĿĮ&#34;:3804,&#34;ĠìĽĲëŀĺ&#34;:3805,&#34;Ġãħİãħİãħİ&#34;:3806,&#34;Ġê³³&#34;:3807,&#34;Ġëª©ìĨĮë¦¬&#34;:3808,&#34;(?)&#34;:3809,&#34;14&#34;:3810,&#34;ĠìķĦëĬĶ&#34;:3811,&#34;ë§ĲëıĦ&#34;:3812,&#34;ìĹ°ê±¸&#34;:3813,&#34;Ġì§Ģë£¨íķ´&#34;:3814,&#34;ë³´ê³łìĭ¶&#34;:3815,&#34;ĠìĤ¬ëŀĳìĿ´&#34;:3816,&#34;ë°ľìĿ´&#34;:3817,&#34;íĽĦë°ĺ&#34;:3818,&#34;íķĺê¸°ë§Į&#34;:3819,&#34;ĠìĽĲìŀĳìĿĦ&#34;:3820,&#34;Ġì²Ńì¶ĺ&#34;:3821,&#34;ëŁŃìłĢëŁŃ&#34;:3822,&#34;¥ĺ&#34;:3823,&#34;ĠìĦ¬&#34;:3824,&#34;ĠìŀĲìľł&#34;:3825,&#34;Ġë³´ê³łìĭ¶ìĿĢ&#34;:3826,&#34;Ġíķ´íĶ¼&#34;:3827,&#34;Ġê°ĲíĥĦ&#34;:3828,&#34;ĠíĶĮ&#34;:3829,&#34;ĠëĤľëĭ¤&#34;:3830,&#34;Ġê²ĮìŀĦ&#34;:3831,&#34;Ġê°Ħë§ĮìĹĲ&#34;:3832,&#34;íĳ¸&#34;:3833,&#34;ĠìĿ´ëŁ°ê±°&#34;:3834,&#34;Ġë¯¸ëŀĺ&#34;:3835,&#34;Ġìŀ¬ë¯¸ìĹĨê³ł&#34;:3836,&#34;ëŁ½ëĭ¤&#34;:3837,&#34;ìĭĿìľ¼ë¡ľ&#34;:3838,&#34;Ġëľ¨&#34;:3839,&#34;ĠëĵľëĿ¼ë§Īë¥¼&#34;:3840,&#34;ëıĦê°Ģ&#34;:3841,&#34;Ġë§ĪëĿ¼&#34;:3842,&#34;ë§ĮëĵľëĬĶ&#34;:3843,&#34;ìĺ¤ë¹ł&#34;:3844,&#34;ĠìĬ¤ì¼Ģ&#34;:3845,&#34;ìĦ±ìĿĢ&#34;:3846,&#34;ĠìĹ°ê¸°ìĹĲ&#34;:3847,&#34;ĠìĽĮ&#34;:3848,&#34;Ġíķ´ì£¼&#34;:3849,&#34;íħĶ&#34;:3850,&#34;ê´Ģê°Ŀ&#34;:3851,&#34;ì¶Ķìĸµ&#34;:3852,&#34;ëĶ°ëľ»&#34;:3853,&#34;Ġìł¤&#34;:3854,&#34;ëĵ¤ë§Į&#34;:3855,&#34;Ġë©Ķìĭľ&#34;:3856,&#34;Ġìķŀìľ¼ë¡ľ&#34;:3857,&#34;Ġì·¨íĸ¥&#34;:3858,&#34;ëļ±&#34;:3859,&#34;ëĩĮ&#34;:3860,&#34;Ġë¶Ļ&#34;:3861,&#34;íĴĢ&#34;:3862,&#34;ìĺĢìĸ´ìļĶ&#34;:3863,&#34;ë©´ìĦľëıĦ&#34;:3864,&#34;ĠìĿ¼ìĸ´&#34;:3865,&#34;Ġê³łë¯¼&#34;:3866,&#34;Ġë°ĶëĢ&#34;:3867,&#34;Ġì²ĺìĿĮë¶ĢíĦ°&#34;:3868,&#34;ìĥĿê°ģë³´ëĭ¤&#34;:3869,&#34;ĠëĪĪìĿĦ&#34;:3870,&#34;Ġëĸ¨ìĸ´ì§ĢëĬĶ&#34;:3871,&#34;Ġë¶Īë¥ľ&#34;:3872,&#34;Ġíį¼&#34;:3873,&#34;ĠìĿĳ&#34;:3874,&#34;ĠìĿ´íķĺ&#34;:3875,&#34;ê±°ëĥĲ&#34;:3876,&#34;ìĹĨìĸ´ìĦľ&#34;:3877,&#34;ì¦Ĳ&#34;:3878,&#34;Ġìŀĺë§Įëĵł&#34;:3879,&#34;Ġê·Ģìĭł&#34;:3880,&#34;ĠìĨĮìŀ¬ë¥¼&#34;:3881,&#34;ĠD&#34;:3882,&#34;Ġë³´ìĿ´ëĬĶ&#34;:3883,&#34;ë°¥&#34;:3884,&#34;ìĭľì²Ń&#34;:3885,&#34;ê·¸ëłĩ&#34;:3886,&#34;ĠìłĦë¬¸&#34;:3887,&#34;ë¬´ìĦŃ&#34;:3888,&#34;Ġê°ĲìłķìĿ´&#34;:3889,&#34;Ġëħ¼&#34;:3890,&#34;ĠìĿ´ìľłê°Ģ&#34;:3891,&#34;Ġëļ&#34;:3892,&#34;ë§¥&#34;:3893,&#34;ìĥ¤&#34;:3894,&#34;ìĻĢìĦľ&#34;:3895,&#34;íĸĪìĿĦ&#34;:3896,&#34;ĠëĬĻ&#34;:3897,&#34;ĠëĤ¨ëĬĶëĭ¤&#34;:3898,&#34;ĠìĬ¤íĨłë¦¬ìĻĢ&#34;:3899,&#34;Ġë¯¸ìķĪ&#34;:3900,&#34;ĠìķĪë´¤&#34;:3901,&#34;Ġê³µíı¬ë&#34;:3902,&#34;Ġë³´ëĬĶê²Į&#34;:3903,&#34;ĠìķĦê¹ĮìĽĮ&#34;:3904,&#34;ĠëĪĪìĿ´&#34;:3905,&#34;Ġì°įìĿĢ&#34;:3906,&#34;ê²Łëĭ¤&#34;:3907,&#34;90&#34;:3908,&#34;ë©ĶìĿ´&#34;:3909,&#34;ĠìķĦë§Ī&#34;:3910,&#34;Ġìĸ´ì°Į&#34;:3911,&#34;ĠëģĬ&#34;:3912,&#34;ë¹Ħê°Ģ&#34;:3913,&#34;ĠëĮĢíķľë¯¼êµŃ&#34;:3914,&#34;ĠëĪĦêµ°&#34;:3915,&#34;Ġë¡&#34;:3916,&#34;ê°Ģìŀ¥&#34;:3917,&#34;ëĵ¤ê³ł&#34;:3918,&#34;ìķĦë²Ħì§Ģ&#34;:3919,&#34;ĠëĤĺìĦľ&#34;:3920,&#34;ëł¤ëĭ¤&#34;:3921,&#34;ĠìĻľìĿ´ëłĩê²Į&#34;:3922,&#34;ĠìĤ¬ëŀĮìĿĦ&#34;:3923,&#34;ĠíķĺëĤĺíķĺëĤĺ&#34;:3924,&#34;Ġê·¸ëŁ°ê°Ģ&#34;:3925,&#34;ëŃĲì§Ģ&#34;:3926,&#34;ĠìŀĪëĭ¤ë©´&#34;:3927,&#34;Ġìľłì¹ĺíķľ&#34;:3928,&#34;ìŀ¬ë¯¸ìŀĪìĸ´ìļĶ&#34;:3929,&#34;ĠìĹ¬ëŁ¬ë&#34;:3930,&#34;ĠíĨµíķ´&#34;:3931,&#34;¬ë¡ľ&#34;:3932,&#34;³´ëĭ¤&#34;:3933,&#34;ĠìĿ´ëŀĺ&#34;:3934,&#34;ìľ¼ë¡ł&#34;:3935,&#34;Ġê¸Ī&#34;:3936,&#34;ì¤ĳêµŃ&#34;:3937,&#34;ìłĢëĬĶ&#34;:3938,&#34;Ġë§¤ëł¥ìĿ´&#34;:3939,&#34;ëĳ¥&#34;:3940,&#34;Ġë¦¬ë©ĶìĿ´íģ¬&#34;:3941,&#34;Ġê°ĲìĤ¬íķ©ëĭĪëĭ¤&#34;:3942,&#34;Ġ+&#34;:3943,&#34;ëĭ¤ì§Ģ&#34;:3944,&#34;ëĬĶê°Ģ&#34;:3945,&#34;ìĺģíĻĶëĿ¼&#34;:3946,&#34;ìĺģíĻĶëĿ¼ê³ł&#34;:3947,&#34;Ġê°ĢìĦľ&#34;:3948,&#34;ìĹ°ê¸°ëł¥&#34;:3949,&#34;ĠíĻĶê°Ģ&#34;:3950,&#34;Ġì¤Ħê±°ë¦¬&#34;:3951,&#34;Ġìłģëĭ¹&#34;:3952,&#34;ĠëĤĺìĻĶìľ¼ë©´&#34;:3953,&#34;ĠëŁ¬ë¸Į&#34;:3954,&#34;11&#34;:3955,&#34;ĠìĺģíĻĶëĥĲ&#34;:3956,&#34;ëĦ¤ìĿ´ë²Ħ&#34;:3957,&#34;Ġë´¤ëĭ¤ê°Ģ&#34;:3958,&#34;ìĹ¬ê¸°&#34;:3959,&#34;ĠìķĮìķĦ&#34;:3960,&#34;ĠìĿ´ëŁ°ìĺģíĻĶ&#34;:3961,&#34;Ġìŀ¬ë¯¸ìŀĪê³ł&#34;:3962,&#34;Ġëħ¹&#34;:3963,&#34;ì§±ì§±&#34;:3964,&#34;ê·¸ëŀĺìĦľ&#34;:3965,&#34;Ġëıħë¦½&#34;:3966,&#34;ĠìĬ¤íĨłë¦¬ë¥¼&#34;:3967,&#34;ìĸ´ëł¸ìĿĦëķĮ&#34;:3968,&#34;ë³įê²Į&#34;:3969,&#34;ĠìĹ¬ìłĦíŀĪ&#34;:3970,&#34;ìİĦ&#34;:3971,&#34;ì¨&#34;:3972,&#34;íķľì§Ģ&#34;:3973,&#34;ëĿ¼ìĬ¤&#34;:3974,&#34;ëŀĲëĭ¤&#34;:3975,&#34;Ġìĺ¬ëĿ¼&#34;:3976,&#34;ìĻĢë¥´&#34;:3977,&#34;Ġíķµ&#34;:3978,&#34;ĠìłĢëĬĶ&#34;:3979,&#34;ĠìĹŃíķł&#34;:3980,&#34;ìľĦíķ´&#34;:3981,&#34;ê²°êµŃ&#34;:3982,&#34;ĠìķĪëĲĺê³ł&#34;:3983,&#34;Ġ80&#34;:3984,&#34;íļ¨ê³¼&#34;:3985,&#34;ĠìķĪë³¸&#34;:3986,&#34;µĿ&#34;:3987,&#34;ìĵ´&#34;:3988,&#34;ìĿ´íĦ°&#34;:3989,&#34;ëĭĪìķĦ&#34;:3990,&#34;Ġë³´ëŁ¬&#34;:3991,&#34;íķłê¹Į&#34;:3992,&#34;Ġëįĺ&#34;:3993,&#34;ĠëĤ´ìļ©ìĿĦ&#34;:3994,&#34;Ġìļ°ìļ¸&#34;:3995,&#34;Ġìĭľê°ĦëĤŃë¹Ħ&#34;:3996,&#34;Ġì²ĺìĿĮìľ¼ë¡ľ&#34;:3997,&#34;Ġíķľíİ¸&#34;:3998,&#34;ĠìĥĿê°ģíķ©ëĭĪëĭ¤&#34;:3999,&#34;ìĿ´ëĿ¼ìĦľ&#34;:4000,&#34;ĠëģĿëĤĺê³ł&#34;:4001,&#34;ìĹŃìĤ¬&#34;:4002,&#34;íķľëĭ¤ë©´&#34;:4003,&#34;ìĤ´ëĭ¤&#34;:4004,&#34;ë¨¹ê³ł&#34;:4005,&#34;ì¾Įíķľ&#34;:4006,&#34;ìķĦëĭĪëĿ¼&#34;:4007,&#34;ĠìŀĲìĭłìĿĺ&#34;:4008,&#34;Ġì²łíķĻ&#34;:4009,&#34;bb&#34;:4010,&#34;ê¸°ë¶Ħ&#34;:4011,&#34;Ġìłķì¹ĺ&#34;:4012,&#34;ĠìĭľìĦł&#34;:4013,&#34;Ġë´¤ìĹĪëĬĶëį°&#34;:4014,&#34;Ġê°Ģì§Ħ&#34;:4015,&#34;ìĿ¼ëĭ¨&#34;:4016,&#34;Ġìŀ¬ë°Įìĸ´&#34;:4017,&#34;ĠìłĦëĭ¬&#34;:4018,&#34;ĠìķĪíķĺê³ł&#34;:4019,&#34;ë¥¸ëĭ¤&#34;:4020,&#34;Ġë³´ê¸°ìĹĶ&#34;:4021,&#34;Ġìľłì¹ĺíķĺê³ł&#34;:4022,&#34;ì§ĢìķĬìĿĢ&#34;:4023,&#34;íĳľíĺĦ&#34;:4024,&#34;ë©ĺíĦ°&#34;:4025,&#34;ìĹ¬ìļ´ìĿ´&#34;:4026,&#34;íīģ&#34;:4027,&#34;an&#34;:4028,&#34;ĵ¨&#34;:4029,&#34;Ġë¥ĺ&#34;:4030,&#34;ìĿ´ìŀĲ&#34;:4031,&#34;ëłī&#34;:4032,&#34;ìĹĪìĸ´&#34;:4033,&#34;ì£¼ìĸ¼&#34;:4034,&#34;íĨ¤&#34;:4035,&#34;ê±¸ë¡ľ&#34;:4036,&#34;ìĽĲìĿ´&#34;:4037,&#34;ìĶ¨ê°Ģ&#34;:4038,&#34;ĠëĪĦêµ¬&#34;:4039,&#34;Ġê´ľíŀĪ&#34;:4040,&#34;**&#34;:4041,&#34;100&#34;:4042,&#34;30&#34;:4043,&#34;ou&#34;:4044,&#34;įëĭĪëĭ¤&#34;:4045,&#34;ìĿ´ë¥¼&#34;:4046,&#34;íķľë§ĪëĶĶë¡ľ&#34;:4047,&#34;ë²¤&#34;:4048,&#34;ĠëĦĪë¬´ëĦĪë¬´&#34;:4049,&#34;ë¯¸ì¹ľ&#34;:4050,&#34;Ġìŀĺë´¤ìĬµëĭĪëĭ¤&#34;:4051,&#34;ëłĪìĬ¤&#34;:4052,&#34;ëŃĺ&#34;:4053,&#34;Ġë³Ħë£¨&#34;:4054,&#34;Ġì§ľì¦ĿëĤĺ&#34;:4055,&#34;ĠíĴĢ&#34;:4056,&#34;ĠëĮĢëĭ¨íķĺëĭ¤&#34;:4057,&#34;ĠìķĪëĲ¨&#34;:4058,&#34;Ġcg&#34;:4059,&#34;ëĤĺìĿĺ&#34;:4060,&#34;ì£¼ìĹ°&#34;:4061,&#34;ëĤ´ìĿ¸ìĥĿ&#34;:4062,&#34;ìĹĲìĦľìĿĺ&#34;:4063,&#34;ĠëĮĢì¶©&#34;:4064,&#34;ëĶĶìĸ´&#34;:4065,&#34;íĬ¸ê°Ģ&#34;:4066,&#34;ĠëĵľëĿ¼ë§ĪëĬĶ&#34;:4067,&#34;ĠëĲĺìĹĪëĭ¤&#34;:4068,&#34;ĠìĹ¬ìŀĲê°Ģ&#34;:4069,&#34;ëĤ´ìļ©ìĿĢ&#34;:4070,&#34;ê¶ģ&#34;:4071,&#34;íĹĪìłĳ&#34;:4072,&#34;15&#34;:4073,&#34;ģìĵ¸&#34;:4074,&#34;íķĺëł¤ëĬĶ&#34;:4075,&#34;ĠìĹĨì§Ģë§Į&#34;:4076,&#34;Ġì°¬&#34;:4077,&#34;Ġì¤ĳë°ĺ&#34;:4078,&#34;ì¶ľìĹ°&#34;:4079,&#34;ëĨ¨&#34;:4080,&#34;Ġìŀ¬ë¯¸ìĹĨìĸ´&#34;:4081,&#34;ĠìłĦê¸°ìĦ¸ê°Ģ&#34;:4082,&#34;ê¸°ìĿĺ&#34;:4083,&#34;ìĭľìķĦ&#34;:4084,&#34;ê¹Įì§Ħ&#34;:4085,&#34;êµ¬íķĺê³ł&#34;:4086,&#34;Ġëª¨ìŀĲ&#34;:4087,&#34;ìĭłìĦł&#34;:4088,&#34;ì¡°ê¸Ī&#34;:4089,&#34;Ġë°Ķê¿&#34;:4090,&#34;Ġê¼´&#34;:4091,&#34;ì´ĪëĶ©&#34;:4092,&#34;íıīë¡łê°Ģ&#34;:4093,&#34;ĠíĻ©ëĭ¹&#34;:4094,&#34;íĿ¥ë¯¸&#34;:4095,&#34;ì§Īëģ&#34;:4096,&#34;ãħģ&#34;:4097,&#34;ĠìķĦìłĢìĶ¨&#34;:4098,&#34;ĠëĤĺìĹĲê²Į&#34;:4099,&#34;ìłĦíĺĢ&#34;:4100,&#34;Ġìŀ¬ë¯¸ë¥¼&#34;:4101,&#34;íİ¸ë³´ëĭ¤&#34;:4102,&#34;ë²Ħëł¸ëĭ¤&#34;:4103,&#34;ê»Ħ&#34;:4104,&#34;ĠìĺĪìģľ&#34;:4105,&#34;ì©Ķ&#34;:4106,&#34;Ġì§Ģê¸Īê¹Įì§Ģ&#34;:4107,&#34;ìĽĥê¸°&#34;:4108,&#34;ìŀĬ&#34;:4109,&#34;Ġë³´ìĭľê¸¸&#34;:4110,&#34;ëĮĢëĬĶ&#34;:4111,&#34;Ġëĭ¤ë§Į&#34;:4112,&#34;ìŀĪìĸ´&#34;:4113,&#34;ĠìĥĪë¡&#34;:4114,&#34;Ġë³´ê³łëĤĺìĦľ&#34;:4115,&#34;ĠìĤ¬ëĬĶ&#34;:4116,&#34;ê²°ë§ĲìĿ´&#34;:4117,&#34;ĠíĿ¥ë¯¸ë¡Ń&#34;:4118,&#34;Ġê·¸ëŁ¼&#34;:4119,&#34;ìĭľëĤĺë¦¬ìĺ¤&#34;:4120,&#34;ĠìŀĪìĬµëĭĪëĭ¤&#34;:4121,&#34;ì°Ŀ&#34;:4122,&#34;ĠìĺģíĻĺ&#34;:4123,&#34;ĠìµľìķħìĿ´ëĭ¤&#34;:4124,&#34;Ġëª°ìŀħìĿ´&#34;:4125,&#34;ĠìĹ°ê¸°ëł¥ìĿ´&#34;:4126,&#34;¬ëł&#34;:4127,&#34;!!!!!&#34;:4128,&#34;ĠëĤĺë¥¼&#34;:4129,&#34;ĠëĤĺíĥĢ&#34;:4130,&#34;ìĪĺëĬĶ&#34;:4131,&#34;ĠëĦĪë¬´ëĤĺëıĦ&#34;:4132,&#34;ëĵľëĿ¼&#34;:4133,&#34;Ġìŀ¬ë°ĮìĹĪìĸ´ìļĶ&#34;:4134,&#34;ĠìĹ¬ìĦ±&#34;:4135,&#34;ëįĶë§Į&#34;:4136,&#34;ë§¤ëł¥&#34;:4137,&#34;Ġê¸°ë¶ĦìĿ´&#34;:4138,&#34;Ġìĸ´ìĥīíķľ&#34;:4139,&#34;ì§Ģì»¬&#34;:4140,&#34;Ġê·¸ëĵ¤ìĿĺ&#34;:4141,&#34;ìĹĪì§Ģ&#34;:4142,&#34;ë²¼&#34;:4143,&#34;ĠìĹ°ê¸°ìĻĢ&#34;:4144,&#34;ĠìĿ¼ëĭ¨&#34;:4145,&#34;ëĲĺì§Ģ&#34;:4146,&#34;ì¦ĪëĭĪ&#34;:4147,&#34;ĠíķĦìļĶìĹĨëĬĶ&#34;:4148,&#34;Ġìłľìŀĳì§Ħ&#34;:4149,&#34;Ġê·ĢìĹ¬ìļ´&#34;:4150,&#34;ĠíķĻêµĲ&#34;:4151,&#34;ĠìłĦì²´ìłģìľ¼ë¡ľ&#34;:4152,&#34;Ġì§ł&#34;:4153,&#34;Ġëĭ¤ìĨĮ&#34;:4154,&#34;ëŀĺê¸°&#34;:4155,&#34;ë¬´ìĦľ&#34;:4156,&#34;ìĤ¬ëŀĮìĿĢ&#34;:4157,&#34;ĠìĻ¸êµŃ&#34;:4158,&#34;ìŀĶìŀĶíķľ&#34;:4159,&#34;ê°ĢëĬ¥&#34;:4160,&#34;ë³´ìĿ´&#34;:4161,&#34;Ġë°Ŀ&#34;:4162,&#34;ë´ĩ&#34;:4163,&#34;íĸĪëĭ¤ëĬĶ&#34;:4164,&#34;ê°ĻìĿĮ&#34;:4165,&#34;Ġë§Įëĵ¤ìĸ´ì§Ħ&#34;:4166,&#34;ĠìĪĺì¤ĢìĿ´&#34;:4167,&#34;Ġì©Ķ&#34;:4168,&#34;Ġê¾¸&#34;:4169,&#34;ĠìĺĪë»&#34;:4170,&#34;ìĿ´ê²ĥëıĦ&#34;:4171,&#34;ìĸ´ì§Ħ&#34;:4172,&#34;ë¡ľëĬĶ&#34;:4173,&#34;ë¡ľë§¨&#34;:4174,&#34;ìķĦëĨĶ&#34;:4175,&#34;ê±°ìļ´&#34;:4176,&#34;Ġëĭ¤ìĭľë´ĲëıĦ&#34;:4177,&#34;ìķĪë³´&#34;:4178,&#34;Ġíı¬ìŀ¥&#34;:4179,&#34;ëıĮìķĦ&#34;:4180,&#34;ĳ¹&#34;:4181,&#34;ĠìĿ´ìłł&#34;:4182,&#34;ìłĲì£¼ëĬĶ&#34;:4183,&#34;ê±°ë¦¬ëĬĶ&#34;:4184,&#34;ë§ĪìłĢ&#34;:4185,&#34;ìĥģìĿ´&#34;:4186,&#34;Ġëı¼&#34;:4187,&#34;Ġëįľ&#34;:4188,&#34;Ġê¸°ë³¸&#34;:4189,&#34;ĠìĪĺëıĦ&#34;:4190,&#34;ĠìłĦìĹĲ&#34;:4191,&#34;Ġê°ľíĮĲ&#34;:4192,&#34;Ġìļ°ìĹ°íŀĪ&#34;:4193,&#34;Ġê¼½&#34;:4194,&#34;Ġë³´ì§Ģë§Ī&#34;:4195,&#34;Ġë³´ì§Ģë§ĪìĦ¸ìļĶ&#34;:4196,&#34;Ġë¦¬ë·°&#34;:4197,&#34;ĠíĿĲë¦Ħ&#34;:4198,&#34;µĿìĺ¤&#34;:4199,&#34;ë¢&#34;:4200,&#34;ë¡¯&#34;:4201,&#34;ìĿ¸ëıĦ&#34;:4202,&#34;ìĭľíĤ¨&#34;:4203,&#34;ĠìŀĪëĤĺ&#34;:4204,&#34;ê³µê°Ĳ&#34;:4205,&#34;ĠìĤ¬ìĿ´&#34;:4206,&#34;ĠìĤ¬ê·¹&#34;:4207,&#34;ĠìłĢëłĩê²Į&#34;:4208,&#34;ì½Ķë¯¹&#34;:4209,&#34;Ġìĭ¬ìĭ¬&#34;:4210,&#34;ìķĦê¹Įìļ´&#34;:4211,&#34;Ġëª¸ë§¤&#34;:4212,&#34;ĠìĨĮìŀ¬ë¡ľ&#34;:4213,&#34;ë§ĺ&#34;:4214,&#34;ì§ĢìĿĺ&#34;:4215,&#34;Ġìŀĩ&#34;:4216,&#34;Ġì¢ĭìĿĦ&#34;:4217,&#34;ìľ¼ë¡ľìĦľ&#34;:4218,&#34;Ġìĸ´ìĿ´ê°Ģ&#34;:4219,&#34;ĠëŃī&#34;:4220,&#34;ëĵ¯ìĿ´&#34;:4221,&#34;Ġìĺ¤ëĿ½&#34;:4222,&#34;ĠìľłìĿ¼&#34;:4223,&#34;ëĦĪë¬´ëĤĺ&#34;:4224,&#34;ĠìłĢëıĦ&#34;:4225,&#34;ĠíĹĽ&#34;:4226,&#34;ê´Ģê³Ħ&#34;:4227,&#34;ĠìķĦëĭĮëĵ¯&#34;:4228,&#34;ìĽłìĿĮ&#34;:4229,&#34;Ġë°ĺìłĦìĿ´&#34;:4230,&#34;Ġê·¸ëłĩëĭ¤&#34;:4231,&#34;ĠíĹĪìłĳíķľ&#34;:4232,&#34;ĠìķĦë¦Ħëĭµê³ł&#34;:4233,&#34;?!&#34;:4234,&#34;ìĿ´ë¦¬&#34;:4235,&#34;ë§ĮìłĲ&#34;:4236,&#34;Ġë°¤&#34;:4237,&#34;ëĮĢìĿĺ&#34;:4238,&#34;ĠíķĺëĦ¤ìļĶ&#34;:4239,&#34;ëª¸&#34;:4240,&#34;Ġìĺ¤ê·¸ëĿ¼&#34;:4241,&#34;Ġë¶ĢíĦ°&#34;:4242,&#34;ĠëĲĺê³ł&#34;:4243,&#34;Ġê³łìĥĿ&#34;:4244,&#34;ëĳĲê³ł&#34;:4245,&#34;¬ë¦Ħ&#34;:4246,&#34;ìķĪëĲ&#34;:4247,&#34;ëĶ°ëĿ¼&#34;:4248,&#34;Ġíı¬ê¸°&#34;:4249,&#34;Ġì±Ļ&#34;:4250,&#34;Ġê³µê°ĲìĿ´&#34;:4251,&#34;Ġì¹´ë©ĶëĿ¼&#34;:4252,&#34;Ġtv&#34;:4253,&#34;Ġì§Ħë¶Ģíķľ&#34;:4254,&#34;ìĺ¬ëķĮ&#34;:4255,&#34;ĪëķĮ&#34;:4256,&#34;ë¡±&#34;:4257,&#34;¬ëŀĢ&#34;:4258,&#34;Ġíķľë§ĪëĶĶë¡ľ&#34;:4259,&#34;Ġë´¤ìľ¼ë©´&#34;:4260,&#34;Ġê°ĢëģĶ&#34;:4261,&#34;Ġ15&#34;:4262,&#34;Ġê°ĻìķĦ&#34;:4263,&#34;ĠëĶ´&#34;:4264,&#34;íĤ¬ë§ģíĥĢìŀĦìļ©&#34;:4265,&#34;ĠíĻľ&#34;:4266,&#34;ĠíĥĪ&#34;:4267,&#34;ĠëĳĲë²Ī&#34;:4268,&#34;Ġë§¤ëł¥ìłģìĿ¸&#34;:4269,&#34;êµ¬ë¥¼&#34;:4270,&#34;ĠìĨĮë¦Ħëıĭ&#34;:4271,&#34;Ġìļ©ìĦľ&#34;:4272,&#34;Ġê´´ë¬¼&#34;:4273,&#34;íķľê²ĥ&#34;:4274,&#34;ĠìĿ´ê¸°&#34;:4275,&#34;íķ´ì¤Ģ&#34;:4276,&#34;Ġë³´ìĭľ&#34;:4277,&#34;ë²Īë³´&#34;:4278,&#34;ĠìĽĶ&#34;:4279,&#34;Ġë¶ĢëģĦ&#34;:4280,&#34;ìĨįìĹĲìĦľ&#34;:4281,&#34;ëĬĲëĿ¼&#34;:4282,&#34;ë¥ĺìĿĺ&#34;:4283,&#34;Ġì§ľì¦ĿëĤĺëĬĶ&#34;:4284,&#34;ĠìĿ´ìľłëĬĶ&#34;:4285,&#34;Ġìĸ´ìĿ´ìĹĨëĬĶ&#34;:4286,&#34;ĠëĪĪë¹Ľ&#34;:4287,&#34;ë¢°&#34;:4288,&#34;Ġë§¡&#34;:4289,&#34;ëĦ£&#34;:4290,&#34;ĠìĹĨëĤĺ&#34;:4291,&#34;ë£¬&#34;:4292,&#34;ìĦ±ìĹĲ&#34;:4293,&#34;ĠìĨĮëħĢ&#34;:4294,&#34;ë´¤ìĿĮ&#34;:4295,&#34;ìŀ¬ë°ĮëĬĶëį°&#34;:4296,&#34;ĠíıĲ&#34;:4297,&#34;ë°°ìļ°ê°Ģ&#34;:4298,&#34;ê¼½&#34;:4299,&#34;ì§Ģë£¨íķ¨&#34;:4300,&#34;Ġì¶Ķì²ľíķ©ëĭĪëĭ¤&#34;:4301,&#34;ê´ľì°®ìĿĢ&#34;:4302,&#34;ê²¹ëĭ¤&#34;:4303,&#34;ĠOST&#34;:4304,&#34;ìĿ´ëŀĺ&#34;:4305,&#34;ìķĦëĭĺ&#34;:4306,&#34;ê¹Įì§ĢëĬĶ&#34;:4307,&#34;ì§ĢìķĬê³ł&#34;:4308,&#34;ê¾¼&#34;:4309,&#34;íķĺìĭľëĬĶ&#34;:4310,&#34;ĠìĬ¬íĶĦëĭ¤&#34;:4311,&#34;ĠìłĬìĿĢ&#34;:4312,&#34;ìŀĪëįĺ&#34;:4313,&#34;ãħĪ&#34;:4314,&#34;ëįľ&#34;:4315,&#34;ìĺ¹&#34;:4316,&#34;Ġë°¥&#34;:4317,&#34;ìŀĲëĭ¨&#34;:4318,&#34;ĠëģĿëĤĺëĬĶ&#34;:4319,&#34;ìĹ¬ë°°ìļ°&#34;:4320,&#34;ëĪĦê°Ģ&#34;:4321,&#34;ì¤ĺìĦľ&#34;:4322,&#34;ĠíĿ¬ìĥĿ&#34;:4323,&#34;ĠìĻłì§Ģ&#34;:4324,&#34;ëıĦìĹĨê³ł&#34;:4325,&#34;ìŀĲê³ł&#34;:4326,&#34;Ġíķĺë£¨&#34;:4327,&#34;ìĻĢëĬĶ&#34;:4328,&#34;ìĭłìĿĺ&#34;:4329,&#34;íķĺëĭ¤ê°Ģ&#34;:4330,&#34;ĠìĥĿê°ģëĤĺëĬĶ&#34;:4331,&#34;ë²Ħëł¸&#34;:4332,&#34;Ġì¹¼&#34;:4333,&#34;ĠìĤ´ì§Ŀ&#34;:4334,&#34;ĠíĻķìĭ¤íŀĪ&#34;:4335,&#34;ì¤ĳê°ĦìĹĲ&#34;:4336,&#34;ĠìķĦë¦Ħëĭµëĭ¤&#34;:4337,&#34;ĠìĪľìĪĺíķľ&#34;:4338,&#34;ì§Ģë§Ī&#34;:4339,&#34;Ġë³´ìĿ´&#34;:4340,&#34;ìĺģíĻĶìŀĦ&#34;:4341,&#34;ë³´ìĹ¬&#34;:4342,&#34;ìŀ¥ë¥´&#34;:4343,&#34;ìĤ¬ìĭ¤&#34;:4344,&#34;Ġë§Īì¹ĺ&#34;:4345,&#34;ëħĦìłĦìĹĲ&#34;:4346,&#34;ìķĪìĹĲ&#34;:4347,&#34;Ġë§Įëĵ¤ê³ł&#34;:4348,&#34;Ġìĥģì²ĺ&#34;:4349,&#34;Ġì¢ĭìķĺëįĺ&#34;:4350,&#34;ë³ĦìłĲ&#34;:4351,&#34;Ġíİĳ&#34;:4352,&#34;Ġíĳľìłķ&#34;:4353,&#34;Ġê±´ì§Ģ&#34;:4354,&#34;Ġë´Ĳìķ¼íķł&#34;:4355,&#34;ĠìĶ¬&#34;:4356,&#34;ê°Ģê°Ģ&#34;:4357,&#34;ê¸°ëįķ&#34;:4358,&#34;ìķĦìĺ¤&#34;:4359,&#34;ĠìŀĪì§Ģ&#34;:4360,&#34;íŀĪëĬĶ&#34;:4361,&#34;Ġë§ĲìĿĦ&#34;:4362,&#34;Ġìĭ¶ìĸ´&#34;:4363,&#34;Ġì¹¨&#34;:4364,&#34;ë°°ìļ°ëĵ¤ìĿ´&#34;:4365,&#34;ĠíıīìĥĿ&#34;:4366,&#34;ĠíķĦìļĶìĹĨëĭ¤&#34;:4367,&#34;ìŀ¥ë©´ìĿ´&#34;:4368,&#34;ĠëĨĢëŀį&#34;:4369,&#34;ĠëĸłëĤĺ&#34;:4370,&#34;ĠìĦ¹ìĭľ&#34;:4371,&#34;°©&#34;:4372,&#34;ĠìĺģíĻĶê´Ģ&#34;:4373,&#34;ëĵ¤ìĹĲ&#34;:4374,&#34;Ġê²©&#34;:4375,&#34;ĠìŀĪê²Į&#34;:4376,&#34;ëįĺì§Ģ&#34;:4377,&#34;Ġë¹Ħê·¹&#34;:4378,&#34;Ġê°ĲëıĻìłģ&#34;:4379,&#34;Ġìŀ¬ë¯¸ìĹĨëĦ¤&#34;:4380,&#34;ìĺĪìĪł&#34;:4381,&#34;ìĺĪìłĦìĹĲ&#34;:4382,&#34;Ġì¡¸ëĿ¼&#34;:4383,&#34;ëĭ¤ìĭľë´ĲëıĦ&#34;:4384,&#34;Ġìĭ¸ìĿ´ì½Ķ&#34;:4385,&#34;Ġìĭ«ëĭ¤&#34;:4386,&#34;ê²¼ëĭ¤&#34;:4387,&#34;Ġìŀ¼ìŀĪê²Į&#34;:4388,&#34;Ġíģ¬ê²Į&#34;:4389,&#34;ĠìĿ´ëŁ¬&#34;:4390,&#34;Ġì§Ĳ&#34;:4391,&#34;ìĺģíĻĶë³´ëĭ¤&#34;:4392,&#34;ĠëĤĺìĿĢ&#34;:4393,&#34;ĠìĺĢ&#34;:4394,&#34;Ġìĺ¤ëŀľ&#34;:4395,&#34;ĠíıīìłĲìĹĲ&#34;:4396,&#34;Ġì§Ģë£¨íķĺì§Ģ&#34;:4397,&#34;ëģ¼ë¦¬&#34;:4398,&#34;ĠìĹŃìĭľëĤĺ&#34;:4399,&#34;Ġìŀ¬ë°ĭê²Į&#34;:4400,&#34;19&#34;:4401,&#34;Ġìº&#34;:4402,&#34;ìĿ´íĽĦ&#34;:4403,&#34;ãħı&#34;:4404,&#34;ëłģ&#34;:4405,&#34;íķ´ì§Ħ&#34;:4406,&#34;ì²«&#34;:4407,&#34;ìĪĺìĹĨëĬĶ&#34;:4408,&#34;íĬ¸ì½¤&#34;:4409,&#34;ĠìĤ¬ê¸°&#34;:4410,&#34;ìķłëĵ¤ìĿ´&#34;:4411,&#34;ìĦ¤ìłķ&#34;:4412,&#34;Ġë¶Ħëħ¸&#34;:4413,&#34;Ġê¸´ìŀ¥&#34;:4414,&#34;ì²ĺìĿĮë¶ĢíĦ°&#34;:4415,&#34;ìĽĲìŀĳìĿĦ&#34;:4416,&#34;ĠìĤ°ë§Į&#34;:4417,&#34;Ġíļ¨&#34;:4418,&#34;ìĨĮìŀ¬ëĬĶ&#34;:4419,&#34;Ġê¹ĶëģĶ&#34;:4420,&#34;ëĬĶê±´&#34;:4421,&#34;ĠìĺģíĻĶë³´ê³ł&#34;:4422,&#34;ìĺģíĻĶê´ĢìĹĲìĦľ&#34;:4423,&#34;Ġê·¸ê²ĥ&#34;:4424,&#34;Ġê·¸ë¦°&#34;:4425,&#34;ìŀĲìĹ°&#34;:4426,&#34;ìĪĺìĿĺ&#34;:4427,&#34;ĠëĵľëŁ¬&#34;:4428,&#34;ëł¥ìĿĦ&#34;:4429,&#34;ĠíĽĦìĨį&#34;:4430,&#34;ĠíķĦìļĶíķľ&#34;:4431,&#34;ĠëĿ¼ê³ł&#34;:4432,&#34;ĠíĻįë³´&#34;:4433,&#34;Ġê·ĢìĹ½ê³ł&#34;:4434,&#34;on&#34;:4435,&#34;ĥ¥&#34;:4436,&#34;ëĤļ&#34;:4437,&#34;ìĭľìĹĲ&#34;:4438,&#34;ê°ĦìĿĺ&#34;:4439,&#34;êµ¬ëĭĪ&#34;:4440,&#34;ì¤ĳìĿĺ&#34;:4441,&#34;ĠìķĬê²Į&#34;:4442,&#34;Ġêµ¬ë&#34;:4443,&#34;Ġìĺ¤ë²Ħ&#34;:4444,&#34;Ġìķłëĵ¤ìĿ´&#34;:4445,&#34;ìŀĶìĿ¸&#34;:4446,&#34;ê°Ĳëıħëĭĺ&#34;:4447,&#34;ìĿ¸ê°ĢìļĶ&#34;:4448,&#34;Ġíĺķíİ¸&#34;:4449,&#34;ĠìĨĮë¦Ħëģ¼&#34;:4450,&#34;Ġê¹Įì§Ģ&#34;:4451,&#34;ad&#34;:4452,&#34;Ġë±&#34;:4453,&#34;Ġë¿&#34;:4454,&#34;ìĿ´ë©°&#34;:4455,&#34;íķĺëł¤&#34;:4456,&#34;ìĦ¯&#34;:4457,&#34;ĠìĿ´ë£¨&#34;:4458,&#34;ìĬ¤íĨł&#34;:4459,&#34;ĠìĹ°ìķł&#34;:4460,&#34;ëªħìĿĺ&#34;:4461,&#34;Ġìĭľê°ĦìĿĦ&#34;:4462,&#34;ìĬ¤íĨłë¦¬ëĬĶ&#34;:4463,&#34;íķĺê¸°ëıĦ&#34;:4464,&#34;ëįĶëĿ¼êµ¬ìļĶ&#34;:4465,&#34;ëıħêµĲ&#34;:4466,&#34;Ġì£Ħëĭ¤&#34;:4467,&#34;ĠíĻĶëł¤íķľ&#34;:4468,&#34;ëĭ¤ìĿĮ&#34;:4469,&#34;Ġìŀ¤&#34;:4470,&#34;ìĿĦê±°&#34;:4471,&#34;Ġê·¸ë¦½&#34;:4472,&#34;ë°ĳìĹĲ&#34;:4473,&#34;Ġíķĺë©´ìĦľ&#34;:4474,&#34;ĠìĹ°ìĺĪ&#34;:4475,&#34;ìĭ¤íĻĶ&#34;:4476,&#34;Ġë³´ê³łìĭ¶ëĭ¤&#34;:4477,&#34;Ġë°ĺìĦ±&#34;:4478,&#34;ĠìĿĺë¬¸&#34;:4479,&#34;Ġëª°ëĿ¼ëıĦ&#34;:4480,&#34;ĠíĬ¹ë³Ħ&#34;:4481,&#34;Ġì£¼ìĿ¸ê³µìĿĺ&#34;:4482,&#34;ë¶Īê°Ģ&#34;:4483,&#34;ĠãħħãħĤ&#34;:4484,&#34;re&#34;:4485,&#34;ĠìĭŃ&#34;:4486,&#34;Ġê·¸ì§Ģ&#34;:4487,&#34;ìĭĿìĿĦ&#34;:4488,&#34;ĠíĺĦìŀ¬&#34;:4489,&#34;íĶ¼ìĨĮ&#34;:4490,&#34;ì£¼ìĿ¸ê³µìĿ´&#34;:4491,&#34;ìŀĲì²´ê°Ģ&#34;:4492,&#34;Ġw&#34;:4493,&#34;ìĿ´ëĭĪ&#34;:4494,&#34;ê²Ģ&#34;:4495,&#34;ìķĦëĭĪëĭ¤&#34;:4496,&#34;ĠìłķìĦľ&#34;:4497,&#34;ĠìŀĪìĸ´ìļĶ&#34;:4498,&#34;ê·¸ìĿ¸&#34;:4499,&#34;Ġì§Ħìłķ&#34;:4500,&#34;Ġê°Ģê¹Į&#34;:4501,&#34;ĠìĬ¤íĬ¸&#34;:4502,&#34;ë¶Ģì¡±&#34;:4503,&#34;Ġìŀ¬ë°ĮëĬĶëį°&#34;:4504,&#34;Ġì¶ķ&#34;:4505,&#34;Ġì¡°íıŃ&#34;:4506,&#34;ĠìĤ¬ëŀĳìĹĲ&#34;:4507,&#34;Ġë¶Īêµ¬íķĺê³ł&#34;:4508,&#34;ì¹ľëĭ¤&#34;:4509,&#34;ĠìĦłìĥĿ&#34;:4510,&#34;Ġê³¼ìĹ°&#34;:4511,&#34;ĠíĢĦë¦¬íĭ°&#34;:4512,&#34;íķĺìĭł&#34;:4513,&#34;ê°ĢëĿ½&#34;:4514,&#34;ìĹĲë§Į&#34;:4515,&#34;Ġëĭ¬ëĿ¼&#34;:4516,&#34;ĠìĭľìĽĲ&#34;:4517,&#34;Ġë¬´ìĭľ&#34;:4518,&#34;ĠìĽ¬&#34;:4519,&#34;Ġë¯¸ìĬ¤&#34;:4520,&#34;ĠìĤ¬ëŀĳíķĺëĬĶ&#34;:4521,&#34;Ġì¢ĭìķĦìĦľ&#34;:4522,&#34;Ġë¶Ħëĵ¤&#34;:4523,&#34;Ġë°ĽìķĦ&#34;:4524,&#34;ĠìķħìĹŃ&#34;:4525,&#34;ì¤ĺìķ¼&#34;:4526,&#34;Ġê¹¨ëĭ«&#34;:4527,&#34;ê°ĢìĬ´ìĿ´&#34;:4528,&#34;Ġë¸ĶëŀĻ&#34;:4529,&#34;ë¿Ķ&#34;:4530,&#34;Ġìī½ê²Į&#34;:4531,&#34;Ġíķ´íĶ¼ìĹĶëĶ©&#34;:4532,&#34;ë§ĪìĬ¤&#34;:4533,&#34;ĠëĤĺìĺ´&#34;:4534,&#34;ì§Ħë¶Ģ&#34;:4535,&#34;ìĤ¬ìĿ´&#34;:4536,&#34;ĠëģĦ&#34;:4537,&#34;Ġíı¬ë&#34;:4538,&#34;ë§ĲìĿ´íķĦìļĶ&#34;:4539,&#34;êµ¬ë¡ľ&#34;:4540,&#34;Ġê²ģëĤĺ&#34;:4541,&#34;ë§Ļ&#34;:4542,&#34;Ġê°ľìĿ¸&#34;:4543,&#34;íİ¸ìĹĲ&#34;:4544,&#34;ë¦¬ëĦ¤&#34;:4545,&#34;ĠìĹ°ì¶ľëıĦ&#34;:4546,&#34;ë¸Ĳ&#34;:4547,&#34;ĠìĦ¤ëłĪ&#34;:4548,&#34;ĠëĤ®ìķĦìĦľ&#34;:4549,&#34;ĠìĹ´ìĭ¬íŀĪ&#34;:4550,&#34;ĠëĸłëĤĺìĦľ&#34;:4551,&#34;ĠOOOê¸°&#34;:4552,&#34;Ġê·¸ê±¸&#34;:4553,&#34;ìłĲì£¼&#34;:4554,&#34;Ġìĸ´ëł¸&#34;:4555,&#34;Ġìŀ¬ë°ĮìĹĪëĬĶëį°&#34;:4556,&#34;Ġìŀ¬ë°ĮìĹĪìĿĮ&#34;:4557,&#34;ĠìķĬìķĦ&#34;:4558,&#34;ĠëĤ¨ìĿĦ&#34;:4559,&#34;ĠìĿ´ëŁ°ê±¸&#34;:4560,&#34;Ġìķ¡ìħĺìĿ´&#34;:4561,&#34;ìł¸ìļĶ&#34;:4562,&#34;Ġê¸°ëĮĢë¥¼&#34;:4563,&#34;íķĻëħĦ&#34;:4564,&#34;ĠëĳĲê³ł&#34;:4565,&#34;ĠíĺĦìĭ¤ìĿĦ&#34;:4566,&#34;ê¿Ī&#34;:4567,&#34;Ġëľ»&#34;:4568,&#34;ìķĦê¹ĮìĽĮ&#34;:4569,&#34;Ġëĳĺëĭ¤&#34;:4570,&#34;Ġìľłì¾Įíķľ&#34;:4571,&#34;ìĸ´ëĸ¤&#34;:4572,&#34;¥´ëħ¸&#34;:4573,&#34;Ġêµ³ìĿ´&#34;:4574,&#34;Ġìħ&#34;:4575,&#34;ìĿ´ë¯¸&#34;:4576,&#34;ì§Ģë§ĪëĿ¼&#34;:4577,&#34;íķĺìħ¨&#34;:4578,&#34;ĠìĿ´ê²ĥëıĦ&#34;:4579,&#34;ĠìĿ´ìģľ&#34;:4580,&#34;ìĿ¸ëĵ¤&#34;:4581,&#34;ìĭľëĤĺ&#34;:4582,&#34;ëŀµ&#34;:4583,&#34;Ġì°¡&#34;:4584,&#34;ìħī&#34;:4585,&#34;Ġëª»íĸĪëĭ¤&#34;:4586,&#34;ĠìŀĲë§ī&#34;:4587,&#34;ë´¤ëįĺ&#34;:4588,&#34;íĿĶ&#34;:4589,&#34;Ġê²°ë¡ł&#34;:4590,&#34;ĠëĬĲëĤĢ&#34;:4591,&#34;Ġë°ľìłĦ&#34;:4592,&#34;ĠëĬĲëĤĮìĿĦ&#34;:4593,&#34;ì»¨&#34;:4594,&#34;Ġãħĭãħĭãħĭãħĭãħĭ&#34;:4595,&#34;Ġìį©&#34;:4596,&#34;CG&#34;:4597,&#34;íĵ¨&#34;:4598,&#34;íķľëĭ¤ê³ł&#34;:4599,&#34;ĠìĿ´ë»Ĳ&#34;:4600,&#34;ëł·&#34;:4601,&#34;...?&#34;:4602,&#34;ĠìķĦìĺ¤&#34;:4603,&#34;ĠëĤĺê°Ģ&#34;:4604,&#34;ìļ°ê³ł&#34;:4605,&#34;Ġìµľê·¼&#34;:4606,&#34;Ġë¬´ì§Ģ&#34;:4607,&#34;ë¬´ë¹Ħ&#34;:4608,&#34;Ġê°ĲëıĻìłģìĿ´ê³ł&#34;:4609,&#34;Ġê±°ì§ĵ&#34;:4610,&#34;ê²¨ìļ´&#34;:4611,&#34;ĠìĿ´íķ´íķł&#34;:4612,&#34;ìŀ¡íķľ&#34;:4613,&#34;ë¶Īìĸ´&#34;:4614,&#34;ëįĶëĿ¼ëıĦ&#34;:4615,&#34;Ġê²°ë§ĲëıĦ&#34;:4616,&#34;ĠìĺģíĻĶìĹĲìļĶ&#34;:4617,&#34;ëŀįëĭĪëĭ¤&#34;:4618,&#34;ĠìĬ¤ì¼ĢìĿ¼&#34;:4619,&#34;ê°Ģì§Ģê³ł&#34;:4620,&#34;ëĤ«&#34;:4621,&#34;ëĤ¼&#34;:4622,&#34;ìķĦìī½&#34;:4623,&#34;Ġìĭ¬ë¦¬&#34;:4624,&#34;Ġê·¸ëŀ¬&#34;:4625,&#34;ìŀĳìĿ´&#34;:4626,&#34;ì°®&#34;:4627,&#34;íĸĪëĬĶì§Ģ&#34;:4628,&#34;ìĭłìĿĢ&#34;:4629,&#34;ê²łìĿĮ&#34;:4630,&#34;Ġë§ĪìĦ¸ìļĶ&#34;:4631,&#34;ĠëĲĺì§Ģ&#34;:4632,&#34;Ġê°Ĳëªħ&#34;:4633,&#34;íĮĲìĹĲ&#34;:4634,&#34;ĠëıĻìĥĿ&#34;:4635,&#34;Ġë³Ħë¡ľëĭ¤&#34;:4636,&#34;ìĤ¬ëŀĮëĵ¤ìĿ´&#34;:4637,&#34;ì§Ģë£¨íķĺëĭ¤&#34;:4638,&#34;ĠíĴĭ&#34;:4639,&#34;Ġê¹ĬìĿ´&#34;:4640,&#34;ì¤Įë§Ī&#34;:4641,&#34;is&#34;:4642,&#34;íķĺëĿ¼&#34;:4643,&#34;íķĺëł¤ê³ł&#34;:4644,&#34;ĠìĹĳ&#34;:4645,&#34;ĠìĿ´ìĺģíĻĶê°Ģ&#34;:4646,&#34;ë§Įëĵľ&#34;:4647,&#34;ìĹĪëĬĶì§Ģ&#34;:4648,&#34;ê²łëĦ¤ìļĶ&#34;:4649,&#34;Ġë¹Ħì£¼ìĸ¼&#34;:4650,&#34;ëĭ¨ìĪľ&#34;:4651,&#34;Ġë°°ìļ°ìĿĺ&#34;:4652,&#34;Ġì¢ĭìķĦíķł&#34;:4653,&#34;íıīìłĲìĹĲ&#34;:4654,&#34;¬ë¦¼&#34;:4655,&#34;Ġìĭłê²½&#34;:4656,&#34;ĠëķĮë¬¸&#34;:4657,&#34;Ġë³¼ë§Įíķ¨&#34;:4658,&#34;ìĵ°ëłĪê¸°ìĺģíĻĶ&#34;:4659,&#34;ì¢ĭìĿĢìĺģíĻĶ&#34;:4660,&#34;ĠìĬ¬íĶĦê³ł&#34;:4661,&#34;ë±&#34;:4662,&#34;Ġìĩ¼&#34;:4663,&#34;Ġëĭ¬ë¦¬&#34;:4664,&#34;Ġìĥ¤&#34;:4665,&#34;Ġê²ĮìĿ´&#34;:4666,&#34;ĠìŀĪìĸ´ìķ¼&#34;:4667,&#34;ê³¼ìĿĺ&#34;:4668,&#34;Ġíķľëį°&#34;:4669,&#34;Ġì¤Į&#34;:4670,&#34;íİ¸ëıĦ&#34;:4671,&#34;Ġë§İëĭ¤&#34;:4672,&#34;ĠìłĢìĺĪìĤ°&#34;:4673,&#34;ë´ĲìļĶ&#34;:4674,&#34;ì¢ĭìķĦíķĺëĬĶ&#34;:4675,&#34;Ġíıīë²Ķíķľ&#34;:4676,&#34;. &#34; &#34; &#34;&#34;:4677,&#34;¦ê²Į&#34;:4678,&#34;ìĿ´ìħĺ&#34;:4679,&#34;ìķ¤&#34;:4680,&#34;ì§ĢëĤĺ&#34;:4681,&#34;ĠìŀĪëĦ¤ìļĶ&#34;:4682,&#34;Ġíĳ¹&#34;:4683,&#34;ĠìĹĨëĬĶëį°&#34;:4684,&#34;ìķĺê³ł&#34;:4685,&#34;ì¤ĳë°ĺ&#34;:4686,&#34;ĠëįĶìĿ´ìĥģ&#34;:4687,&#34;íķĺëĭ¤ëĭĪ&#34;:4688,&#34;ĠìĽĢ&#34;:4689,&#34;ĠëĤ¨ì£¼&#34;:4690,&#34;ĠìĿ¼ìĥģ&#34;:4691,&#34;ĠëĲĺê²Į&#34;:4692,&#34;ì²ĺêµ¬ëĭĪ&#34;:4693,&#34;ĠíŀĲë§ģ&#34;:4694,&#34;007&#34;:4695,&#34;ëŃĲëĥĲ&#34;:4696,&#34;Ġê·¸ëłĩëĭ¤ê³ł&#34;:4697,&#34;Ġìķħëĭ¹&#34;:4698,&#34;Ġìī¬&#34;:4699,&#34;ë§ĪìĿĮìĿ´&#34;:4700,&#34;Ġ&lt;&#34;:4701,&#34;Ġíĩ´&#34;:4702,&#34;ĠìĹĨëĭ¤ëĬĶ&#34;:4703,&#34;ĠëĤĺì¤ĳìĹĲ&#34;:4704,&#34;íĸĪëĦ¤&#34;:4705,&#34;ĠìĹ°ê²°&#34;:4706,&#34;Ġë³¼ìĪĺë¡Ŀ&#34;:4707,&#34;Ġë³¼ëķĮë§Īëĭ¤&#34;:4708,&#34;ê³ĦìĿĺ&#34;:4709,&#34;Ġìŀ¬ë¯¸ìŀĪìĿĮ&#34;:4710,&#34;Ġìĭ¶ëĦ¤ìļĶ&#34;:4711,&#34;íĽĮë¥Ń&#34;:4712,&#34;ë©ĭì§Ħ&#34;:4713,&#34;ìķĦê¹ĮìĽĢ&#34;:4714,&#34;ëĤĺìĻĢìĦľ&#34;:4715,&#34;Bê¸ī&#34;:4716,&#34;at&#34;:4717,&#34;ŀĳ&#34;:4718,&#34;Ġëº&#34;:4719,&#34;ëĬĶìĺģíĻĶ&#34;:4720,&#34;¬ë°&#34;:4721,&#34;Ġì§ķ&#34;:4722,&#34;ìľ¼ë¡ľìį¨&#34;:4723,&#34;íķ¨ìĹĲ&#34;:4724,&#34;ìĤ¬ìĿĺ&#34;:4725,&#34;ĠìłĦíĺķìłģìĿ¸&#34;:4726,&#34;Ġë§Ĳíķł&#34;:4727,&#34;ĠëĿ¼ìĿ´&#34;:4728,&#34;Ġëª©ìĨĮ&#34;:4729,&#34;ê½¤&#34;:4730,&#34;Īëł&#34;:4731,&#34;ë¡ľëıĦ&#34;:4732,&#34;ìļ°ìĻĢ&#34;:4733,&#34;Ġìĭľê°ģ&#34;:4734,&#34;Ġì°Ŀ&#34;:4735,&#34;ĠìĥĿê°ģíķĺê³ł&#34;:4736,&#34;ë¬´ì§Ģ&#34;:4737,&#34;ìŀ¬ë¯¸ê°Ģ&#34;:4738,&#34;ĠìĤ¬ëŀĮìĿĺ&#34;:4739,&#34;ëª¨ëĳĲ&#34;:4740,&#34;ĠíķĺëĬĶì§Ģ&#34;:4741,&#34;ìĺ¨ëĭ¤&#34;:4742,&#34;íĭ°ë¸Į&#34;:4743,&#34;ĠìĬ¬íĶĶ&#34;:4744,&#34;ĠìĤ°ìľ¼ë¡ľ&#34;:4745,&#34;Ġë°ĶëĢĮ&#34;:4746,&#34;Ġíİĳíİĳ&#34;:4747,&#34;ì§Ħìłķíķľ&#34;:4748,&#34;ìłĦì²´&#34;:4749,&#34;ìĦ±ìļ°&#34;:4750,&#34;ĠëĤ¨ëħĢ&#34;:4751,&#34;íĿ¡&#34;:4752,&#34;ê´ĢëŀĮ&#34;:4753,&#34;ĠìĥĿìĥĿ&#34;:4754,&#34;Ġë°ĺìłĦëıĦ&#34;:4755,&#34;ìķĦìĿ´ëĵ¤ìĿ´&#34;:4756,&#34;¬ëįĺ&#34;:4757,&#34;BS&#34;:4758,&#34;ĠëĩĮ&#34;:4759,&#34;ê²ĮìŀĦ&#34;:4760,&#34;Ġë³´ìķĺëĭ¤&#34;:4761,&#34;ë°¤&#34;:4762,&#34;Ġì¢ĭìĬµëĭĪëĭ¤&#34;:4763,&#34;Ġëª»íķł&#34;:4764,&#34;íļį&#34;:4765,&#34;ëŀĢëĭ¤&#34;:4766,&#34;Ġìķ¡ìħĺìĿĢ&#34;:4767,&#34;Ġë§Įëĵłëĭ¤&#34;:4768,&#34;Ġì§±ì§±&#34;:4769,&#34;Ġìŀ¬ëĤľ&#34;:4770,&#34;ĠìĿĮìķħëıĦ&#34;:4771,&#34;Ġì©Į&#34;:4772,&#34;Ġê´ĳê³ł&#34;:4773,&#34;ĠT&#34;:4774,&#34;ì§Ģë§Ĳ&#34;:4775,&#34;íķĺêµ¬&#34;:4776,&#34;ëį©&#34;:4777,&#34;ëĤĺë§Į&#34;:4778,&#34;ê±°ë©´&#34;:4779,&#34;ëĮĢë¥¼&#34;:4780,&#34;ĠìŀĪìľ¼ë©´&#34;:4781,&#34;ê·¸ë¦¬&#34;:4782,&#34;ìĦ±ìĿĺ&#34;:4783,&#34;ĠëĮĢíĳľ&#34;:4784,&#34;ĠìķĬì§Ģë§Į&#34;:4785,&#34;ĠíĿ¡&#34;:4786,&#34;ìĪŃ&#34;:4787,&#34;ĠëĨį&#34;:4788,&#34;ëŁ½ê³ł&#34;:4789,&#34;Ġê°ķëł¬&#34;:4790,&#34;ì§ĳëĭĪëĭ¤&#34;:4791,&#34;ìĶ¬ìĿĢ&#34;:4792,&#34;Ġì¡´ê²½&#34;:4793,&#34;ĠëĪĪë¬¼ìĿĦ&#34;:4794,&#34;ĠìĿ´ìłľìķ¼&#34;:4795,&#34;Ġë¹¼ê³ł&#34;:4796,&#34;âĻ¡âĻ¡&#34;:4797,&#34;ìĬ¤íĥĢìĿ¼&#34;:4798,&#34;ìħĶìĦľ&#34;:4799,&#34;ë´£ëĬĶëį°&#34;:4800,&#34;Ġë´£ëĬĶëį°&#34;:4801,&#34;ĠA&#34;:4802,&#34;Ġìª½&#34;:4803,&#34;ĠìĺģíĻĶëĿ¼ëĬĶ&#34;:4804,&#34;ë¡ľìĦľ&#34;:4805,&#34;ĠìķĦì¹¨&#34;:4806,&#34;íģ¬ë¦¬ìĬ¤&#34;:4807,&#34;Ġì²´&#34;:4808,&#34;ĠìĤ´ê³ł&#34;:4809,&#34;ĠìĺĪìłĦ&#34;:4810,&#34;Ġìĭ¬ê°ģ&#34;:4811,&#34;ĠìĬ¬íİ&#34;:4812,&#34;ĠëĨĴëĭ¤&#34;:4813,&#34;ĠìĥĿìķł&#34;:4814,&#34;ìĸ¼êµ´&#34;:4815,&#34;Ġì»¨&#34;:4816,&#34;ì§Ģê¸Īê¹Įì§Ģ&#34;:4817,&#34;Ġì¢ħêµĲ&#34;:4818,&#34;ĠìĿ´ìģĺê³ł&#34;:4819,&#34;ĠìĮ&#34;:4820,&#34;ìĿ´ê¸°&#34;:4821,&#34;ìĿ¸íĬ¸&#34;:4822,&#34;ì£¼ìłľ&#34;:4823,&#34;íŀĲ&#34;:4824,&#34;ë¬´ìĹĩ&#34;:4825,&#34;ëıĻìĥĿ&#34;:4826,&#34;Ġì¢ĢëįĶ&#34;:4827,&#34;Ġìķ¡ìħĺìĺģíĻĶ&#34;:4828,&#34;ĠëıĻë¬¼&#34;:4829,&#34;ar&#34;:4830,&#34;ëĭ¬ëĿ¼&#34;:4831,&#34;íķµ&#34;:4832,&#34;ê°ĸ&#34;:4833,&#34;ìķķ&#34;:4834,&#34;ì§Ģìĸ´&#34;:4835,&#34;ĠìķĦëł¨&#34;:4836,&#34;ìľ¼ëĭĪê¹Į&#34;:4837,&#34;ê±°ìŀĦ&#34;:4838,&#34;ë²¨&#34;:4839,&#34;ĠìĬ¤íı¬&#34;:4840,&#34;ë¯¸ê°Ģ&#34;:4841,&#34;ĠìĹ°ê¸°ìŀĲ&#34;:4842,&#34;Ġìŀ¬ë°ĮìĬµëĭĪëĭ¤&#34;:4843,&#34;Ġë¬´íĺĳ&#34;:4844,&#34;ë¶ĦìľĦ&#34;:4845,&#34;íģ¬ë¦°&#34;:4846,&#34;ĠìĿ¼ë¶Ģ&#34;:4847,&#34;ìµľê·¼&#34;:4848,&#34;ìĹŃëĮĢ&#34;:4849,&#34;ìĿ´ëŁ°ìĺģíĻĶ&#34;:4850,&#34;Ġê·¹ì¹ĺ&#34;:4851,&#34;íĺ¸ê°Ģ&#34;:4852,&#34;Ġëĭ¨ìĹ°&#34;:4853,&#34;ìĦĿìĿ´&#34;:4854,&#34;Ġëªĩë²Ī&#34;:4855,&#34;Ġê½Ŀ&#34;:4856,&#34;Ġì§Ħìĭ¬ìľ¼ë¡ľ&#34;:4857,&#34;ĠìłĲìĪĺë¥¼&#34;:4858,&#34;ĠëĽ°ìĸ´ëĤľ&#34;:4859,&#34;Ġíĥľìĸ´&#34;:4860,&#34;ê½ĥ&#34;:4861,&#34;Ġìĵ¸ëį°&#34;:4862,&#34;Ġë½ĳ&#34;:4863,&#34;Ġë§Īë¬´ë¦¬&#34;:4864,&#34;ìĿ´ìłķëıĦ&#34;:4865,&#34;ĠìĺģíĻĶìĿ¸ëĵ¯&#34;:4866,&#34;Ġëĭ®&#34;:4867,&#34;ĠìĹ°ìĨį&#34;:4868,&#34;Ġì§Ģê¸Īë´ĲëıĦ&#34;:4869,&#34;ê·¸ëłĩê²Į&#34;:4870,&#34;Ġìĭľë¦¬&#34;:4871,&#34;ìķĺì§Ģë§Į&#34;:4872,&#34;Ġê°Ģìł¸&#34;:4873,&#34;ĠìľĮ&#34;:4874,&#34;Ġë¬´ë¹Ħ&#34;:4875,&#34;ĠëĤ¨ê²¨&#34;:4876,&#34;íĬ¸ë§¨&#34;:4877,&#34;Ġì§Ģë£¨íķ´ìĦľ&#34;:4878,&#34;ëĤłëķĮ&#34;:4879,&#34;ì©¡&#34;:4880,&#34;ĠíĳľìłĪ&#34;:4881,&#34;ê¼´&#34;:4882,&#34;ìĸĳìĿ´&#34;:4883,&#34;ĠìĿ¸ìĥĿìĿĦ&#34;:4884,&#34;ĠìķĦìī¬ìĽĢ&#34;:4885,&#34;Ġìĭ¸êµ¬ëł¤&#34;:4886,&#34;ì¡¸ìŀĳ&#34;:4887,&#34;ê³µíı¬ìĺģíĻĶ&#34;:4888,&#34;ìıĺ&#34;:4889,&#34;ver&#34;:4890,&#34;ìĿ´ìĺģíĻĶë¥¼&#34;:4891,&#34;ëĵ£&#34;:4892,&#34;ìĹĲëĮĢíķľ&#34;:4893,&#34;ëĤĺëĦ¤&#34;:4894,&#34;Ġë³´ëĭĪê¹Į&#34;:4895,&#34;ë§ĲìĿĦ&#34;:4896,&#34;ìĻĵ&#34;:4897,&#34;Ġê²ī&#34;:4898,&#34;íĮ©&#34;:4899,&#34;ìĨĮëħĢ&#34;:4900,&#34;Ġê°ĻìķĦìĦľ&#34;:4901,&#34;Ġë§ĲíķĺëĬĶ&#34;:4902,&#34;Ġë³¸ëĭ¤ë©´&#34;:4903,&#34;ĠìĨĮìŀ¥&#34;:4904,&#34;Ġë¯¸ì¹ĺ&#34;:4905,&#34;ë¦¬ë©°&#34;:4906,&#34;Ġê³łìłĦ&#34;:4907,&#34;Ġê³łíĨµ&#34;:4908,&#34;Ġê±°ê¸°&#34;:4909,&#34;ĠìŀĳíĴĪìĿ´ëĭ¤&#34;:4910,&#34;ìĶ¨ìĿĺ&#34;:4911,&#34;ĠìĨįìĹĲ&#34;:4912,&#34;ĠìĿĮìķħìĿ´&#34;:4913,&#34;Ġëĸłìĺ¤&#34;:4914,&#34;ĠìĤ¶ìĿĺ&#34;:4915,&#34;ĠìĶģìĵ¸&#34;:4916,&#34;~~~~~~~~&#34;:4917,&#34;ĠìĿĺë¯¸ë¥¼&#34;:4918,&#34;it&#34;:4919,&#34;Ġ^&#34;:4920,&#34;Ġgood&#34;:4921,&#34;ê°Ĵ&#34;:4922,&#34;ë§ĮìĿĦ&#34;:4923,&#34;ëĦ·&#34;:4924,&#34;ĠìĦŀ&#34;:4925,&#34;ìŀ¬ë¯¸ëĬĶ&#34;:4926,&#34;ĠìĤ¬ëŀĳê³¼&#34;:4927,&#34;ĠìķĦê¹Ŀì§Ģ&#34;:4928,&#34;Ġë©ĭì§Ģëĭ¤&#34;:4929,&#34;ê°ĢëĬĶì¤Ħ&#34;:4930,&#34;Ġì§Ģê¸ĪìĿĢ&#34;:4931,&#34;ëĲ¬&#34;:4932,&#34;ĠìķĦìĿ´ëĵ¤ìĿ´&#34;:4933,&#34;ìĬ¤ëŁ¬ìĽĢ&#34;:4934,&#34;ĠíĳľíĺĦíķľ&#34;:4935,&#34;ĠìĺģíĻĶìĺĢìĸ´ìļĶ&#34;:4936,&#34;ìĻĦë²½&#34;:4937,&#34;Ġëĵ±ìŀ¥ìĿ¸ë¬¼&#34;:4938,&#34;ing&#34;:4939,&#34;Ġì¥Ĳ&#34;:4940,&#34;ĠìĬĪíį¼&#34;:4941,&#34;ll&#34;:4942,&#34;ħ¸&#34;:4943,&#34;ê°Ģë¥¼&#34;:4944,&#34;ĠëĤ´ëł¤&#34;:4945,&#34;ê²łìĸ´ìļĶ&#34;:4946,&#34;ĠìķĪëı¼&#34;:4947,&#34;Ġë³¸ìĺģíĻĶ&#34;:4948,&#34;Ġë¹¡&#34;:4949,&#34;ĠëģĿëĤľ&#34;:4950,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪëĭ¤&#34;:4951,&#34;Ġë°ĺë³µ&#34;:4952,&#34;ĠìŀĳíĴĪìĿĦ&#34;:4953,&#34;ê´ĢìĿ´&#34;:4954,&#34;Ġìŀ¥ë©´ìĹĲìĦľ&#34;:4955,&#34;ê°ĲëıħìĿĢ&#34;:4956,&#34;ë³Ħë£¨&#34;:4957,&#34;ĠìĹĲíĶ¼ìĨĮ&#34;:4958,&#34;ìĥĿê°ģìĹĨìĿ´&#34;:4959,&#34;ĠìķĦìĿ´ëĵ¤&#34;:4960,&#34;Ġëª¨ëĵłê²Į&#34;:4961,&#34;Ġì¹ľêµ¬ëĵ¤&#34;:4962,&#34;Ġìķķê¶Į&#34;:4963,&#34;ì¥Ĳ&#34;:4964,&#34;Ġìĸ´ëł¤ìļ´&#34;:4965,&#34;ie&#34;:4966,&#34;¨¸&#34;:4967,&#34;ĠìĮį&#34;:4968,&#34;ĠìĺģíĻĶëĵ¤&#34;:4969,&#34;ê±°ê°Ļëĭ¤&#34;:4970,&#34;ìĨĶ&#34;:4971,&#34;Ġì§Ģì¼ľ&#34;:4972,&#34;ĠëįĶë¶Īìĸ´&#34;:4973,&#34;Ġë³¼ê²Į&#34;:4974,&#34;ìĺģìĿ´&#34;:4975,&#34;íĿ¬ë&#34;:4976,&#34;Ġê±°ìĬ&#34;:4977,&#34;Ġëĭ¤ìĭľíķľë²Ī&#34;:4978,&#34;ìĭľê°ĦìĹĲ&#34;:4979,&#34;Ġìļ¸ê³ł&#34;:4980,&#34;Ġìļ¸ìĹĪëĭ¤&#34;:4981,&#34;ĠìĻ¸ëª¨&#34;:4982,&#34;Ġë¨¹ê³ł&#34;:4983,&#34;Ġê²½íĹĺ&#34;:4984,&#34;ĠêµŃë¯¼&#34;:4985,&#34;Ġìĸ¸ìłľëĤĺ&#34;:4986,&#34;¬ëįĶ&#34;:4987,&#34;ĠëĬ¥ëł¥&#34;:4988,&#34;ĠëıħíĬ¹íķľ&#34;:4989,&#34;Ġê²Ģìĥī&#34;:4990,&#34;44&#34;:4991,&#34;bs&#34;:4992,&#34;ë¤&#34;:4993,&#34;ìĿ´ìĺģíĻĶëĬĶ&#34;:4994,&#34;ëĭ¤ëĵ¤&#34;:4995,&#34;íķĺì§Ħ&#34;:4996,&#34;ĠìŀĪëįĺ&#34;:4997,&#34;ëĤĺëĭ¤&#34;:4998,&#34;ìķĦë¹ł&#34;:4999,&#34;ìĥģíĻ©&#34;:5000,&#34;ĠëĮĢë¶Ģë¶Ħ&#34;:5001,&#34;ĠìłĦíĪ¬&#34;:5002,&#34;ê°ľê·¸&#34;:5003,&#34;ê°ĲëıĦ&#34;:5004,&#34;ĠìĿ´ëŁ°ê²Į&#34;:5005,&#34;ĠíĿī&#34;:5006,&#34;ìĽĲëŀĺ&#34;:5007,&#34;ëĲĺìĦľ&#34;:5008,&#34;Ġì¡°íķ©&#34;:5009,&#34;ìĹ°ê¸°ëĬĶ&#34;:5010,&#34;Ġë°ľê²¬&#34;:5011,&#34;Ġãħłãħłãħł&#34;:5012,&#34;Ġë§Ŀì³Ĳ&#34;:5013,&#34;ĠìĬ¤ë¦´ëŁ¬ë&#34;:5014,&#34;íĺ¼ìŀĲ&#34;:5015,&#34;Ġëª¨ë¥´ê²łê³ł&#34;:5016,&#34;ĠíıŃëł¥&#34;:5017,&#34;ĠëģĮìĸ´&#34;:5018,&#34;Ġë°Ķê¿Ķ&#34;:5019,&#34;Ġì°Ŀì°Ŀ&#34;:5020,&#34;ĠìĿµ&#34;:5021,&#34;ìļ°ê¸°&#34;:5022,&#34;Ġìŀ¬ë¯¸ìŀĪëĦ¤ìļĶ&#34;:5023,&#34;Ġíķľì°¸&#34;:5024,&#34;ĠìĬ¤ìĬ¤ë¡ľ&#34;:5025,&#34;ë¯¸ìķĪ&#34;:5026,&#34;ìĨĮë¦Ħ&#34;:5027,&#34;ìĺĢì§Ģë§Į&#34;:5028,&#34;Ġê°ĲëıĻê³¼&#34;:5029,&#34;Ġë¯¸ëª¨&#34;:5030,&#34;Ġì§Ģë£¨íķł&#34;:5031,&#34;ê¸´ëĭ¤&#34;:5032,&#34;ĠìĿĺìĭ¬&#34;:5033,&#34;ĠìĬ¬íį¼&#34;:5034,&#34;ĠëĨĴìķĦ&#34;:5035,&#34;Ġê²Įëĭ¤ê°Ģ&#34;:5036,&#34;ĠìķĦìī½ëĦ¤ìļĶ&#34;:5037,&#34;ĠëĬĲëģ¼ëĬĶ&#34;:5038,&#34;âĻ¥âĻ¥âĻ¥âĻ¥&#34;:5039,&#34;Ġë¹łìł¸ëĵ¤&#34;:5040,&#34;ĠìĦ¬ìĦ¸&#34;:5041,&#34;ìĸ´ë¥¸&#34;:5042,&#34;ìłĦìŀĳ&#34;:5043,&#34;ìĤ¬ê±´&#34;:5044,&#34;ĠìĻľì¼Ģ&#34;:5045,&#34;ë²Ħë¦¬ê³ł&#34;:5046,&#34;ĠìĿ¼ë°ĺ&#34;:5047,&#34;ìĺģíĻĺ&#34;:5048,&#34;ĠíĹĲ&#34;:5049,&#34;íĿĺ&#34;:5050,&#34;Ġì²ĺëŁ¼&#34;:5051,&#34;ĠíĺĦëĮĢ&#34;:5052,&#34;ë°©ìĤ¬&#34;:5053,&#34;Ġë°ķìĪĺë¥¼&#34;:5054,&#34;ĠìĻ¸ê³Ħ&#34;:5055,&#34;ĠìĽĥê¸°ê³ł&#34;:5056,&#34;Ġë¹¼ê³łëĬĶ&#34;:5057,&#34;ĠëĶ°ëľ»íķ´ì§ĢëĬĶ&#34;:5058,&#34;Ġëĭ¤íģĲë©ĺíĦ°&#34;:5059,&#34;ìŀ¬ë¯¸ìĹĨìĸ´&#34;:5060,&#34;ì³¤ëĭ¤&#34;:5061,&#34;ĠìĿ´íĽĦë¡ľ&#34;:5062,&#34;ĠíĴĭíĴĭ&#34;:5063,&#34;ë®¤&#34;:5064,&#34;íĻĶëł¤&#34;:5065,&#34;íķĺê²łëĭ¤&#34;:5066,&#34;ĠìķĦë¥ĺ&#34;:5067,&#34;ê±°ëĿ¼&#34;:5068,&#34;ìĹĪìĿĦê¹Į&#34;:5069,&#34;Ġëĭ¤ë£¬&#34;:5070,&#34;Ġê²¬&#34;:5071,&#34;ëĵľìĿĺ&#34;:5072,&#34;ìĦ±ì¹ĺ&#34;:5073,&#34;ê²ĥë§Į&#34;:5074,&#34;ĠìĿ¸íĦ°&#34;:5075,&#34;ìľłì¾Į&#34;:5076,&#34;Ġíķłê¹Į&#34;:5077,&#34;ĠëıĻìķĪ&#34;:5078,&#34;ĠëĤľíķ´&#34;:5079,&#34;ë§¤ìļ°&#34;:5080,&#34;íķĺëĤĺëıĦ&#34;:5081,&#34;ĠëĲ¨&#34;:5082,&#34;ĠìĿ¸ìĥĿìĿĺ&#34;:5083,&#34;Ġ40&#34;:5084,&#34;ìłĦê°ľê°Ģ&#34;:5085,&#34;Ġë¯¼ë§Ŀ&#34;:5086,&#34;Ġëĺĳê°Ļ&#34;:5087,&#34;ìĸ´ìĦ¤íĶĪ&#34;:5088,&#34;ĠëŃīíģ´&#34;:5089,&#34;íĮĢ&#34;:5090,&#34;ë¡Ģ&#34;:5091,&#34;Ġìĭ±&#34;:5092,&#34;ìłķìĿĢ&#34;:5093,&#34;íĸĪëĦ¤ìļĶ&#34;:5094,&#34;ëĤ´ìĥĿ&#34;:5095,&#34;ê·¸ë§Į&#34;:5096,&#34;ëĥĪ&#34;:5097,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:5098,&#34;Ġë§Ĳë¡ľ&#34;:5099,&#34;Ġëª»íķ´&#34;:5100,&#34;Ġìłľë¡ľ&#34;:5101,&#34;ĠëģĶ&#34;:5102,&#34;ë²ĦíĬ¸&#34;:5103,&#34;ë¬¸ìłľ&#34;:5104,&#34;ìŀ¬ë°Įìĸ´&#34;:5105,&#34;ĠìĤ¬ëŀĳìĿ´ìķ¼ê¸°&#34;:5106,&#34;íĶĦëŀĳìĬ¤&#34;:5107,&#34;Ġì¢ĭìķĺìĿĦ&#34;:5108,&#34;ìĪľìĪĺ&#34;:5109,&#34;ĠëĪĪìĹĲ&#34;:5110,&#34;ì³ĲìĦľ&#34;:5111,&#34;ĠíĭĢìĸ´&#34;:5112,&#34;ĠìķĦì§ģê¹Įì§Ģ&#34;:5113,&#34;Ġì¤ĳê°Ħì¤ĳê°Ħ&#34;:5114,&#34;ĠìĦ¸ìĥģìĹĲ&#34;:5115,&#34;ìĭľíĤ¤ëĬĶ&#34;:5116,&#34;ì«&#34;:5117,&#34;ìĿ¸ëĵ¤ìĿ´&#34;:5118,&#34;êµ¬ëŀĳ&#34;:5119,&#34;Ġê·¸ëŁ¬ë&#34;:5120,&#34;Ġê·¸ëŁŃìłĢëŁŃ&#34;:5121,&#34;ì¹¸&#34;:5122,&#34;ì¹¼&#34;:5123,&#34;Ġì¢ĭì§Ģë§Į&#34;:5124,&#34;Ġìĺ¬ëł¤&#34;:5125,&#34;Ġì£¼ë³Ģ&#34;:5126,&#34;ë¹Į&#34;:5127,&#34;Ġê³µë¶Ģ&#34;:5128,&#34;ëĸł&#34;:5129,&#34;ë¨¸ë¦¬&#34;:5130,&#34;ëľ©&#34;:5131,&#34;ĠìĨįíİ¸&#34;:5132,&#34;Ġê°Īëĵ±&#34;:5133,&#34;Ġë¯¿ìĿĦ&#34;:5134,&#34;Ġë¶ģíķľ&#34;:5135,&#34;Ġë»&#34;:5136,&#34;íķŃ&#34;:5137,&#34;ê°ĵ&#34;:5138,&#34;ìĿ´ëŁ¬&#34;:5139,&#34;ìł¼&#34;:5140,&#34;ë¦¬ê¸°&#34;:5141,&#34;ìľĦë¡ľ&#34;:5142,&#34;Ġë³´ê¸¸&#34;:5143,&#34;Ġë°ĭ&#34;:5144,&#34;ìŀ¥ìĹĲìĦľ&#34;:5145,&#34;Ġë´¤ëįĶëĭĪ&#34;:5146,&#34;ê²ĥëĵ¤&#34;:5147,&#34;íĮį&#34;:5148,&#34;íİĻ&#34;:5149,&#34;ê°ľìĹ°&#34;:5150,&#34;ëįĶëŁ½ê²Į&#34;:5151,&#34;Ġë§İìķĦ&#34;:5152,&#34;Ġë°°ìļ°ëĵ¤ëıĦ&#34;:5153,&#34;Ġì¢ĭìķĦíķĺëĬĶëį°&#34;:5154,&#34;ìĭĿìĿĺ&#34;:5155,&#34;íĤ¤ëĬĶ&#34;:5156,&#34;íħĮìĿ¼&#34;:5157,&#34;Ġê²°ë§ĲìĿĢ&#34;:5158,&#34;ĠëĴ¤ë¡ľ&#34;:5159,&#34;ì¯§&#34;:5160,&#34;ãħłãħłãħłãħłãħłãħłãħłãħł&#34;:5161,&#34;Ġì¦Ĳê²ģê²Į&#34;:5162,&#34;ãĢ&#34;:5163,&#34;ĠìķĪë¬´&#34;:5164,&#34;ëĤĺëĦ¤ìļĶ&#34;:5165,&#34;ĠëĤ³&#34;:5166,&#34;ìŀĲëĤĺ&#34;:5167,&#34;ĠìĭľíĬ¸ì½¤&#34;:5168,&#34;Ġìłķë§Ĳë¡ľ&#34;:5169,&#34;ìĭłìĿĦ&#34;:5170,&#34;ĠìķĬëĤĺ&#34;:5171,&#34;Ġë¹ĦíĺĦìĭ¤&#34;:5172,&#34;ìŀ¬ë°Įê³ł&#34;:5173,&#34;ĠëĤĺìĺ¤ë©´&#34;:5174,&#34;Ġëĵ¤ìĹĪëĭ¤&#34;:5175,&#34;ê°ĲëıĻìłģìĿ´&#34;:5176,&#34;ĠíĺĦìĭ¤ìłģìĿ¸&#34;:5177,&#34;ì§Ģë£¨íķľ&#34;:5178,&#34;ĠìķĪë³´ê³ł&#34;:5179,&#34;!!!!!!!!!!!!!!!!&#34;:5180,&#34;Ńī&#34;:5181,&#34;ê°±&#34;:5182,&#34;ìĦ¹&#34;:5183,&#34;ìĸ´ìłľ&#34;:5184,&#34;ëĭĪê¹Ĳ&#34;:5185,&#34;Ġëª½&#34;:5186,&#34;ìłĦíĺķìłģìĿ¸&#34;:5187,&#34;ìĻĢìļ°&#34;:5188,&#34;ĠìĥĿê°ģíķĺëĬĶ&#34;:5189,&#34;ê¸´íķľëį°&#34;:5190,&#34;Ġëª°ìŀħíķ´ìĦľ&#34;:5191,&#34;Ġë°©ë²ķ&#34;:5192,&#34;Ġë»Ķíķĺê³ł&#34;:5193,&#34;Ġë¬¸íĻĶ&#34;:5194,&#34;Ġìĥģìĥģëł¥&#34;:5195,&#34;ĠëĬĺìĸ´&#34;:5196,&#34;13&#34;:5197,&#34;ov&#34;:5198,&#34;Ġf&#34;:5199,&#34;ĠìĹĺ&#34;:5200,&#34;êµ¬ëĿ¼&#34;:5201,&#34;Ġì¢Į&#34;:5202,&#34;ĠìĹĨìĹĪëįĺ&#34;:5203,&#34;ì¹Ļ&#34;:5204,&#34;ìłĦëĵľ&#34;:5205,&#34;ê·¸ëĮĢë¡ľ&#34;:5206,&#34;Ġì¢ĭìķĺì§Ģë§Į&#34;:5207,&#34;ĠìĺĪìłĦìĹĲ&#34;:5208,&#34;ĠìĺĪê³łíİ¸&#34;:5209,&#34;Ġìļ°ë¦¬ê°Ģ&#34;:5210,&#34;Ġëª¨ë¥´ê²łì§Ģë§Į&#34;:5211,&#34;ìĤ¼ë¥ĺ&#34;:5212,&#34;Ġíŀĺëĵ¤ëĭ¤&#34;:5213,&#34;ê°Ķëĭ¤&#34;:5214,&#34;ĠìķĶíĬ¼&#34;:5215,&#34;Ġì±ĦëĦĲ&#34;:5216,&#34;Ġë°ĭë°ĭ&#34;:5217,&#34;Ġ!!!&#34;:5218,&#34;ĠìĹħ&#34;:5219,&#34;ìķĦìī¬&#34;:5220,&#34;ĠìķĦëĵ¤&#34;:5221,&#34;íķ´ì§Ģ&#34;:5222,&#34;Ġë³´ê¸´&#34;:5223,&#34;Ġë³´ìĿ¸ëĭ¤&#34;:5224,&#34;ìłĲìĪĺ&#34;:5225,&#34;ìłķìĿĺ&#34;:5226,&#34;ìłģìĿĢ&#34;:5227,&#34;Ġìĭľìĭľ&#34;:5228,&#34;íķ¨ìĿĢ&#34;:5229,&#34;ĠìłĦìŀĳ&#34;:5230,&#34;ĠìķĬìĿĦê¹Į&#34;:5231,&#34;Ġë¬´ì²Ļ&#34;:5232,&#34;ìĭ¤íķľ&#34;:5233,&#34;Ġë§İê³ł&#34;:5234,&#34;Īëį°&#34;:5235,&#34;Ġì§Ģë£¨íķĺê²Į&#34;:5236,&#34;ë³´ê³łìĭ¶ìĿĢ&#34;:5237,&#34;ãħİãħİãħİãħİ&#34;:5238,&#34;ë¬¼ìĿ´&#34;:5239,&#34;Ġì¢ĭìķĺìľ¼ëĤĺ&#34;:5240,&#34;Ġê¸°ëĮĢíĸĪëĬĶëį°&#34;:5241,&#34;ìĤ¬ëŀĮëĵ¤ìĿĢ&#34;:5242,&#34;ĠíĬ¹ìĪĺ&#34;:5243,&#34;Ġíİ¸ê²¬&#34;:5244,&#34;ĠíĥĢìŀĦ&#34;:5245,&#34;ĠìĽĥìĿĮìĿ´&#34;:5246,&#34;Ġê°ĲìłķìĿĦ&#34;:5247,&#34;VD&#34;:5248,&#34;ìĩ&#34;:5249,&#34;Ġìį¼&#34;:5250,&#34;ìŀŃ&#34;:5251,&#34;ìļĶìĿ¼&#34;:5252,&#34;Ġë³´ë©°&#34;:5253,&#34;ìļ©ìľ¼ë¡ľ&#34;:5254,&#34;íĸĪëĤĺ&#34;:5255,&#34;íĸĪìĸ´&#34;:5256,&#34;Ġëª¨ë¥¼&#34;:5257,&#34;ìħĢ&#34;:5258,&#34;ê²łìĬµëĭĪëĭ¤&#34;:5259,&#34;Ġëª»íķľëĭ¤&#34;:5260,&#34;Ġë©ĺ&#34;:5261,&#34;Ġìļ°ì£¼&#34;:5262,&#34;ìŀ¬ë°ĮëĦ¤ìļĶ&#34;:5263,&#34;ĠìĽĥê¹Ģ&#34;:5264,&#34;ìĹĩëĭ¤&#34;:5265,&#34;ĠíĨ°&#34;:5266,&#34;ë°ĽìĿĢ&#34;:5267,&#34;ëĺĳ&#34;:5268,&#34;ëıħíĬ¹&#34;:5269,&#34;ĠìŀĬìĿĦ&#34;:5270,&#34;ĠìĿ´ìľłë¥¼&#34;:5271,&#34;ì¸µ&#34;:5272,&#34;ĠëłĪìĿ´&#34;:5273,&#34;ĪëįĶëĭĪ&#34;:5274,&#34;Ġì¼ĢìĿ´ë¸Ķ&#34;:5275,&#34;Ġìĸ´ë¦°ìĿ´&#34;:5276,&#34;Ġë²Įìį¨&#34;:5277,&#34;ì¥¬&#34;:5278,&#34;Ġëįķë¶ĦìĹĲ&#34;:5279,&#34;ĠíĹĲë¦¬ìļ°ëĵľ&#34;:5280,&#34;Ġëī´&#34;:5281,&#34;ãħ£&#34;:5282,&#34;.........&#34;:5283,&#34;ĠìĭľëıĦ&#34;:5284,&#34;ë²Ħê·¸&#34;:5285,&#34;ë²Ħë¦¬ëĬĶ&#34;:5286,&#34;ë²ĦìĬ¤íĦ°&#34;:5287,&#34;ë³´ê³łìĭ¶ëĭ¤&#34;:5288,&#34;ĠìĭłíĮĮ&#34;:5289,&#34;Ġì£½ìĿĦ&#34;:5290,&#34;Ġê´Ģíķľ&#34;:5291,&#34;ë¶ĪìĮį&#34;:5292,&#34;Ġìłķìĭłë³ĳ&#34;:5293,&#34;ĠìŀĬíĺĢì§Ģì§Ģ&#34;:5294,&#34;Ġì±Ļê²¨&#34;:5295,&#34;ĠìĽĢì§ģ&#34;:5296,&#34;ĻëĭĪëĭ¤&#34;:5297,&#34;ìĿ¸ê³¼&#34;:5298,&#34;Ġëĭ¤ìĸĳ&#34;:5299,&#34;íķĺê³łìŀĲ&#34;:5300,&#34;ĠëĮĢíķ´ìĦľ&#34;:5301,&#34;ĠëĬĲìĻĢë¥´&#34;:5302,&#34;ì¢ĭìĿĮ&#34;:5303,&#34;Ġëĸ¡&#34;:5304,&#34;ëĬĲëĭĪ&#34;:5305,&#34;0000&#34;:5306,&#34;Ġë¡ľê·¸ìĿ¸&#34;:5307,&#34;ìŀ¬ë¯¸ìŀĪëĭ¤&#34;:5308,&#34;ëŁ¬ëĶĶ&#34;:5309,&#34;ìĸ´ë¦´ìłģ&#34;:5310,&#34;ëİĢ&#34;:5311,&#34;Ġë§¨ëĤł&#34;:5312,&#34;ìĸĺê¸°&#34;:5313,&#34;ë¯¿ê³ł&#34;:5314,&#34;ĠëŁ&#34;:5315,&#34;ìĿ´ë¡ľ&#34;:5316,&#34;ìĹĲë¡ľ&#34;:5317,&#34;ìĸ´ì©Į&#34;:5318,&#34;ìĹĨëĦ¤ìļĶ&#34;:5319,&#34;ìĥģìĺģ&#34;:5320,&#34;ìĺ¤ìĺ¤&#34;:5321,&#34;ĠìĦ¼&#34;:5322,&#34;Ġë´¤ìĿĦëķĮ&#34;:5323,&#34;Ġìŀ¬ë°Įëĭ¤ê³ł&#34;:5324,&#34;ë£¨ìĬ¤&#34;:5325,&#34;Ġë¬´ìĸ¸&#34;:5326,&#34;Ġì²Ļ&#34;:5327,&#34;ì²´ê°Ģ&#34;:5328,&#34;Ġë¹ĦíĮĲ&#34;:5329,&#34;ìķĪíĥĢ&#34;:5330,&#34;Ġì¢ĭìķĦíķ´ìĦľ&#34;:5331,&#34;ĠìŀĳíĴĪìĿĢ&#34;:5332,&#34;ĠëĦĺì¹ĺëĬĶ&#34;:5333,&#34;íĸīë³µ&#34;:5334,&#34;Ġë³´ìĹ¬ì£¼ê³ł&#34;:5335,&#34;ìĸ¸ëĭĪ&#34;:5336,&#34;Ġë³¼ë§Įíķĺëĭ¤&#34;:5337,&#34;ĠëĮĢìĤ¬ê°Ģ&#34;:5338,&#34;ë¦¬ìĬ¤ë§Ī&#34;:5339,&#34;Ġì¢ĭê²łìĸ´ìļĶ&#34;:5340,&#34;Ġì§ģìłĳ&#34;:5341,&#34;ĠìĥĪë¡Ń&#34;:5342,&#34;om&#34;:5343,&#34;ªħ&#34;:5344,&#34;ìĿ´íģ¬&#34;:5345,&#34;¬ëŁ¬&#34;:5346,&#34;ë¦¬íı¬&#34;:5347,&#34;ìĺ¬ë¦¬&#34;:5348,&#34;ìķ¼ë§Ĳë¡ľ&#34;:5349,&#34;Ġì¢ĭê²Į&#34;:5350,&#34;ìŀĪëĤĺ&#34;:5351,&#34;¬ëŀĺ&#34;:5352,&#34;ëħ¼&#34;:5353,&#34;ë£©&#34;:5354,&#34;ìĻĢìĿĺ&#34;:5355,&#34;ëł¤ìļĶ&#34;:5356,&#34;ìĺĢê³ł&#34;:5357,&#34;ĠìĿ¼íĴĪ&#34;:5358,&#34;ĠìĤ¬ëŀĳìĿĺ&#34;:5359,&#34;ĠìĥģìĹħ&#34;:5360,&#34;ëĸĦ&#34;:5361,&#34;Ġê²°ê³¼&#34;:5362,&#34;ëĭĪë¡ľ&#34;:5363,&#34;Ġì§ĳìĹĲìĦľ&#34;:5364,&#34;ĠìĭłìĦłíķľ&#34;:5365,&#34;ĠìĹīìĦ±íķľ&#34;:5366,&#34;,.,.&#34;:5367,&#34;ĠìķĪíĥĢê¹Ŀëĭ¤&#34;:5368,&#34;ĠëĿ&#34;:5369,&#34;¬ëĵľ&#34;:5370,&#34;ĠìĿ´ìļ©&#34;:5371,&#34;ìļĶìĨĮ&#34;:5372,&#34;ìķĦëĤ´&#34;:5373,&#34;ìłĲìĿ´ëĭ¤&#34;:5374,&#34;ìłķìļ°&#34;:5375,&#34;ëĶĺ&#34;:5376,&#34;ìłľëĮĢë¡ľ&#34;:5377,&#34;Ġìĺ¤ë°Ķ&#34;:5378,&#34;Ġìĵ°ëłĪê¸°ëĭ¤&#34;:5379,&#34;ì¢Ģë¹Ħ&#34;:5380,&#34;ĠëĬĲê»´ì§Ħëĭ¤&#34;:5381,&#34;ì¸¡&#34;:5382,&#34;ëĳĳ&#34;:5383,&#34;Ġì«Į&#34;:5384,&#34;Ġíĭ°ë¹ĦìĹĲìĦľ&#34;:5385,&#34;Ġë¹Įëł¤&#34;:5386,&#34;ĠëŁ¬ëĭĿíĥĢìŀĦ&#34;:5387,&#34;Ġë®¤ì§Ģì»¬&#34;:5388,&#34;ì¦Īë¥¼&#34;:5389,&#34;ĠI&#34;:5390,&#34;ìĿ´ë¦Ħ&#34;:5391,&#34;ê¸°ìĹĲëĬĶ&#34;:5392,&#34;ĠìķĦíĶĶ&#34;:5393,&#34;Ġ12&#34;:5394,&#34;ĠìĨĶì§ģ&#34;:5395,&#34;Ġëª¨ë¥´ê²Į&#34;:5396,&#34;ĠëĶ°ë¡ľ&#34;:5397,&#34;ë¨¹ëĬĶ&#34;:5398,&#34;ĠìĹ°ê¸°ëł¥ëıĦ&#34;:5399,&#34;Ġê·¸ëłĩì§Ģ&#34;:5400,&#34;ëĦĪìĬ¤&#34;:5401,&#34;ĠìŀĪìĹĪì§Ģë§Į&#34;:5402,&#34;Ġthe&#34;:5403,&#34;Ġëĥī&#34;:5404,&#34;¨¼&#34;:5405,&#34;..^^&#34;:5406,&#34;ãħĹ&#34;:5407,&#34;íķĺëįĺëį°&#34;:5408,&#34;ìĦľëıĦ&#34;:5409,&#34;ìłĲìłķëıĦ&#34;:5410,&#34;ìĤ¬ëĿ¼&#34;:5411,&#34;ĠìĹĨì§Ģ&#34;:5412,&#34;ĠëĤĺë§Į&#34;:5413,&#34;ì§ĦìĿ´&#34;:5414,&#34;Ġíķľë§ĪëĶĶ&#34;:5415,&#34;Ġê¸°ìĪł&#34;:5416,&#34;ëł¤ëĤĺ&#34;:5417,&#34;íĸĪëĭ¤ê³ł&#34;:5418,&#34;ĠëįĶëŁ¬ìļ´&#34;:5419,&#34;ĠëĵľëŁ½ê²Į&#34;:5420,&#34;Ġëĵľë¦½ëĭĪëĭ¤&#34;:5421,&#34;ìĥĿíĻľ&#34;:5422,&#34;ĠìĹ¬íĸī&#34;:5423,&#34;ĠìŀĳìĿĢ&#34;:5424,&#34;ë°Ķë¡ľ&#34;:5425,&#34;Ġì£½ìĿ´ê³ł&#34;:5426,&#34;ĠíıīìĿ´&#34;:5427,&#34;ĠëĪĦêµ¬ëĤĺ&#34;:5428,&#34;ĠíĤ¬ë§ģíĥĢìŀĦ&#34;:5429,&#34;ĠãħĪ&#34;:5430,&#34;Ġì§Īì§Īëģ&#34;:5431,&#34;Ġë²łìĬ¤íĬ¸&#34;:5432,&#34;Ġë¬´ìĦŃëĭ¤&#34;:5433,&#34;Ġê¹¨ëĭ¬&#34;:5434,&#34;Ġë¶ĪëŁ¬&#34;:5435,&#34;Ġì¦Ĳê±°ìļ´&#34;:5436,&#34;íķ´ì§Ħëĭ¤&#34;:5437,&#34;ĠìĹĨìĸ´ìļĶ&#34;:5438,&#34;ìļ´ëĵľ&#34;:5439,&#34;Ġ18&#34;:5440,&#34;Ġìŀĺíķĺê³ł&#34;:5441,&#34;ìŀĦìĿĦ&#34;:5442,&#34;ë³¸ìĺģíĻĶ&#34;:5443,&#34;íı´&#34;:5444,&#34;ĠìĿ¼ìĿ´&#34;:5445,&#34;ëªħìĿĦ&#34;:5446,&#34;Ġì£½ìĿĢ&#34;:5447,&#34;íĽĦíļĮ&#34;:5448,&#34;ì»¥&#34;:5449,&#34;ĠëĤ¨ìŀĲê°Ģ&#34;:5450,&#34;Ġíĺ¸ëŁ¬&#34;:5451,&#34;ëĨĪëĵ¤&#34;:5452,&#34;Īë²ķ&#34;:5453,&#34;Ġë¬ĺìĤ¬&#34;:5454,&#34;zz&#34;:5455,&#34;ĠìĿ´ëķĮ&#34;:5456,&#34;ìĸ´ëĬĲ&#34;:5457,&#34;ìķĦëĬĶ&#34;:5458,&#34;ë³´ëĦ¤&#34;:5459,&#34;íĨłë¡Ŀ&#34;:5460,&#34;ĠìĿ¸íķ´&#34;:5461,&#34;ĠìŀĲì£¼&#34;:5462,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿĺ&#34;:5463,&#34;ì¼ĵ&#34;:5464,&#34;Ġë¶Īì¾Į&#34;:5465,&#34;ĠìĻĦìłĦíŀĪ&#34;:5466,&#34;Ġë°ľë¡ľ&#34;:5467,&#34;Ġìĵ°ëŀĺê¸°&#34;:5468,&#34;Ġìŀĳê°Ģê°Ģ&#34;:5469,&#34;ĠìĿ´ìģĺëĭ¤&#34;:5470,&#34;ì·¨íĸ¥&#34;:5471,&#34;ë½ķ&#34;:5472,&#34;22&#34;:5473,&#34;©ĺ&#34;:5474,&#34;Ġãħĩãħĩ&#34;:5475,&#34;İģ&#34;:5476,&#34;ìĹĳ&#34;:5477,&#34;ê¸ĭ&#34;:5478,&#34;ìĸ´ìĥī&#34;:5479,&#34;ë©ľë¡ľ&#34;:5480,&#34;ìŀĪìĸ´ìĦľ&#34;:5481,&#34;ĠìĺĨ&#34;:5482,&#34;ë´ħëĭĪëĭ¤&#34;:5483,&#34;ìłľë¡ľ&#34;:5484,&#34;Ġê¸°ê°Ģ&#34;:5485,&#34;ìŀĦìĬ¤&#34;:5486,&#34;ê°ĲìĿĢ&#34;:5487,&#34;Ġë³¼ìĪĺìŀĪ&#34;:5488,&#34;Ġìĭ¶ìĿĦ&#34;:5489,&#34;ìķłê°Ģ&#34;:5490,&#34;ĠìĤ¬ëŀĳìĬ¤ëŁ¬ìļ´&#34;:5491,&#34;ìĨįìĿĺ&#34;:5492,&#34;Ġê³łë§Ī&#34;:5493,&#34;Ġë°Ķê¾¸&#34;:5494,&#34;ĠíŀĺìĿ´&#34;:5495,&#34;Ġê°ĢìĬ´ìķĦ&#34;:5496,&#34;ìŀ¥ë©´ìĿĢ&#34;:5497,&#34;Ġê°Ħëĭ¤&#34;:5498,&#34;ĠìĭłìĦłíķĺê³ł&#34;:5499,&#34;Ġì¶ĶìĸµìĿĺ&#34;:5500,&#34;Ġë³´ìķĺìĬµëĭĪëĭ¤&#34;:5501,&#34;Ġê°Ģì¹ĺê°Ģ&#34;:5502,&#34;ĠìĺĪë»Ĳ&#34;:5503,&#34;ë°©ìĤ¬ìĪĺ&#34;:5504,&#34;Ġëł&#34;:5505,&#34;ëĭī&#34;:5506,&#34;ìĿ´ë²Ī&#34;:5507,&#34;ìĹĪëĥĲ&#34;:5508,&#34;ĠìĺģíĻĶëĦ¤&#34;:5509,&#34;ĠìķĦíĶĦ&#34;:5510,&#34;Ġê·¸ëĭ¤ì§Ģ&#34;:5511,&#34;Ġìĸ´ì²ĺêµ¬ëĭĪ&#34;:5512,&#34;Ġë§ĮëĤĺ&#34;:5513,&#34;íİľ&#34;:5514,&#34;ìĦ¸ê¸°&#34;:5515,&#34;Ġëª»ìĥĿ&#34;:5516,&#34;ëħĦë§ĮìĹĲ&#34;:5517,&#34;ëĲĺëĦ¤ìļĶ&#34;:5518,&#34;ë³´ê³łëĤĺìĦľ&#34;:5519,&#34;íħĿ&#34;:5520,&#34;ëĤ¨ëĬĶ&#34;:5521,&#34;ê²¨ì§Ħ&#34;:5522,&#34;ĠìĦ±ìĿ¸&#34;:5523,&#34;ëĲ©ëĭĪëĭ¤&#34;:5524,&#34;Ġê³¼ìŀ¥&#34;:5525,&#34;ĠëĶ±íŀĪ&#34;:5526,&#34;ì²ĺìĿĮìĹĶ&#34;:5527,&#34;Ġë¶Ģì¡±íķĺëĭ¤&#34;:5528,&#34;Ġìĭ¶ìĿĢëį°&#34;:5529,&#34;Ġë¬¸ìłľê°Ģ&#34;:5530,&#34;Ġë¿ĲìĿ´ëĭ¤&#34;:5531,&#34;ĠìĻľìĿ´ëŀĺ&#34;:5532,&#34;ĠìĦ¹ìĬ¤&#34;:5533,&#34;ìĸ´ëł¸ìĿĦ&#34;:5534,&#34;ĠëĬĲê¼Ī&#34;:5535,&#34;ì§ĢëıĦìķĬê³ł&#34;:5536,&#34;íľĺ&#34;:5537,&#34;ĦìĥĪ&#34;:5538,&#34;ŃëĭĪëĭ¤&#34;:5539,&#34;ìĹĲíľ´&#34;:5540,&#34;Ġê·¸ê±°&#34;:5541,&#34;ìłĲì¤Į&#34;:5542,&#34;ìĬ¤íĭ°&#34;:5543,&#34;ìĬ¤ì¼Ģ&#34;:5544,&#34;Ġë´¤ìĿĦ&#34;:5545,&#34;íķ¨ìĿĺ&#34;:5546,&#34;Ġì¤¬&#34;:5547,&#34;ìĭłë¶Ħ&#34;:5548,&#34;Ġëª»íķ¨&#34;:5549,&#34;Ġê°ľë¿Ķ&#34;:5550,&#34;Ġë©Ī&#34;:5551,&#34;ìĽĲìĿĺ&#34;:5552,&#34;ëĲĺìĹĪëĭ¤&#34;:5553,&#34;ë¦¬ëį&#34;:5554,&#34;ìķłëĭĪë©ĶìĿ´ìħĺ&#34;:5555,&#34;ĠëªħíĴĪ&#34;:5556,&#34;ĠíķĺëĤĺìĿĺ&#34;:5557,&#34;ë³¼ë§Įíķľ&#34;:5558,&#34;Ġë³´ìĹ¬ì¤Ģëĭ¤&#34;:5559,&#34;Ġëĭ¨ì§Ģ&#34;:5560,&#34;ëĬ¥ëł¥&#34;:5561,&#34;ë§Įëĵ¤ìĸ´&#34;:5562,&#34;ëıħë¦½&#34;:5563,&#34;ĠìĨĮìŀ¬ê°Ģ&#34;:5564,&#34;Ġê¸´ìŀ¥ê°ĲëıĦ&#34;:5565,&#34;ĪëĶ§&#34;:5566,&#34;Ġë¬´ìĹĩìĿĦ&#34;:5567,&#34;ì¶©ê²©&#34;:5568,&#34;ê¹Ĭê²Į&#34;:5569,&#34;ĠíķĻìĥĿ&#34;:5570,&#34;ĠíŀĪìĸ´ë¡ľ&#34;:5571,&#34;Ġë©įì²Ńíķľ&#34;:5572,&#34;ìĿ½&#34;:5573,&#34;íĻĢ&#34;:5574,&#34;ëĤĻ&#34;:5575,&#34;ëį¤&#34;:5576,&#34;ĠìĿ´ëģĮ&#34;:5577,&#34;ë¡ľëĵľ&#34;:5578,&#34;ëĦ¹&#34;:5579,&#34;ë°ĸ&#34;:5580,&#34;ë¶ķ&#34;:5581,&#34;ĠìĹĨìĬµëĭĪëĭ¤&#34;:5582,&#34;ĠìĥĪëģ¼&#34;:5583,&#34;Ġìĺ·&#34;:5584,&#34;ìĦ±ê³¼&#34;:5585,&#34;Ġìŀ¬ë°ĮëĦ¤&#34;:5586,&#34;ĠìĥĿê°ģìĹĨìĿ´&#34;:5587,&#34;ê´ľ&#34;:5588,&#34;Ġë¯¸ëĵľ&#34;:5589,&#34;ë²Ħë¦¼&#34;:5590,&#34;ĠìŀĪëĬĶì§Ģ&#34;:5591,&#34;ĠíķĺëĬĶê±°&#34;:5592,&#34;ì´Į&#34;:5593,&#34;ĠìľĦíķ´ìĦľ&#34;:5594,&#34;ĠìĨįìĹĲìĦľ&#34;:5595,&#34;ĠìłķëıĦìĿĺ&#34;:5596,&#34;ëıĪì£¼ê³ł&#34;:5597,&#34;ĠìĿ¸ìĥģìłģìĿ´&#34;:5598,&#34;Ġê¿ĪìĿĦ&#34;:5599,&#34;ĠëĽ°ìĸ´ëĦĺ&#34;:5600,&#34;ĠìĿ´ìłķëıĦë©´&#34;:5601,&#34;ĠìķĮê²łëĬĶëį°&#34;:5602,&#34;Ġë°ĳ&#34;:5603,&#34;ìĹ°ìĿ´&#34;:5604,&#34;ĠìĥĿê°ģíķ¨&#34;:5605,&#34;ëħĦìĿĺ&#34;:5606,&#34;Ġë¹Ħíķĺë©´&#34;:5607,&#34;ĠìķĪë´Ĳ&#34;:5608,&#34;Ġìĥģê´Ģ&#34;:5609,&#34;ì¶Ķê¸°&#34;:5610,&#34;ê²¨ìĦľ&#34;:5611,&#34;ĠìĿ´íķ´íķĺê¸°&#34;:5612,&#34;ĠëĨĴê²Į&#34;:5613,&#34;ë²Ķì£Ħ&#34;:5614,&#34;ê²ģëĭĪëĭ¤&#34;:5615,&#34;ĠëĶĶì¦ĪëĭĪ&#34;:5616,&#34;Ġì²ŃìĨĮëħĦ&#34;:5617,&#34;ê²ĥê°ĻìķĦìļĶ&#34;:5618,&#34;ĠíĴįê²½&#34;:5619,&#34;ĠíĪ¬ìŀĲ&#34;:5620,&#34;Ġì§ĿìĿ´&#34;:5621,&#34;Ġì°Įì§Ī&#34;:5622,&#34;Ġìĸ´ë¦´ëķĮ&#34;:5623,&#34;ĠìĿĳìĽĲ&#34;:5624,&#34;.,&#34;:5625,&#34;¬ĺ&#34;:5626,&#34;ëĮĵê¸Ģ&#34;:5627,&#34;Ġì¯&#34;:5628,&#34;ĠìµĿìĺ¤&#34;:5629,&#34;ëĭĮ&#34;:5630,&#34;ëĤĺìĺ¬&#34;:5631,&#34;ĠíķĺìĿ´&#34;:5632,&#34;ĠìŀĪêµ¬ëĤĺ&#34;:5633,&#34;ê·¸ëĤĺë§Ī&#34;:5634,&#34;Ġê°Ģë³įê²Į&#34;:5635,&#34;íĮĶìĿ´&#34;:5636,&#34;ëŁ¬ìĽĮ&#34;:5637,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:5638,&#34;Ġë§ĲìķĦë¨¹&#34;:5639,&#34;Ġíķ´ëĿ¼&#34;:5640,&#34;ĠìķĦê¹Įìļ¸&#34;:5641,&#34;ì¼ĢìĿ´ë¸Ķ&#34;:5642,&#34;ìĶ¬ìĿ´&#34;:5643,&#34;Ġëª¨ë¥´ê²łìĿĮ&#34;:5644,&#34;ĠìıŁ&#34;:5645,&#34;Ġì¤ĳìļĶíķľ&#34;:5646,&#34;ĠíĥĦíĥĦíķľ&#34;:5647,&#34;Ġê¹ľì§Ŀ&#34;:5648,&#34;ìĿ´ëıĦ&#34;:5649,&#34;ëĭĪëĭ¹&#34;:5650,&#34;ìĭľìĽĲ&#34;:5651,&#34;ìĿĮìĿ´&#34;:5652,&#34;ëª¬&#34;:5653,&#34;ĠìłĦë°ĺ&#34;:5654,&#34;Ġìļ°ëł¤&#34;:5655,&#34;Ġìŀ¬ë¯¸ìĹĨê²Į&#34;:5656,&#34;ë°ľìĹ°ê¸°&#34;:5657,&#34;ĠìĦ±ê²©&#34;:5658,&#34;Ġë©ĭìŀĪëĭ¤&#34;:5659,&#34;000&#34;:5660,&#34;ëĵľëĿ¼ë§Īê°Ģ&#34;:5661,&#34;Ġì§ľì¦ĿëĤľëĭ¤&#34;:5662,&#34;ĠìĪĺì¤ĢìĿĺ&#34;:5663,&#34;Ġì¹ĺê³ł&#34;:5664,&#34;ëķħ&#34;:5665,&#34;ĠêµĲìľ¡&#34;:5666,&#34;ĠíĥĢìĿ´&#34;:5667,&#34;Ġìłľìŀĳë¹Ħ&#34;:5668,&#34;ìĿ¸ê°ĦìĿĺ&#34;:5669,&#34;âĺĨ&#34;:5670,&#34;¬¸&#34;:5671,&#34;Ńìłľ&#34;:5672,&#34;ĠëĮ&#34;:5673,&#34;ìĿ´ìģĺ&#34;:5674,&#34;Ġìķ¤&#34;:5675,&#34;ê¸°ì§Ģ&#34;:5676,&#34;ë§ĮìĿĢ&#34;:5677,&#34;Ġë³´ëł¤ê³ł&#34;:5678,&#34;ì£¼ê°Ģ&#34;:5679,&#34;ĠìłķìĿĺ&#34;:5680,&#34;Ġìĸ´ìļ°&#34;:5681,&#34;ĠëĤ´ë©´&#34;:5682,&#34;ìķĺìĿĦ&#34;:5683,&#34;ìŀĦìĥĪ&#34;:5684,&#34;Ġë³¼ê±°&#34;:5685,&#34;Ġíķ´ê²°&#34;:5686,&#34;Ġíķłë§ĲìĿ´&#34;:5687,&#34;Ġê´Ģìĭ¬&#34;:5688,&#34;ìĤ´ìĿ¸&#34;:5689,&#34;íĪ¬ë&#34;:5690,&#34;ĠëĤ¨ìŀĲìĿĺ&#34;:5691,&#34;Ġë°ĽìĿĦ&#34;:5692,&#34;ĠìºĲë¦ŃíĦ°ê°Ģ&#34;:5693,&#34;ìĹ¬ëŁ¬ë&#34;:5694,&#34;ê·ĢìĹ½&#34;:5695,&#34;ĠìĨĮë¦ĦìĿ´&#34;:5696,&#34;ëĮĢíķľë¯¼êµŃ&#34;:5697,&#34;Ġê²¨ìļ°&#34;:5698,&#34;Ġê¹¨ëĭ«ê²Į&#34;:5699,&#34;ë·&#34;:5700,&#34;ĪĦ&#34;:5701,&#34;ìĿ´ëĵ¤&#34;:5702,&#34;ê°Ģë©´&#34;:5703,&#34;ìĿ¸ê²ĥ&#34;:5704,&#34;ê¹ģ&#34;:5705,&#34;ê±°ìĿĺ&#34;:5706,&#34;ìŀĲëĵ¤ìĿ´&#34;:5707,&#34;ë¶Ģìŀĳ&#34;:5708,&#34;ĠìŀĲìĤ´&#34;:5709,&#34;Ġìļ°ìłķ&#34;:5710,&#34;Ġëħ¸ëĭµ&#34;:5711,&#34;ê°ķíĺ¸&#34;:5712,&#34;Ġë°©ìĺģ&#34;:5713,&#34;Ġì°įìĸ´&#34;:5714,&#34;Ġ50&#34;:5715,&#34;ìŀ¬ë°ĭëĭ¤&#34;:5716,&#34;Ġíĸīë³µíķľ&#34;:5717,&#34;Ġê°ľìĹ°ìĦ±ìĿ´&#34;:5718,&#34;Ġê°ľìĹ°ìĦ±ëıĦ&#34;:5719,&#34;ìĺģìĥģë¯¸&#34;:5720,&#34;Ġëĺĳê°ĻìĿĢ&#34;:5721,&#34;Īë²½ìĹĲ&#34;:5722,&#34;.^^&#34;:5723,&#34;ĠìĹ¿&#34;:5724,&#34;ë¦¬ì¹´&#34;:5725,&#34;ëłĲ&#34;:5726,&#34;Ġë³´ìŀĲ&#34;:5727,&#34;ĠìŀĪìĹĪëĬĶëį°&#34;:5728,&#34;íĸĪìľ¼ëĤĺ&#34;:5729,&#34;ĠìķĬëĦ¤ìļĶ&#34;:5730,&#34;ĠìĿ´ëŁ°ìĺģíĻĶê°Ģ&#34;:5731,&#34;ëĲĺê²Į&#34;:5732,&#34;Ġëĭ¹ìĹ°&#34;:5733,&#34;ĠíĹĪìĦ¸&#34;:5734,&#34;ì»·&#34;:5735,&#34;ĠëĤ®ê²Į&#34;:5736,&#34;Ġë¹łì§Ħ&#34;:5737,&#34;ìķĦìĿ´ê°Ģ&#34;:5738,&#34;ìķ½íķľ&#34;:5739,&#34;Ġìķŀìľ¼ë¡ľëıĦ&#34;:5740,&#34;ë³´ì§Ģë§Ī&#34;:5741,&#34;Ġêµ°ëĮĢ&#34;:5742,&#34;ĠìķĦë¦Ħëĭ¤ìĽĢ&#34;:5743,&#34;Ġì§Ŀíīģ&#34;:5744,&#34;Ġíı¬ë¥´ëħ¸&#34;:5745,&#34;40&#34;:5746,&#34;ĠãĦ±&#34;:5747,&#34;..;;&#34;:5748,&#34;Ġíĳ¸&#34;:5749,&#34;ë¡ľë§Į&#34;:5750,&#34;ìķĦìłĢìĶ¨&#34;:5751,&#34;Ġê·¸ê±´&#34;:5752,&#34;Ġëĭ¥&#34;:5753,&#34;ë³´ìŀĲ&#34;:5754,&#34;Ġë°ĳìĹĲ&#34;:5755,&#34;Ġë§Įíķľ&#34;:5756,&#34;ê·¸ë¦¼&#34;:5757,&#34;ì¹ĺê°Ģ&#34;:5758,&#34;íķĺëĬĶê±´&#34;:5759,&#34;ĠìķĪìĵ°&#34;:5760,&#34;ĠìķĮìĪĺ&#34;:5761,&#34;Ġì§Ģë£¨íĸĪëĭ¤&#34;:5762,&#34;ëħ¸ëĭµ&#34;:5763,&#34;ìĹĨëĬĶìĺģíĻĶ&#34;:5764,&#34;ĠëĤĺìĺ¤ì§Ģ&#34;:5765,&#34;Ġë¶Ħëĵ¤ìĿĢ&#34;:5766,&#34;ĠëªħìŀĳìĿ´&#34;:5767,&#34;Ġë¡ľë´ĩ&#34;:5768,&#34;Ġìľłì¹ĺíķĺëĭ¤&#34;:5769,&#34;ĠìĿĮìķħê³¼&#34;:5770,&#34;Ġíķľë²Īì¯¤&#34;:5771,&#34;Ġëĳĺì§¸&#34;:5772,&#34;Ġì©Ĳëĭ¤&#34;:5773,&#34;ĠëĭĪì½ľ&#34;:5774,&#34;ìĦ¸ìĥģìĹĲ&#34;:5775,&#34;ĠMì°½&#34;:5776,&#34;OOOê¸°&#34;:5777,&#34;Ġì£½ëĬĶì¤Ħ&#34;:5778,&#34;³¸&#34;:5779,&#34;¬ëłĪ&#34;:5780,&#34;ìĸ´ë¦¬&#34;:5781,&#34;ĠëĤ©&#34;:5782,&#34;ìĺģíĻĶìĿ¸ëį°&#34;:5783,&#34;ìĿĮìĿĦ&#34;:5784,&#34;Ġëĭ¤ëĭĪ&#34;:5785,&#34;Ġìĸ´ë¨¸ëĭĪ&#34;:5786,&#34;Ġìĺ¬ë¦¬&#34;:5787,&#34;ĠìĿ¸ëĤ´&#34;:5788,&#34;ë¬¼ìĿĦ&#34;:5789,&#34;Ġìĺģíĺ¼&#34;:5790,&#34;ĠíĥĲ&#34;:5791,&#34;Ġê°ķëł¥&#34;:5792,&#34;Ġë§Įëĵ¤ìĸ´ëıĦ&#34;:5793,&#34;Ġëĸ¨ìĸ´ì§Ħëĭ¤&#34;:5794,&#34;Ġìķ¼ëıĻ&#34;:5795,&#34;Ġìłľëª©ìĿ´&#34;:5796,&#34;Ġê¸°ëĭ¤ëł¤&#34;:5797,&#34;ĠìķĪë³¼&#34;:5798,&#34;ëłĪë©ĺ&#34;:5799,&#34;ìĺĽëĤłìĹĲ&#34;:5800,&#34;ìĭľì²Ńë¥ł&#34;:5801,&#34;Ġìĺ¤ëŀľë§Į&#34;:5802,&#34;ìĹł&#34;:5803,&#34;ĠíĦ&#34;:5804,&#34;ĠìķĦìĺĪ&#34;:5805,&#34;Ġë³´ëĭ¤ëĬĶ&#34;:5806,&#34;ë³´ëĭĪê¹Į&#34;:5807,&#34;ìłķìłģìĿ¸&#34;:5808,&#34;ìŀ¥ìĿĦ&#34;:5809,&#34;Ġì§Ģêµ¬&#34;:5810,&#34;íĨ±&#34;:5811,&#34;ë¯¸ëĦ¤&#34;:5812,&#34;Ġê¸°íļĮ&#34;:5813,&#34;ëł¤ëĭ¤ê°Ģ&#34;:5814,&#34;ë¬µ&#34;:5815,&#34;êµ¬ìĦ±&#34;:5816,&#34;Ġìŀĺìĸ´ìļ¸&#34;:5817,&#34;ĠìĥĿê°ģëĤĺìĦľ&#34;:5818,&#34;Ġìĺ¤ëŀ«&#34;:5819,&#34;Ġê¹İ&#34;:5820,&#34;ëĭ¤ëĬĶê±°&#34;:5821,&#34;ëĭ¤ëĬĶê±¸&#34;:5822,&#34;ë´¤ëĦ¤ìļĶ&#34;:5823,&#34;ĠíķĺëĬĶê²Į&#34;:5824,&#34;ìĹĲê²ĮëĬĶ&#34;:5825,&#34;ë¡Ŀë²ĦìĬ¤íĦ°&#34;:5826,&#34;ĠìľĦëĮĢíķľ&#34;:5827,&#34;ĠíķľêµŃìĺģíĻĶëĬĶ&#34;:5828,&#34;ì§Ģë£¨íķ´&#34;:5829,&#34;íĨµëł¹&#34;:5830,&#34;ĠìĹ¬ìļ´ìĿĦ&#34;:5831,&#34;ìķ½ê°Ħ&#34;:5832,&#34;ĠíĨµì¾Į&#34;:5833,&#34;ĠëĬĲëĤĦìĪĺ&#34;:5834,&#34;ĠíĿĶíķľ&#34;:5835,&#34;ĠíķĦë¦Ħ&#34;:5836,&#34;70&#34;:5837,&#34;~!!!&#34;:5838,&#34;ëĽ°&#34;:5839,&#34;Ġ/&#34;:5840,&#34;Ġl&#34;:5841,&#34;ĠìŀŃ&#34;:5842,&#34;ë¦¬íķľ&#34;:5843,&#34;Ġì§Īë&#34;:5844,&#34;ìľ¼ë¡ľëĬĶ&#34;:5845,&#34;íĺĪ&#34;:5846,&#34;ì¶¤&#34;:5847,&#34;ĠëĮĢë³¸&#34;:5848,&#34;ì¢Į&#34;:5849,&#34;Ġìµľê³łìŀħëĭĪëĭ¤&#34;:5850,&#34;ë¬¸íĻĶ&#34;:5851,&#34;íĤ¬&#34;:5852,&#34;Ġê°ĲíĿ¥&#34;:5853,&#34;ĠíĮĮìĿ´&#34;:5854,&#34;ìĿ´ëŁ°ê²Į&#34;:5855,&#34;ĠìĹ°ì¶ľê³¼&#34;:5856,&#34;ĠìĦ¸ëł¨&#34;:5857,&#34;íĺķëĭĺ&#34;:5858,&#34;ëĭĪë¥¼&#34;:5859,&#34;ê°ĲëıĻìłģìĿ¸&#34;:5860,&#34;Ġê°ĢìĬ´ìĹĲ&#34;:5861,&#34;Ġ70&#34;:5862,&#34;ë³ĳë§Ľ&#34;:5863,&#34;Ġê½ĥ&#34;:5864,&#34;Ġë¬´ìĹĩìĿ¸ì§Ģ&#34;:5865,&#34;ĠëĳĺìĿ´&#34;:5866,&#34;Ġì»¤íĶĮ&#34;:5867,&#34;Ġê°ĸê³ł&#34;:5868,&#34;ëħĦëıĦìĹĲ&#34;:5869,&#34;ĠìĺĪìģĺê³ł&#34;:5870,&#34;íĭ°ë¹ĦìĹĲìĦľ&#34;:5871,&#34;¸Ķ&#34;:5872,&#34;íĻĶë©´&#34;:5873,&#34;ì§Ģë©´&#34;:5874,&#34;ìĭ¬ë&#34;:5875,&#34;ê²ĮìļĶ&#34;:5876,&#34;íķ´ì£¼ìĦ¸ìļĶ&#34;:5877,&#34;Ġê·¸ê²ĥëıĦ&#34;:5878,&#34;ĠíķĺëĬĺ&#34;:5879,&#34;Ġë´ħëĭĪëĭ¤&#34;:5880,&#34;ì§Ħìĭ¤&#34;:5881,&#34;ìŀĪìĹĪëĭ¤&#34;:5882,&#34;ìłĦë¬¸&#34;:5883,&#34;Ġì§ĢëĤĺëıĦ&#34;:5884,&#34;ëĵľìĭľ&#34;:5885,&#34;ĠëıĦìłĦ&#34;:5886,&#34;ĠìĪĺê³ł&#34;:5887,&#34;ĠìĥĿê°ģíķĺë©´&#34;:5888,&#34;ĠìķĦëĭĻëĭĪëĭ¤&#34;:5889,&#34;Ġë§Įëĵ¤ëĭ¤ëĭĪ&#34;:5890,&#34;ìĦłìĥĿ&#34;:5891,&#34;Ġìĵ°ëłĪê¸°ê°ĻìĿĢ&#34;:5892,&#34;Ġíĥģ&#34;:5893,&#34;ì¢ħìĿ¼&#34;:5894,&#34;ĠìĹ´ë°Ľ&#34;:5895,&#34;ĠìŀĲì²´ëĬĶ&#34;:5896,&#34;Ġì¹´ë¦¬ìĬ¤ë§Ī&#34;:5897,&#34;íĻįì½©&#34;:5898,&#34;ĠìĹ¬ê¸°ìĦľ&#34;:5899,&#34;Ġì°©íķľ&#34;:5900,&#34;ĠíĹĲë¦¬ìĽĥ&#34;:5901,&#34;ìķĪëĲ¨&#34;:5902,&#34;ëłĪë©ĺíĥĢ&#34;:5903,&#34;°ľ&#34;:5904,&#34;ëĿ&#34;:5905,&#34;ë©į&#34;:5906,&#34;ìľĮ&#34;:5907,&#34;ì¤¬ëĭ¤&#34;:5908,&#34;íĺĲ&#34;:5909,&#34;ìĹ¬íĸī&#34;:5910,&#34;Ġê¸°ëıħêµĲ&#34;:5911,&#34;ëķĮëĬĶ&#34;:5912,&#34;ĠìĿ¸ê¸°&#34;:5913,&#34;ĠìłľìĻ¸&#34;:5914,&#34;ëĥĲê³ł&#34;:5915,&#34;ë²ĪìĿĢ&#34;:5916,&#34;Ġë°°ìļ°ë¥¼&#34;:5917,&#34;Ġì¡°ìŀĳ&#34;:5918,&#34;Ġë°ĺê°ľëıĦ&#34;:5919,&#34;ĠíķłìĪĺ&#34;:5920,&#34;Ġìļ¸ì»¥&#34;:5921,&#34;ĠíĬ¹ìĿ´&#34;:5922,&#34;ìĿ´ê±´ëŃĲ&#34;:5923,&#34;Ġìļ°ë¦¬ìĿĺ&#34;:5924,&#34;ĠìłĦê°ľëıĦ&#34;:5925,&#34;ìŀ¡ê³ł&#34;:5926,&#34;ê¸°ëĮĢìķĪíķĺê³ł&#34;:5927,&#34;Ġìŀłê¹Ĳ&#34;:5928,&#34;Ġê¸´ìŀ¥ê°ĲìĿ´&#34;:5929,&#34;ĠêµŃëĤ´&#34;:5930,&#34;????????&#34;:5931,&#34;Ġëĵ£ê³ł&#34;:5932,&#34;en&#34;:5933,&#34;ª¼&#34;:5934,&#34;ëĬĶëĮĢ&#34;:5935,&#34;ĠìĿ´ëıĦ&#34;:5936,&#34;ë§Įëĭ¤&#34;:5937,&#34;ìŀĲëĵ¤ìĿĺ&#34;:5938,&#34;ĠëĤ´ìĿ¸ìĥĿ&#34;:5939,&#34;Ġê¸°ì¤Ģ&#34;:5940,&#34;ë¥´ê²Į&#34;:5941,&#34;Ġë§Įëĵ¤ë©´&#34;:5942,&#34;ë´¤ì§Ģë§Į&#34;:5943,&#34;Ġë°ĶëŀįëĭĪëĭ¤&#34;:5944,&#34;ëĸ¡&#34;:5945,&#34;ĠìĤ´ëł¤&#34;:5946,&#34;Ġíı´&#34;:5947,&#34;ìĶ¨ëĬĶ&#34;:5948,&#34;ĠíĸĪìľ¼ë©´&#34;:5949,&#34;ê¸ĢìİĦ&#34;:5950,&#34;íĺķëŀĺ&#34;:5951,&#34;ĠëĬĲëĤĮìĿĺ&#34;:5952,&#34;ëĶ°ìľĦ&#34;:5953,&#34;ê·¸ëŀĺíĶ½&#34;:5954,&#34;âĻ¥âĻ¥âĻ¥&#34;:5955,&#34;Ġì¶ľìĹ°ì§Ħ&#34;:5956,&#34;ìłĲëĮĢëĬĶ&#34;:5957,&#34;ĠìŀĲìĭłìĿ´&#34;:5958,&#34;ĠëĬĲê¼Īëĭ¤&#34;:5959,&#34;ë¦¬íı¬íĦ°&#34;:5960,&#34;ĠG&#34;:5961,&#34;Ġ~~&#34;:5962,&#34;ķëĭĪëĭ¤&#34;:5963,&#34;ì§ĢìļĶ&#34;:5964,&#34;ĠëĤĻ&#34;:5965,&#34;ĠìķĦíĶĪ&#34;:5966,&#34;ìĭľìĤ¬íļĮ&#34;:5967,&#34;ĠíķĺëĦ¤&#34;:5968,&#34;Ġëĭ¤ë¥¼&#34;:5969,&#34;ê±´ë§Į&#34;:5970,&#34;ĠìĬ¤íģ¬ë¦°&#34;:5971,&#34;ë¯¸ìĬ¤&#34;:5972,&#34;ê²ĥê°ĻìĿĢ&#34;:5973,&#34;ë¶ĦìĿĢ&#34;:5974,&#34;ĠìķĪìĹĲ&#34;:5975,&#34;ĠëĤ¨ìĿĢ&#34;:5976,&#34;íĬ¸ìĿĺ&#34;:5977,&#34;ì¢ĭìķĺëĭ¤&#34;:5978,&#34;ĠëģĿìĿ´&#34;:5979,&#34;Ġë´Ĳì¤Ħ&#34;:5980,&#34;ĠëħĦ&#34;:5981,&#34;íħĮìĿ´&#34;:5982,&#34;ĠìŀĳíĴĪìĦ±&#34;:5983,&#34;Ġíĥĳ&#34;:5984,&#34;Ġëĸ¨&#34;:5985,&#34;Ġíİ¼&#34;:5986,&#34;ĠìķĦìĿ´ëıĮ&#34;:5987,&#34;ĠìŀĶëľ©&#34;:5988,&#34;Ġë°ķì§Ħ&#34;:5989,&#34;ĠìĿ¼ë³¸ìĺģíĻĶ&#34;:5990,&#34;ĠìĿ´ìłľìĦľìķ¼&#34;:5991,&#34;ë³Ħë¡ľëĭ¤&#34;:5992,&#34;ĠìŀĬì§Ģ&#34;:5993,&#34;Ġë¹¼ë©´&#34;:5994,&#34;ëłĪìĿ´ìħĺ&#34;:5995,&#34;Ġëĭ¨ìĪľíķľ&#34;:5996,&#34;ë¶Īë¥ľ&#34;:5997,&#34;ost&#34;:5998,&#34;¥ë¯¸&#34;:5999,&#34;ãģ&#34;:6000,&#34;ìĿ´ìĬ¨&#34;:6001,&#34;íķĺêµ°&#34;:6002,&#34;Ġìĭ¬ë¦¬ë&#34;:6003,&#34;ìłĲìĿ´ìĥģ&#34;:6004,&#34;Ġìŀ¬ë°©&#34;:6005,&#34;ì²©&#34;:6006,&#34;Ġëĭ¤ë£¨&#34;:6007,&#34;ìŀ¥ìķł&#34;:6008,&#34;ëĵľë¥¼&#34;:6009,&#34;ëĵľë¦½ëĭĪëĭ¤&#34;:6010,&#34;Ġìµľê°ķ&#34;:6011,&#34;ĠëĮĢíĻĶ&#34;:6012,&#34;ì¤ĳìĿ´&#34;:6013,&#34;ê°Ļê³ł&#34;:6014,&#34;íıīìłĲìĿĦ&#34;:6015,&#34;ĠíķĺëĤĺê°ĻìĿ´&#34;:6016,&#34;ìĿ´ëŁ°ê±¸&#34;:6017,&#34;ìĻĶëĭ¤&#34;:6018,&#34;ëĭĪëİģ&#34;:6019,&#34;ĠìĦ¤ëĵĿ&#34;:6020,&#34;ĠìĿ¸ê°ĦìĿ´&#34;:6021,&#34;Ġë¶ĦìľĦê¸°ê°Ģ&#34;:6022,&#34;ĠìĭľëĮĢë¥¼&#34;:6023,&#34;ĠíĥĦìĥĿ&#34;:6024,&#34;íĦ¸&#34;:6025,&#34;ëĤ®ìĿĢ&#34;:6026,&#34;íĮĮìĿ´ìĸ´&#34;:6027,&#34;ĠìķĪíĥĢê¹Įìļ´&#34;:6028,&#34;íĵ¨íĦ°&#34;:6029,&#34;ĠëŁ°&#34;:6030,&#34;Ġg&#34;:6031,&#34;ìĸ´ì§Ģ&#34;:6032,&#34;ìĸ´ìĹ¬&#34;:6033,&#34;ëĿ¼ê°Ģ&#34;:6034,&#34;ìķĦëĭĪë&#34;:6035,&#34;íķ´ì£¼ê³ł&#34;:6036,&#34;ë¶Ļ&#34;:6037,&#34;ìĥģìĿĺ&#34;:6038,&#34;ìľ¼ë¡ľë§Į&#34;:6039,&#34;Ġìĸ´ì§Ģ&#34;:6040,&#34;ê°ģë³¸&#34;:6041,&#34;ìłľìĿ¼&#34;:6042,&#34;ĠìĹ°ê¸°ìŀĺ&#34;:6043,&#34;ĠìľĦë¡ľ&#34;:6044,&#34;ê°ľëĬĶ&#34;:6045,&#34;ĠìĥĿê°ģíĸĪëĬĶëį°&#34;:6046,&#34;ĠìĿ¸ëıĦ&#34;:6047,&#34;ìĹĪëĭ¤ëĬĶ&#34;:6048,&#34;ë²Ħëł¤&#34;:6049,&#34;ĠìĤ¬ê³ł&#34;:6050,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪìĸ´ìļĶ&#34;:6051,&#34;ĠìĤ¬ëŀĳìĿĢ&#34;:6052,&#34;ê²½ì°°&#34;:6053,&#34;ìĿ´ëŁ°ê±°&#34;:6054,&#34;Ġìĸ¼ë§Ī&#34;:6055,&#34;ëĤĺìĺ¤ê³ł&#34;:6056,&#34;Ġìļķë§Ŀ&#34;:6057,&#34;Ġëĭ¬ëĭ¬&#34;:6058,&#34;ë¯¸ëĦ¤ìĿ´íĦ°&#34;:6059,&#34;´íĵ¨íĦ°&#34;:6060,&#34;Ġãħĩ&#34;:6061,&#34;ì§Ģë§Ĳê³ł&#34;:6062,&#34;ê°ĢìĿĺ&#34;:6063,&#34;ê¸°ìłĦìĹĲ&#34;:6064,&#34;ëĤĺìģľ&#34;:6065,&#34;ë¦¬ìļ°ëĵľ&#34;:6066,&#34;ìłĲìĹĲìĦľ&#34;:6067,&#34;ìłĲë§ĮìłĲ&#34;:6068,&#34;ìļ°ìĹ°íŀĪ&#34;:6069,&#34;ìķ¼íķł&#34;:6070,&#34;ê³¼ìĦľ&#34;:6071,&#34;Ġê°Ģë©´&#34;:6072,&#34;Ġê°Ģë²¼&#34;:6073,&#34;ìĨĮë£¡&#34;:6074,&#34;Ġëĵľë¦¼&#34;:6075,&#34;ëħĦìŀĳ&#34;:6076,&#34;Ġë§İìķĦìĦľ&#34;:6077,&#34;Ġìĭłë¹Ħ&#34;:6078,&#34;ĠëĦĺê²Į&#34;:6079,&#34;ĠíĶ½&#34;:6080,&#34;Ġì½ĶëĤľ&#34;:6081,&#34;Ġë°°ìĹŃ&#34;:6082,&#34;Ġë§¤ëł¥ìĿĦ&#34;:6083,&#34;Ġì§ĳì°©&#34;:6084,&#34;Ġëª¨ëĳĲê°Ģ&#34;:6085,&#34;ëıĪìĿ´&#34;:6086,&#34;Ġëĵ±ëĵ±&#34;:6087,&#34;ë§Įíģ¼ìĿĢ&#34;:6088,&#34;Ġì§Īë¦¬ì§Ģ&#34;:6089,&#34;ê·ĢìĹ¬&#34;:6090,&#34;ĠëłĪìłĦëĵľ&#34;:6091,&#34;ĠëĨĵê³ł&#34;:6092,&#34;Ġì´Īë°ĺìĹĲ&#34;:6093,&#34;ìĹīìĦ±&#34;:6094,&#34;ì§§&#34;:6095,&#34;ëį°ìĿ´&#34;:6096,&#34;ëĦĪë&#34;:6097,&#34;ĠìķĦì¤Įë§Ī&#34;:6098,&#34;ìĺĪë&#34;:6099,&#34;ìĥĪë¡&#34;:6100,&#34;ìĺģíĻĶìŀħëĭĪëĭ¤&#34;:6101,&#34;ë²Ĺ&#34;:6102,&#34;ìŀ¥íķľ&#34;:6103,&#34;Ġê²ª&#34;:6104,&#34;ëĵľë¡ľ&#34;:6105,&#34;íĨ°&#34;:6106,&#34;ë¶Ģê°Ģ&#34;:6107,&#34;ìłĢìĺĪìĤ°&#34;:6108,&#34;ë¹Ĺ&#34;:6109,&#34;ì¡°ìłĪ&#34;:6110,&#34;Ġì¹Ń&#34;:6111,&#34;Ġê±°ê¸°ìĹĲ&#34;:6112,&#34;ì¤ĦìĿ´ìķ¼&#34;:6113,&#34;ëĭĺëĵ¤&#34;:6114,&#34;ĠìĤ´ë¦°&#34;:6115,&#34;ê²°íĺ¼&#34;:6116,&#34;ìĽłì§Ģë§Į&#34;:6117,&#34;ĠìĨĲë°ľìĿ´&#34;:6118,&#34;Ġëį°ë¦¬ê³ł&#34;:6119,&#34;ëįķíĻĶ&#34;:6120,&#34;ìĿ¸ì¤ĦìķĮ&#34;:6121,&#34;íģ¬ë¡ľ&#34;:6122,&#34;ĠìŀĶìĿ¸íķľ&#34;:6123,&#34;ë¹¨ë¦¬&#34;:6124,&#34;Ġê·¸ëħĢìĿĺ&#34;:6125,&#34;ëª°ìŀħëıĦ&#34;:6126,&#34;ìĸ´ëĶĶìĦľ&#34;:6127,&#34;Ġë±Ģ&#34;:6128,&#34;Ġp&#34;:6129,&#34;Ġì¸&#34;:6130,&#34;ìĹĲëĮĢ&#34;:6131,&#34;ìĭľë©´&#34;:6132,&#34;ìľ¼ë¦¬&#34;:6133,&#34;ë²ĪëįĶ&#34;:6134,&#34;ìŀĪìĬµëĭĪëĭ¤&#34;:6135,&#34;Ġíķľëªħ&#34;:6136,&#34;ëĵľë¦¼&#34;:6137,&#34;ĠìłĦìľ¨&#34;:6138,&#34;ëłĪìĿ¸&#34;:6139,&#34;Ġì£¼ê¸°ëıĦ&#34;:6140,&#34;ë¹ĦìĬ·&#34;:6141,&#34;Ġë¹Ħë°Ģ&#34;:6142,&#34;íĭ¸&#34;:6143,&#34;ìłģìĿ´ì§Ģ&#34;:6144,&#34;ìµľê³łìµľê³ł&#34;:6145,&#34;ĠìĿĺìĻ¸ë¡ľ&#34;:6146,&#34;ìĺĪê³łíİ¸&#34;:6147,&#34;Ġëĵ¤ê³ł&#34;:6148,&#34;ë°°ìļ°ìĿĺ&#34;:6149,&#34;ì©Ŀ&#34;:6150,&#34;ĠìĹ¬ìŀĲìĿĺ&#34;:6151,&#34;ãħľãħľãħľãħľ&#34;:6152,&#34;Ġìĵ°ê³ł&#34;:6153,&#34;Ġë¶Ģë¶ĦëıĦ&#34;:6154,&#34;Ġëĭµëĭµíķĺê³ł&#34;:6155,&#34;íķĺíķĺíķĺ&#34;:6156,&#34;Ġìĸ´ìļ¸ë¦¬ëĬĶ&#34;:6157,&#34;Ġìıĺ&#34;:6158,&#34;ìĿ´ìĺĢëĭ¤&#34;:6159,&#34;ìĿ´ìĹĪìĿĮ&#34;:6160,&#34;íķĺë£¨&#34;:6161,&#34;íķĺê¸¸&#34;:6162,&#34;Ġìķī&#34;:6163,&#34;ĠìĿ´ìĹ°ê±¸&#34;:6164,&#34;ëĭĪíį¼&#34;:6165,&#34;Ġë³´ëĭ¤ëĭĪ&#34;:6166,&#34;ìĥĪë¡ľ&#34;:6167,&#34;Ġê·¸ìŀĲì²´&#34;:6168,&#34;ê±°ëĵł&#34;:6169,&#34;ìŀĲë¡ľ&#34;:6170,&#34;ì§ľë¡ľ&#34;:6171,&#34;Ġë§ĮëĤ¨&#34;:6172,&#34;ìŀ¬ë¥¼&#34;:6173,&#34;ê²łëĭ¤ëĬĶ&#34;:6174,&#34;Ġë³´ê³łëĤĺë©´&#34;:6175,&#34;ĠìĿ¼ê¹¨&#34;:6176,&#34;ĠëŃĲëĿ¼&#34;:6177,&#34;ë°ĶìĿ´&#34;:6178,&#34;ìĿ¸ëį°ëıĦ&#34;:6179,&#34;ĠìķĦëĭĪìķ¼&#34;:6180,&#34;ì½Ķë©ĶëĶĶ&#34;:6181,&#34;ì©Ĳëĭ¤&#34;:6182,&#34;ìĽłìĸ´ìļĶ&#34;:6183,&#34;..........&#34;:6184,&#34;Ġì£¼ìĿ¸ê³µìĿĢ&#34;:6185,&#34;Ġë³´ê¸°ìĹĲëĬĶ&#34;:6186,&#34;Ġë§İìĿĢëį°&#34;:6187,&#34;Ġëĸ¨ìĸ´ì§Ģê³ł&#34;:6188,&#34;ĠíĿ¥ë¯¸ë¡ľ&#34;:6189,&#34;ĠìĤ´ìķĦê°ĢëĬĶ&#34;:6190,&#34;Ġìĭľë¦¬ì¦ĪëĬĶ&#34;:6191,&#34;Ġë§Įëĵ¤ìĹĪëĬĶì§Ģ&#34;:6192,&#34;ìķĦë¦Ħëĭµ&#34;:6193,&#34;Ġìĸ´ì©ĶìĪĺ&#34;:6194,&#34;Ġê³µíı¬ë¬¼&#34;:6195,&#34;Ġíļ¨ê³¼&#34;:6196,&#34;íĻĶëł¤íķľ&#34;:6197,&#34;vd&#34;:6198,&#34;ĪìĿ´&#34;:6199,&#34;ľ©&#34;:6200,&#34;ìĿ´ìĺģíĻĶê°Ģ&#34;:6201,&#34;ĠìĺģíĻĶíĻĶ&#34;:6202,&#34;ĠìĺģíĻĶë³´ëĬĶ&#34;:6203,&#34;ëį´&#34;:6204,&#34;ëĬĶëį°ëıĦ&#34;:6205,&#34;Ġë´ī&#34;:6206,&#34;ìłģìŀĦ&#34;:6207,&#34;ê·¸ëħĢ&#34;:6208,&#34;Ġë´¤ìĿĦê¹Į&#34;:6209,&#34;Ġê°Ģì§Ģ&#34;:6210,&#34;ë¶Ģë¡ľ&#34;:6211,&#34;Ġìľ¡&#34;:6212,&#34;Ġë¬´ìĪł&#34;:6213,&#34;ĠìĥĿê°ģíķł&#34;:6214,&#34;ìĺĢìĸ´&#34;:6215,&#34;ë¹Ľ&#34;:6216,&#34;Ġíķ´ìĦĿ&#34;:6217,&#34;ĠìĽĥê²¼&#34;:6218,&#34;Ġì°¸ìľ¼ë¡ľ&#34;:6219,&#34;ëŃĲê°Ģ&#34;:6220,&#34;ĠíĺĦìĭ¤ìĦ±&#34;:6221,&#34;íıīë²Ķ&#34;:6222,&#34;Ġëª°ìŀħê°Ĳ&#34;:6223,&#34;Ġìľłì¹ĺíķ¨&#34;:6224,&#34;ë°Ľê³ł&#34;:6225,&#34;Ġê¸¸ê²Į&#34;:6226,&#34;ĠìĦ¸ìĥģìĿĦ&#34;:6227,&#34;ĠìķĦë¦Ħëĭµê²Į&#34;:6228,&#34;ìĭ¸ìĽĢ&#34;:6229,&#34;Ġì§Ħë¶Ģíķĺê³ł&#34;:6230,&#34;ĠíĮ¨ëŁ¬ëĶĶ&#34;:6231,&#34;Ġì¢ĭê²łëĦ¤ìļĶ&#34;:6232,&#34;Ġì¿ł&#34;:6233,&#34;Ġê°ĲìłķìĿ´ìŀħ&#34;:6234,&#34;as&#34;:6235,&#34;cg&#34;:6236,&#34;Ĩĵ&#34;:6237,&#34;Ġëĳ&#34;:6238,&#34;ãħĬ&#34;:6239,&#34;ì§Ģê¸Īë´ĲëıĦ&#34;:6240,&#34;ĠìŀĪëĦ¤&#34;:6241,&#34;ĠìĿ´ëŁ´&#34;:6242,&#34;ìĸ´ë²Ħë¦°&#34;:6243,&#34;ëĭĪê°Ģ&#34;:6244,&#34;ìķĦëĵ¤&#34;:6245,&#34;ìĿ¸ëĵ¤ìĿĺ&#34;:6246,&#34;êµīìŀ¥íŀĪ&#34;:6247,&#34;íķ´ì§Ģê³ł&#34;:6248,&#34;Ġê·¸ë§Įíģ¼&#34;:6249,&#34;ìĬ¤íĤ¤&#34;:6250,&#34;ìłķíĻĶ&#34;:6251,&#34;ìļ°ê°Ģ&#34;:6252,&#34;ìĥģëĭ¹íŀĪ&#34;:6253,&#34;ĠìĭľíĤ¤&#34;:6254,&#34;Ġê¸°ë°ľ&#34;:6255,&#34;êµŃìĺģ&#34;:6256,&#34;ĠìĥĿê°ģìľ¼ë¡ľ&#34;:6257,&#34;Ġê°ľë§īìŀ¥&#34;:6258,&#34;ëĲĺë©´&#34;:6259,&#34;Ġì¡°ìŀ¡&#34;:6260,&#34;Ġë´¤ëĬĶëį°ëıĦ&#34;:6261,&#34;Ġíķłë¨¸ëĭĪ&#34;:6262,&#34;Ġì£½ìĿ´ëĬĶ&#34;:6263,&#34;ìĺĪìłĦ&#34;:6264,&#34;íĮĲíĥĢì§Ģ&#34;:6265,&#34;Ġë¬µ&#34;:6266,&#34;ĠìĪł&#34;:6267,&#34;ĠëıĻìĭľìĹĲ&#34;:6268,&#34;ĠìĿ´ê±°ë³´ê³ł&#34;:6269,&#34;ĠëıĪìĿĦ&#34;:6270,&#34;ìĽłìĬµëĭĪëĭ¤&#34;:6271,&#34;ĠìĿ´ìĥģìĿĺ&#34;:6272,&#34;íĽĪíĽĪ&#34;:6273,&#34;Ġíķľë²ĪëıĦ&#34;:6274,&#34;íķľêµŃìĺģíĻĶ&#34;:6275,&#34;Ġëħ¸ëŀĺëıĦ&#34;:6276,&#34;íķ´ìķ¼ì§Ģ&#34;:6277,&#34;Ġë³Ģíĥľ&#34;:6278,&#34;ĠëŃĺê¹Į&#34;:6279,&#34;ĠìŀĲìĭłìĿĦ&#34;:6280,&#34;ĠëĵľëĿ¼ë§ĪëĿ¼&#34;:6281,&#34;ĠíĮ¬ìĿ´&#34;:6282,&#34;ëĮĦ&#34;:6283,&#34;Ġìľłëªħíķľ&#34;:6284,&#34;¤ì¼Ģ&#34;:6285,&#34;Ġ=&#34;:6286,&#34;Ġy&#34;:6287,&#34;ëĬĶëĵ¯&#34;:6288,&#34;ê°ĢëĦ¤&#34;:6289,&#34;ĠìĺģíĻĶìĿ¸ê°Ģ&#34;:6290,&#34;ìĸ´ê°ĢëĬĶ&#34;:6291,&#34;ĠìķĦëıĻ&#34;:6292,&#34;ìłĲì£¼ê³ł&#34;:6293,&#34;Ġëĭĺ&#34;:6294,&#34;ìĥģíķľ&#34;:6295,&#34;ìĺ¤ë¸Į&#34;:6296,&#34;ëħķ&#34;:6297,&#34;Ġìĸ´ëĳĲ&#34;:6298,&#34;Ġì§ĢëĤ¬&#34;:6299,&#34;ëĥĪëĭ¤&#34;:6300,&#34;Ġìĭľê±¸&#34;:6301,&#34;ĠìĬ¤íĮĮìĿ´&#34;:6302,&#34;ì¹ĺëħ¸&#34;:6303,&#34;íĸĪì§Ģ&#34;:6304,&#34;Ġëª¨íĹĺ&#34;:6305,&#34;Ġë³¸ê²Į&#34;:6306,&#34;ë¹¡&#34;:6307,&#34;Ġê°ĲëıĻìĿĺ&#34;:6308,&#34;ë¦¬ëĦ¤ìļĶ&#34;:6309,&#34;Ġì¡°ìłĪ&#34;:6310,&#34;Ġëĭ¤ìĭľëĬĶ&#34;:6311,&#34;ĠìĤ´ëĭ¤&#34;:6312,&#34;ĠíķĺëĤĺë¡ľ&#34;:6313,&#34;ĠìĺĪê³ł&#34;:6314,&#34;ê²¨ìļ¸&#34;:6315,&#34;íĶ¼ìĬ¤&#34;:6316,&#34;ĠìĿ´ê±´ëŃĲ&#34;:6317,&#34;ĠìķĦê¹ĮìĽłëĭ¤&#34;:6318,&#34;Ġê¹Ģê¸°ëįķ&#34;:6319,&#34;Ġìĭ¬ì§Ģìĸ´&#34;:6320,&#34;ìĹĩëĬĶëį°&#34;:6321,&#34;ìŀ¬ë¯¸ìŀĪê³ł&#34;:6322,&#34;Ġëªĩë²ĪìĿĦ&#34;:6323,&#34;ĠìĹ°ê¸°ëł¥ìĿĢ&#34;:6324,&#34;Ġì¹ĺë°Ģ&#34;:6325,&#34;ìĵ°ê³ł&#34;:6326,&#34;ĠìĿĺë¯¸ê°Ģ&#34;:6327,&#34;Ġëĭµëĭµíķľ&#34;:6328,&#34;ì¡¸ëĿ¼&#34;:6329,&#34;ê°ĳëĭĪëĭ¤&#34;:6330,&#34;ê±°ê°ĻìķĦìļĶ&#34;:6331,&#34;ĠìĿĢê·¼&#34;:6332,&#34;ëļĿ&#34;:6333,&#34;Ġëıĭë³´ìĿ´ëĬĶ&#34;:6334,&#34;Ġê²¨ìļ¸&#34;:6335,&#34;ë¬ĺíķľ&#34;:6336,&#34;ĠëĤĺë¨¸ì§Ģ&#34;:6337,&#34;ĠìłĦë¬¸ê°Ģ&#34;:6338,&#34;ë¢°ë§¤&#34;:6339,&#34;ë·Ķ&#34;:6340,&#34;ut&#34;:6341,&#34;Ġ;&#34;:6342,&#34;Ġë¥¼&#34;:6343,&#34;íĻĶìĿ´íĮħ&#34;:6344,&#34;Ġíľĺ&#34;:6345,&#34;ìłĲìľ¼ë¡ľ&#34;:6346,&#34;ìĭľìĺ¤&#34;:6347,&#34;ìŀĲìĻĢ&#34;:6348,&#34;ìĹĨê²Į&#34;:6349,&#34;Ġíķĺê¸¸ëŀĺ&#34;:6350,&#34;ì¹Ń&#34;:6351,&#34;ìŀĳìĿĢ&#34;:6352,&#34;íĥĳ&#34;:6353,&#34;Ġìŀ¬ë¯¸ìĻĢ&#34;:6354,&#34;Ġê°Ģê³ł&#34;:6355,&#34;Ġê°Ģë¥´&#34;:6356,&#34;íķĺëĬĶê²ĥ&#34;:6357,&#34;ëŀĺê³¤&#34;:6358,&#34;ì¤ĳìĹĲìĦľ&#34;:6359,&#34;Ġì£¼ê¸°&#34;:6360,&#34;ëķĮë¬¸&#34;:6361,&#34;ëĵłì§Ģ&#34;:6362,&#34;Ġë³¸ê±°&#34;:6363,&#34;ĠìĿ´ëŁ°ìĺģíĻĶë¥¼&#34;:6364,&#34;ìĭ¬ìľ¼ë¡ľ&#34;:6365,&#34;ĠìĬ¤íĨłë¦¬ìĿĺ&#34;:6366,&#34;Ġì¤ĳìĹĲìĦľ&#34;:6367,&#34;Ġê±°ë¶Ģ&#34;:6368,&#34;ĠìĺģêµŃ&#34;:6369,&#34;ìľĦìĹĲ&#34;:6370,&#34;ë´Ĳì£¼&#34;:6371,&#34;Ġë¶Ħëĵ¤ìĿ´&#34;:6372,&#34;ĠëıĦë§Ŀ&#34;:6373,&#34;Ġìŀ¥ë©´ëıĦ&#34;:6374,&#34;ĠëªħìŀĳìĿĦ&#34;:6375,&#34;ë³µìĿĦ&#34;:6376,&#34;Ġêµ¬ìĦ±ìĿ´&#34;:6377,&#34;ĠëĲ©ëĭĪëĭ¤&#34;:6378,&#34;ĠëĤ®ëĦ¤&#34;:6379,&#34;íıīìĿ´&#34;:6380,&#34;ëĬĶê±°ìķ¼&#34;:6381,&#34;Ġë³´ì§Ģë§ĪëĿ¼&#34;:6382,&#34;ĠìĨĲìĹĲ&#34;:6383,&#34;ĠëĶĶíħĮìĿ¼&#34;:6384,&#34;ê±´ê°ĢìļĶ&#34;:6385,&#34;Ġê´ľì°®ìĿĢëį°&#34;:6386,&#34;ĠìĿ½ìĸ´&#34;:6387,&#34;Ġì§ĢëĤĺì¹ĺê²Į&#34;:6388,&#34;łìłģìľ¼ë¡ľ&#34;:6389,&#34;ëĬ¦ê²Į&#34;:6390,&#34;ì§ĢëĦ¤&#34;:6391,&#34;ëıĦë¥¼&#34;:6392,&#34;ĠìĹł&#34;:6393,&#34;ëĤĺëĭĪ&#34;:6394,&#34;ë³´ìĺģ&#34;:6395,&#34;ìĽĶìĿ´&#34;:6396,&#34;íķĺê³łìĭ¶&#34;:6397,&#34;ê·¸ëŁ¼&#34;:6398,&#34;ê²łì£ł&#34;:6399,&#34;ìķħíķľ&#34;:6400,&#34;ĠìŀĲìĭĿ&#34;:6401,&#34;Ġìľłì§Ģ&#34;:6402,&#34;ì¢ĭëĦ¤ìļĶ&#34;:6403,&#34;Ġê°ĲìķĪ&#34;:6404,&#34;Ġìĺģìĸ´&#34;:6405,&#34;Ġìĺģíĸ¥&#34;:6406,&#34;!!!!!!&#34;:6407,&#34;íĭ°ì¦Į&#34;:6408,&#34;ĠìĿ´ìķ¼ê¸°ëĬĶ&#34;:6409,&#34;Ġë³Ħë¡ľê³ł&#34;:6410,&#34;Ġìĭ¬íķĺê²Į&#34;:6411,&#34;ëĵ±ìŀ¥&#34;:6412,&#34;ë°ĽëĬĶ&#34;:6413,&#34;ìĤ¬ë¡ľ&#34;:6414,&#34;ĠìĥĿíĻľ&#34;:6415,&#34;Ġë°ĺìłĦìĿĢ&#34;:6416,&#34;Ġë°ĽìĿĢ&#34;:6417,&#34;Ġìĵ°ëĬĶ&#34;:6418,&#34;ĠìĶ¹&#34;:6419,&#34;ëĪĦëĤĺ&#34;:6420,&#34;Ġëį°ìĿ´&#34;:6421,&#34;ĠíĽĮë¥Ńíķĺëĭ¤&#34;:6422,&#34;Ġë¬´ìĦľìĽĮ&#34;:6423,&#34;ë¶Ģë¶ĦìĿ´&#34;:6424,&#34;ĠíĻķìĭ¤&#34;:6425,&#34;ìłľëª©ìĿ´&#34;:6426,&#34;ĠêµĲíĽĪìĿĦ&#34;:6427,&#34;Ġë³¼ìĪĺìŀĪëĬĶ&#34;:6428,&#34;ìĭ«ìĸ´&#34;:6429,&#34;ëĵľëĿ¼ë§Īë¥¼&#34;:6430,&#34;ĠìĺģìĽĲíŀĪ&#34;:6431,&#34;ĠDVD&#34;:6432,&#34;ĹĦ&#34;:6433,&#34;ê¸°ê¹Įì§Ģ&#34;:6434,&#34;ìłĲìłĲ&#34;:6435,&#34;ìĭľë¦¬&#34;:6436,&#34;ë³´ìĿ´ëĬĶ&#34;:6437,&#34;Ġ13&#34;:6438,&#34;íķĺëĬĶìĺģíĻĶ&#34;:6439,&#34;ĠëĮĢì¤ĳ&#34;:6440,&#34;íĸĪìĹĪëĬĶëį°&#34;:6441,&#34;ĠìķĪìĬµ&#34;:6442,&#34;ê°ľê°Ģ&#34;:6443,&#34;Ġëª»íķĺëĭ¤&#34;:6444,&#34;Ġìķłëĵ¤ìĿĢ&#34;:6445,&#34;ê¸´íķĺì§Ģë§Į&#34;:6446,&#34;ĠìĽĥê³ł&#34;:6447,&#34;Ġìŀ¬ë¯¸ìĹĨìĹĪëĭ¤&#34;:6448,&#34;ë°ĶëŀĮ&#34;:6449,&#34;íĹĪë¬´&#34;:6450,&#34;ĠëķĮëł¤&#34;:6451,&#34;ëĬĲê»´&#34;:6452,&#34;ĠëªħìŀĳìŀħëĭĪëĭ¤&#34;:6453,&#34;ì©ľ&#34;:6454,&#34;Ġìŀ¼ëĤĺê²Į&#34;:6455,&#34;ìĥĿê°ģìĿ´&#34;:6456,&#34;ĠìŀĪëĭ¤ëĭĪ&#34;:6457,&#34;Ġì§ľì¦ĿëĤĺìĦľ&#34;:6458,&#34;íĪ¬ë¦¬&#34;:6459,&#34;Ġì§ĳìĹĲ&#34;:6460,&#34;Ġê²°ë§ĲìĿĦ&#34;:6461,&#34;Ġëĭ´ìķĦ&#34;:6462,&#34;ĠìĤ¶ìĹĲ&#34;:6463,&#34;ìĺ¬íķ´&#34;:6464,&#34;ìŀĸìķĦìļĶ&#34;:6465,&#34;Ġë²ĶìĿ¸&#34;:6466,&#34;Ġì£¼ìłľë¥¼&#34;:6467,&#34;Ġìĥīëĭ¤ë¥¸&#34;:6468,&#34;ãħħãħĤ&#34;:6469,&#34;Ġíĺ¹ìĿĢ&#34;:6470,&#34;íĢ´&#34;:6471,&#34;ĠìĨĮì¤ĳíķľ&#34;:6472,&#34;ĠëĤĺìģĺì§Ģ&#34;:6473,&#34;íĮ©íĬ¸&#34;:6474,&#34;ëī´&#34;:6475,&#34;ìĿ´ìĸ´&#34;:6476,&#34;ìĸ´íľ´&#34;:6477,&#34;ĠëĤŃ&#34;:6478,&#34;ë§ĮëıĦ&#34;:6479,&#34;ëĿ¼ìĿĺ&#34;:6480,&#34;Ġë³´ëįĺ&#34;:6481,&#34;ìĭľì¹´&#34;:6482,&#34;ĲëıĻ&#34;:6483,&#34;Ġì°°&#34;:6484,&#34;ĠëĮĢìĤ¬ëĵ¤&#34;:6485,&#34;íĸĪìĿĦëķĮ&#34;:6486,&#34;íĮł&#34;:6487,&#34;ì¤ĳíĽĪ&#34;:6488,&#34;Ġë¬´ë¦¬&#34;:6489,&#34;ë¶ĦìĿĺ&#34;:6490,&#34;ĠëĤ¨ëĦ¤ìļĶ&#34;:6491,&#34;Ġìľłë°ľ&#34;:6492,&#34;ĠìĤ¬ì§Ħ&#34;:6493,&#34;íĶĦëĭĿ&#34;:6494,&#34;Ġìĭľê°Ħê°ĢëĬĶì¤Ħ&#34;:6495,&#34;íĽĦìĿĺ&#34;:6496,&#34;Ġê°ķíķľ&#34;:6497,&#34;ìĽĥê¹Ģ&#34;:6498,&#34;Ġìĭ¬ìĺ¤&#34;:6499,&#34;ëĵľëĿ¼ë§ĪëĬĶ&#34;:6500,&#34;ĠëĤ®ì§Ģ&#34;:6501,&#34;ĠìłģìłĪ&#34;:6502,&#34;ĠìķĦìī½ì§Ģë§Į&#34;:6503,&#34;ĠíĿ¥ë¯¸ë¡ľìļ´&#34;:6504,&#34;ĠìĽĥê¸°ì§ĢëıĦ&#34;:6505,&#34;Ġìĸ´ìĥīíķĺê³ł&#34;:6506,&#34;ì»¤íĶĮ&#34;:6507,&#34;ê¹ĬìĿĢ&#34;:6508,&#34;Ġì¶ĶìĸµìĿ´&#34;:6509,&#34;ê°ĳìŀĲê¸°&#34;:6510,&#34;Ġì§ĳì¤ĳìĿ´&#34;:6511,&#34;íķĺìŀĲëĬĶ&#34;:6512,&#34;ë¿Į&#34;:6513,&#34;50&#34;:6514,&#34;Ġ007&#34;:6515,&#34;ìĿ´ìĹ°ê±¸&#34;:6516,&#34;íķĺëŁ¬&#34;:6517,&#34;ê°ĢìĹĲ&#34;:6518,&#34;ĠìĹ½&#34;:6519,&#34;Ġë§¹&#34;:6520,&#34;ìĸ´ëĤľ&#34;:6521,&#34;ìķĦì¹ĺ&#34;:6522,&#34;ĠìķĦìĹŃ&#34;:6523,&#34;ĮĢìĿĺ&#34;:6524,&#34;ìĭľê±¸&#34;:6525,&#34;ìļ°ìļ¸&#34;:6526,&#34;ìĪĺë¡ľ&#34;:6527,&#34;ìĪĺìŀĳ&#34;:6528,&#34;ìĥģìĪĺ&#34;:6529,&#34;ãħłãħľ&#34;:6530,&#34;Ġìĸ´ëĳ&#34;:6531,&#34;Ġë´¤ëĦ¤&#34;:6532,&#34;ë¶Ģë¥¼&#34;:6533,&#34;ĠìĪĺìŀħ&#34;:6534,&#34;ë¶Ħíķľ&#34;:6535,&#34;ìĦ¸íı¬&#34;:6536,&#34;ê°ĲìĤ¬&#34;:6537,&#34;Ġê°Ļê³ł&#34;:6538,&#34;Ġìµľê³łìĺĢëĭ¤&#34;:6539,&#34;íİ¸ì§ĳ&#34;:6540,&#34;ìĺĢëĦ¤&#34;:6541,&#34;ìĺĢìľ¼ë©´&#34;:6542,&#34;Ġë³¼ê±°ë¦¬&#34;:6543,&#34;ìłĢëŁ°&#34;:6544,&#34;ìĦłìĿĦ&#34;:6545,&#34;Ġìĭ¶ìĸ´ìļĶ&#34;:6546,&#34;íĺĦìŀ¬&#34;:6547,&#34;íıīìłĲìĿĢ&#34;:6548,&#34;ê°ĻìĿĢìĺģíĻĶ&#34;:6549,&#34;ĠìķĦëĭĪì§Ģ&#34;:6550,&#34;ì¦ĪìĿĺ&#34;:6551,&#34;Ġë§ĮëĵłìĺģíĻĶ&#34;:6552,&#34;ĠìĿ´ìĥģíķĺê²Į&#34;:6553,&#34;ë¹łì§Ħ&#34;:6554,&#34;ìŁģìĿ´&#34;:6555,&#34;Ġëĭ´ê²¨&#34;:6556,&#34;Ġë²łëĵľ&#34;:6557,&#34;Ġê°ĳëĭĪëĭ¤&#34;:6558,&#34;Ġìĸĳìĭ¬&#34;:6559,&#34;ëĬĲëĤĮìĿ´&#34;:6560,&#34;íĸĩëĭ¤&#34;:6561,&#34;Ġìĸ´ëĬĲìłķëıĦ&#34;:6562,&#34;Ġì²«ìĤ¬ëŀĳ&#34;:6563,&#34;Ġì°©ê°ģ&#34;:6564,&#34;Ġìĭľì²Ńë¥łìĿ´&#34;:6565,&#34;Ġë¿Į&#34;:6566,&#34;-^&#34;:6567,&#34;°Ķ&#34;:6568,&#34;ìĮ&#34;:6569,&#34;ãħĵ&#34;:6570,&#34;ìĦ¬&#34;:6571,&#34;ëıĦìķĦëĭĪê³ł&#34;:6572,&#34;ĠìĺģíĻĶìĻĢ&#34;:6573,&#34;ĠìķĦê¸°&#34;:6574,&#34;ê¹ľ&#34;:6575,&#34;ë§Īì¹ĺ&#34;:6576,&#34;Ġíķĺëĭ¤ê°Ģ&#34;:6577,&#34;ìŀĪìĿĦ&#34;:6578,&#34;ì°¡&#34;:6579,&#34;ìĦ±ê¸°&#34;:6580,&#34;Ġìŀ¬ë°Įëĭ¤ëĬĶ&#34;:6581,&#34;Ġê°ĲëıĻìĿĢ&#34;:6582,&#34;Ġê°ĲëıĻìłģìĿ´ëĭ¤&#34;:6583,&#34;ë°ķíķľ&#34;:6584,&#34;ëĤ¨ì£¼&#34;:6585,&#34;íĹĲë¦¬&#34;:6586,&#34;ĠìĺĪëĬ¥&#34;:6587,&#34;ëĬĲëģ¼&#34;:6588,&#34;ĠëĳĲë²Īì§¸&#34;:6589,&#34;ĠíĻĶëģĪ&#34;:6590,&#34;ĠìłĲìĪĺê°Ģ&#34;:6591,&#34;ĠëĤ®ìķĦ&#34;:6592,&#34;Ġë§¤ëł¥ìłģìĿ´&#34;:6593,&#34;ìĬ¤ëŁ½ì§Ģ&#34;:6594,&#34;ìŀ¬ë¯¸ìŀĪìĿĮ&#34;:6595,&#34;ĠìķĦëĭĪëĿ¼ê³ł&#34;:6596,&#34;Ġìŀ¬ëĬ¥&#34;:6597,&#34;Ġë³´ì§Ģë§Ĳ&#34;:6598,&#34;ì´ĪìĹĲ&#34;:6599,&#34;ĠìĨĲìĥī&#34;:6600,&#34;ìĹ¬ìŀĲê°Ģ&#34;:6601,&#34;ìĬ¬íĶĦ&#34;:6602,&#34;Ġìĸ´ìĦ¤íĶĦê³ł&#34;:6603,&#34;ëĿ¼ìĿ´ìĸ¸&#34;:6604,&#34;Ġê´Ģê°ĿìĿĦ&#34;:6605,&#34;Ġê¶ģê¸Īíķĺëĭ¤&#34;:6606,&#34;ë°ĺìłĦìĿ´&#34;:6607,&#34;ĠìĹīìĦ±íķĺê³ł&#34;:6608,&#34;Ġsf&#34;:6609,&#34;Ġì½ľ&#34;:6610,&#34;Ġìĭ¤ìłľë¡ľ&#34;:6611,&#34;ĠíĹ¤ìĸ´&#34;:6612,&#34;ìĻłì§Ģ&#34;:6613,&#34;ĠìĹŃëĮĢê¸ī&#34;:6614,&#34;?...&#34;:6615,&#34;es&#34;:6616,&#34;±ìłķ&#34;:6617,&#34;ĠE&#34;:6618,&#34;ì§ĢíĺĦ&#34;:6619,&#34;ìľĪ&#34;:6620,&#34;ìĬ¤ìĹĲ&#34;:6621,&#34;Ġìĺ¥&#34;:6622,&#34;ĠìĺĽ&#34;:6623,&#34;ìĹ¬ì§Ħ&#34;:6624,&#34;ìĿ¼ìĿĦ&#34;:6625,&#34;ìĿ¼ëĵ¯&#34;:6626,&#34;íķ¨ëıĦ&#34;:6627,&#34;ĠìķĬìķĦìĦľ&#34;:6628,&#34;Ġë¬´íķľ&#34;:6629,&#34;ĠëĤ´ìļ©ê³¼&#34;:6630,&#34;ì¡°ìĹ°&#34;:6631,&#34;ĠìľĦëĮĢ&#34;:6632,&#34;ë°©ê¸Ī&#34;:6633,&#34;ĠìĹĲíľ´&#34;:6634,&#34;Ġë³´ê¸°ìĹĲ&#34;:6635,&#34;ĠëĨĴìķĦìĦľ&#34;:6636,&#34;ì£¼ìĿ¸ê³µìĿĺ&#34;:6637,&#34;ìĸ¼ë§ĪëĤĺ&#34;:6638,&#34;Ġëªĩëªĩ&#34;:6639,&#34;ìĸ´ìķ¼ì§Ģ&#34;:6640,&#34;Ġì¦Ĳê²¨&#34;:6641,&#34;ĠêµĲê³¼ìĦľ&#34;:6642,&#34;Ġëª©ìĪ¨&#34;:6643,&#34;Ġíĺķëĭĺ&#34;:6644,&#34;ĠìĽĥê¸°ëĭ¤&#34;:6645,&#34;Ġë¬´ìĦŃì§ĢëıĦ&#34;:6646,&#34;ĠíĻķìĿ¸&#34;:6647,&#34;Ġì°¾ìķĦë³¼&#34;:6648,&#34;Ġì§ĳì¤ĳíķ´ìĦľ&#34;:6649,&#34;Ġë¸Ķë¡Ŀë²ĦìĬ¤íĦ°&#34;:6650,&#34;ëĨĴìĿĢ&#34;:6651,&#34;ĠìĺģíĻĺëį°&#34;:6652,&#34;Ġëĭ¤ìĸĳíķľ&#34;:6653,&#34;²Ī&#34;:6654,&#34;íĩ´&#34;:6655,&#34;ëĭ¬ë&#34;:6656,&#34;ìłĬ&#34;:6657,&#34;ëıĮìĿ´&#34;:6658,&#34;íķĺëįĶëĿ¼&#34;:6659,&#34;ĠëĤ¬&#34;:6660,&#34;Ġë³´ìķĺëĬĶëį°&#34;:6661,&#34;ë©´ìĹĲìĦľ&#34;:6662,&#34;ë°į&#34;:6663,&#34;ìłĦìĦ¤&#34;:6664,&#34;ĠìŀĪëĭ¤ê³ł&#34;:6665,&#34;Ġì§Ģê¸Īë³´&#34;:6666,&#34;Ġíķľëĭ¤ëĬĶ&#34;:6667,&#34;ì¹ĺê³¤&#34;:6668,&#34;Ġì£¼ë§Ĳ&#34;:6669,&#34;ê°ĻìķĦ&#34;:6670,&#34;Ġë§Ĳìķĺ&#34;:6671,&#34;Ġë³¼ëķĮ&#34;:6672,&#34;Ġì¡°ìļ©&#34;:6673,&#34;ëĨį&#34;:6674,&#34;íķĺì§Ģë§ĪëĿ¼&#34;:6675,&#34;Ġê²°ì½Ķ&#34;:6676,&#34;ĠëĤĺìĺ¤ëĦ¤&#34;:6677,&#34;ĠìķĦê¹Ŀê³ł&#34;:6678,&#34;ì¼°&#34;:6679,&#34;Ġì§ľì§ĳ&#34;:6680,&#34;ĠëĳĲëł¤&#34;:6681,&#34;ĠìĿ´ìĥģíķĺê³ł&#34;:6682,&#34;ĠëĤ®ëĭ¤&#34;:6683,&#34;ĠìĦłëıĻ&#34;:6684,&#34;ĠìŀĶíĺ¹&#34;:6685,&#34;Ġê°ĢìĬ´ìĿĦ&#34;:6686,&#34;ì¢ĭìĿĢëį°&#34;:6687,&#34;ìŀĩëĬĶ&#34;:6688,&#34;Ġìĭ¸ìļ°ëĬĶ&#34;:6689,&#34;ìĹ¬ëŁ¬&#34;:6690,&#34;ëĤ¸ëĭ¤&#34;:6691,&#34;Ġìĸ´ìĦ¤íĶĦ&#34;:6692,&#34;ëĺĲíķľ&#34;:6693,&#34;Ġëª°ìŀħëıĦê°Ģ&#34;:6694,&#34;ĠìķĪëĲł&#34;:6695,&#34;ìķŀìľ¼ë¡ľ&#34;:6696,&#34;ë¹¼ê³ł&#34;:6697,&#34;ĠìĬ¤íĬ¸ëłĪìĬ¤&#34;:6698,&#34;ĠìĿ¼ê¹¨ìĽĮ&#34;:6699,&#34;ìĿ´ìłł&#34;:6700,&#34;ë§Īë¬´&#34;:6701,&#34;ê³łìĥĿ&#34;:6702,&#34;ĠíĮĢ&#34;:6703,&#34;ë¡¤&#34;:6704,&#34;ìĺ¬ëĿ¼&#34;:6705,&#34;ë§Īê°Ģ&#34;:6706,&#34;ìĹĪêµ¬ëĤĺ&#34;:6707,&#34;Ġëĭ¤ìĦ¯&#34;:6708,&#34;ìĪĺíĺĦ&#34;:6709,&#34;ìŀ¥ìĿĺ&#34;:6710,&#34;ëĵľë¦¬&#34;:6711,&#34;Ġ11&#34;:6712,&#34;Ġìŀ¬ë°Įìĸ´ìĦľ&#34;:6713,&#34;ê°ĻìķĦìļĶ&#34;:6714,&#34;ìŀ¬ë¯¸ìĹĨê³ł&#34;:6715,&#34;ëıĻíĻĶ&#34;:6716,&#34;ĠëŃĲíķĺëĤĺ&#34;:6717,&#34;ìĨįìľ¼ë¡ľ&#34;:6718,&#34;ë§Īë¥¼&#34;:6719,&#34;ìĹ°ê¸°ë¥¼&#34;:6720,&#34;Ġëª°ëŀĲëĭ¤&#34;:6721,&#34;ĠìĿ´íķ´ë¥¼&#34;:6722,&#34;ĠëĺĲëĭ¤ë¥¸&#34;:6723,&#34;Ġê´Ģëł¨&#34;:6724,&#34;ĠìľĦíĹĺ&#34;:6725,&#34;Ġì§ľì¦Ŀë§Į&#34;:6726,&#34;ì£½ëĬĶ&#34;:6727,&#34;Ġìĸµì§ĢìĬ¤ëŁ¬ìļ´&#34;:6728,&#34;ĠëĤĺìĻĶëĭ¤&#34;:6729,&#34;ìŀ¬ë°ĭê²Į&#34;:6730,&#34;ìŀ¬ë°ĭìĸ´ìļĶ&#34;:6731,&#34;Ġìĸ´ì©ľ&#34;:6732,&#34;ĠíĮĲëĭ¨&#34;:6733,&#34;ĠëıĮìķĦê°Ģ&#34;:6734,&#34;ëªĩë²ĪìĿĦ&#34;:6735,&#34;Ġë§ĲëıĦìķĪëĲĺëĬĶ&#34;:6736,&#34;ëİħ&#34;:6737,&#34;Ġë°Ģëł¤&#34;:6738,&#34;ë©ĶìĿ´ëĵľ&#34;:6739,&#34;ĠìĹ°ìĺĪìĿ¸&#34;:6740,&#34;Ġìĸ´ìļ°ëŁ¬&#34;:6741,&#34;est&#34;:6742,&#34;íķĺê±°ëĤĺ&#34;:6743,&#34;íķľìĭ¬&#34;:6744,&#34;íķľê°ľëıĦ&#34;:6745,&#34;ĠìĺģíĻĶë§Į&#34;:6746,&#34;ĠìĹ¬ë¦Ħ&#34;:6747,&#34;ìĿ¸ê±´&#34;:6748,&#34;ĠìķĦëĤĺ&#34;:6749,&#34;ĠìķĦëĤ´&#34;:6750,&#34;ìĭľê°Ģ&#34;:6751,&#34;ĠìłķíĻķ&#34;:6752,&#34;ê·¸ëĭ¥&#34;:6753,&#34;ĠìĭľëģĦ&#34;:6754,&#34;ìĿ¼ìĿĺ&#34;:6755,&#34;ìĤ¬ìĻĢ&#34;:6756,&#34;ìĭłê³ł&#34;:6757,&#34;ĠìŀĺíķĺëĬĶ&#34;:6758,&#34;íĬľ&#34;:6759,&#34;Ġë¶Ģë¥´&#34;:6760,&#34;Ġì¡°ìĦł&#34;:6761,&#34;ìķłëĵ¤ìĿĢ&#34;:6762,&#34;ĠëªħíĻĶ&#34;:6763,&#34;Ġìŀ¬ë¯¸ìĹĨìĸ´ìĦľ&#34;:6764,&#34;ê²½ìĿ´&#34;:6765,&#34;íĮĲìĿ´&#34;:6766,&#34;ĠíĸĪìĿĮ&#34;:6767,&#34;ìĤ´ìķĦ&#34;:6768,&#34;Ġë³Ħë¡ľìŀĦ&#34;:6769,&#34;ì°¸ê³ł&#34;:6770,&#34;Ġëª¨ëĵłê±¸&#34;:6771,&#34;ì¢ħêµĲ&#34;:6772,&#34;ì²ĺìĿĮìľ¼ë¡ľ&#34;:6773,&#34;íĮ¨ìĬ¤&#34;:6774,&#34;Ġìŀ¬ë°ĭëĭ¤&#34;:6775,&#34;Ġëħ¸ëŀĺê°Ģ&#34;:6776,&#34;Ġìŀĳê°Ģëĭĺ&#34;:6777,&#34;Ġì©Ŀ&#34;:6778,&#34;Ġë¹µë¹µ&#34;:6779,&#34;ĠìĺĪìģĺëĭ¤&#34;:6780,&#34;íŀĺëĵ¤&#34;:6781,&#34;Ġíķľìĭ¬íķľ&#34;:6782,&#34;ª¨ë¡ľ&#34;:6783,&#34;ĠíĽĪíĽĪíķľ&#34;:6784,&#34;Ġëıħë¦½ìĺģíĻĶ&#34;:6785,&#34;ì¨Į&#34;:6786,&#34;Ġê·¸ë¦½ëĭ¤&#34;:6787,&#34;ĠìĹĲíĶ¼ìĨĮëĵľ&#34;:6788,&#34;Ġe&#34;:6789,&#34;ìĿ´ê²ĥ&#34;:6790,&#34;ë©¸&#34;:6791,&#34;Ġê·¸ëŀ&#34;:6792,&#34;ìľ¼ëł¤ê³ł&#34;:6793,&#34;ìŀĲê¾¸&#34;:6794,&#34;ĠìĹĨê²Į&#34;:6795,&#34;ĠìĹĨìĹĪìĿĮ&#34;:6796,&#34;ĠëĮĢëĨĵê³ł&#34;:6797,&#34;êµ¬ê°Ģ&#34;:6798,&#34;Ġë¬´ë£Į&#34;:6799,&#34;ĠëĬĶ&#34;:6800,&#34;ĠíĺĲ&#34;:6801,&#34;Ġìĺ¤ë¹ł&#34;:6802,&#34;ĠìĤ¬ëŀĮëıĦ&#34;:6803,&#34;ĠëĲĺëĦ¤ìļĶ&#34;:6804,&#34;Ġìĭ¶ì§Ģ&#34;:6805,&#34;ì¤ĦìĿĢ&#34;:6806,&#34;Ġê²°ìłķ&#34;:6807,&#34;ë¶ĢíĦ°ê°Ģ&#34;:6808,&#34;ìĭľê°ĦìĿĦ&#34;:6809,&#34;ĠìĹ°ì¶ľìĿĢ&#34;:6810,&#34;Ġì§ľìŀĦìĥĪ&#34;:6811,&#34;Ġìŀ¥ë©´ìĿĦ&#34;:6812,&#34;Ġê´ľì°®ìķĺëĭ¤&#34;:6813,&#34;ìĻĶëĬĶëį°&#34;:6814,&#34;Ġë§ĪìĿĮìľ¼ë¡ľ&#34;:6815,&#34;ëĤ´ê°Ģë³¸&#34;:6816,&#34;Ġì§ľì¦ĿëĤ¨&#34;:6817,&#34;íĪ°&#34;:6818,&#34;Ġë´¤ëĭ¤ë©´&#34;:6819,&#34;Ġëĸ¨ìĸ´ì§Ĳ&#34;:6820,&#34;ìµľê³łìĿĺìĺģíĻĶ&#34;:6821,&#34;ĠíĥĢê³ł&#34;:6822,&#34;ìĿĮìķħìĿ´&#34;:6823,&#34;íģ¬ëł&#34;:6824,&#34;ĠëķľìĹĲ&#34;:6825,&#34;Ġëĭ¤ë¥´ê²Į&#34;:6826,&#34;Ġë¹µíĦ°&#34;:6827,&#34;Ġë§ĪìĿ´íģ´&#34;:6828,&#34;Ġëģ¼ìĽĮ&#34;:6829,&#34;ìį¼&#34;:6830,&#34;ĠP&#34;:6831,&#34;ëĭ·&#34;:6832,&#34;íķĢ&#34;:6833,&#34;ê°ĪëķĮ&#34;:6834,&#34;ìĹĲìĿ´&#34;:6835,&#34;ĠìĺģíĻĶìĿ¸ì§Ģ&#34;:6836,&#34;ĠìĿ´ìłķ&#34;:6837,&#34;ìĸ´ìļ©&#34;:6838,&#34;ëĿ¼ëħ¸&#34;:6839,&#34;ëł´&#34;:6840,&#34;ĠìķĦëĥĲ&#34;:6841,&#34;Ġìłķìŀĳ&#34;:6842,&#34;ë¶ĢìĿĺ&#34;:6843,&#34;ĠìĥĿê°ģëıĦ&#34;:6844,&#34;Ġêµ´&#34;:6845,&#34;ëĵ¤ìĿ´ëĤĺ&#34;:6846,&#34;ĠìŀĲëŀĳ&#34;:6847,&#34;ĠìķĮê²łëĭ¤&#34;:6848,&#34;ìĺģìĽħ&#34;:6849,&#34;Ġë°°ìļ°ëıĦ&#34;:6850,&#34;ì§ģíķľ&#34;:6851,&#34;Ġê°ĲíŀĪ&#34;:6852,&#34;ìŀ¬ë°ĮëĬĶ&#34;:6853,&#34;Ġìŀ¬ë¯¸ìĹĨìĸ´ìļĶ&#34;:6854,&#34;ë¯¼ìĿ´&#34;:6855,&#34;ê·¹ìĿĦ&#34;:6856,&#34;ë³¼ëķĮë§Īëĭ¤&#34;:6857,&#34;ìĽĥê²¨&#34;:6858,&#34;Ġë§ŀì¶°&#34;:6859,&#34;íĹĪíĹĪ&#34;:6860,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:6861,&#34;ĠìķĦê¹Įìļ´ìĺģíĻĶ&#34;:6862,&#34;Ġì¶ľìĹ°íķľ&#34;:6863,&#34;ĠìĿ½ê³ł&#34;:6864,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:6865,&#34;ëĪĪë¬¼ìĿ´&#34;:6866,&#34;ìĭ«ëĭ¤&#34;:6867,&#34;ê¸°ìĸµìĹĲ&#34;:6868,&#34;ìº¬&#34;:6869,&#34;ê·¸ëŁ¬ëĤĺ&#34;:6870,&#34;ĠìĿ´ëĶ°ìľĦ&#34;:6871,&#34;ĠíĶĮë¡¯&#34;:6872,&#34;ë¡ľë§¨ìĬ¤&#34;:6873,&#34;ĠìķĦë¥ĺìŀĳ&#34;:6874,&#34;ì·&#34;:6875,&#34;ëĭĪëĿ¼&#34;:6876,&#34;ìĿ´ìĹ¬&#34;:6877,&#34;ë§Įíķĺëĭ¤&#34;:6878,&#34;ë¦¬ìĻĢ&#34;:6879,&#34;Ġìłł&#34;:6880,&#34;ë©ľ&#34;:6881,&#34;íķ´ìĦł&#34;:6882,&#34;Ġë³´íĨµ&#34;:6883,&#34;Ġëª¬&#34;:6884,&#34;ìľ¼ìĭł&#34;:6885,&#34;ĠëĦĮ&#34;:6886,&#34;ì¹ĺì§Ģ&#34;:6887,&#34;ë¶Ģëª¨&#34;:6888,&#34;Ġë³¸ê±´ëį°&#34;:6889,&#34;ĠëĤ¨ê¸°&#34;:6890,&#34;Ġê²ĥìĹĲ&#34;:6891,&#34;ìĹĪëĭ¤ê³ł&#34;:6892,&#34;Ġë¹ħ&#34;:6893,&#34;ìŀ¬ë¯¸ìŀĪëĬĶ&#34;:6894,&#34;ĠìŀĳìľĦ&#34;:6895,&#34;Ġë¶Ģíĥģ&#34;:6896,&#34;ĠìķĪë´&#34;:6897,&#34;ĠíĹĮ&#34;:6898,&#34;Ġê³łë§Ļ&#34;:6899,&#34;Ġë§¤ìĿ¼&#34;:6900,&#34;ìĭĿìĿĢ&#34;:6901,&#34;Ġìķ¡ìħĺìĿĦ&#34;:6902,&#34;Ġì¶Ķê²©&#34;:6903,&#34;ì°¨ë³Ħ&#34;:6904,&#34;Ġê´ľì°®ìĿĮ&#34;:6905,&#34;íĺķìłľ&#34;:6906,&#34;ìŀĺë´¤ìĬµëĭĪëĭ¤&#34;:6907,&#34;ìĹĩìĿĮ&#34;:6908,&#34;ìķĪëĲľ&#34;:6909,&#34;ìĥĿê°ģìĿĦ&#34;:6910,&#34;ĠìķĦìĿ´ê°Ģ&#34;:6911,&#34;Ġë°ĽëĬĶ&#34;:6912,&#34;Ġë³µìŀ¡&#34;:6913,&#34;ĠìĺģíĻĶëĿ¼ëĭĪ&#34;:6914,&#34;ìĭľëĮĢë¥¼&#34;:6915,&#34;ë³´ì§Ģë§ĪìĦ¸ìļĶ&#34;:6916,&#34;íķĺì§ĢìķĬê³ł&#34;:6917,&#34;ëĶ°ëľ»íķľ&#34;:6918,&#34;ĠìľłìĿ¼íķľ&#34;:6919,&#34;Ġê²¬ìŀĲëĭ¨&#34;:6920,&#34;ìĩĦ&#34;:6921,&#34;Ġn&#34;:6922,&#34;ħĢ&#34;:6923,&#34;ëį¸&#34;:6924,&#34;ë¡ľì§Ģ&#34;:6925,&#34;...^^&#34;:6926,&#34;ìĿ¸ì§ĢëıĦ&#34;:6927,&#34;Ġëĭ¬ëł¤&#34;:6928,&#34;ìĽ¬&#34;:6929,&#34;ë§ĪìŀĲ&#34;:6930,&#34;ĠìĹĨëĤĺìļĶ&#34;:6931,&#34;ì£¼ê¸°&#34;:6932,&#34;ìĥģìĿĢ&#34;:6933,&#34;ĠìķĪê°Ħëĭ¤&#34;:6934,&#34;ê¹Įì§ĢëıĦ&#34;:6935,&#34;Ġë¯¸ìĬ¤íĦ°&#34;:6936,&#34;ĠìĤ¬ëŀĮëĵ¤ìĹĲê²Į&#34;:6937,&#34;ì¢ĭê²Į&#34;:6938,&#34;Ġê³µê°Ħ&#34;:6939,&#34;ĠëĤ´ìļ©ìĿ¸ì§Ģ&#34;:6940,&#34;ì¤Ħê±°ë¦¬&#34;:6941,&#34;ĠìĤ´ë©´ìĦľ&#34;:6942,&#34;Ġìŀ¼ìŀĪëĭ¤&#34;:6943,&#34;ì½§&#34;:6944,&#34;ĠìĦľë¶Ģ&#34;:6945,&#34;ìŀ¡ìĿĦ&#34;:6946,&#34;ëĬĶê±°ì§Ģ&#34;:6947,&#34;Ġê·¹ìŀ¥ê°ĢìĦľ&#34;:6948,&#34;ìķ¡ìħĺìĿĢ&#34;:6949,&#34;Ġëĭ´ë°°&#34;:6950,&#34;ĠìķĦìī¬ìĽĢìĿ´&#34;:6951,&#34;ĠìĺģíĻĶìĺĢìĿĮ&#34;:6952,&#34;ĠëĤļìĭľ&#34;:6953,&#34;Ġê¿Īê¾¸&#34;:6954,&#34;ëª¨ë¥´ê³ł&#34;:6955,&#34;Ġë¹ĦëĶĶìĺ¤ë¡ľ&#34;:6956,&#34;ĠìķķëıĦ&#34;:6957,&#34;ì¤¬ìľ¼ë©´&#34;:6958,&#34;ìĻłë§Įíķĺë©´&#34;:6959,&#34;Ġì¤ĺìķ¼&#34;:6960,&#34;Ġëĭ¤ìļ´ë°ĽìķĦ&#34;:6961,&#34;ĠíķĺëĤĺíķĺëĤĺê°Ģ&#34;:6962,&#34;Ġë¥ĺìĬ¹&#34;:6963,&#34;ĠìĬ¬íİĲ&#34;:6964,&#34;ht&#34;:6965,&#34;¡íĮ¨&#34;:6966,&#34;ĠL&#34;:6967,&#34;ëĭ¼&#34;:6968,&#34;ìĸ´ìĿĺ&#34;:6969,&#34;ë¡ľìį¨&#34;:6970,&#34;ëĿ¼ëĿ¼&#34;:6971,&#34;ë¦¬ëĤĺ&#34;:6972,&#34;Ġë³´ëĤ´&#34;:6973,&#34;ìłķê³¼&#34;:6974,&#34;ìłĦìĿĦ&#34;:6975,&#34;ê·¸ëķĮ&#34;:6976,&#34;ìĬµëĭĪê¹Į&#34;:6977,&#34;ëŀĺëıĦ&#34;:6978,&#34;Ġìŀ¬ë°ĮìĿĦ&#34;:6979,&#34;Ġì¤įëĭĪëĭ¤&#34;:6980,&#34;ë¶ĦìĿĦ&#34;:6981,&#34;ìĦ¸ìĹ¬&#34;:6982,&#34;ê°Ĳìłķ&#34;:6983,&#34;Ġë³¸ë°©ìĤ¬ìĪĺ&#34;:6984,&#34;ĠëģĿëĤ¬&#34;:6985,&#34;ë§Ŀìŀĳ&#34;:6986,&#34;Ġê±°ë¶ģ&#34;:6987,&#34;ì§ĪìĿ´&#34;:6988,&#34;Ġìĭľê°ĦìĹĲ&#34;:6989,&#34;Ġëĸ¼&#34;:6990,&#34;Ġê·¹ë³µ&#34;:6991,&#34;Ġìļ¸ë¦¬ëĬĶ&#34;:6992,&#34;Ġëĭ¨íİ¸&#34;:6993,&#34;Ġì§ľì¦ĿëĤĺê³ł&#34;:6994,&#34;íĿ¬ê°Ģ&#34;:6995,&#34;ìķ¡ìħĺìĿ´&#34;:6996,&#34;ĠëłĪìķĮ&#34;:6997,&#34;Ġê·ĢìĹ¬ìĽĮ&#34;:6998,&#34;Ġìľłì¾Įíķĺê³ł&#34;:6999,&#34;Ġë¶ĪìĮįíķĺëĭ¤&#34;:7000,&#34;Ġë§ĽìĿ´&#34;:7001,&#34;Ġof&#34;:7002,&#34;Ġìĺ¬íķ´&#34;:7003,&#34;Ġë§Ĳíķĺê³łìŀĲ&#34;:7004,&#34;ĠëĨĢëŀįëĭ¤&#34;:7005,&#34;ĠìĿ´ë»ĲìĦľ&#34;:7006,&#34;Ġ&gt;&#34;:7007,&#34;ìĿ´ìļĶ&#34;:7008,&#34;ëĤĢ&#34;:7009,&#34;ë¦ĩ&#34;:7010,&#34;ìĥĺ&#34;:7011,&#34;ë³´ëĤ´&#34;:7012,&#34;ìļ°ìĿĺ&#34;:7013,&#34;ìłĦìĿ´&#34;:7014,&#34;Ġìłķì²´&#34;:7015,&#34;ĠìĬ¤íİĺ&#34;:7016,&#34;ĠìĽ°&#34;:7017,&#34;ĠìŀĲìĦ¸&#34;:7018,&#34;ĠëĤ¨íİ¸&#34;:7019,&#34;Ġìĺ¤ë¸Į&#34;:7020,&#34;ë²Ħì§Ģê°Ģ&#34;:7021,&#34;Ġë¶Ģìĭ¤&#34;:7022,&#34;Ġ21&#34;:7023,&#34;Ġê°Ĳê°ģ&#34;:7024,&#34;Ġë°ĺëĵľìĭľ&#34;:7025,&#34;ë°Ķë³´&#34;:7026,&#34;ĠìŀĳíĴĪìŀħëĭĪëĭ¤&#34;:7027,&#34;ĠëķĮë§Īëĭ¤&#34;:7028,&#34;ì²ľìŀ¬&#34;:7029,&#34;íĸĪëĭ¤ë©´&#34;:7030,&#34;ìĭľê°ĦëĤŃë¹Ħ&#34;:7031,&#34;Ġë°°ìĭł&#34;:7032,&#34;ì§ĳìĹĲ&#34;:7033,&#34;ê°ĲëıĻìłģìĿ´ê³ł&#34;:7034,&#34;ĠìłķëıĦë©´&#34;:7035,&#34;ì¡ĮìĿĮ&#34;:7036,&#34;Ġë°Ľê³ł&#34;:7037,&#34;Ġìĺģìĥģê³¼&#34;:7038,&#34;Ġë¨¸ë¦¬ê°Ģ&#34;:7039,&#34;ĠìĨĮìŀ¬ëĬĶ&#34;:7040,&#34;Ġëª¨ë¥´ê²łëĦ¤ìļĶ&#34;:7041,&#34;ì¿¡&#34;:7042,&#34;ìľ¤ë°ľ&#34;:7043,&#34;ĠíĿĲë¥´ëĬĶ&#34;:7044,&#34;ĠëģĮê³ł&#34;:7045,&#34;ĠìĿ´ë¦ĦìĿĦ&#34;:7046,&#34;Ġì±ħìŀĦ&#34;:7047,&#34;ĠíĥĪì¶ľ&#34;:7048,&#34;íķŃìĥģ&#34;:7049,&#34;ìĿ´ìĹĲ&#34;:7050,&#34;íķĺê·ł&#34;:7051,&#34;ëĤ©&#34;:7052,&#34;ëĵŃ&#34;:7053,&#34;ìķĦë§Ī&#34;:7054,&#34;ìĹĨìĿĦ&#34;:7055,&#34;ĠëĤĺëĦ¤ìļĶ&#34;:7056,&#34;Ġëĭ¤ìĿ´&#34;:7057,&#34;ìŀ¥ìĭ¤&#34;:7058,&#34;ĠëĵŃëĭĪëĭ¤&#34;:7059,&#34;Ġì§Ģê²¹&#34;:7060,&#34;íķłë§Įíķľ&#34;:7061,&#34;Ġëį¤&#34;:7062,&#34;ìĤ¬ìĹĲ&#34;:7063,&#34;Ġìŀĺë´¤ìĸ´ìļĶ&#34;:7064,&#34;íĬĢ&#34;:7065,&#34;ĠëĬ¦&#34;:7066,&#34;ĠìĿ¸íĺķ&#34;:7067,&#34;Ġê°ľëħĲ&#34;:7068,&#34;ì¤Ģíĺ¸&#34;:7069,&#34;ë³´ëĭ¤ëıĦ&#34;:7070,&#34;ëįĶëįĶ&#34;:7071,&#34;Ġìļ°ë¦¬ëĬĶ&#34;:7072,&#34;ĠìĽĥìľ¼ë©´ìĦľ&#34;:7073,&#34;Ġìĭ¤ëł¥&#34;:7074,&#34;ĠëıĻìĦ±ìķł&#34;:7075,&#34;ëª»íķ´&#34;:7076,&#34;Ġë§īíĮĲìĹĲ&#34;:7077,&#34;ĠìĦ¤ìłķìĿ´&#34;:7078,&#34;Ġë§Ŀíķľ&#34;:7079,&#34;Ġë§ŀì§Ģ&#34;:7080,&#34;ĠìĦłë¬¼&#34;:7081,&#34;Ġì§ĳìĸ´&#34;:7082,&#34;ê¸°ëıĦíķĺê³ł&#34;:7083,&#34;ĠìĹ´ìĹ°&#34;:7084,&#34;ê¸°ëĮĢíķĺê³ł&#34;:7085,&#34;Ġëĭ´ìĿĢ&#34;:7086,&#34;Ġì´Īëĵ±íķĻìĥĿ&#34;:7087,&#34;ê´ĳê³ł&#34;:7088,&#34;ĠëĤļìĺĢëĭ¤&#34;:7089,&#34;Ġìĺ¤ëŀĺëĲľ&#34;:7090,&#34;Ġëĭ¤ë¥´ëĭ¤&#34;:7091,&#34;Ġëĭ¹ìĭłìĿĢ&#34;:7092,&#34;Ġì§§ìĿĢ&#34;:7093,&#34;ĠìŀĬíĺĢì§Ģ&#34;:7094,&#34;Ġê°ĢëĵĿíķľ&#34;:7095,&#34;ĠëģĶì°į&#34;:7096,&#34;ĠëĿĦ&#34;:7097,&#34;ĠF&#34;:7098,&#34;ĠN&#34;:7099,&#34;..!&#34;:7100,&#34;¬ëĦ¤&#34;:7101,&#34;ĠìĿ´ë¯¼&#34;:7102,&#34;ìķĦì¹¨&#34;:7103,&#34;Ġê·¸ëĵ¤&#34;:7104,&#34;ìłĲë¶ĢíĦ°&#34;:7105,&#34;Ġë°ĸ&#34;:7106,&#34;ĠìĹĨìľ¼ë©´&#34;:7107,&#34;Ġì¢ĭëĭ¤ê³ł&#34;:7108,&#34;ì£¼ë©´&#34;:7109,&#34;ìŀĪì§Ģë§Į&#34;:7110,&#34;ĠëĦĪë¬´ëıĦ&#34;:7111,&#34;ĠëĮĢìĭł&#34;:7112,&#34;Ġìŀ¬ë°Įëįĺëį°&#34;:7113,&#34;ĠìķĪëĤĺ&#34;:7114,&#34;Ġìµľê³łë´ī&#34;:7115,&#34;ĠëģĪ&#34;:7116,&#34;Ġë³¸ëĵ¯&#34;:7117,&#34;Ġê°ĲëıĻë°Ľ&#34;:7118,&#34;Ġê¹į&#34;:7119,&#34;ĠíĿ¬ë&#34;:7120,&#34;Ġíĸĩ&#34;:7121,&#34;Ġì¤ĳëıħ&#34;:7122,&#34;ĠìķłêµŃ&#34;:7123,&#34;ë¬¼ë¡ł&#34;:7124,&#34;ìŀ¬ë°ĮìĹĪìĸ´ìļĶ&#34;:7125,&#34;Ġìŀ¬ë¯¸ìĹĨëĭ¤ê³ł&#34;:7126,&#34;ãĦ²&#34;:7127,&#34;ëĤ¨ê¸°&#34;:7128,&#34;ìĦ¤ë§Ī&#34;:7129,&#34;ì¦Īê°Ģ&#34;:7130,&#34;ĠìķĦê¹ĿëĦ¤ìļĶ&#34;:7131,&#34;Ġë§Īì§Ģë§īê¹Įì§Ģ&#34;:7132,&#34;ĠìĦ¤ë§Ī&#34;:7133,&#34;ĠìłĦê°ľìĻĢ&#34;:7134,&#34;Ġë°ĺìłĦìĿĦ&#34;:7135,&#34;ê¶ĮìĿ´&#34;:7136,&#34;Ġì¦Ĳê¸¸&#34;:7137,&#34;ìĵ°ëĬĶ&#34;:7138,&#34;ìŀ¬ë°ĭìĿĮ&#34;:7139,&#34;Ġìŀłìĭľ&#34;:7140,&#34;ê²¬ìŀĲëĭ¨&#34;:7141,&#34;Ġì¼Ģë¦ŃíĦ°&#34;:7142,&#34;Ġìĸ¼êµ´ìĿ´&#34;:7143,&#34;Ġê½¤ëĤĺ&#34;:7144,&#34;ìķĦëĭĮê°Ģ&#34;:7145,&#34;ìĪĻíķľ&#34;:7146,&#34;Ġì½©&#34;:7147,&#34;Ġë¬»ìĸ´&#34;:7148,&#34;âĢ¦&#34;:7149,&#34;ĠëĨĢëĿ¼ìļ´&#34;:7150,&#34;ĠíĶ¼íķ´ìŀĲ&#34;:7151,&#34;ĠìĿ¼ë¶ĢëŁ¬&#34;:7152,&#34;ìĿ´ìłķëıĦë©´&#34;:7153,&#34;Ġëĭ¤íģĲë©ĺíĦ°ë¦¬&#34;:7154,&#34;Ġyou&#34;:7155,&#34;ĳëĮĢ&#34;:7156,&#34;ĠëĹĦ&#34;:7157,&#34;ìĿ´ì£ł&#34;:7158,&#34;ìŀĪëĥĲ&#34;:7159,&#34;ì§ĢìĹĲ&#34;:7160,&#34;ëĤ©ëĭĪëĭ¤&#34;:7161,&#34;¬ëĦ¤ìļĶ&#34;:7162,&#34;ëıĦë¡ľ&#34;:7163,&#34;ëłĽ&#34;:7164,&#34;ĠìĹĨëĭ¤ê³ł&#34;:7165,&#34;ĠëĤ´ê²Į&#34;:7166,&#34;ķĮë¬¸ìĹĲ&#34;:7167,&#34;ë¯¸ì¹ĺ&#34;:7168,&#34;íŀĪê³ł&#34;:7169,&#34;íĸĪìĿĦê¹Į&#34;:7170,&#34;ĠìĪĺë§İìĿĢ&#34;:7171,&#34;ë£¨íĨł&#34;:7172,&#34;Ġë¬´ê²Į&#34;:7173,&#34;ë¶Ħëĵ¤ìĿĢ&#34;:7174,&#34;ê°ľìĿ¸&#34;:7175,&#34;ĠìĿ¸ëĶĶ&#34;:7176,&#34;Ġìµľê³łë¡ľ&#34;:7177,&#34;Ġêµ¬ë¦¬&#34;:7178,&#34;ìłĢê¸°&#34;:7179,&#34;ìķĪëı¼&#34;:7180,&#34;Ġìķłë§¤&#34;:7181,&#34;ĠìłĢê¸ī&#34;:7182,&#34;ë°ĶëĿ¼&#34;:7183,&#34;ĠìĹŃìĤ¬ë¥¼&#34;:7184,&#34;Ġë°Ķíĥķ&#34;:7185,&#34;ĠëıĻìĸĳ&#34;:7186,&#34;ë³¼ìĪĺë¡Ŀ&#34;:7187,&#34;ë³´ëĬĶê²Į&#34;:7188,&#34;Ġê´ľì°®ê³ł&#34;:7189,&#34;Ġì¤Ħê±°&#34;:7190,&#34;Ġë§¤ëł¥ìĹĲ&#34;:7191,&#34;ĠìłķëıĦê°Ģ&#34;:7192,&#34;ĠìĥĿê¸´&#34;:7193,&#34;ĠìķĦë¬´ëŁ°&#34;:7194,&#34;ĠíĮĮìĽĮ&#34;:7195,&#34;Ġê³§&#34;:7196,&#34;ìµľìķħìĿ´ëĭ¤&#34;:7197,&#34;ìĬ¬íĶĦëĭ¤&#34;:7198,&#34;ĠìķĮë°Ķëĵ¤&#34;:7199,&#34;ĠìĤ¬ìĭ¤ìĿĦ&#34;:7200,&#34;ì±ħìĿĦ&#34;:7201,&#34;ĠìķĬìķĺëįĺ&#34;:7202,&#34;ë¡Ńê³ł&#34;:7203,&#34;ê±°ê°ĻìĿĮ&#34;:7204,&#34;ìĿ´íķ´ê°Ģ&#34;:7205,&#34;Ġë¹ĽìĿĦ&#34;:7206,&#34;ĠìłĦìĦ¤ìĿĺ&#34;:7207,&#34;ìĬĪíį¼&#34;:7208,&#34;ëĳ¥ìĿ´&#34;:7209,&#34;Ġìĸ´ëł¸ìĿĦëķĮ&#34;:7210,&#34;¬ë°ķ&#34;:7211,&#34;ĠíĬ¹ìĪĺíļ¨ê³¼&#34;:7212,&#34;ĠëĮĦ&#34;:7213,&#34;ow&#34;:7214,&#34;¨¹&#34;:7215,&#34;ĠX&#34;:7216,&#34;ľ¨&#34;:7217,&#34;ìĿ´ëĵł&#34;:7218,&#34;ê°Ģë³įê²Į&#34;:7219,&#34;ëıĦìĿĺ&#34;:7220,&#34;ê²ĮíķĺëĬĶ&#34;:7221,&#34;ëĭĪì½ľ&#34;:7222,&#34;ëį°ì²´&#34;:7223,&#34;ìĺµ&#34;:7224,&#34;ìĬ¤íĨ¤&#34;:7225,&#34;ê¹ĮìļĶ&#34;:7226,&#34;Ġíķĺìĭľ&#34;:7227,&#34;ìļ°ìĬ¤&#34;:7228,&#34;Ġìĺ¹&#34;:7229,&#34;íĥĲ&#34;:7230,&#34;ĠíķľìĪ¨&#34;:7231,&#34;íķ¨ê»ĺ&#34;:7232,&#34;ë¶Ħë§ĮìĹĲ&#34;:7233,&#34;ĠìķĪëĵ¤&#34;:7234,&#34;ê°ĻìķĦìĦľ&#34;:7235,&#34;ĠìĽ¨&#34;:7236,&#34;ìŀĦìĹĲëıĦ&#34;:7237,&#34;ĠìĥĿê°ģëĤľëĭ¤&#34;:7238,&#34;ìľłë¦¬&#34;:7239,&#34;ìŀ¬ë¯¸ìŀĩ&#34;:7240,&#34;ĠëŃĲëĭĪ&#34;:7241,&#34;Ġìķłê¸°&#34;:7242,&#34;Ġê³łëĩĮ&#34;:7243,&#34;ìľĦìĽĲ&#34;:7244,&#34;ĠíĥĿ&#34;:7245,&#34;ì´¬ìĺģ&#34;:7246,&#34;Ġë©ĭì§Ģê³ł&#34;:7247,&#34;Ġê´ľì°®ëĭ¤&#34;:7248,&#34;ìĤ¬ëŀĮìĿĺ&#34;:7249,&#34;ë¨¹ìĿĢ&#34;:7250,&#34;ê·¼ëŀĺ&#34;:7251,&#34;ĠíĺĦìĭ¤ìĿ´&#34;:7252,&#34;Ġì§ľì¦ĿìĿ´&#34;:7253,&#34;ìĬ¤ëŁ¬ìĽĮ&#34;:7254,&#34;ë¶ĪíĹĪ&#34;:7255,&#34;ë³ĳíĹĮ&#34;:7256,&#34;ĠãħĤ&#34;:7257,&#34;ìĹ¬ë²Ħ&#34;:7258,&#34;Ġêµ¿êµ¿&#34;:7259,&#34;ìĿĮìķħëıĦ&#34;:7260,&#34;íĥĪì¶ľ&#34;:7261,&#34;ìŀ¬ë¯¸ìĹĨìĸ´ìļĶ&#34;:7262,&#34;Ġê¶ģê¸Īíķ´ìĦľ&#34;:7263,&#34;ìĭľëĮĢìĹĲ&#34;:7264,&#34;Ġìŀ¥ë¥´ê°Ģ&#34;:7265,&#34;Ġbê¸ī&#34;:7266,&#34;¶ĢíĦ°&#34;:7267,&#34;ê¸°ìĸµìĿ´&#34;:7268,&#34;Ġdvd&#34;:7269,&#34;ì§Ħë¶Ģíķľ&#34;:7270,&#34;ìĥĪë¡ľìļ´&#34;:7271,&#34;al&#34;:7272,&#34;ic&#34;:7273,&#34;Ġk&#34;:7274,&#34;ĠìŃī&#34;:7275,&#34;ëĬĶê±¸&#34;:7276,&#34;ìłĪë¡ľ&#34;:7277,&#34;ĠìŀĲëıĻ&#34;:7278,&#34;ĠìĺģíĻĶë³´ë©´ìĦľ&#34;:7279,&#34;ëĤĺê°Ħ&#34;:7280,&#34;ë§Įì¡±&#34;:7281,&#34;ë¦¬ìĹĦ&#34;:7282,&#34;ìķĦëħ¸&#34;:7283,&#34;ìĺģíĻĶìĹĲìĦľ&#34;:7284,&#34;ìĹĨì§Ģ&#34;:7285,&#34;ĠëĤĺìĺ¬ë&#34;:7286,&#34;ì§ĦìķĬ&#34;:7287,&#34;Ġìŀ¬ë¯¸ë¡ľ&#34;:7288,&#34;ĠëĤ´ìĥĿ&#34;:7289,&#34;íķłë¿Ĳ&#34;:7290,&#34;ìĹ°ìķł&#34;:7291,&#34;Ġê°Ģë³į&#34;:7292,&#34;ë¯¸ëĬĶ&#34;:7293,&#34;Ġ199&#34;:7294,&#34;ëĵłê°Ģ&#34;:7295,&#34;Ġì²©&#34;:7296,&#34;Ġë§ĲìŀĲ&#34;:7297,&#34;ë¬´ìĭľ&#34;:7298,&#34;ĠìĤ¬ìļ©&#34;:7299,&#34;Ġì¡°íĻĶ&#34;:7300,&#34;ĠíĮĿ&#34;:7301,&#34;íķĺì§Ģë§Ī&#34;:7302,&#34;ëĭµëĭĪëĭ¤&#34;:7303,&#34;Ġë©ĭìŀĪëĬĶ&#34;:7304,&#34;íĺ¸ê°Ĳ&#34;:7305,&#34;ìĽłê³ł&#34;:7306,&#34;ì¹´íĶĦ&#34;:7307,&#34;ì¹´ë©ĶëĿ¼&#34;:7308,&#34;Ġë§Ŀì¹ľ&#34;:7309,&#34;ì°¸ìĭł&#34;:7310,&#34;ê»ĺìļĶ&#34;:7311,&#34;Ġì°įê³ł&#34;:7312,&#34;ĠëĨĢëŀĲ&#34;:7313,&#34;Ġë´Ĳìķ¼ì§Ģ&#34;:7314,&#34;Ġìĭ¸ìĽĢ&#34;:7315,&#34;©´ìĦľ&#34;:7316,&#34;Ġê°ģìĥī&#34;:7317,&#34;ĠìĪĺìŀĳìĿ´ëĭ¤&#34;:7318,&#34;ĠëĶ°ëľ»íķĺê³ł&#34;:7319,&#34;ĠCê¸ī&#34;:7320,&#34;ĠìŀĶìŀĶíķĺê³ł&#34;:7321,&#34;íķĻêµĲìĹĲìĦľ&#34;:7322,&#34;ìŀĶìŀĶíķĺê³ł&#34;:7323,&#34;;;;;;;;;&#34;:7324,&#34;Ġë¦¬ìĸ¼ë¦¬íĭ°&#34;:7325,&#34;ĠìĨĮìĦ¤ìĿĦ&#34;:7326,&#34;ĠìĭĿìĥģíķľ&#34;:7327,&#34;ĠíĿ¬ë§ĿìĿĦ&#34;:7328,&#34;íĽĮë¥Ńíķľ&#34;:7329,&#34;199&#34;:7330,&#34;60&#34;:7331,&#34;½ķ&#34;:7332,&#34;ëĭ¤ë¥´&#34;:7333,&#34;íķĺìĿ´&#34;:7334,&#34;ĠìĺģíĻĶëĤĺ&#34;:7335,&#34;ĠìĺģíĻĶìķ¼&#34;:7336,&#34;ìĿĢëį°&#34;:7337,&#34;ëĵ¤ë¡ľ&#34;:7338,&#34;ë¥ľ&#34;:7339,&#34;ë³´ê¸´&#34;:7340,&#34;ìĬ¤ë§¨&#34;:7341,&#34;ëŀŃ&#34;:7342,&#34;ìĹĨëĤĺ&#34;:7343,&#34;ì£¼ê¸°ëıĦ&#34;:7344,&#34;ìĥģìłģìĿ¸&#34;:7345,&#34;Ġì§Ģê²¨&#34;:7346,&#34;Ġíķľë°©&#34;:7347,&#34;íķłì§Ģ&#34;:7348,&#34;ĠëĮĢíĨµëł¹&#34;:7349,&#34;ë¶ĢëĬĶ&#34;:7350,&#34;êµ¬ìĿĺ&#34;:7351,&#34;ë¶ĦëıĻìķĪ&#34;:7352,&#34;Ġì£¼ì§Ģ&#34;:7353,&#34;ê°Ĳê³¼&#34;:7354,&#34;íĬ¸ë¡ľ&#34;:7355,&#34;ĠìĿ¼ìľ¼&#34;:7356,&#34;íĤ¬ë§ģíĥĢìŀĦ&#34;:7357,&#34;Ġê°ĲëıħìĿĦ&#34;:7358,&#34;Ġìŀ¬ë¯¸ìŀĪìĬµëĭĪëĭ¤&#34;:7359,&#34;Ġë°ĺëĮĢ&#34;:7360,&#34;ë°ķì£½&#34;:7361,&#34;Ġë§¤ëĭĪìķĦ&#34;:7362,&#34;ĠëĤĺìĺ¤ëĬĶëį°&#34;:7363,&#34;ëĿ¼ëĬĶê²Į&#34;:7364,&#34;Ġë¡ľì½Ķ&#34;:7365,&#34;Ġìĸ´ëĸ¨&#34;:7366,&#34;ĠëĬĲëĤĮìĿĢ&#34;:7367,&#34;ĠíŀĺëĤ´&#34;:7368,&#34;Ġì£¼ìĿ¸ê³µëĵ¤&#34;:7369,&#34;ê°ĲëıĻëıĦ&#34;:7370,&#34;ĠëĭµìĿ´&#34;:7371,&#34;ëĤ¬ìĿĮ&#34;:7372,&#34;Ġê¹Ĭê²Į&#34;:7373,&#34;ĠìłĲìĪĺëĬĶ&#34;:7374,&#34;ĠìłĲìĪĺì¤Ģ&#34;:7375,&#34;ĪëįĶëĿ¼ë©´&#34;:7376,&#34;ìľłì¹ĺíķľ&#34;:7377,&#34;ìľłì¹ĺíķĺê³ł&#34;:7378,&#34;Ġë§Įëĵ¤ìĹĪìĿĦê¹Į&#34;:7379,&#34;Ġê·ĢìĹ½ëĭ¤&#34;:7380,&#34;Ġãħīãħīãħī&#34;:7381,&#34;ĠíĥĦíĥĦíķĺê³ł&#34;:7382,&#34;ì¶ĶìĸµìĿĺ&#34;:7383,&#34;21&#34;:7384,&#34;od&#34;:7385,&#34;ot&#34;:7386,&#34;¤ëĭ¤&#34;:7387,&#34;ìĿĺë¥¼&#34;:7388,&#34;ìķĦì§Ģ&#34;:7389,&#34;Ġê·¸ê²ĥìĿĦ&#34;:7390,&#34;ê¹ĥ&#34;:7391,&#34;Ġì¢ĭìĿĦëĵ¯&#34;:7392,&#34;Ġëĭ¤ìĭł&#34;:7393,&#34;ìŀĪì§Ģ&#34;:7394,&#34;Ġìĥĺ&#34;:7395,&#34;ìķĺìľ¼ë©´&#34;:7396,&#34;êµŃìĿĺ&#34;:7397,&#34;ĠìķĪìĸ´ìļ¸&#34;:7398,&#34;Ġìµľê³łìŀĦ&#34;:7399,&#34;ë¬´íĺĳ&#34;:7400,&#34;ë¹Ħì¶Ķ&#34;:7401,&#34;ìĺĢëĤĺ&#34;:7402,&#34;ê±¸ìŀĳ&#34;:7403,&#34;Ġë¹Ħíĸī&#34;:7404,&#34;ëıĻìĽĲ&#34;:7405,&#34;Ġìĭ¤íĹĺ&#34;:7406,&#34;ì§ĪìķĬ&#34;:7407,&#34;ìĬ¨ìĿ´&#34;:7408,&#34;ëŀĢíĭ°&#34;:7409,&#34;ĠìķĦê¹ĿëĦ¤&#34;:7410,&#34;Ġë¶Īê°ĢëĬ¥&#34;:7411,&#34;Ġì²ĺìĿĮìĹĶ&#34;:7412,&#34;Ġê·¹ëĭ¨&#34;:7413,&#34;ì©į&#34;:7414,&#34;Ġëĭ¹ìĹ°íŀĪ&#34;:7415,&#34;ĠëĬĲëĤĮìĿ´ëĭ¤&#34;:7416,&#34;ê°ĲëıĻìĿ´&#34;:7417,&#34;Īë°©&#34;:7418,&#34;ì£½ë°ķì£½&#34;:7419,&#34;Ġì»¸&#34;:7420,&#34;ĠíĮĮê´´&#34;:7421,&#34;Ġëĭ´ê¸´&#34;:7422,&#34;ĠìŀĬê³ł&#34;:7423,&#34;Ġìķ¼íķľ&#34;:7424,&#34;Ġë¬´ìĹĩìĿ¸ê°Ģ&#34;:7425,&#34;ĠíıŃë°ľ&#34;:7426,&#34;ĠíıŃíĴį&#34;:7427,&#34;ì²łíķĻ&#34;:7428,&#34;ë§ĪìĿĮìĹĲ&#34;:7429,&#34;Ġëª»ë³´ê²łëĭ¤&#34;:7430,&#34;Ġìŀ¤ëĭ¤&#34;:7431,&#34;Ġìĭľë¦¬ì¦&#34;:7432,&#34;íķĺëĶĶ&#34;:7433,&#34;íķĺëĭĪê¹Į&#34;:7434,&#34;ê°ĢëģĶ&#34;:7435,&#34;ëĤĺê°ĢëĬĶ&#34;:7436,&#34;ëĿ¼ìĿ¸&#34;:7437,&#34;ëŁ¬ëĿ¼&#34;:7438,&#34;ìŀĲë§ī&#34;:7439,&#34;ìĽŁ&#34;:7440,&#34;ĠìĹĨëĥĲ&#34;:7441,&#34;ìłģìľ¼ë¡ł&#34;:7442,&#34;Ġë§ĮëĤľ&#34;:7443,&#34;Ġìŀ¬ë¯¸ìŀĩ&#34;:7444,&#34;ìĤ¬ê·¹&#34;:7445,&#34;ê²łëĬĶëį°&#34;:7446,&#34;ê°ľëħĲ&#34;:7447,&#34;~~^^&#34;:7448,&#34;ìĹĪëĭ¤ë©´&#34;:7449,&#34;ìĶ¹&#34;:7450,&#34;ìŀ¬ë¯¸ìĹĨëĬĶ&#34;:7451,&#34;Ġë§Įëĵ¤ìĸ´ìĦľ&#34;:7452,&#34;ĠëĤ´ìļ©ìĹĲ&#34;:7453,&#34;ĠìĤ¬ëŀĳíķ´ìļĶ&#34;:7454,&#34;ìľĦê°Ģ&#34;:7455,&#34;ìľĦíķľ&#34;:7456,&#34;Ġëĭ¤ìĭľë³´ëĭĪ&#34;:7457,&#34;ìĬ¨ìĿĺ&#34;:7458,&#34;íļĮê°Ģ&#34;:7459,&#34;Ġíĥĵ&#34;:7460,&#34;ĠìĿ´íķ´ìķĪ&#34;:7461,&#34;ĠìĪĻ&#34;:7462,&#34;Ġê´ľì°®ìķĺëĬĶëį°&#34;:7463,&#34;ìŀĺëª»&#34;:7464,&#34;Ġê¹Ģë¯¼&#34;:7465,&#34;ëĤ´ìļ©ìĿ¸ì§Ģ&#34;:7466,&#34;ĠëĲ¬&#34;:7467,&#34;Ġê³¼íķĻ&#34;:7468,&#34;ì¡Įëįĺ&#34;:7469,&#34;Ġë§İìĿĢê±¸&#34;:7470,&#34;ĠíĴĪ&#34;:7471,&#34;ĠìķĬìĿĢëį°&#34;:7472,&#34;ìķ¡ìħĺëıĦ&#34;:7473,&#34;ì´Īëĵ±íķĻêµĲ&#34;:7474,&#34;ĠìĨĮìŀ¬ìĻĢ&#34;:7475,&#34;Ġìĭľìŀĳíķ´ìĦľ&#34;:7476,&#34;Ġìŀ¬ë¯¸ëıĦìĹĨê³ł&#34;:7477,&#34;ëłĪìĿ´ëĵľ&#34;:7478,&#34;ìķĹëĭ¤&#34;:7479,&#34;Ġëķ¡&#34;:7480,&#34;ĠëķĲ&#34;:7481,&#34;Ġìķ½ê°ĦìĿĺ&#34;:7482,&#34;êµ³êµ³&#34;:7483,&#34;ì´Īë°ĺìĹĲ&#34;:7484,&#34;ĠìĨĮì¤ĳíķ¨ìĿĦ&#34;:7485,&#34;Ġìĸ´ë¦´ìłģ&#34;:7486,&#34;ìĽ¨ìĿ´&#34;:7487,&#34;ĠìĹ¬ëŁ¬ê°Ģì§Ģ&#34;:7488,&#34;ê¶ģê¸Ī&#34;:7489,&#34;ĠíĬ¹ë³Ħíķľ&#34;:7490,&#34;Ġëĳĺì§¸ì¹ĺê³ł&#34;:7491,&#34;ë·°&#34;:7492,&#34;ëĭ¤ë§Į&#34;:7493,&#34;ê¸°ìĻĢ&#34;:7494,&#34;ëĤĺë¥¼&#34;:7495,&#34;ìķĦìĹŃ&#34;:7496,&#34;Ġê°Ŀ&#34;:7497,&#34;ëį°ë¯¸&#34;:7498,&#34;ĠìķĦëŀĺ&#34;:7499,&#34;ĠìķĦíĮĮ&#34;:7500,&#34;ìĭľê¸°&#34;:7501,&#34;ë³´ìĭľê¸¸&#34;:7502,&#34;ëĮĢìĹĲ&#34;:7503,&#34;ìŀĪìľ¼ë©´&#34;:7504,&#34;ìĥģìĹĲ&#34;:7505,&#34;Ġìĸ´íľ´&#34;:7506,&#34;ìĭłëıĦ&#34;:7507,&#34;Ġê±±ìłķ&#34;:7508,&#34;ëķĮë§¤&#34;:7509,&#34;ìĨĮê°Ģ&#34;:7510,&#34;ëĵ¤ìĿ´ëŀĳ&#34;:7511,&#34;ìłķë§Ĳë¡ľ&#34;:7512,&#34;Ġë¯¸ì³¤&#34;:7513,&#34;ëĲĺëĤĺ&#34;:7514,&#34;ìĦłìĿ´&#34;:7515,&#34;ìĦłíĥĿ&#34;:7516,&#34;íĭĭ&#34;:7517,&#34;ëıĻë¬¼&#34;:7518,&#34;ĠëŃĲê³ł&#34;:7519,&#34;ĠëŃĲíķĺëĬĶ&#34;:7520,&#34;ĠìĹŃëŁī&#34;:7521,&#34;Ġë°Ķëĭ¤&#34;:7522,&#34;ĠìĹĨëĬĶê²Į&#34;:7523,&#34;ì²Ńì¶ĺ&#34;:7524,&#34;ĠìĭłìĿĺ&#34;:7525,&#34;Ġì£½ìĹ¬&#34;:7526,&#34;ĠìĿ´íķ´íķłìĪĺ&#34;:7527,&#34;Ġë°°íĬ¸ë§¨&#34;:7528,&#34;ìŀĺë§Įëĵ¤&#34;:7529,&#34;...........&#34;:7530,&#34;íĮĮì¹ĺëħ¸&#34;:7531,&#34;ĠìłĲìĿ´&#34;:7532,&#34;ë¹łìł¸&#34;:7533,&#34;íıīìĥĿ&#34;:7534,&#34;ì§ĢìķĬëĬĶëĭ¤&#34;:7535,&#34;ĠìĥĿìĹĲ&#34;:7536,&#34;Ġë´Ĳìķ¼íķĺëĬĶ&#34;:7537,&#34;ëĪĦêµ°&#34;:7538,&#34;ìĽĶíķľ&#34;:7539,&#34;ĠìĭľëĮĢìĹĲ&#34;:7540,&#34;ĠìĿ´ë¦ĦìĿ´&#34;:7541,&#34;Ġê°ĸëĭ¤&#34;:7542,&#34;ìķĮë°Ķëĵ¤&#34;:7543,&#34;Ġìŀĺëª»ëĲľ&#34;:7544,&#34;Ġì«ĵ&#34;:7545,&#34;Ġì¹ľêµ¬ê°Ģ&#34;:7546,&#34;Ġë§ĮìłĲìĹĲ&#34;:7547,&#34;âĺħâĺħ&#34;:7548,&#34;ë¦¬ë©´ìĦľ&#34;:7549,&#34;ĠíĿ¡ìŀħ&#34;:7550,&#34;Ġê°Ģë²¼ìļ´&#34;:7551,&#34;ìĹĲëĮĢíķ´&#34;:7552,&#34;?..&#34;:7553,&#34;ĳ¼&#34;:7554,&#34;íķĺìĭ¤&#34;:7555,&#34;¬ë½ķ&#34;:7556,&#34;Ġíĺģ&#34;:7557,&#34;ìĭ¼&#34;:7558,&#34;ëĤĺìģĺ&#34;:7559,&#34;ĠëĤ¯&#34;:7560,&#34;ëĵ¤íķľíħĮ&#34;:7561,&#34;ìĿ¸ê²Į&#34;:7562,&#34;Ġë³´ëĿ¼ê³ł&#34;:7563,&#34;Ġê·¸ê²ĥìĿ´&#34;:7564,&#34;ĠëĤĺìĺ¬ëķĮ&#34;:7565,&#34;ĠíķĺìĹ¬&#34;:7566,&#34;ìķ¼íķľëĭ¤&#34;:7567,&#34;ì£¼ì§Ģ&#34;:7568,&#34;ì£¼ìĿ¼&#34;:7569,&#34;ìłģìĿ´ëĿ¼&#34;:7570,&#34;ĠìĻ¤ì¼Ģ&#34;:7571,&#34;ê³¼ê±°&#34;:7572,&#34;ìķĺëĦ¤&#34;:7573,&#34;íĶĶìĿ´&#34;:7574,&#34;ì¹ĺìĿĺ&#34;:7575,&#34;ĠìĨĶ&#34;:7576,&#34;Ġìŀ¬ë°ĮìĹĪëįĺ&#34;:7577,&#34;ĠìķĪê°Ģ&#34;:7578,&#34;ĠëŃ£&#34;:7579,&#34;ĠìĻľê³¡&#34;:7580,&#34;íĬ¸ë¥¼&#34;:7581,&#34;ìłĢì§Ī&#34;:7582,&#34;ëįĶë§¨&#34;:7583,&#34;ë²Ħë¦´&#34;:7584,&#34;Ġìľłíĸī&#34;:7585,&#34;Ġìŀ¥ìķł&#34;:7586,&#34;ĠìĤ¬ëŀĮìĿ´ëĿ¼ë©´&#34;:7587,&#34;ĠëĤ´ìļ©ìĿĺ&#34;:7588,&#34;ëĭĺìĿĢ&#34;:7589,&#34;Ġë§¤ë¯¸&#34;:7590,&#34;Ġë§¤ëģĦ&#34;:7591,&#34;ë§Īë¥´&#34;:7592,&#34;Ġìĭľê°Ħë§Į&#34;:7593,&#34;Ġì£½ê³ł&#34;:7594,&#34;ĠëıĦë¬´ì§Ģ&#34;:7595,&#34;ĠíķľêµŃìĹĲìĦľ&#34;:7596,&#34;Ġëĭ¹íĻ©&#34;:7597,&#34;Ġìļ¸ë©´ìĦľ&#34;:7598,&#34;Ġë§Ŀê°Ģ&#34;:7599,&#34;Ġë§ŀìķĦ&#34;:7600,&#34;ĠìłķëıĦëĬĶ&#34;:7601,&#34;ìĤ¬ëŀĳìĿĦ&#34;:7602,&#34;Ġíİ¸ìķĪ&#34;:7603,&#34;Ġê³µê°Ĳíķł&#34;:7604,&#34;Ġë²Ħë¦¬ê³ł&#34;:7605,&#34;Ġíĺ¸ê¸°&#34;:7606,&#34;ìŀĸìĿĢ&#34;:7607,&#34;ĪëįĶëĿ¼&#34;:7608,&#34;Īë³´ëĭ¤&#34;:7609,&#34;ĠìĬ¤íĨłë¦¬ëĿ¼&#34;:7610,&#34;ê·¸ëŁ°ëį°&#34;:7611,&#34;Ġë¨¼ê°Ģ&#34;:7612,&#34;ìľ¼ë©´ìĦľëıĦ&#34;:7613,&#34;ìĸ´ë¦°ìĭľìłĪ&#34;:7614,&#34;Ġë¶Īíİ¸íķľ&#34;:7615,&#34;ĠãĦ·ãĦ·ãĦ·&#34;:7616,&#34;ĠìļĶìĨĮê°Ģ&#34;:7617,&#34;Ġë£¨ì¦Ī&#34;:7618,&#34;ĠìľłìĿ¼íķĺê²Į&#34;:7619,&#34;Ġì»¨ìħī&#34;:7620,&#34;ëıħíĬ¹íķľ&#34;:7621,&#34;Ġìłłìŀ¥&#34;:7622,&#34;ì¹´íĶĦë¦¬ìĺ¤&#34;:7623,&#34;ĠK&#34;:7624,&#34;ĠìĺģíĻĶìĺĪìļĶ&#34;:7625,&#34;ĠìĹ¼&#34;:7626,&#34;ëĤĺìĿ´íĬ¸&#34;:7627,&#34;ĠìłĪë¡ľ&#34;:7628,&#34;ëĵ¤ëģ¼ë¦¬&#34;:7629,&#34;ìłķìĹĲ&#34;:7630,&#34;ìłķì©¡&#34;:7631,&#34;ë§ĪìĿĺ&#34;:7632,&#34;ê¹Įì§Ģë§Į&#34;:7633,&#34;ìļ°ë¦¬ê°Ģ&#34;:7634,&#34;ìĥģê³¼&#34;:7635,&#34;ê°ĦìĹĲ&#34;:7636,&#34;Ġìĸ´ëĶ&#34;:7637,&#34;Ġìĸ´ëķ&#34;:7638,&#34;ëĵľëŁ½ê²Į&#34;:7639,&#34;ì¹ĺë¥¼&#34;:7640,&#34;Ġë¬´ëĤľ&#34;:7641,&#34;ë¶Ħëªħ&#34;:7642,&#34;ĠìĥĿê°ģëĤĺ&#34;:7643,&#34;Ġë§ĲìķĦìķ¼&#34;:7644,&#34;Ġë§Ĳê³łëĬĶ&#34;:7645,&#34;Ġëª»ë´Ĳì£¼&#34;:7646,&#34;íı¼&#34;:7647,&#34;ĠìľłìķĦ&#34;:7648,&#34;ëĲĺëĬĶëį°&#34;:7649,&#34;ì¡°ìĦł&#34;:7650,&#34;Ġëªħìŀ¥ë©´&#34;:7651,&#34;Ġëªħë³µìĿĦ&#34;:7652,&#34;ìĹŃíķł&#34;:7653,&#34;ë°ĺê°ľëıĦ&#34;:7654,&#34;Ġíķłë§Ĳ&#34;:7655,&#34;ëĭµê²Į&#34;:7656,&#34;Ġê¸°ìĸµëĤĺëĬĶ&#34;:7657,&#34;ìķĮê³ł&#34;:7658,&#34;ìĤ¬ëŀĮìĿĦ&#34;:7659,&#34;ë¨¹ìĿĦ&#34;:7660,&#34;íħĮëŁ¬&#34;:7661,&#34;ë¦¼ìĿ´&#34;:7662,&#34;ĠíŀĺìĿĦ&#34;:7663,&#34;Ġì¤ĦìĪĺ&#34;:7664,&#34;Ġë§Įëĵ¤ìĸ´ëĿ¼&#34;:7665,&#34;ĠìĦľìļ¸&#34;:7666,&#34;ì¡Įìĸ´ìļĶ&#34;:7667,&#34;ĠìĿ¸ìĥĿìĹĲ&#34;:7668,&#34;ĠìĪĺì¤ĢìĿĦ&#34;:7669,&#34;Ġì¶Ķì²ľíķĺê³ł&#34;:7670,&#34;ëģĿëĤĺ&#34;:7671,&#34;Ġì¦Ĳê¸°&#34;:7672,&#34;Ġë³´ê²ĮëĲľ&#34;:7673,&#34;Ġëį°ë·Ķ&#34;:7674,&#34;Ġì«Ħ&#34;:7675,&#34;Ġë¹ĽëĤĺëĬĶ&#34;:7676,&#34;ëļ±ë§ŀ&#34;:7677,&#34;ĠëļĿ&#34;:7678,&#34;Ġìĵ¸ëį°ìĹĨìĿ´&#34;:7679,&#34;ãĢĤ&#34;:7680,&#34;ovie&#34;:7681,&#34;ìķĪíĥĢê¹Ŀ&#34;:7682,&#34;Ġì§ľì§ĳê¸°&#34;:7683,&#34;ĠíĺĲìĺ¤&#34;:7684,&#34;ëŀĢíĭ°ëħ¸&#34;:7685,&#34;^-^&#34;:7686,&#34;im&#34;:7687,&#34;êº&#34;:7688,&#34;ĨĴ&#34;:7689,&#34;ê°Ģìļ´&#34;:7690,&#34;ê¸°ë²ķ&#34;:7691,&#34;ìķĦíĶĦ&#34;:7692,&#34;ìĬ¤ìĻĢ&#34;:7693,&#34;ìĬ¤íİĺ&#34;:7694,&#34;ì§Ħì§Ģ&#34;:7695,&#34;ìĺ¤ëŀ«&#34;:7696,&#34;ê·¸ëŁŃìłĢëŁŃ&#34;:7697,&#34;ì°¬ëŀĢ&#34;:7698,&#34;ë¶ĢìĹĲ&#34;:7699,&#34;ê°ľë¥¼&#34;:7700,&#34;ìłĢë¦¬&#34;:7701,&#34;ĠìĨĮìĨĮíķľ&#34;:7702,&#34;ìĽĲìĿĢ&#34;:7703,&#34;ĠëĵľëĿ¼ë§ĪìŀħëĭĪëĭ¤&#34;:7704,&#34;Ġë°°ìļ°ëĬĶ&#34;:7705,&#34;ì¡°ìķĦ&#34;:7706,&#34;ĠìĤ¬ëŀĳíķ´&#34;:7707,&#34;ìļ¸ë¿Ĳ&#34;:7708,&#34;Ġì¢ĭìķĺìĿĦíħĲëį°&#34;:7709,&#34;ìĺĪìģĺ&#34;:7710,&#34;Ġìķ¡ìħĺê³¼&#34;:7711,&#34;êµĲíĽĪ&#34;:7712,&#34;Ġëª°ëĿ¼&#34;:7713,&#34;ìĭľê°ĦëıĻìķĪ&#34;:7714,&#34;ĠëĳĲê·¼&#34;:7715,&#34;ëª»íķĺëĬĶ&#34;:7716,&#34;ê²°ë¡ł&#34;:7717,&#34;ĠìĹ¬ìŀĲëĬĶ&#34;:7718,&#34;ëĤ´ìļ©ìĿĦ&#34;:7719,&#34;Ġë³¼ë§ĮíĸĪëĭ¤&#34;:7720,&#34;Ġê±´ê°Ģ&#34;:7721,&#34;Ġì°¾ìķĦìĦľ&#34;:7722,&#34;ĠìĻ¸ë¡ľ&#34;:7723,&#34;Ġë¨¹ëĬĶ&#34;:7724,&#34;Ġë§ĲìĿ´íķĦìļĶ&#34;:7725,&#34;ëģĿëĤĺê³ł&#34;:7726,&#34;Ġëĸ¨ìĸ´ì§Ģ&#34;:7727,&#34;Ġê³±&#34;:7728,&#34;ĠìĤ¶ìĿ´&#34;:7729,&#34;Ġìŀłëĵ¤&#34;:7730,&#34;ìĹ¬ëħ&#34;:7731,&#34;ìĽĲìŀĳìĿĺ&#34;:7732,&#34;Ġë¬´ìĦŃê³ł&#34;:7733,&#34;Ġëĵ¤ìĸ´ê°Ģ&#34;:7734,&#34;ì¿µ&#34;:7735,&#34;ĠìķĬìķĺì§Ģë§Į&#34;:7736,&#34;ìķĪëĲĺê³ł&#34;:7737,&#34;ĠìķĶê±¸&#34;:7738,&#34;ìŀĲì²´ëĬĶ&#34;:7739,&#34;ìħĶìķ¼&#34;:7740,&#34;Ġíģ´ëŀĺ&#34;:7741,&#34;Ġë³´ìķĺëįĺ&#34;:7742,&#34;Ġëĭ¬ì½¤&#34;:7743,&#34;ĠìĿ´ê²ĥë³´ëĭ¨&#34;:7744,&#34;ìĸ´ë¦°ìĿ´&#34;:7745,&#34;ĠìĥĿê°ģíķ´ë³´ê²Į&#34;:7746,&#34;ĠíĿĳìĿ¸&#34;:7747,&#34;Ġê°ĢëĬ¥íķľ&#34;:7748,&#34;ĠíĿĺëŁ¬ê°ĢëĬĶ&#34;:7749,&#34;Ġ:)&#34;:7750,&#34;ìĭłìĦłíķľ&#34;:7751,&#34;ë§ĺìĹĲ&#34;:7752,&#34;Ġê°Ģê¹Įìļ´&#34;:7753,&#34;Ġê±°ì§ĵë§Ĳ&#34;:7754,&#34;OST&#34;:7755,&#34;nd&#34;:7756,&#34;£Į&#34;:7757,&#34;íķĳ&#34;:7758,&#34;ê³łëıĦ&#34;:7759,&#34;íķĺìķĦ&#34;:7760,&#34;íķĺëĬĺ&#34;:7761,&#34;ìĿĢê±°&#34;:7762,&#34;ìĸ´ë¥¼&#34;:7763,&#34;ìĿ¸ìĹĲ&#34;:7764,&#34;ĠìķĦëĨĶ&#34;:7765,&#34;íķ´íĶ¼&#34;:7766,&#34;ìĺģíĻĶë³´ê³ł&#34;:7767,&#34;ë³´êµ¬&#34;:7768,&#34;ìĬ¤íı¬&#34;:7769,&#34;ìŀĲë¦¬&#34;:7770,&#34;ĠëĤĺìĿĦëĵ¯&#34;:7771,&#34;ìĥģìĥģ&#34;:7772,&#34;íĺĶ&#34;:7773,&#34;ê·¸ìĿĺ&#34;:7774,&#34;ĠìĭľìĤ¬íļĮ&#34;:7775,&#34;Ġìµľë¯¼&#34;:7776,&#34;ìĭłê¸°&#34;:7777,&#34;Ġìŀĺíķľëĭ¤&#34;:7778,&#34;Ġì£¼ìĦ±ì¹ĺ&#34;:7779,&#34;íİ¸ìĹĲìĦľ&#34;:7780,&#34;íĥĢê³ł&#34;:7781,&#34;ë²Ħë¦¬&#34;:7782,&#34;Ġë²¨&#34;:7783,&#34;ĠìĤ¬ìļ´ëĵľ&#34;:7784,&#34;ëªħíķľ&#34;:7785,&#34;Ġì¡°ì°¨&#34;:7786,&#34;Ġìĭ¶ìĹĪëĭ¤&#34;:7787,&#34;Ġìĭ¶ìĹĪëįĺ&#34;:7788,&#34;Ġì¢ĭìķĦíķĺì§Ģë§Į&#34;:7789,&#34;íķĺì§Ģë§Ĳ&#34;:7790,&#34;ĠìĹŃê²¨&#34;:7791,&#34;ê»Ģ&#34;:7792,&#34;ëħ¸ì¶ľ&#34;:7793,&#34;íıīìłĲë³´ê³ł&#34;:7794,&#34;ĠíĻĢ&#34;:7795,&#34;Ġë¶ĦëŁī&#34;:7796,&#34;ì¹ľêµ¬ëĵ¤&#34;:7797,&#34;ìĤ´ìĿ´&#34;:7798,&#34;ë³¼ìĪĺ&#34;:7799,&#34;íķĺê¸°ìĹĶ&#34;:7800,&#34;Ġë§Įëĵ¤ìĸ´ëĤ¸&#34;:7801,&#34;Ġìłģìĸ´ëıĦ&#34;:7802,&#34;ĠìĿ¸ìĥĿìĿ´&#34;:7803,&#34;ìĤ¬ëŀĳê³¼&#34;:7804,&#34;Ġëª¨ìĬµìĿĢ&#34;:7805,&#34;Ġê³µê°ĲëıĦ&#34;:7806,&#34;ĠëĨĢëŀĲëĭ¤&#34;:7807,&#34;ëŁ¬ëĥĲ&#34;:7808,&#34;ĠìŀłìĿ´&#34;:7809,&#34;ĠëĬĲëģ¼ê³ł&#34;:7810,&#34;Ġë¹¼ê³¤&#34;:7811,&#34;íķĺìĭľê³ł&#34;:7812,&#34;Ġìŀ¼ìŀĪìĸ´ìļĶ&#34;:7813,&#34;Ġë°°ê²½ìľ¼ë¡ľ&#34;:7814,&#34;ĠëĵľëĿ¼ë§Īë¡ľ&#34;:7815,&#34;íıŃëł¥&#34;:7816,&#34;Ġì£¼ìłľê°Ģ&#34;:7817,&#34;ĠìķĮìķĺìĿĮ&#34;:7818,&#34;íķĦë²Ħê·¸&#34;:7819,&#34;ĠìŀĥìĿĢ&#34;:7820,&#34;ìĹĦì²ŃëĤľ&#34;:7821,&#34;Ġê°Ķëĭ¤&#34;:7822,&#34;ĠëĥĦìĥĪ&#34;:7823,&#34;Ġëĭ¤íĸīìĿ´ëĭ¤&#34;:7824,&#34;Ġìĸ´ëłµëĭ¤&#34;:7825,&#34;ĠìĿ´ëŀĺìĦľ&#34;:7826,&#34;ĠìķĪë³¸ëĭ¤&#34;:7827,&#34;ĠíĹĽìĽĥìĿĮ&#34;:7828,&#34;ê°ľìĹ°ìĦ±&#34;:7829,&#34;ì¢ħìĿ¼ê´Ģ&#34;:7830,&#34;ë¶ĪíĹĪìłĦ&#34;:7831,&#34;ª©&#34;:7832,&#34;³¼&#34;:7833,&#34;ê³±&#34;:7834,&#34;ìļ¤&#34;:7835,&#34;ì§Ģë¶Ģ&#34;:7836,&#34;ëĤŃ&#34;:7837,&#34;ĠìĺģíĻĶìĿ´&#34;:7838,&#34;ĠìĺģíĻĶìĿ¸ì¤Ħ&#34;:7839,&#34;ìĿĦìĪĺê°Ģ&#34;:7840,&#34;Ġë§´&#34;:7841,&#34;íķ´ìł¸&#34;:7842,&#34;Ġë³´ëĭ¨&#34;:7843,&#34;ìłĲìĹĲ&#34;:7844,&#34;ìŀĲëıĦ&#34;:7845,&#34;ì§Ħíĸī&#34;:7846,&#34;ĠìĹ°ê·¹&#34;:7847,&#34;ĲëıĮ&#34;:7848,&#34;ìķĺëĦ¤ìļĶ&#34;:7849,&#34;Ġëª¨ë¥¸ëĭ¤&#34;:7850,&#34;êµŃë¯¼&#34;:7851,&#34;ë¶Ħëĵ¤ìĿ´&#34;:7852,&#34;ìħ§&#34;:7853,&#34;ĠìķĪì¢ĭìķĦ&#34;:7854,&#34;~~!!&#34;:7855,&#34;Ġì§Ħì§ľë¡ľ&#34;:7856,&#34;Ġêµīìŀ¥&#34;:7857,&#34;Ġê°ľìĦ±&#34;:7858,&#34;ĠìĨĮíĻĶ&#34;:7859,&#34;ë¹ħ&#34;:7860,&#34;^^*&#34;:7861,&#34;Ġìĭ¶ìĸ´ìĦľ&#34;:7862,&#34;ĠíĮį&#34;:7863,&#34;ì¦ĿìĿ´&#34;:7864,&#34;ìĺĪì§Ħ&#34;:7865,&#34;Ġëª¨ë¥´ì§Ģë§Į&#34;:7866,&#34;ìŀĪëĬĶìĺģíĻĶ&#34;:7867,&#34;Ġë§Īì§Ģë§īìĹĶ&#34;:7868,&#34;Ġë§Ŀì¹ĺ&#34;:7869,&#34;Ġë³´ê¸°ê°Ģ&#34;:7870,&#34;ëĶ©ëķĮ&#34;:7871,&#34;Ġìłģëĭ¹íŀĪ&#34;:7872,&#34;ëįĶëĿ¼ë©´&#34;:7873,&#34;ìĬ¹ìłĦ&#34;:7874,&#34;ëŁ¬ë¸Į&#34;:7875,&#34;ì²ĺìĿĮìĹĲëĬĶ&#34;:7876,&#34;Ġìĸ´ëĶĶë¡ľ&#34;:7877,&#34;ĠìĨĮìŀ¬ëıĦ&#34;:7878,&#34;Ġì¹´ë©Ķ&#34;:7879,&#34;ëĪĦêµ¬&#34;:7880,&#34;ĠìķŀëĴ¤&#34;:7881,&#34;ĠëĶĶìĽĮ&#34;:7882,&#34;ĠìĺģíĻĶëĿ¼ìĦľ&#34;:7883,&#34;Ġê·¸ìłĢê·¸ëŁ°&#34;:7884,&#34;Ġì±ħìĿĦ&#34;:7885,&#34;Ġë§Įëĵ¤ìĹĪëĤĺ&#34;:7886,&#34;Ġëķħ&#34;:7887,&#34;ĠíĿĶëĵ¤&#34;:7888,&#34;ĠìĻĶìĬµëĭĪëĭ¤&#34;:7889,&#34;Ġíĸ¥ìĹ°&#34;:7890,&#34;ĠìĹ¬ì£¼ê°Ģ&#34;:7891,&#34;ĠìĹ¬ëŁ¬ë¶Ħ&#34;:7892,&#34;ĠíķĻêµĲìĹĲìĦľ&#34;:7893,&#34;Ġê°ķëł¬íķľ&#34;:7894,&#34;ë¤Ħ&#34;:7895,&#34;ìķĦëĭĪë©´&#34;:7896,&#34;íģ¬ëłĪëĶ§&#34;:7897,&#34;18&#34;:7898,&#34;SF&#34;:7899,&#34;ay&#34;:7900,&#34;et&#34;:7901,&#34;ê°Ģë¦¬&#34;:7902,&#34;ê²ī&#34;:7903,&#34;ëĿ¼ìĹĲ&#34;:7904,&#34;ëĦĮ&#34;:7905,&#34;ìĭľìĿĺ&#34;:7906,&#34;ë³´ëĦ¤ìļĶ&#34;:7907,&#34;ìĬ¤íĭ±&#34;:7908,&#34;Ġë°ı&#34;:7909,&#34;ìŀĲë©´&#34;:7910,&#34;íķĺê³łëĬĶ&#34;:7911,&#34;Ġìµľì´Ī&#34;:7912,&#34;ì¹ĺëıĦ&#34;:7913,&#34;êµ¬ìĹŃ&#34;:7914,&#34;Ġëª¨ìļķ&#34;:7915,&#34;Ġìłķë§ĲìĿ´ì§Ģ&#34;:7916,&#34;Ġë¬´ëĦĪ&#34;:7917,&#34;ĠìŀĲê¸°ê°Ģ&#34;:7918,&#34;íİ¸ê³¼&#34;:7919,&#34;ĠíıīìłĲë³´ê³ł&#34;:7920,&#34;Ġë©ĶìĿ´&#34;:7921,&#34;Ġë¹Ļ&#34;:7922,&#34;ĠëģĿëĤłëķĮ&#34;:7923,&#34;Ġìļ°ë¦¬ëĵ¤&#34;:7924,&#34;Ġê°ĲëıħëıĦ&#34;:7925,&#34;ĠìĤ¬ëŀĳíķ©ëĭĪëĭ¤&#34;:7926,&#34;Ġë°ĺìĺģ&#34;:7927,&#34;ëĳĲë²Ī&#34;:7928,&#34;ĠíĽĦìĹĲ&#34;:7929,&#34;ĠëĶ°ë¦Ħ&#34;:7930,&#34;ì§ĳìĸ´&#34;:7931,&#34;ì§ĢìķĬëĭ¤&#34;:7932,&#34;Ġì°¾ëĬĶ&#34;:7933,&#34;ĠìķĦë¬´ëıĦ&#34;:7934,&#34;íĥĦíĥĦ&#34;:7935,&#34;ì¢ħìĺģ&#34;:7936,&#34;Ġì¹ĺìľł&#34;:7937,&#34;ĠëĬĲê»´ì¡Įëĭ¤&#34;:7938,&#34;ĠëıħìĿ¼&#34;:7939,&#34;ì¸Ħ&#34;:7940,&#34;ê·ĢìĹ¬ìļ´&#34;:7941,&#34;ĠëĴ¤ìĹĲ&#34;:7942,&#34;Ġìĭľë¦¬ì¦Īì¤ĳ&#34;:7943,&#34;Ġê¸¸ìĿ´&#34;:7944,&#34;Ġ60&#34;:7945,&#34;ĠíĻ©ëĭ¹íķľ&#34;:7946,&#34;Ġê¸ĢìİĦ&#34;:7947,&#34;ì±ĦëĦĲ&#34;:7948,&#34;íĸĩëĬĶëį°&#34;:7949,&#34;ìĨĮìŀ¬ê°Ģ&#34;:7950,&#34;ë³´ì§Ģë§ĪëĿ¼&#34;:7951,&#34;íĿ¥íĸī&#34;:7952,&#34;ì§ľì¦ĿëĤĺ&#34;:7953,&#34;ĠëķĢ&#34;:7954,&#34;Ġì¢ĭê²łìĬµëĭĪëĭ¤&#34;:7955,&#34;ĠìĹĦë§Īê°Ģ&#34;:7956,&#34;ĠëŁ¬ìĭľìķĦ&#34;:7957,&#34;Ġëľ¬ê¸ĪìĹĨëĬĶ&#34;:7958,&#34;ì§Ģëª»íķł&#34;:7959,&#34;Ġëª©ìĨĮë¦¬ê°Ģ&#34;:7960,&#34;ĠìĿ¸íĦ°ëĦ·&#34;:7961,&#34;ì¥¬ìĸ¼&#34;:7962,&#34;Ġì§Īì§ĪëģĮ&#34;:7963,&#34;Ġ,,&#34;:7964,&#34;ĠìŁ&#34;:7965,&#34;ëĭĲ&#34;:7966,&#34;ìĿĺìĭĿ&#34;:7967,&#34;ê¸°ìľĦíķ´&#34;:7968,&#34;ĠìĹ®&#34;:7969,&#34;ìĺĽ&#34;:7970,&#34;ìĺģíĻĶìłľ&#34;:7971,&#34;Ġê·¸ëĭ¹ìĭľ&#34;:7972,&#34;ìĬ¤íħĶ&#34;:7973,&#34;ìŀĲê·¹&#34;:7974,&#34;ë§Īë¦¬&#34;:7975,&#34;ìĹĨìĸ´ìļĶ&#34;:7976,&#34;ìĹĨìĹĪëĭ¤&#34;:7977,&#34;ĠëĤĺìĿĮ&#34;:7978,&#34;ĠëĤĺì¤ĳ&#34;:7979,&#34;ĠëĤĺëŀĳ&#34;:7980,&#34;Ġíķĺë©°&#34;:7981,&#34;Ġìłķë³´&#34;:7982,&#34;Ġì§Ģê²¹ëĭ¤&#34;:7983,&#34;ĠìĦ¬ë&#34;:7984,&#34;ë¯¸íĻĶ&#34;:7985,&#34;ìĭłë¶Ħëĵ¤&#34;:7986,&#34;ĠìĽħ&#34;:7987,&#34;~~~~~&#34;:7988,&#34;ìĭ¤ìłľ&#34;:7989,&#34;Ġë§ĲìĶĢ&#34;:7990,&#34;ĠìĹ¬íĥľ&#34;:7991,&#34;ĠìŀĲê³ł&#34;:7992,&#34;ĠìłľìŀĦìĬ¤&#34;:7993,&#34;Ġë¹ķëĭĪëĭ¤&#34;:7994,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿĦ&#34;:7995,&#34;Ġë¶Ģëĭ´&#34;:7996,&#34;ĠëģĿìĹĲ&#34;:7997,&#34;ĠíĮ¬ëĵ¤&#34;:7998,&#34;ĠìłĢíıīê°Ģ&#34;:7999,&#34;ë°ĶíĥĢ&#34;:8000,&#34;ìŀ¼ìŀĪ&#34;:8001,&#34;ì¶ĶëĬĶ&#34;:8002,&#34;íĽĦìĹĲ&#34;:8003,&#34;Ġìŀ¥ë©´ëĵ¤ìĿ´&#34;:8004,&#34;Ġíıīê·ł&#34;:8005,&#34;ĠëıĻëĦ¤&#34;:8006,&#34;ëª»íķľ&#34;:8007,&#34;Ġê´ľì°®ê²Į&#34;:8008,&#34;ë³µìĪĺ&#34;:8009,&#34;ĠìĹ¬ìŀĲëĵ¤&#34;:8010,&#34;ìĽĥê¸´&#34;:8011,&#34;Ġìĭ¬ìŀ¥&#34;:8012,&#34;ĠìĦłìĤ¬&#34;:8013,&#34;ë°ĽìĿĦ&#34;:8014,&#34;Ġìłģëĭ¹íķľ&#34;:8015,&#34;Ġë°ķìĪĺ&#34;:8016,&#34;ìĤ¬ëŀĳìĿĢ&#34;:8017,&#34;ìĻ¸êµŃ&#34;:8018,&#34;Ġê·¸ëłĩì§Ģë§Į&#34;:8019,&#34;Ġê¿Ģ&#34;:8020,&#34;ĠìĽĲìŀĳìĿĺ&#34;:8021,&#34;ëħĦëĮĢìĹĲ&#34;:8022,&#34;ë¸ĶëŀĻ&#34;:8023,&#34;ê·Ģìĭł&#34;:8024,&#34;Ġë³ĢíĻĶ&#34;:8025,&#34;íĺľìĦł&#34;:8026,&#34;ëĳĶ&#34;:8027,&#34;ĠìŀĪìĹĪìľ¼ë©´&#34;:8028,&#34;Ġê·ĢìĹ¬ìĽĢ&#34;:8029,&#34;ê·¸ëŁ°ì§Ģ&#34;:8030,&#34;ĠìķĮìķĺëĦ¤&#34;:8031,&#34;íģ¬ë£¨&#34;:8032,&#34;ĠìĻłë§Įíķĺë©´&#34;:8033,&#34;................................&#34;:8034,&#34;Ġíĺ¹ìĭľ&#34;:8035,&#34;ê³³ìĹĲ&#34;:8036,&#34;ìį©&#34;:8037,&#34;Ġì§Ħì§Ģíķĺê²Į&#34;:8038,&#34;ë¡ľë§¨íĭ±&#34;:8039,&#34;Ġë³´ìĭľë©´&#34;:8040,&#34;Ġë§¡ìĿĢ&#34;:8041,&#34;ìłĦì²´ìłģìľ¼ë¡ľ&#34;:8042,&#34;íķµëħ¸ìŀ¼&#34;:8043,&#34;ĠìķĪë¬´ìĦľ&#34;:8044,&#34;ê°±ìĿ´&#34;:8045,&#34;ĠìĿ´ëıĦìłĢëıĦ&#34;:8046,&#34;ck&#34;:8047,&#34;~âĻ¥&#34;:8048,&#34;Ġ&amp;&#34;:8049,&#34;ĠW&#34;:8050,&#34;ŃĪ&#34;:8051,&#34;ìĿ´íĶĦ&#34;:8052,&#34;ìķ¡&#34;:8053,&#34;Īë¹Ħ&#34;:8054,&#34;ĠìĿ´ëĭ¤&#34;:8055,&#34;ĠìĿ´ëłĩ&#34;:8056,&#34;ĠìĿ´ë³´ëĭ¤&#34;:8057,&#34;ë¦¬ìĸ¼&#34;:8058,&#34;ìķĦì§Ħì§ľ&#34;:8059,&#34;ìķĦëĪĦ&#34;:8060,&#34;ì²¼&#34;:8061,&#34;ĠëĤĺë¬´&#34;:8062,&#34;ĠëĤĺìĿ´ê°Ģ&#34;:8063,&#34;Ġëĭ¤íģ¬&#34;:8064,&#34;Ġìĸ´ìłľ&#34;:8065,&#34;Ġê¸°íļį&#34;:8066,&#34;ë¬»&#34;:8067,&#34;ĠìĪĺë¡Ŀ&#34;:8068,&#34;ë¶ĦíŀĪ&#34;:8069,&#34;ê²łìĸ´&#34;:8070,&#34;ê²łëĭ¤ê³ł&#34;:8071,&#34;Ġë§Īìĭľê¸¸&#34;:8072,&#34;ëĵ¯íķĺëĭ¤&#34;:8073,&#34;ìľłë°ľ&#34;:8074,&#34;ì²´ìłģ&#34;:8075,&#34;ëĦĪë¬´ëĤĺëıĦ&#34;:8076,&#34;ĠëĵľëĿ¼ë§ĪìĿĺ&#34;:8077,&#34;ĠìĤ¬ìĿ´ì½Ķ&#34;:8078,&#34;ìĺģìĿĺ&#34;:8079,&#34;Ġíķ¸&#34;:8080,&#34;ì¡°ìļ©&#34;:8081,&#34;ìŀ¬ë°ĮëĦ¤&#34;:8082,&#34;Ġë°Ķëĭ¥&#34;:8083,&#34;ëĸ¨&#34;:8084,&#34;Ġìĭ¤ê°Ĳ&#34;:8085,&#34;ìŀ¼ìŀĪìĸ´ìļĶ&#34;:8086,&#34;ëĿ¼ê³łìļĶ&#34;:8087,&#34;ĠìĤ´ë¦¬ì§Ģ&#34;:8088,&#34;Ġíķłë¦¬ìļ°ëĵľ&#34;:8089,&#34;ĠìķĦëĭĪìŀĸìķĦ&#34;:8090,&#34;ĠìĿ´íķ´ë¶Īê°Ģ&#34;:8091,&#34;Ġì§ľë¦¿&#34;:8092,&#34;ĠíķľêµŃìĿĺ&#34;:8093,&#34;ĠëĤľë¬´&#34;:8094,&#34;ìĿĦê¹ĮìļĶ&#34;:8095,&#34;Ġì§Ģê¸ĪìĿĺ&#34;:8096,&#34;íĥľíĺĦ&#34;:8097,&#34;Ġì§ľì¦ĿëĤĺê²Į&#34;:8098,&#34;ĠìĦľìĸĳ&#34;:8099,&#34;ĠìĥĿê²¼&#34;:8100,&#34;ĠìĬ¤ë¦´ëıĦ&#34;:8101,&#34;Ġë°©íķ´&#34;:8102,&#34;ëĭ¤ëĭĪëĬĶ&#34;:8103,&#34;ìĦĿê·ľ&#34;:8104,&#34;ĠìĹ°ê¸°ëł¥ìĹĲ&#34;:8105,&#34;íĳľìłķ&#34;:8106,&#34;ĠìĽĲìŀĳìĿ´&#34;:8107,&#34;ì§Ģê¸ĪëıĦ&#34;:8108,&#34;êº¼ë©´&#34;:8109,&#34;ìĻķêµŃ&#34;:8110,&#34;Ġë³´ê²ĮëĲĺ&#34;:8111,&#34;Ġë¦¬ì¦Ī&#34;:8112,&#34;ìĬ¬íĶĪ&#34;:8113,&#34;Ġë¬´ìĦŃê²Į&#34;:8114,&#34;Ġìŀ¼ìŀĪìĿĮ&#34;:8115,&#34;ĠíħĲ&#34;:8116,&#34;ë¡Ńê²Į&#34;:8117,&#34;Ġë¯¼íıĲ&#34;:8118,&#34;ì§Ħìĭ¬ìľ¼ë¡ľ&#34;:8119,&#34;ĠíĤ¤ìĬ¤&#34;:8120,&#34;Ġìĸ¸ìłł&#34;:8121,&#34;ë§ĪìĿ´íģ´&#34;:8122,&#34;Ġë¯¸ìĨĮê°Ģ&#34;:8123,&#34;Ġëħ¹ìķĦ&#34;:8124,&#34;ìłĲë§ĮìłĲìĹĲ&#34;:8125,&#34;Ġë±ĢíĮĮìĿ´ìĸ´&#34;:8126,&#34;Ġìĸ´ëĳ¡&#34;:8127,&#34;dd&#34;:8128,&#34;©į&#34;:8129,&#34;Ġìª¼&#34;:8130,&#34;ìĿ´ëķĮ&#34;:8131,&#34;Ġìķµ&#34;:8132,&#34;ĠìĿĦ&#34;:8133,&#34;ĠìĺģíĻĶê°Ļ&#34;:8134,&#34;Īë¥¼&#34;:8135,&#34;ìĿĦê²ĥ&#34;:8136,&#34;ìļĶíķľ&#34;:8137,&#34;ĠëĤ¬ëĭ¤&#34;:8138,&#34;ìĺ·&#34;:8139,&#34;ìĺģíĻĶê´Ģ&#34;:8140,&#34;ìłĲìĿ´ëĤĺ&#34;:8141,&#34;ê±°ê°ĻìĿĢëį°&#34;:8142,&#34;ìłķë¯¼&#34;:8143,&#34;ì§Ħì°½&#34;:8144,&#34;Ġëĭ¤ëħĢ&#34;:8145,&#34;ìłĦìĿĺ&#34;:8146,&#34;Ġìĸ´ì§¸&#34;:8147,&#34;ëĵľë§Į&#34;:8148,&#34;ĠìĬ¤íĥ&#34;:8149,&#34;ĠìĬ¤íĭ°&#34;:8150,&#34;ĠëıĪë²&#34;:8151,&#34;Ġì°¢&#34;:8152,&#34;íŀĪìĸ´ë¡ľ&#34;:8153,&#34;ëŁ¬ì§Ģ&#34;:8154,&#34;ìħ°&#34;:8155,&#34;ìĨĮìŀ¥&#34;:8156,&#34;Ġë§ĲìĿ¸ê°Ģ&#34;:8157,&#34;ìĭ¬ìĹĲ&#34;:8158,&#34;ĠëĵľëĿ¼ë§Īëĭ¤&#34;:8159,&#34;Ġë°°ìļ°ëĵ¤ìĿĢ&#34;:8160,&#34;ë¯Ģë¡ľ&#34;:8161,&#34;Ġìļ°ìĹ°&#34;:8162,&#34;ì¡°íĺĦ&#34;:8163,&#34;ë°Ķëĭ¥&#34;:8164,&#34;ĠìĹŃê²¨ìļ´&#34;:8165,&#34;Ġìĭľê°ĦìķĦê¹Ŀëĭ¤&#34;:8166,&#34;ëĬĲëĥĲ&#34;:8167,&#34;Ġë¶Īê³¼&#34;:8168,&#34;Ġë©ĭì§Ģê²Į&#34;:8169,&#34;ì£¤&#34;:8170,&#34;ìĿ´ê±°ë³´ê³ł&#34;:8171,&#34;Ġë°°ê¸ī&#34;:8172,&#34;íĺķìĿ´&#34;:8173,&#34;Ġì°¨ìĿ´&#34;:8174,&#34;Ġë¹łë¥¸&#34;:8175,&#34;ĠíķĦìļĶê°Ģ&#34;:8176,&#34;ĠìĺģìĥģìĿ´&#34;:8177,&#34;ìĻ¸ìĿĺ&#34;:8178,&#34;ê¸°ëĮĢë¥¼&#34;:8179,&#34;Ġë²Ħë¦°&#34;:8180,&#34;ãĦ·ãĦ·ãĦ·&#34;:8181,&#34;ĠëĤ«ê²łëĭ¤&#34;:8182,&#34;Ġë¹łìł¸ìĦľ&#34;:8183,&#34;ĠíĦ°ë¯¸ëĦ¤ìĿ´íĦ°&#34;:8184,&#34;ê·¹ìŀ¥íĮĲ&#34;:8185,&#34;ĠìĥģíĻ©ìĿĦ&#34;:8186,&#34;ĠìĪ¨ê²¨ì§Ħ&#34;:8187,&#34;ĠìĻĶëĭ¤&#34;:8188,&#34;ĠìĤ¬ë¬´&#34;:8189,&#34;ĠìĿĺëıĦê°Ģ&#34;:8190,&#34;íĶĦë¡ľê·¸ëŀ¨&#34;:8191,&#34;ë²Īë´¤&#34;:8192,&#34;Ġì¿µ&#34;:8193,&#34;Ġëľ¬ê¸ĪìĹĨìĿ´&#34;:8194,&#34;Ġë¶Ģëª¨ëĭĺ&#34;:8195,&#34;Ġêµ¬ë¶Ħ&#34;:8196,&#34;ë±ħ&#34;:8197,&#34;Ġìĵ¸ëį°ìĹĨëĬĶ&#34;:8198,&#34;ì«Į&#34;:8199,&#34;ĠíĦ¸&#34;:8200,&#34;am&#34;:8201,&#34;ig&#34;:8202,&#34;le&#34;:8203,&#34;ê°ĢìľĦ&#34;:8204,&#34;ëıĦìķĪ&#34;:8205,&#34;ĠìĿ´ìĿĢ&#34;:8206,&#34;ĠìĿ´íĨłë¡Ŀ&#34;:8207,&#34;ìĸ´ìĬ¤&#34;:8208,&#34;ëĵ¤ìĹ¬&#34;:8209,&#34;ìĬ¤ì¹´&#34;:8210,&#34;ĠìĤ½&#34;:8211,&#34;ĠìĤŃìłľ&#34;:8212,&#34;ìĤŃ&#34;:8213,&#34;ĠëĤĺëłĪìĿ´ìħĺ&#34;:8214,&#34;Ġíķĺê¸°&#34;:8215,&#34;Ġìłķëĭ¹&#34;:8216,&#34;Ġë¶ķ&#34;:8217,&#34;íķłìĪĺê°Ģ&#34;:8218,&#34;Ġì§Ħë¦¬&#34;:8219,&#34;ĠìµľìĨĮ&#34;:8220,&#34;ê²ĥìĹĲ&#34;:8221,&#34;íĸĪëĭ¤ê°Ģ&#34;:8222,&#34;ìĤ¬ëıĦ&#34;:8223,&#34;ĠëŃ¥ë¯¸&#34;:8224,&#34;ĠëĵľëĦ¤ìļĶ&#34;:8225,&#34;Ġê°Ļìķĺëĭ¤&#34;:8226,&#34;ë¹Ħíķ´&#34;:8227,&#34;Ġë³¼ëł¤ê³ł&#34;:8228,&#34;ĠìľłëŁ½&#34;:8229,&#34;ĠíĸĪëįĶëĭĪ&#34;:8230,&#34;ìĺģìĽĲ&#34;:8231,&#34;Ġìķłíĭĭ&#34;:8232,&#34;Ġìĭ¶ìĬµëĭĪëĭ¤&#34;:8233,&#34;ìŀ¬ë°ĮìĬµëĭĪëĭ¤&#34;:8234,&#34;íı¬ìĿ¸íĬ¸&#34;:8235,&#34;Ġë³´ëĬĶê±°&#34;:8236,&#34;Ġê¸°ëĮĢìĿ´ìĥģ&#34;:8237,&#34;Ġê°ķê°Ħ&#34;:8238,&#34;ĠëıĻìĺģìĥģ&#34;:8239,&#34;ĠìķĦê¹ĮìĽĮìĦľ&#34;:8240,&#34;Ġê´ľì°®ìķĺìĿĮ&#34;:8241,&#34;ê¹Ģê¸°ëįķ&#34;:8242,&#34;Ġëĭ¨ì²´&#34;:8243,&#34;Ġë¹łì§ĢëĬĶ&#34;:8244,&#34;ĠìĺģìĥģëıĦ&#34;:8245,&#34;íķľíħĮëĬĶ&#34;:8246,&#34;ì´Īëĵ±íķĻìĥĿ&#34;:8247,&#34;Ġê°ľë´īíķľ&#34;:8248,&#34;ĠìĨĲë°ľ&#34;:8249,&#34;ĠíĿ¥ë¯¸ë¥¼&#34;:8250,&#34;ĠãħĦ&#34;:8251,&#34;Ġë¶Ģë¶ĦìĿĢ&#34;:8252,&#34;ĠìĭľëĤĺë¦¬ìĺ¤ê°Ģ&#34;:8253,&#34;ĠìŀĪìĹĪê³ł&#34;:8254,&#34;ĠëıĮìķĦë³´ê²Į&#34;:8255,&#34;ê²ĥê°ĻëĦ¤ìļĶ&#34;:8256,&#34;Ġëĺĳê°ĻìĿ´&#34;:8257,&#34;ìŀĳê°Ģê°Ģ&#34;:8258,&#34;ĠëĬ¥ê°Ģ&#34;:8259,&#34;ì²¨ìĹĶ&#34;:8260,&#34;Ġë§ĪìĿ´ëĦĪìĬ¤&#34;:8261,&#34;ĠìŀĲê·¹ìłģìĿ¸&#34;:8262,&#34;Ġì§Ħì§Ģíķľ&#34;:8263,&#34;Ġíģ¬ë¦¬ìĬ¤ë§ĪìĬ¤&#34;:8264,&#34;ĠëĪĦêµ°ì§Ģ&#34;:8265,&#34;ìŀĪëįĺëį°&#34;:8266,&#34;ĠìĦłìĥĿëĭĺ&#34;:8267,&#34;Ġìĸ´ëł¸ìĿĦ&#34;:8268,&#34;ë¶ĦìľĦê¸°&#34;:8269,&#34;Ġíİ¼ì³Ĳ&#34;:8270,&#34;Ġë°Ķíĥķìľ¼ë¡ľ&#34;:8271,&#34;ed&#34;:8272,&#34;Ġis&#34;:8273,&#34;ì§Ģê¸Īë³´&#34;:8274,&#34;ĠìķĦíĶĦëĭ¤&#34;:8275,&#34;Ġë³´ìĿ´ì§Ģ&#34;:8276,&#34;Ġê·¸ìķ¼ë§Ĳë¡ľ&#34;:8277,&#34;ìĬ¤íĶ¼&#34;:8278,&#34;ìŀĲìĿ¸&#34;:8279,&#34;ìĹĨëĬĶëį°&#34;:8280,&#34;ĠíķĺíĴĪ&#34;:8281,&#34;ìĪĺìĦł&#34;:8282,&#34;ë²ħ&#34;:8283,&#34;ìĥģíĥľ&#34;:8284,&#34;Ġìłķìļ°&#34;:8285,&#34;Ġê²¬ë&#34;:8286,&#34;íķĺê³łìĭ¶ìĿĢ&#34;:8287,&#34;Ġìĸ´ê±°ì§Ģ&#34;:8288,&#34;ĠìĦŃ&#34;:8289,&#34;ê³µì£¼&#34;:8290,&#34;ìĹ¬íĥľ&#34;:8291,&#34;ĠìĹ°ê¸°íķĺëĬĶ&#34;:8292,&#34;ĠìĪĺë©´&#34;:8293,&#34;êµ¬ìĻĢ&#34;:8294,&#34;ìĭ¤íĮ¨&#34;:8295,&#34;Ġë§ĲíĪ¬&#34;:8296,&#34;íĥĢì¿ł&#34;:8297,&#34;ìĺĢì§Ģ&#34;:8298,&#34;Ġíĺĳ&#34;:8299,&#34;íĬ¸ëĬĶ&#34;:8300,&#34;Ġìĺ¤ëĬĶ&#34;:8301,&#34;Ġë¹Ħíĺ¸ê°Ĳ&#34;:8302,&#34;ëģĹ&#34;:8303,&#34;ìĭ¬ìĭ¬&#34;:8304,&#34;Ġë¶ĢìŀĲìĹ°&#34;:8305,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪìĬµëĭĪëĭ¤&#34;:8306,&#34;Ġê³łìĸĳìĿ´&#34;:8307,&#34;íħľ&#34;:8308,&#34;ë§Īëŀĳ&#34;:8309,&#34;ì¶Ķìĸ´&#34;:8310,&#34;ìĶ¨ëıĦ&#34;:8311,&#34;ĠíĻĶìŀ¥ìĭ¤&#34;:8312,&#34;íķĺëĤĺíķĺëĤĺ&#34;:8313,&#34;ìŀĺìĥĿ&#34;:8314,&#34;íķĺê¸°ìĹĲ&#34;:8315,&#34;Ġì¡¸ìĹħ&#34;:8316,&#34;Ġì¶©ìĭ¤&#34;:8317,&#34;ĠìłĦê°ľëĬĶ&#34;:8318,&#34;ì¡ĮìĬµëĭĪëĭ¤&#34;:8319,&#34;ëŀľìĬ¤&#34;:8320,&#34;Ġë¨¹ì¹ł&#34;:8321,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:8322,&#34;Ġê°Ģì¡±ìĿĺ&#34;:8323,&#34;ĠêµŃê°Ģ&#34;:8324,&#34;ĠìĿ¸ìĥģìłģìĿ¸&#34;:8325,&#34;ĠìĦ¸ìĥģìĿ´&#34;:8326,&#34;ĠìĹĶëĶ©ìĿ´&#34;:8327,&#34;2014&#34;:8328,&#34;ê°ĶëĬĶëį°&#34;:8329,&#34;ĠìŀĦíĮ©íĬ¸&#34;:8330,&#34;ìĭľëĮĢìĿĺ&#34;:8331,&#34;ĠêµĲíĽĪëıĦ&#34;:8332,&#34;ĠìłĦì²´ìłģìĿ¸&#34;:8333,&#34;ìķŀìĹĲ&#34;:8334,&#34;Ġê°ľê·¸ë§¨&#34;:8335,&#34;ĠìŀĺìĥĿê²¼&#34;:8336,&#34;Ġë¹Īìķ½&#34;:8337,&#34;ĠìĺģìĽĲíķľ&#34;:8338,&#34;ëĮĢëĭ¨íķľ&#34;:8339,&#34;Ġëįĺìł¸&#34;:8340,&#34;Ġë°ĿíĺĢ&#34;:8341,&#34;ĠíĴĭíĴĭíķľ&#34;:8342,&#34;ìłĦë¬¸ê°Ģ&#34;:8343,&#34;op&#34;:8344,&#34;§íĮħ&#34;:8345,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:8346,&#34;ħķ&#34;:8347,&#34;ĵ¯&#34;:8348,&#34;ëĭ¬ë¦¬&#34;:8349,&#34;ìĿ´ê²ĥìĿ´&#34;:8350,&#34;ëıĦíķľ&#34;:8351,&#34;ë¦¬ëŀĳ&#34;:8352,&#34;ĠìĺģíĻĶì§Ģ&#34;:8353,&#34;ĠìĺģíĻĶìĿ¼&#34;:8354,&#34;ëĦĲëıĮ&#34;:8355,&#34;Ġë³´ëł¤&#34;:8356,&#34;ìĭľì§Ģ&#34;:8357,&#34;ìĭľëĦ¤&#34;:8358,&#34;ìľ¼ìħ¨&#34;:8359,&#34;ĠëĤĺìĹĲê²Ĳ&#34;:8360,&#34;Ġíķĺëĭ¤ëĭĪ&#34;:8361,&#34;Ġì¢ĭëĦ¤&#34;:8362,&#34;ê°ĦíŀĪ&#34;:8363,&#34;Ġìĺ³&#34;:8364,&#34;Ġê°ĢìĪĺ&#34;:8365,&#34;Ġê°ĢíŀĪ&#34;:8366,&#34;ì¹ĺë©´&#34;:8367,&#34;ĠëĮĢê²°&#34;:8368,&#34;ìĿ¼ìĿĢ&#34;:8369,&#34;ëŀĺëĵľ&#34;:8370,&#34;ìĤ¬ê³ł&#34;:8371,&#34;Ġë¬´ê²ģ&#34;:8372,&#34;ìħľ&#34;:8373,&#34;ĠìķĪëĤĺìĺ¨ëĭ¤&#34;:8374,&#34;Ġë§Īì°¬&#34;:8375,&#34;Ġëª»ë§Įëĵ¤&#34;:8376,&#34;ëĤľíķ´&#34;:8377,&#34;ë³¸ì§Ģ&#34;:8378,&#34;Ġê²ĥìĿ¸ê°Ģ&#34;:8379,&#34;ĠìĿ´ëŁ°ê±´&#34;:8380,&#34;ĠëģĿëıĦ&#34;:8381,&#34;ëıĻê·¼&#34;:8382,&#34;ĠìĤ¬ëŀĳíķĺê³ł&#34;:8383,&#34;Ġë³´ëĬĶê²ĥ&#34;:8384,&#34;ëĸ¼&#34;:8385,&#34;ìŀ¼ìŀĩ&#34;:8386,&#34;êµ°ëĮĢ&#34;:8387,&#34;ĠíķłëķĮ&#34;:8388,&#34;Ġì£½ìĿ´&#34;:8389,&#34;ìĺĪìĥģ&#34;:8390,&#34;ê·¹íŀĪ&#34;:8391,&#34;ë¶ĢíĦ°ëĬĶ&#34;:8392,&#34;ĠìĽĲì¡°&#34;:8393,&#34;ĠíķľêµŃìĿ¸&#34;:8394,&#34;Ġë§Įëĵłê±°&#34;:8395,&#34;Ġìŀ¼ìŀĩ&#34;:8396,&#34;ì§ĳìĹĲìĦľ&#34;:8397,&#34;ĠìĿ´ìĥģìĿĢ&#34;:8398,&#34;ĠëĤ®ëĦ¤ìļĶ&#34;:8399,&#34;ĠìłģìłĪíķľ&#34;:8400,&#34;ìŀ¡ëĬĶ&#34;:8401,&#34;íĿ¬ìĿĺ&#34;:8402,&#34;Ġìŀ¬ê°ľë´ī&#34;:8403,&#34;ëķ¡&#34;:8404,&#34;Ġëĭ´ê³ł&#34;:8405,&#34;êº¼ìķ¼&#34;:8406,&#34;Ġë§īìŀ¥ëĵľëĿ¼ë§Ī&#34;:8407,&#34;ëįķë¶ĦìĹĲ&#34;:8408,&#34;Ġ2014&#34;:8409,&#34;ì¹ĺê³łëĬĶ&#34;:8410,&#34;ĠìķłëĭĪëĬĶ&#34;:8411,&#34;Ġì²¨ìĿ´ëĭ¤&#34;:8412,&#34;ĠíĻĺê²½&#34;:8413,&#34;ëĨĢëĵľ&#34;:8414,&#34;Ġìľłì¾Įíķĺê²Į&#34;:8415,&#34;ìĸµì§Ģë¡ľ&#34;:8416,&#34;ìłľìŀĳë¹Ħ&#34;:8417,&#34;ëĨ¨ëĦ¤&#34;:8418,&#34;ë¬´ìĦľìļ´&#34;:8419,&#34;Ġê²°ë¡łìĿĢ&#34;:8420,&#34;ĠìĮ©&#34;:8421,&#34;ĠìĬ¤íı¬ì¸ł&#34;:8422,&#34;.-&#34;:8423,&#34;Dë¡ľ&#34;:8424,&#34;²ł&#34;:8425,&#34;ìµĿìĺ¤&#34;:8426,&#34;ĠR&#34;:8427,&#34;ìŀĥ&#34;:8428,&#34;ìĹĲíļ¨&#34;:8429,&#34;ìĦľë¡ľ&#34;:8430,&#34;ìĸ´ìĸ´&#34;:8431,&#34;ë§Įíķĺê³ł&#34;:8432,&#34;ìĺ¬ë¦¬ë&#34;:8433,&#34;ìĭľìĬ¤&#34;:8434,&#34;ì£¼íĸī&#34;:8435,&#34;ĠìŀĪìľ¼ëĤĺ&#34;:8436,&#34;ĠìĹ°ìĥģ&#34;:8437,&#34;ê·¸ëĭ¤ì§Ģ&#34;:8438,&#34;Ġíķľë²ĪëįĶ&#34;:8439,&#34;ìĨĮë¦¬ê°Ģ&#34;:8440,&#34;Ġëª»ë³¸&#34;:8441,&#34;ë¬´ì¡°ê±´&#34;:8442,&#34;Ġê²ĥëĵ¤ìĿ´&#34;:8443,&#34;Ġë§Įëĵ¤ìĪĺ&#34;:8444,&#34;ĠëĵľëĿ¼ë§Īì¤ĳ&#34;:8445,&#34;ë¦¬ë³´&#34;:8446,&#34;ë¦¬ëĥĲ&#34;:8447,&#34;ĠëĲĺëĬĶëį°&#34;:8448,&#34;Ġì¡°ëĭĪëİģ&#34;:8449,&#34;Ġìļ°ë¢°ë§¤&#34;:8450,&#34;íĤ¥&#34;:8451,&#34;Ġìŀ¬ë¯¸ìŀĪìĸ´&#34;:8452,&#34;ëĤ¨ëħĢ&#34;:8453,&#34;ì½ĶëĤľ&#34;:8454,&#34;Ġì¶Ķë¦¬&#34;:8455,&#34;ĠíĸĪìľ¼ëĤĺ&#34;:8456,&#34;Ġë¶ĪìķĪ&#34;:8457,&#34;Ġë©ĭìŀĪê³ł&#34;:8458,&#34;ì¹ľêµ¬ëŀĳ&#34;:8459,&#34;ĠëıĦëĳĳ&#34;:8460,&#34;ĠíıīìĿĦ&#34;:8461,&#34;ëª»íķĺê³ł&#34;:8462,&#34;ĠìĿ´ê±°ë³´ëĭ¨&#34;:8463,&#34;Ġìŀ¼ìŀĪëĬĶ&#34;:8464,&#34;Ġìļ¸ìĹĪìĸ´ìļĶ&#34;:8465,&#34;ĠìĹĲìĦľ&#34;:8466,&#34;Ġê°Ģìŀ¥íķľ&#34;:8467,&#34;ìĤ¬ë¯¸&#34;:8468,&#34;Ġê³¼ëĮĢ&#34;:8469,&#34;ëĤĺìĺ¤ëĦ¤&#34;:8470,&#34;ëŀľëĵľ&#34;:8471,&#34;Ġíı¬ìĿ¸íĬ¸&#34;:8472,&#34;ĠìķĦë¬´íĬ¼&#34;:8473,&#34;ë§ŀëĬĶ&#34;:8474,&#34;Ġëĭ´ëĭ´&#34;:8475,&#34;ì°½ìłķ&#34;:8476,&#34;ìºĲë¦¬&#34;:8477,&#34;Ġì§Īë¦¬&#34;:8478,&#34;Ġíĺķìłľ&#34;:8479,&#34;Ġíĸīë³µíķĺê²Į&#34;:8480,&#34;ìĿ¼ë³¸ìĺģíĻĶ&#34;:8481,&#34;Ġëĭµëĭµíķĺëĭ¤&#34;:8482,&#34;Ġê¸Ģê³ł&#34;:8483,&#34;ĠìĺģíĻĶëĿ¼ì§Ģë§Į&#34;:8484,&#34;ĠìĸĳìķĦì¹ĺ&#34;:8485,&#34;ĠìĭľëĮĢìĿĺ&#34;:8486,&#34;Ġë¶ĪìĮįíķľ&#34;:8487,&#34;Ġëĭ¨ìĪľíŀĪ&#34;:8488,&#34;ìºħ&#34;:8489,&#34;Ġë©ĶìĦ¸ì§Ģ&#34;:8490,&#34;Ġì°¸ìĭłíķľ&#34;:8491,&#34;ĠëĤĺë¨¸ì§ĢëĬĶ&#34;:8492,&#34;Ġê³µíı¬ë¥¼&#34;:8493,&#34;ĠíıĲì§Ģ&#34;:8494,&#34;ĠìıŁìķĦ&#34;:8495,&#34;ìĥĪë¡Ŀ&#34;:8496,&#34;Ġìĸ´ëĳĲìļ´&#34;:8497,&#34;ì¨Įëĵł&#34;:8498,&#34;ĠìĽ°ë©ĶìĿ´ëĵľ&#34;:8499,&#34;ĠìŀĲëıĻì°¨&#34;:8500,&#34;ãĤ&#34;:8501,&#34;ķħ&#34;:8502,&#34;ìĹĲëĭ¤&#34;:8503,&#34;ìĹĲëĭ¤ê°Ģ&#34;:8504,&#34;ê¸°ìĪł&#34;:8505,&#34;ĠìĿ´ë»&#34;:8506,&#34;...!&#34;:8507,&#34;ë©Ģ&#34;:8508,&#34;ìĿ¸ìĺģíĻĶ&#34;:8509,&#34;ìĿ¸ê±¸&#34;:8510,&#34;ìľ¼ëŁ¬&#34;:8511,&#34;ëĮĢê³ł&#34;:8512,&#34;Ġì¢ĭìľ¼ëĤĺ&#34;:8513,&#34;ì£¼ìĦ±ì¹ĺ&#34;:8514,&#34;Ġíķľíİ¸ìĿĺ&#34;:8515,&#34;ëĵľëĦ¤ìļĶ&#34;:8516,&#34;ëĵľìĽĮ&#34;:8517,&#34;Ġìĭľì¼ľ&#34;:8518,&#34;ĠìĬ¤íħĿ&#34;:8519,&#34;ĠìµľìĨĮíķľ&#34;:8520,&#34;Ġëª¨ìķĦ&#34;:8521,&#34;Ġë§ĲìĿĢ&#34;:8522,&#34;ëħĦê°Ħ&#34;:8523,&#34;Ġê²ĥë§Į&#34;:8524,&#34;Ġë³¼ê¹Į&#34;:8525,&#34;Ġë¶ĢíĻľ&#34;:8526,&#34;ì¢ĭìķĺìĸ´ìļĶ&#34;:8527,&#34;ĠëĵľëĿ¼ë§ĪìĹĲ&#34;:8528,&#34;ĠëģĿëĤĺìĦľ&#34;:8529,&#34;ëıĻìĿ´&#34;:8530,&#34;ĠëŃĲìŀĦ&#34;:8531,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪìĿĮ&#34;:8532,&#34;Ġìĭ¶ëĭ¤ë©´&#34;:8533,&#34;Ġê³łë¬¸&#34;:8534,&#34;ëŁ½ì§Ģ&#34;:8535,&#34;ê´ĢìĿĦ&#34;:8536,&#34;Ġìĭľê°ĦëķĮ&#34;:8537,&#34;ĠëķĮëĬĶ&#34;:8538,&#34;ĠìĺĪì¸¡&#34;:8539,&#34;ì°¨íĶ¼&#34;:8540,&#34;íĭ°ê°Ģ&#34;:8541,&#34;ĠíĸĪìĬµëĭĪëĭ¤&#34;:8542,&#34;Ġë©ĭì§Ģ&#34;:8543,&#34;Ġê´Ģìĭ¬ìĿ´&#34;:8544,&#34;ìłķëıĦìĿĺ&#34;:8545,&#34;ĠëĶ°ìľĦ&#34;:8546,&#34;ë§¤ìĿ´ìħĺ&#34;:8547,&#34;Ġë¡ľë§Ŀ&#34;:8548,&#34;ìŀĺë§Įëĵł&#34;:8549,&#34;ëĭĪë²Ħ&#34;:8550,&#34;Ġëĭ¨ìĸ´&#34;:8551,&#34;ì°¸ëĤĺ&#34;:8552,&#34;ë°°ëĬĶ&#34;:8553,&#34;ĠëĤ¨ìŀĲì£¼ìĿ¸ê³µ&#34;:8554,&#34;ĠìĿ¸ê°ĦìĿĢ&#34;:8555,&#34;Ġìĭ¤ë§ĿìĿ´&#34;:8556,&#34;ĠíĤ¹&#34;:8557,&#34;Ġëª¨ìĬµìĹĲ&#34;:8558,&#34;ĠìºĲë¦ŃíĦ°ëĵ¤&#34;:8559,&#34;ĠìºĲë¦ŃíĦ°ë¥¼&#34;:8560,&#34;ãĦ·ãĦ·ãĦ·ãĦ·&#34;:8561,&#34;ĠìĨĲê°ĢëĿ½&#34;:8562,&#34;ĠëĴ¤ì£½ë°ķì£½&#34;:8563,&#34;Ġìŀĳê°ĢìĿĺ&#34;:8564,&#34;ëħ¸ìŀ¼ëħ¸ìŀ¼&#34;:8565,&#34;ëĪĪìĿ´&#34;:8566,&#34;ĠìłĪë§Ŀ&#34;:8567,&#34;íĮ¬ìĿ´&#34;:8568,&#34;íĻķìĭ¤íŀĪ&#34;:8569,&#34;ĠíĴĢìĸ´ëĤ¸&#34;:8570,&#34;ëĤĺìĻĶìľ¼ë©´&#34;:8571,&#34;Ġíķµëħ¸ìŀ¼&#34;:8572,&#34;íķĻëħĦëķĮ&#34;:8573,&#34;ĠìĻ¸ê³ĦìĿ¸&#34;:8574,&#34;ìĬ¤íĭ°ë¸Ĳ&#34;:8575,&#34;ê´ľíŀĪ&#34;:8576,&#34;ìłĬìĿĢ&#34;:8577,&#34;&gt;&gt;&#34;:8578,&#34;´ëĵľ&#34;:8579,&#34;Ġx&#34;:8580,&#34;ìĿ´ìłķ&#34;:8581,&#34;ê³½&#34;:8582,&#34;ê³łìĭ¶ëĭ¤&#34;:8583,&#34;íĻĶëĬĶ&#34;:8584,&#34;ĠìŀĪëĥĲ&#34;:8585,&#34;ëıĦìłĢíŀĪ&#34;:8586,&#34;íķľê°Ģ&#34;:8587,&#34;ê¸°ìŀĲ&#34;:8588,&#34;ê¸°ë³´ëĭ¨&#34;:8589,&#34;ĠìĿ´ë§Įíķľ&#34;:8590,&#34;ĠìĿ´ìģ¨&#34;:8591,&#34;ìĸ´ëĤĺìĦľ&#34;:8592,&#34;ë§Įëĵ¬&#34;:8593,&#34;Ġìłĸ&#34;:8594,&#34;ëĵ¤ëŁ¬&#34;:8595,&#34;ìķĦëĵ¤ìĿ´&#34;:8596,&#34;Ġë³´ìķĦëıĦ&#34;:8597,&#34;Ġê·¸ëĵ¤ìĿ´&#34;:8598,&#34;Ġê·¸ëıĻìķĪ&#34;:8599,&#34;ìĬ¤ëłĪ&#34;:8600,&#34;ìĬ¤íĦ´&#34;:8601,&#34;ë§Ĳíķł&#34;:8602,&#34;ì¤įëĭĪëĭ¤&#34;:8603,&#34;Ġëĭ¤ë³´ê³ł&#34;:8604,&#34;ìĪĺìłķ&#34;:8605,&#34;ìŀ¥ìĿĢ&#34;:8606,&#34;ĠìĬµ&#34;:8607,&#34;Ġì§ĢíĤ¤&#34;:8608,&#34;ĠëĤ´ëıĪ&#34;:8609,&#34;Ġê¸°íĥĢ&#34;:8610,&#34;ĠìķĬìĬµëĭĪëĭ¤&#34;:8611,&#34;Ġê±·&#34;:8612,&#34;ìĨĮìĭľ&#34;:8613,&#34;Ġìµľê³łëĿ¼ê³ł&#34;:8614,&#34;Ġê°ľëĤĺ&#34;:8615,&#34;ĠìŀĲë³¸&#34;:8616,&#34;Ġë³¸ê²ĥ&#34;:8617,&#34;ìľłë¨¸&#34;:8618,&#34;ìµľê°ķ&#34;:8619,&#34;Ġê³µì§ľë¡ľ&#34;:8620,&#34;ĠëģĿìĿĦ&#34;:8621,&#34;Ġì¡°ì¹´&#34;:8622,&#34;Ġíķ´ìķ¼ì§Ģ&#34;:8623,&#34;ë¬¼ë¡ľ&#34;:8624,&#34;Ġì¹ł&#34;:8625,&#34;ëª¨ë¥¼&#34;:8626,&#34;ĠìĽĥê¸°ëĬĶ&#34;:8627,&#34;Ġìĥģì§ķ&#34;:8628,&#34;íĶĦëłĪ&#34;:8629,&#34;ĠíķłìķĦë²Ħì§Ģ&#34;:8630,&#34;ì½Ķëĵľ&#34;:8631,&#34;Ġê¸°ëĮĢìķĪíķĺê³ł&#34;:8632,&#34;ì²ľìĽĲ&#34;:8633,&#34;ĠìĹ°ì¶ľëł¥ìĿ´&#34;:8634,&#34;ĠíĸĪìĿĦê¹Į&#34;:8635,&#34;ìĤ´ëķĮ&#34;:8636,&#34;Ġë³´ìĹ¬ì¤Ħ&#34;:8637,&#34;íĽ¨&#34;:8638,&#34;íķĺê¸°ê°Ģ&#34;:8639,&#34;Ġê¼Ńë³´ìĦ¸ìļĶ&#34;:8640,&#34;ìĺĢëĭ¤ë©´&#34;:8641,&#34;Ġêµ¬ìĦ±ëıĦ&#34;:8642,&#34;ê°ĲëıĻìłģ&#34;:8643,&#34;ê°ĲëıĻê³¼&#34;:8644,&#34;Ġê±¸ê¹Į&#34;:8645,&#34;ĠìĨįìķĺëĭ¤&#34;:8646,&#34;Ġìľłì¹ĺíķ´ìĦľ&#34;:8647,&#34;Ġãħİãħİãħİãħİ&#34;:8648,&#34;Ġë°ĺìłĦìĹĲ&#34;:8649,&#34;ĠìĿ¸ê°Ħëĵ¤&#34;:8650,&#34;ìŀ¥ë©´ìĹĲìĦľ&#34;:8651,&#34;ì£½ìĿĮ&#34;:8652,&#34;ìĹŃìĭľëĤĺ&#34;:8653,&#34;ìķĦìĿ´ëıĮ&#34;:8654,&#34;ë¸Įë¦¬&#34;:8655,&#34;Ġë²Ħëł¤&#34;:8656,&#34;ë»ĶíĸĪëĭ¤&#34;:8657,&#34;Ġê°Ģì¡±ìĺģíĻĶ&#34;:8658,&#34;Ġëª¨ë¥´ê²łëĦ¤&#34;:8659,&#34;ëıĮëł¤&#34;:8660,&#34;ĠìķŀìĹĲ&#34;:8661,&#34;ê²ģëĤĺ&#34;:8662,&#34;ëĨĪìĿ´&#34;:8663,&#34;ĠíĮĲíĥĢ&#34;:8664,&#34;ìłĲëĮĢê°Ģ&#34;:8665,&#34;Ġê¸¸ìĿĦ&#34;:8666,&#34;ĠìĿ´ëĶ´ê±¸&#34;:8667,&#34;íķĦë¦Ħ&#34;:8668,&#34;íķĺìŀĲë©´&#34;:8669,&#34;ĠëĤ®ìĿĢì§Ģ&#34;:8670,&#34;Ġë§ĮëĵľëĦ¤&#34;:8671,&#34;Ġìŀĺë§Įëĵ¤ìĹĪëĭ¤&#34;:8672,&#34;ë²Īë´Ĳ&#34;:8673,&#34;ĠíĭĢë¦¼&#34;:8674,&#34;Ġì°½íĶ¼&#34;:8675,&#34;ĠìķĪë§ŀëĬĶ&#34;:8676,&#34;ĠìĽĮëĤĻ&#34;:8677,&#34;ë§¤ëł¥ìłģìĿ¸&#34;:8678,&#34;Ġìī¬ìļ´&#34;:8679,&#34;Ġíĥľìĸ´ëĤĺìĦľ&#34;:8680,&#34;ìĻĦë²½íķľ&#34;:8681,&#34;ëĤ´ìĥĿìĹĲ&#34;:8682,&#34;ìķĪëĲľëĭ¤&#34;:8683,&#34;ĠìłĲìĪĺì¤Ģê²ĥëĵ¤&#34;:8684,&#34;Ġê°Ŀê´Ģ&#34;:8685,&#34;²ķ&#34;:8686,&#34;ıëĭ¤&#34;:8687,&#34;ìĿ´ë¯¼&#34;:8688,&#34;ìĿ´ìĹĪëįĺ&#34;:8689,&#34;ìĹ£&#34;:8690,&#34;ê°Ģë©°&#34;:8691,&#34;ĠìĿ´ëĿ¼ëĬĶ&#34;:8692,&#34;ìĸ´ë¨¸ëĭĪ&#34;:8693,&#34;...(&#34;:8694,&#34;Ġê°ĵ&#34;:8695,&#34;êµ¬ë¦¬&#34;:8696,&#34;Ġë³´ëĿ¼&#34;:8697,&#34;Ġì¢ĭì§Ģ&#34;:8698,&#34;ì£¼ëĬĶëį°&#34;:8699,&#34;ì§ĦìĿĺ&#34;:8700,&#34;Ġíķľëĭ¤ê³ł&#34;:8701,&#34;ìķĺëĤĺ&#34;:8702,&#34;íķłë§ĲìĿ´&#34;:8703,&#34;Ġëį®&#34;:8704,&#34;êµ¬ëĬĶ&#34;:8705,&#34;Ġë¬´ìĭĿ&#34;:8706,&#34;ë¶ĦëıĦ&#34;:8707,&#34;íİ¸ê¹Įì§Ģ&#34;:8708,&#34;íĬ¸ë¦Ń&#34;:8709,&#34;Ġìŀ¥êµŃìĺģ&#34;:8710,&#34;Ġë²ħ&#34;:8711,&#34;Ġë¶Ģìłķ&#34;:8712,&#34;ĠìķĪë´ĲëıĦ&#34;:8713,&#34;ëªħìĿĢ&#34;:8714,&#34;ĠëģĿëĤł&#34;:8715,&#34;ĠëĤ´ìļ©ìłĦê°ľ&#34;:8716,&#34;ĠëĲĺëıĮìķĦ&#34;:8717,&#34;ëıĻê±´&#34;:8718,&#34;ì¡°íıŃ&#34;:8719,&#34;ëª¨ìĸĳ&#34;:8720,&#34;Ġëĭ¤ìĭľê¸Ī&#34;:8721,&#34;ëŃī&#34;:8722,&#34;Ġëĵ¤ê²Į&#34;:8723,&#34;Ġë§Īì§Ģë§īìĿ´&#34;:8724,&#34;Ġë³´ìĹ¬ì¤¬&#34;:8725,&#34;Ġë°°ê¼½&#34;:8726,&#34;íķĺëĤĺìļĶ&#34;:8727,&#34;ìĽłëĬĶëį°&#34;:8728,&#34;ãħľãħľãħľ&#34;:8729,&#34;Ġë§ŀëĤĺ&#34;:8730,&#34;ĠëĪĦêµ¬ë&#34;:8731,&#34;ìĤ¬ëŀĳíķĺëĬĶ&#34;:8732,&#34;ĠìķĦëĭĪëĿ¼ë©´&#34;:8733,&#34;íĿ¬ìĦł&#34;:8734,&#34;íĻ©ëĭ¹&#34;:8735,&#34;Ġë¨¸ë¦¿&#34;:8736,&#34;ìĿ´ëĿ¼ëĬĶê²Į&#34;:8737,&#34;ì²ĺìĿĮìĹĲ&#34;:8738,&#34;Ġê°Ģì¡±ìĿ´&#34;:8739,&#34;Ġê³¤&#34;:8740,&#34;ĠìŀłìĿĦ&#34;:8741,&#34;ĠëĮĢìĤ¬ëıĦ&#34;:8742,&#34;ëįķìĹĲ&#34;:8743,&#34;Ġìłľëª©ìĿĦ&#34;:8744,&#34;ĠëĦ¤íĭ°ì¦Į&#34;:8745,&#34;Ġê¸ĢìĿĦ&#34;:8746,&#34;ì°¬ìļ±&#34;:8747,&#34;Ġê·ĢìĹ¬ìĽĮìļĶ&#34;:8748,&#34;Ġì¶©ë¶Ħíķľ&#34;:8749,&#34;Ġth&#34;:8750,&#34;ĠìĬ¤íĨłë¦¬ë¡ľ&#34;:8751,&#34;Ġì»¤ë²Ħ&#34;:8752,&#34;ìłłìŀ¥&#34;:8753,&#34;ëĴ¤ë¡ľ&#34;:8754,&#34;ĠìĿ´ë¯¸ì§Ģ&#34;:8755,&#34;ĠìłĪìłľ&#34;:8756,&#34;Ġì§ģìĹħ&#34;:8757,&#34;ĠíĹĪë¬´íķľ&#34;:8758,&#34;¬ë¦°ëĭ¤&#34;:8759,&#34;Ġìĺ¤ê¸Ģê±°ë¦¬ëĬĶ&#34;:8760,&#34;ì¹ľêµ¬ê°Ģ&#34;:8761,&#34;Ġë®¤ì§ģ&#34;:8762,&#34;Ġê·¸ëł¤ëĤ¸&#34;:8763,&#34;Ġê±°ì§Ģê°ĻìĿĢ&#34;:8764,&#34;Ġëĭ¤ìļ´ë°ĽìķĦìĦľ&#34;:8765,&#34;ĠìĿ´íķĺëıĦ&#34;:8766,&#34;ìĤ´ëĭ¤ìĤ´ëĭ¤&#34;:8767,&#34;ĠíĽĦìĨįìŀĳ&#34;:8768,&#34;Ġê°Ĳëªħê¹Ĭê²Į&#34;:8769,&#34;ëĭ¨ìĪľíķľ&#34;:8770,&#34;ĠëĽ°ìĸ´ëĦĺëĬĶ&#34;:8771,&#34;ĠìłĦë°ĺìłģìľ¼ë¡ľ&#34;:8772,&#34;ëłĪë©ĺíĥĢìĿ¸&#34;:8773,&#34;ĠìĦ¤ëĵĿëł¥&#34;:8774,&#34;Ġì²©ë³´&#34;:8775,&#34;.!&#34;:8776,&#34;bc&#34;:8777,&#34;įĶ&#34;:8778,&#34;ìĹ¬ë¦Ħ&#34;:8779,&#34;ì§ĢìĦŃ&#34;:8780,&#34;ê²»&#34;:8781,&#34;ìĿĢëĵ¯&#34;:8782,&#34;ìĿĢê·¼&#34;:8783,&#34;ìĿĦì§Ģ&#34;:8784,&#34;ĠìĿ´ì¤Ģ&#34;:8785,&#34;ĠìĿ´ìĨĮë£¡&#34;:8786,&#34;ìķĦìĿĺ&#34;:8787,&#34;ìķĦëĭ´&#34;:8788,&#34;ĠìķĦëĵ¤ìĿ´&#34;:8789,&#34;ëŁ¬ë¦¬&#34;:8790,&#34;ëĮĢìŀĳ&#34;:8791,&#34;ìļ°ë¦¬ëĬĶ&#34;:8792,&#34;ìŀ¥ìĹĲ&#34;:8793,&#34;Ġìłķìĥģ&#34;:8794,&#34;ê³¼ìĹ°&#34;:8795,&#34;Ġìĺ®&#34;:8796,&#34;íĥĵ&#34;:8797,&#34;ĠíķľìĪľê°Ħ&#34;:8798,&#34;ìŀ¬ë¡ľ&#34;:8799,&#34;Ġì§ĦìĪĺ&#34;:8800,&#34;Ġë´¤ëĤĺ&#34;:8801,&#34;ìĹ¬ìĦ±&#34;:8802,&#34;ĠìĪĺëĬĶ&#34;:8803,&#34;íĦ°ë¦¬&#34;:8804,&#34;ì¤ĳíķĻêµĲ&#34;:8805,&#34;ëŁ¬ìĬ¤&#34;:8806,&#34;ëķĮëıĦ&#34;:8807,&#34;Ġë§Īëĭ¤&#34;:8808,&#34;ìĭ¤ëł¥&#34;:8809,&#34;Ġëª»ë¯¸&#34;:8810,&#34;ëĶĶì¦ĪëĭĪ&#34;:8811,&#34;íĬ¸ëĿ¼&#34;:8812,&#34;ĠìĿ´ëŁ°ìĺģíĻĶëĬĶ&#34;:8813,&#34;ĠìĨĮíĨµ&#34;:8814,&#34;ëįĶìļ±&#34;:8815,&#34;ìŀ¬ë°ĮìĹĪëĭ¤&#34;:8816,&#34;ĠìĽĥìĹĪëĭ¤&#34;:8817,&#34;Ġë°ĶëĿ¼ëĬĶ&#34;:8818,&#34;Ġìĥģíĥľ&#34;:8819,&#34;ì§Īì§Īëģ&#34;:8820,&#34;ĠëķĮê°Ģ&#34;:8821,&#34;ĠëĦĺìĸ´ìĦľ&#34;:8822,&#34;ľì°¬&#34;:8823,&#34;ëł¸ëĦ¤&#34;:8824,&#34;Ġë¡ľëĵľ&#34;:8825,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:8826,&#34;Ġìĸ´ëĸł&#34;:8827,&#34;ëª©ìĨĮë¦¬&#34;:8828,&#34;Ġê¹Ģì¹ĺ&#34;:8829,&#34;ìĽĥê¸°ê³ł&#34;:8830,&#34;Ġãħłãħłãħłãħł&#34;:8831,&#34;Ġì¤ĦìĿ´ìķ¼&#34;:8832,&#34;ìķĪë´¤&#34;:8833,&#34;ĠìķĦìĿ´ëĶĶìĸ´&#34;:8834,&#34;ìĬ¤ëŁ¬ìĽłëĭ¤&#34;:8835,&#34;ìĵ°ëłĪê¸°ìĵ°ëłĪê¸°&#34;:8836,&#34;íĿ¬ë§Ŀ&#34;:8837,&#34;ìķĦìĿ´ëĵ¤&#34;:8838,&#34;ë¿ĲìĿ´ëĭ¤&#34;:8839,&#34;Ġì¤Ģë¹Ħ&#34;:8840,&#34;ìĬ¹ìļ°&#34;:8841,&#34;ĠíĳľíĺĦìĿ´&#34;:8842,&#34;Ġëĵ±ë¡Ŀ&#34;:8843,&#34;íĹĪìĪł&#34;:8844,&#34;Ġëĸ¨ìĸ´ëľ¨&#34;:8845,&#34;ĠìķĦìī¬ìĽĮìļĶ&#34;:8846,&#34;ĠìŀĬìĿĦìĪĺ&#34;:8847,&#34;ĠìĨĮìŀ¬ìĿĺ&#34;:8848,&#34;Ġì¡°ê¸Īë§Į&#34;:8849,&#34;ĠìĤ´ìķĦìŀĪëĬĶ&#34;:8850,&#34;íļ¨ì£¼&#34;:8851,&#34;ĠìĿ¸ìĥģìłģ&#34;:8852,&#34;Ġìĭľì²ŃìŀĲ&#34;:8853,&#34;Ġê¹¨ìķĮ&#34;:8854,&#34;ì¤ĳê°Ħì¤ĳê°Ħ&#34;:8855,&#34;Ġíĺ¼ëŀĢ&#34;:8856,&#34;ìĭľíĤ¤ê³ł&#34;:8857,&#34;ìĭľíĤ¤ì§Ģ&#34;:8858,&#34;ãħĩãħĩãħĩãħĩ&#34;:8859,&#34;Ġì£ĦìĨ¡&#34;:8860,&#34;Ġìĺģìĥģë¯¸ê°Ģ&#34;:8861,&#34;ê·¸ëŀ¬&#34;:8862,&#34;Ġìĸ¸ëĭĪ&#34;:8863,&#34;ìĭľë¦¬ì¦Īì¤ĳ&#34;:8864,&#34;Ġê·¸ë¦¬ìĽĮ&#34;:8865,&#34;ĠìĺĽëĤłìĹĲ&#34;:8866,&#34;íŀĺëĵł&#34;:8867,&#34;ĠìĬ¹ë¦¬&#34;:8868,&#34;ãĦ¹ãħĩ&#34;:8869,&#34;ì°¾ìķĦ&#34;:8870,&#34;âĺħâĺĨ&#34;:8871,&#34;Ġì°¬ìĸĳ&#34;:8872,&#34;Ġíĺķíİ¸ìĹĨëĬĶ&#34;:8873,&#34;Īëł¨&#34;:8874,&#34;Ġëª½íĻĺ&#34;:8875,&#34;ìĺµëĭĪëĭ¤&#34;:8876,&#34;Ġìĸ´ëķł&#34;:8877,&#34;¤ë²Ħ&#34;:8878,&#34;Ġu&#34;:8879,&#34;Ġ;;;&#34;:8880,&#34;ģĿ&#34;:8881,&#34;..!!&#34;:8882,&#34;ì§ĢìĽĲ&#34;:8883,&#34;ê°Ģë©´ìĦľ&#34;:8884,&#34;Ġìķ¨&#34;:8885,&#34;ëĤĺëĿ¼ëĬĶ&#34;:8886,&#34;ĠìĿ´ìĦ±&#34;:8887,&#34;ĠìĿ´ë³Ħ&#34;:8888,&#34;Ġë§ģ&#34;:8889,&#34;ìĸ´ì°Į&#34;:8890,&#34;ìķĦëł¨&#34;:8891,&#34;ìĿ¸ìĥģ&#34;:8892,&#34;ìĿ¸ëĶĶ&#34;:8893,&#34;ĠìķĦìļ°&#34;:8894,&#34;Ġë³´êµ¬&#34;:8895,&#34;ë³´ëįĺ&#34;:8896,&#34;ìĬ¤ëŀĢ&#34;:8897,&#34;ëŀĮìĿ´&#34;:8898,&#34;ë§ĪëĬĶ&#34;:8899,&#34;ĠëĤĺê³ł&#34;:8900,&#34;ĠíķĺëıĦ&#34;:8901,&#34;Ġíķĺê¸°ìĹĶ&#34;:8902,&#34;ì£¼ìľ¨&#34;:8903,&#34;Ġê²ł&#34;:8904,&#34;ĠìŀĪê¸´&#34;:8905,&#34;ĠìŀĪê²łì§Ģë§Į&#34;:8906,&#34;ê°ĦëıĦ&#34;:8907,&#34;Ġê¸°ê´´&#34;:8908,&#34;íĮ¡&#34;:8909,&#34;Ġë¬´ê±°ìļ´&#34;:8910,&#34;ĠìķĪì¢ĭ&#34;:8911,&#34;ĠìķĪëĤĺìĺ´&#34;:8912,&#34;Ġëª»íķĺê²łëĭ¤&#34;:8913,&#34;Ġë³¸ì§Ī&#34;:8914,&#34;ìłĢê²ĥ&#34;:8915,&#34;ìĽĲìĹĲ&#34;:8916,&#34;ĠìĿ¼ìĿĦ&#34;:8917,&#34;ĠìĤ¬íĪ¬ë¦¬&#34;:8918,&#34;Ġë°°ìļ°ëĵ¤ìĿĦ&#34;:8919,&#34;Ġíķ´ë¦¬íı¬íĦ°&#34;:8920,&#34;Ġê°ĲìĿ´&#34;:8921,&#34;Ġê³łìĸ´&#34;:8922,&#34;íķĺì§Ģë§Ĳê³ł&#34;:8923,&#34;ĠìĹŃê²¹ëĭ¤&#34;:8924,&#34;ë¯¼ìĿĺ&#34;:8925,&#34;Ġíı°&#34;:8926,&#34;ëĬĲìĻĢë¥´&#34;:8927,&#34;ĠëĶ°ë¶Ħ&#34;:8928,&#34;ĠëĤľìŀ¡&#34;:8929,&#34;ë°©ìĭĿ&#34;:8930,&#34;ĠìºĲë¦Ń&#34;:8931,&#34;ëĪĪë&#34;:8932,&#34;Ġë³Ħê±°&#34;:8933,&#34;ê°ĲëıĻìĿĦ&#34;:8934,&#34;Ġë³¼ë§Įíķ©ëĭĪëĭ¤&#34;:8935,&#34;ìĤ¬ëĵľ&#34;:8936,&#34;ĠìłĦê°ľìĹĲ&#34;:8937,&#34;ì±Ļ&#34;:8938,&#34;ëĭ¤ìļ´ë°Ľ&#34;:8939,&#34;ìŀ¥ë©´ëıĦ&#34;:8940,&#34;ĠíĶ¼íĦ°&#34;:8941,&#34;ĠíĳľíĺĦíķł&#34;:8942,&#34;Ġëĭ´ë°±&#34;:8943,&#34;Ġìĭ¸ìļ°&#34;:8944,&#34;Ġ--&#34;:8945,&#34;íķłìĪĺìŀĪëĬĶ&#34;:8946,&#34;Ġë²łìĿ´&#34;:8947,&#34;ĠìķŀìĦľ&#34;:8948,&#34;oooo&#34;:8949,&#34;Ġìŀ¡ìķĦ&#34;:8950,&#34;ìħ¨ìľ¼ë©´&#34;:8951,&#34;íķĺìĭľê¸¸&#34;:8952,&#34;ĠìłĦìŁģìĿĺ&#34;:8953,&#34;ĠìłĦìŁģìĺģíĻĶ&#34;:8954,&#34;íı¬ë¨¸&#34;:8955,&#34;ĠíħĮìĿ´&#34;:8956,&#34;ĠìĹŃìĤ¬ìĥģ&#34;:8957,&#34;ëĴ¤ìĹĲ&#34;:8958,&#34;ëª¨ë¥´ê²Į&#34;:8959,&#34;ëª¨ë¥´ê²łëĭ¤&#34;:8960,&#34;Ġë²Ĺìĸ´ëĤĺ&#34;:8961,&#34;ĠìķĮëł¤ì£¼ëĬĶ&#34;:8962,&#34;ìĦ¼ìĬ¤&#34;:8963,&#34;ìĹ¬ê¸°ìĦľ&#34;:8964,&#34;Ġìĺ¤ê·¸ëĿ¼ëĵľëĬĶ&#34;:8965,&#34;ĠìĤ°ë§Įíķĺê³ł&#34;:8966,&#34;ĠíķĦìļĶíķľê°Ģ&#34;:8967,&#34;íİĻíĬ¸&#34;:8968,&#34;ì¼ĵëª¬&#34;:8969,&#34;Ġìļ°ëł¤ë¨¹&#34;:8970,&#34;ë©įì²Ń&#34;:8971,&#34;Ġì¹Ńì°¬&#34;:8972,&#34;ĠìĦ¬ëľ©&#34;:8973,&#34;Ġë§Īì°¬ê°Ģì§Ģ&#34;:8974,&#34;Good&#34;:8975,&#34;bad&#34;:8976,&#34;Ļíķ©&#34;:8977,&#34;ê³łíķľ&#34;:8978,&#34;ãħĲ&#34;:8979,&#34;Ġìķ¡&#34;:8980,&#34;ĠìĺģíĻĶëŀĳ&#34;:8981,&#34;ë¦¬ìĸ¸&#34;:8982,&#34;ëĭĪìĬ¤&#34;:8983,&#34;Ġì§¬ë½ķ&#34;:8984,&#34;ë©´ìĿĦ&#34;:8985,&#34;ìĭľëıĦ&#34;:8986,&#34;Ġëĭ¿&#34;:8987,&#34;ìŀĲìĭĿ&#34;:8988,&#34;ìĽį&#34;:8989,&#34;ĠëĦĲ&#34;:8990,&#34;ë§ĪìłĢëıĦ&#34;:8991,&#34;ĠìĹĨëįĺ&#34;:8992,&#34;ĠëĤĺìĹ´&#34;:8993,&#34;ìĺ¤ê³ł&#34;:8994,&#34;Ġìĸ´ìłķì©¡&#34;:8995,&#34;ì°Ĳ&#34;:8996,&#34;Ġíķľê±°&#34;:8997,&#34;Ġì§Ħíķľ&#34;:8998,&#34;íķĺëĬĶê²ĥëıĦ&#34;:8999,&#34;ê°ľìĿĺ&#34;:9000,&#34;ê°Ĳìĥģ&#34;:9001,&#34;Ġë§Īìķ½&#34;:9002,&#34;Ġìµľê³łëĦ¤ìļĶ&#34;:9003,&#34;Ġê°ľëĺ¥&#34;:9004,&#34;ë³¸ìĥī&#34;:9005,&#34;Ġë³´ê³łìŀĪëĬĶëį°&#34;:9006,&#34;ìłĢëĤĺ&#34;:9007,&#34;ë²ĪìĿ´ëĤĺ&#34;:9008,&#34;ĠìĤ¬ìĥģ&#34;:9009,&#34;ëĭ¤ëĬĶê±´&#34;:9010,&#34;ëıĻìļ±&#34;:9011,&#34;Ġìļ°ëĬĶ&#34;:9012,&#34;ĠëŃĲëĿ¼ê³ł&#34;:9013,&#34;Ġê°Ĳëıħê³¼&#34;:9014,&#34;Ġê³łëıħ&#34;:9015,&#34;Ġê±°ê¸°ëĭ¤&#34;:9016,&#34;Ġê±°ëĵŃ&#34;:9017,&#34;ĠìĥģëĮĢ&#34;:9018,&#34;Ġìĵ°ëłĪê¸°ë¥¼&#34;:9019,&#34;íıīìłĲì¡°ìłĪ&#34;:9020,&#34;Ġìĭłê³ł&#34;:9021,&#34;íļĮë¶ĢíĦ°&#34;:9022,&#34;ë´Ĳìķ¼ì§Ģ&#34;:9023,&#34;ìĿ´ëŁ°ìĺģíĻĶê°Ģ&#34;:9024,&#34;ĠìĿĺíķ´&#34;:9025,&#34;ìĺĪê³ł&#34;:9026,&#34;ìł¸ìķ¼&#34;:9027,&#34;Ġê¸°ëĮĢìĹĨìĿ´&#34;:9028,&#34;íĮĲìĿĦ&#34;:9029,&#34;ĠíĸĪìĸ´ìļĶ&#34;:9030,&#34;ĠíĸĪëĬĶì§Ģ&#34;:9031,&#34;Ġëĵ¤ìĸ´ìĦľ&#34;:9032,&#34;Ġë©ĭì§Ĳ&#34;:9033,&#34;ĠìĽĲíķĺëĬĶ&#34;:9034,&#34;íĺ¸ëŁ¬&#34;:9035,&#34;ĠëıĦìĻĢ&#34;:9036,&#34;ĠëĤľë¦¬&#34;:9037,&#34;Ġëĭ¹ìŀ¥&#34;:9038,&#34;ĠìĹ¬ìŀĲë¥¼&#34;:9039,&#34;Ġë§īíĮĲ&#34;:9040,&#34;ìĹĩìĸ´ìļĶ&#34;:9041,&#34;ìĪľìĿ´&#34;:9042,&#34;ĠëĨĴì§Ģ&#34;:9043,&#34;ì¼ĢìĿ´&#34;:9044,&#34;ì¡Įìľ¼ë©´&#34;:9045,&#34;ë¶Īíĺ¸ê°Ģ&#34;:9046,&#34;Ġì½Ķë¯¸ëĶĶìĺģíĻĶ&#34;:9047,&#34;ĠëĿ¼ìĬ¤íĬ¸&#34;:9048,&#34;ĠëĬĲê»´ì§Ģ&#34;:9049,&#34;ë§Įíģ¼ìĿ´ëĤĺ&#34;:9050,&#34;ĠíĮĮê²©&#34;:9051,&#34;ĠìĽĲìŀĳìĹĲ&#34;:9052,&#34;ê¿Ģ&#34;:9053,&#34;ĠìłĪëĮĢë¡ľ&#34;:9054,&#34;ĠêµŃìĸ´&#34;:9055,&#34;íĶĮë¦°&#34;:9056,&#34;ì¿¨&#34;:9057,&#34;Ġíļį&#34;:9058,&#34;ê²ĥê°ĻìĿĮ&#34;:9059,&#34;íķĺíķĺíķĺíķĺ&#34;:9060,&#34;Ġë¸Įë£¨ìĬ¤&#34;:9061,&#34;Ġë§Īëĥ¥&#34;:9062,&#34;Ġíı¬ìĬ¤íĦ°ê°Ģ&#34;:9063,&#34;ë¶ģíķľ&#34;:9064,&#34;ĠíĿĳë°±&#34;:9065,&#34;ĠíĤ¬ë§ģíĥĢìŀĦìļ©ìľ¼ë¡ľ&#34;:9066,&#34;ë²Įìį¨&#34;:9067,&#34;ìĨĮìĦ¤ìĿĦ&#34;:9068,&#34;ìĺ¤ê¸Ģê±°&#34;:9069,&#34;Ġë©Ķìĭľì§Ģ&#34;:9070,&#34;****&#34;:9071,&#34;ìķĦê¹Įìļ´ìĺģíĻĶ&#34;:9072,&#34;Ġë°¤ìĹĲ&#34;:9073,&#34;ìŀ¥ë¥´ê°Ģ&#34;:9074,&#34;Ġê²©íĪ¬&#34;:9075,&#34;Ġê¹ĶëģĶíķľ&#34;:9076,&#34;very&#34;:9077,&#34;Ġê±°ê¸°ìĦľ&#34;:9078,&#34;ĠíĿīëĤ´&#34;:9079,&#34;ĠìĬ¤ë¦´ëŁ¬ë¬¼&#34;:9080,&#34;ë®¤ì§Ģì»¬&#34;:9081,&#34;ìľłì¾Įíķĺê³ł&#34;:9082,&#34;ĠìĿ´ëģĮìĸ´&#34;:9083,&#34;ĠëĤ©ëĵĿ&#34;:9084,&#34;ĠëĳĶ&#34;:9085,&#34;ëĵ±ìŀ¥ìĿ¸ë¬¼&#34;:9086,&#34;ê°ĲìĤ¬íķ©ëĭĪëĭ¤&#34;:9087,&#34;BC&#34;:9088,&#34;íĳ¼&#34;:9089,&#34;Ġh&#34;:9090,&#34;Ġ??&#34;:9091,&#34;ĠâĢ&#34;:9092,&#34;ê³łëĵ±íķĻêµĲ&#34;:9093,&#34;Ġêº&#34;:9094,&#34;íķĺê¸¸ëŀĺ&#34;:9095,&#34;ê°Ģìķ¼&#34;:9096,&#34;ë§Įíķ´ìĦł&#34;:9097,&#34;Ġê°±&#34;:9098,&#34;ìĿ¸ì§ĢëĬĶ&#34;:9099,&#34;ìĬ¤ìĹĲìĦľ&#34;:9100,&#34;ìľ¼ëł¤&#34;:9101,&#34;!!âĻ¥&#34;:9102,&#34;ëĮĢê¸°&#34;:9103,&#34;ê¹Įë´Ĳ&#34;:9104,&#34;ìĹĨì§Ģë§Į&#34;:9105,&#34;Ġì¢ĭëįĶëĿ¼&#34;:9106,&#34;ì§Ħìłķ&#34;:9107,&#34;ĠìĥĪë²½ìĹĲ&#34;:9108,&#34;ìĨĶì§ģ&#34;:9109,&#34;ëĤ´ì§Ģ&#34;:9110,&#34;íĥ±&#34;:9111,&#34;íŀĲë§ģ&#34;:9112,&#34;ìĹ°ìĺĪ&#34;:9113,&#34;ê³µë¶Ģ&#34;:9114,&#34;ì¹ĺëĭ¤&#34;:9115,&#34;Ġëª¨ìĸĳ&#34;:9116,&#34;ĠìķĬìķĦìļĶ&#34;:9117,&#34;ëłĪìķĮ&#34;:9118,&#34;ĠìĥĿê°ģìĿĢ&#34;:9119,&#34;ĠìĥĿê°ģë§Į&#34;:9120,&#34;ìŀħëĭĪê¹Į&#34;:9121,&#34;Ġëª»íķ´ìĦľ&#34;:9122,&#34;ĠìĹ¬ì¹ľ&#34;:9123,&#34;ë¹ĦìĹĲ&#34;:9124,&#34;ĠëĤ¨ìĿĺ&#34;:9125,&#34;ĠëĤ¨ëĬĶê²Į&#34;:9126,&#34;ìłĢëĥ¥&#34;:9127,&#34;ĠìĨĮê°ľ&#34;:9128,&#34;ĠìĨĮëħĦ&#34;:9129,&#34;ëįĶêµ°&#34;:9130,&#34;Ġë¯¸íķĻ&#34;:9131,&#34;Ġë¯¸ëħĢ&#34;:9132,&#34;ê³Ħë¥¼&#34;:9133,&#34;íĭĪ&#34;:9134,&#34;Ġê³µë£¡&#34;:9135,&#34;ãħľãħł&#34;:9136,&#34;ĠëĲĺë©´&#34;:9137,&#34;Ġë°ĺìĿĳ&#34;:9138,&#34;ìĹŃìĿĦ&#34;:9139,&#34;ê²½ìĿĦ&#34;:9140,&#34;ìĹ°ê¸°ìĹĲ&#34;:9141,&#34;ĠìķĦëĭĪìĹĪëĭ¤&#34;:9142,&#34;ê·¹ìĿĺ&#34;:9143,&#34;Ġë§Īì§Ģë§īíļĮ&#34;:9144,&#34;ëł¸ëįĺ&#34;:9145,&#34;ĠëıĪëĤ´ê³ł&#34;:9146,&#34;ê¹Ģë¯¼&#34;:9147,&#34;íķĺê¸°ëĬĶ&#34;:9148,&#34;ĠìĿĮìĭĿ&#34;:9149,&#34;Ġë§Ŀíķł&#34;:9150,&#34;ĠìļĶìĥĪ&#34;:9151,&#34;íıīìĿĦ&#34;:9152,&#34;ëĬĶê±°ëĥĲ&#34;:9153,&#34;íĨµìĪĺ&#34;:9154,&#34;Ġì°įëĬĶ&#34;:9155,&#34;Ġìĭ¤ë§ĿìĿ´ëĭ¤&#34;:9156,&#34;ĠìºĲë¦ŃíĦ°ëıĦ&#34;:9157,&#34;ĠìºĲë¦ŃíĦ°ëĵ¤ìĿ´&#34;:9158,&#34;ĠëĨĢëŀĢ&#34;:9159,&#34;ëŁ¬ë©´&#34;:9160,&#34;ë¸ĮëĿ¼&#34;:9161,&#34;íĸ¥ìĿ´&#34;:9162,&#34;ĠëĤĺìĻĶëįĺ&#34;:9163,&#34;ĠëĤĺìĻĢìķ¼&#34;:9164,&#34;Ġê¸´ìŀ¥ê°ĲìĿĦ&#34;:9165,&#34;ĠìłķìĭłìĿ´&#34;:9166,&#34;ĠëĮĢìĤ¬ìĻĢ&#34;:9167,&#34;ìĭ¶ìĿĢëį°&#34;:9168,&#34;ëĤ¨ìŀĲìĿĺ&#34;:9169,&#34;ëĿ¼ìĿ´ì¦Ī&#34;:9170,&#34;ĠìļĶì¦ĺìĿĢ&#34;:9171,&#34;Ġê°ķì¶Ķíķ©ëĭĪëĭ¤&#34;:9172,&#34;Ġì¼Ģë¹Ī&#34;:9173,&#34;íĥĪë¦¬ìķĦ&#34;:9174,&#34;Ġì¶©ê²©ìłģìĿ¸&#34;:9175,&#34;Ġë¯¼ì¡±&#34;:9176,&#34;êµ³ìĿ´&#34;:9177,&#34;ĠìĹ¬ì£¼ìĿ¸ê³µìĿ´&#34;:9178,&#34;Ġëĵ£ê¸°&#34;:9179,&#34;Ġìĸ´ë¥¸ìĿ´&#34;:9180,&#34;ĠíħĮëŁ¬&#34;:9181,&#34;Ġíķľìĭ¬íķĺëĭ¤&#34;:9182,&#34;ë¹¼ê³¤&#34;:9183,&#34;ĠìĽĥê²¨ìĦľ&#34;:9184,&#34;ĠëĪĦêµ°ê°Ģ&#34;:9185,&#34;ë²Īë³´ê³ł&#34;:9186,&#34;ìĿ´ëŀĺìĦľ&#34;:9187,&#34;ìĿ´íĽĦë¡ľ&#34;:9188,&#34;!~&#34;:9189,&#34;ìª&#34;:9190,&#34;ŀĢ&#34;:9191,&#34;ìĿ´íķľ&#34;:9192,&#34;ìĿ´ìĻĢ&#34;:9193,&#34;ãħĮ&#34;:9194,&#34;íķľíİ¸&#34;:9195,&#34;ĠìĺģíĻĶê°Ļëĭ¤&#34;:9196,&#34;ìĸ´ì©Ķ&#34;:9197,&#34;Ġë³´ìĿ´ê³ł&#34;:9198,&#34;Ġê·¸ê°Ģ&#34;:9199,&#34;ìĭľë¥¼&#34;:9200,&#34;ë³´ëŁ¬&#34;:9201,&#34;ĠìĹĨìĹĪ&#34;:9202,&#34;ĠëĤĺë©´&#34;:9203,&#34;ĠíķĺìĦ¸ìļĶ&#34;:9204,&#34;ìĪĺëĭĺ&#34;:9205,&#34;ìĥģìļ°&#34;:9206,&#34;ĠìŀĪìĹĪìĿĮ&#34;:9207,&#34;íķĺê³łëıĦ&#34;:9208,&#34;¬ëŀĲ&#34;:9209,&#34;ëĤ´ìĦľ&#34;:9210,&#34;ìĹ°ìĿĺ&#34;:9211,&#34;Ġë´¤ê³ł&#34;:9212,&#34;ìĹ¬ëıĦ&#34;:9213,&#34;ìĿ¼ì§Ģ&#34;:9214,&#34;ìĿ¼ëŁ¬&#34;:9215,&#34;Ġëª¨ë¥¸&#34;:9216,&#34;ĠìłĦíĻĶ&#34;:9217,&#34;ë¶Ħì§ľë¦¬&#34;:9218,&#34;ê°Ĳíķľ&#34;:9219,&#34;ê·Ħ&#34;:9220,&#34;ĠìłľìĿ´ìĬ¨&#34;:9221,&#34;ëįĶìĿ´ìĥģ&#34;:9222,&#34;Ġë²¤&#34;:9223,&#34;Ġë¶Ģë¶Ģ&#34;:9224,&#34;ë²ĪìĹĲ&#34;:9225,&#34;Ġíķĳ&#34;:9226,&#34;ĠíķŃ&#34;:9227,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪëĬĶëį°&#34;:9228,&#34;ìŀ¬ë°ĮìĹĪëĬĶëį°&#34;:9229,&#34;ĠìĽĥìĿĦ&#34;:9230,&#34;ëĤ¨ìĿĦ&#34;:9231,&#34;Ġëĭ¤ìĭľë³´ê³ł&#34;:9232,&#34;ì²ŃìĨĮëħĦ&#34;:9233,&#34;ĠíķĺëĤĺëĬĶ&#34;:9234,&#34;ĠëĤĺìĺ¤ëĦ¤ìļĶ&#34;:9235,&#34;Ġìķ¡ìħĺìĶ¬&#34;:9236,&#34;Ġì½ĶìĬ¤&#34;:9237,&#34;êµĲìľ¡&#34;:9238,&#34;ĠìĿ´íķ´ëıĦ&#34;:9239,&#34;Ġë¶Īê°Ģ&#34;:9240,&#34;Ġì²ĺìĿĮìĿ´ëĦ¤&#34;:9241,&#34;Ġì²ĺìĿĮë³¸ëĭ¤&#34;:9242,&#34;Ġê·¹íĺĲ&#34;:9243,&#34;ìłķëıĦëĬĶ&#34;:9244,&#34;íĤ¤ëĵľ&#34;:9245,&#34;ë¥ĺìĺģíĻĶ&#34;:9246,&#34;ĠíıīíĻĶ&#34;:9247,&#34;ìĿ´ê±°ëĤĺ&#34;:9248,&#34;Ġê·¸ëŁ°ê±°&#34;:9249,&#34;ìķĮìķĦ&#34;:9250,&#34;Ġê´ľì°®ëĦ¤ìļĶ&#34;:9251,&#34;............&#34;:9252,&#34;íķĺê¸°ê¹Įì§Ģ&#34;:9253,&#34;Ġìĭ¬íĺķëŀĺ&#34;:9254,&#34;ĠíĹĪëĤĺ&#34;:9255,&#34;ĠíĺĦìĭ¤ê³¼&#34;:9256,&#34;ĠíĺĦìĭ¤ìłģìĿ´&#34;:9257,&#34;ê»ĺìĦľ&#34;:9258,&#34;ĠíĶ¼ìķĦëħ¸&#34;:9259,&#34;Ġìĸµì§ĢìĬ¤ëŁ½ê³ł&#34;:9260,&#34;ìĬ¹íĹĮ&#34;:9261,&#34;Ġìŀħìŀ¥ìĹĲìĦľ&#34;:9262,&#34;Ġíĺ¸íĿ¡&#34;:9263,&#34;ĠìĻĢìĦľ&#34;:9264,&#34;Ġì¹ľêµ¬ëŀĳ&#34;:9265,&#34;ì§Ģê¸ĪìĿĢ&#34;:9266,&#34;Ġìĸ´ëĶĶìĹĲ&#34;:9267,&#34;Ġì¡°ê¸ĪìĿĢ&#34;:9268,&#34;Ġë³´ê²ĮëĲĺëĬĶ&#34;:9269,&#34;ìĽĲìŀĳìĿ´&#34;:9270,&#34;Ġìĸ´ìĦ¤íĶĦëĭ¤&#34;:9271,&#34;Ġë¬¸ìłľë¥¼&#34;:9272,&#34;ĠìŀĶìŀĶíķĺê²Į&#34;:9273,&#34;Ġìĸĺê¸°ë¥¼&#34;:9274,&#34;Ġê°ĲëıĻìłģìĿ´ëĦ¤ìļĶ&#34;:9275,&#34;ìĨĲê°ĢëĿ½&#34;:9276,&#34;ĠíĴįìŀĲ&#34;:9277,&#34;Ġìĸ´ìļ¸ë¦¬ì§Ģ&#34;:9278,&#34;ìķĶíĬ¼&#34;:9279,&#34;Ġì½ĺ&#34;:9280,&#34;Ġíĺ¹ìĭľëĤĺ&#34;:9281,&#34;Ġìį°&#34;:9282,&#34;Ġë¬ĺíķľ&#34;:9283,&#34;Ġë²Ĺìĸ´&#34;:9284,&#34;Ġmovie&#34;:9285,&#34;ĠìĥĪìĤ¼&#34;:9286,&#34;ì§ĪëķĮ&#34;:9287,&#34;ì¤ĦìķĮìķĺëĭ¤&#34;:9288,&#34;ë½Ģ&#34;:9289,&#34;ĠìĹ¬ëŁ¬ë²Ī&#34;:9290,&#34;íĿ¥ë¯¸ì§Ħì§Ħ&#34;:9291,&#34;Ġìµľê·¼ìĹĲ&#34;:9292,&#34;ëĤ«ëĭ¤&#34;:9293,&#34;ĠëĿ¼ìĿ´ìĸ¸&#34;:9294,&#34;Ġì¸¡&#34;:9295,&#34;ìĺ¤ëŀ«ë§ĮìĹĲ&#34;:9296,&#34;ëĿ¼ìĹĲëª½&#34;:9297,&#34;25&#34;:9298,&#34;il&#34;:9299,&#34;vs&#34;:9300,&#34;ľìĭľ&#34;:9301,&#34;íķĺìĿĺ&#34;:9302,&#34;ê¸°ë°ľ&#34;:9303,&#34;ĠìĿ´ê²ĥìĿ´&#34;:9304,&#34;ìĸ´ëłµ&#34;:9305,&#34;ë¦¬ëĬĶëį°&#34;:9306,&#34;ĠìłĪë&#34;:9307,&#34;ìķĦëŀĺ&#34;:9308,&#34;ĠìķĦíĮł&#34;:9309,&#34;íķ´ê°ĢëĬĶ&#34;:9310,&#34;Ġë³´ê²łëĭ¤&#34;:9311,&#34;Ġë³´ìĦĿ&#34;:9312,&#34;ìĥĮ&#34;:9313,&#34;Ġê·¸ìĿ´ìĥģ&#34;:9314,&#34;ë°ĳ&#34;:9315,&#34;ëŀ´&#34;:9316,&#34;ĠìĤĲ&#34;:9317,&#34;ëĮĢì¤ĳ&#34;:9318,&#34;Ġì§Ģê²½&#34;:9319,&#34;ê·¸ëĭ¹ìĭľ&#34;:9320,&#34;Ġíķľê°ľëıĦ&#34;:9321,&#34;ìķĺìĸ´&#34;:9322,&#34;ë¯¸ë¥¼&#34;:9323,&#34;ë¯¸ëŀĺ&#34;:9324,&#34;ê²ĥê°ĻìĿĢëį°&#34;:9325,&#34;ìĿ¼ìĹĲ&#34;:9326,&#34;Ġìŀĺë³´ê³ł&#34;:9327,&#34;ĠëĬ¦ê²Į&#34;:9328,&#34;ê°ĲíŀĪ&#34;:9329,&#34;Ġê°Ļì§Ģë§Į&#34;:9330,&#34;ĠëĤ¨ê¸°ëĬĶ&#34;:9331,&#34;íĬ¸ìĻĢ&#34;:9332,&#34;Ġìĺ¤íķ´&#34;:9333,&#34;ĠìĬ¤íĨłë¦¬ìłĦê°ľ&#34;:9334,&#34;ìķĪìĹĲìĦľ&#34;:9335,&#34;ìĽĲìĹĲìĦľ&#34;:9336,&#34;Ġì§Ģë£¨íķ¨ìĿĦ&#34;:9337,&#34;ìĽĮì¦Ī&#34;:9338,&#34;ĠëĲĺìĦľ&#34;:9339,&#34;Ġíķ´ì¤Ģëĭ¤&#34;:9340,&#34;íĤ´&#34;:9341,&#34;Ġìĭ¶ìĹĪëĬĶëį°&#34;:9342,&#34;Ġê°ĲìĦ±ìĿĦ&#34;:9343,&#34;ĠìĤ¬ëŀĳìĬ¤ëŁ½ëĭ¤&#34;:9344,&#34;Ġìŀ¬ë¯¸ìĹĨëĦ¤ìļĶ&#34;:9345,&#34;Ġìĭľê°ĦëıĦ&#34;:9346,&#34;ĠíķĺëĤĺê°Ģ&#34;:9347,&#34;ìµľê³łìŀħëĭĪëĭ¤&#34;:9348,&#34;Ġì£½ìĸ´&#34;:9349,&#34;ì¦ĿìĿĦ&#34;:9350,&#34;ìĹĲê²ĮëıĦ&#34;:9351,&#34;ĠíĽĦíķĺê²Į&#34;:9352,&#34;ĠìĦ±ìĿ¸ìĿ´&#34;:9353,&#34;ĠíĸĪëĤĺ&#34;:9354,&#34;Ġì§ľë¦¬&#34;:9355,&#34;Ġíģ¬ë&#34;:9356,&#34;ì½ķ&#34;:9357,&#34;ĠìĿ´ìĥģíķ´&#34;:9358,&#34;ìĸ¸ë§¨&#34;:9359,&#34;ĠìłģìĿĢ&#34;:9360,&#34;ĠìłģëĤĺëĿ¼&#34;:9361,&#34;Ġê±´ì§Ī&#34;:9362,&#34;ìŀ¬ë¯¸ìŀĪìĹĪìĸ´ìļĶ&#34;:9363,&#34;ĠìķĦë¬´ëĤĺ&#34;:9364,&#34;Ġìĸµìļ¸&#34;:9365,&#34;ĠìĹ´ê´ĳ&#34;:9366,&#34;ĠìºĲë¦ŃíĦ°ëĬĶ&#34;:9367,&#34;ë§ŀê³ł&#34;:9368,&#34;ĠìĺģíĻĶìĺĢëĬĶëį°&#34;:9369,&#34;ĪëĶ°&#34;:9370,&#34;ìĬ¬íį¼&#34;:9371,&#34;ìķĦê¹Ŀê³ł&#34;:9372,&#34;Ġëĵ¤ìĸ´ê°Ħ&#34;:9373,&#34;ĠìķĪë³´ëĬĶê²Į&#34;:9374,&#34;ĠëĤļìĿ´ì§Ģ&#34;:9375,&#34;Ġë°°ê²½ìĿĮìķħ&#34;:9376,&#34;ìĻĦìĦ±ëıĦ&#34;:9377,&#34;ĠãħİãĦ·ãĦ·&#34;:9378,&#34;ĠìĽĥìĿĮìĿĦ&#34;:9379,&#34;Ġê²°êµŃìĿĢ&#34;:9380,&#34;íŀĪë´¤&#34;:9381,&#34;ë§ĪìĿĮìĿĦ&#34;:9382,&#34;Ġëĭ¹ìĭľìĹĲ&#34;:9383,&#34;ĠìłĪìłķ&#34;:9384,&#34;Ġê°Ģì¹ĺëıĦ&#34;:9385,&#34;,,,,,,,,&#34;:9386,&#34;ĠìĤ¼ë¥ĺìĺģíĻĶ&#34;:9387,&#34;Ġìĸ´ì©Įë©´&#34;:9388,&#34;ìŀ¬ë¯¸ëıĦìĹĨê³ł&#34;:9389,&#34;ìĿ´íķĺëĵľ&#34;:9390,&#34;Ġì¿¨&#34;:9391,&#34;ê¼¬ë§Ī&#34;:9392,&#34;Ġë©ĶìĦ¸ì§Ģë¥¼&#34;:9393,&#34;ì¤ĦìķĮìķĺëĬĶëį°&#34;:9394,&#34;íĹĪìłĳíķľ&#34;:9395,&#34;ĠìĥĪë¡Ŀ&#34;:9396,&#34;¬ëł¸&#34;:9397,&#34;ìµľê·¼ìĹĲ&#34;:9398,&#34;Ġê²īë©ĭ&#34;:9399,&#34;ĠìĿµìĪĻ&#34;:9400,&#34;ëıħë¦½ìĺģíĻĶ&#34;:9401,&#34;ëŀľìĬ¤íı¬ë¨¸&#34;:9402,&#34;_^&#34;:9403,&#34;id&#34;:9404,&#34;´£&#34;:9405,&#34;ëĩ&#34;:9406,&#34;Ġ!!!!&#34;:9407,&#34;Ġì¾Į&#34;:9408,&#34;ĠìĮĵ&#34;:9409,&#34;ê°Īë&#34;:9410,&#34;ëĭ¤ì½Ķ&#34;:9411,&#34;ê³°&#34;:9412,&#34;ëĬĶê²ĥëıĦ&#34;:9413,&#34;ê³łìłĦ&#34;:9414,&#34;Ġíī&#34;:9415,&#34;ĠìĺģíĻĶëĵ¤ìĿ´&#34;:9416,&#34;Ġë§Īë²ķ&#34;:9417,&#34;ìķĦë¦¬&#34;:9418,&#34;ìĿ¸íĦ°&#34;:9419,&#34;ë³´ëł¤ê³ł&#34;:9420,&#34;ìĬ¤íĭ´&#34;:9421,&#34;ëĮĢì¶©&#34;:9422,&#34;ĠìĹĨìĸ´ëıĦ&#34;:9423,&#34;ì§Ħíķľ&#34;:9424,&#34;ìŀĪìĿĦê¹Į&#34;:9425,&#34;ìłĦëĭ¬&#34;:9426,&#34;ĠìŀĪìĹĪìĬµëĭĪëĭ¤&#34;:9427,&#34;ĠìĹ°ê´Ģ&#34;:9428,&#34;Ġíķľê°Ģ&#34;:9429,&#34;ì¹ĺë§Į&#34;:9430,&#34;Ġìŀ¬ë°ĮìĹĪ&#34;:9431,&#34;ĠëĬĳëĮĢ&#34;:9432,&#34;Ġì£¼ìľ¤ë°ľ&#34;:9433,&#34;Ġë§ĪëĭĪ&#34;:9434,&#34;Ġëª»íķĺëĦ¤&#34;:9435,&#34;ĠìĹ¬ìĭł&#34;:9436,&#34;ĠìŀĲìķĦ&#34;:9437,&#34;Ġê²ĥëĵ¤&#34;:9438,&#34;Ġìĺ¤ì§Ģ&#34;:9439,&#34;Ġê¹¡íĮ¨&#34;:9440,&#34;ìŀ¬ë¯¸ìŀĪëĦ¤ìļĶ&#34;:9441,&#34;ë²ĦíĬ¼&#34;:9442,&#34;ëªħíĻĶ&#34;:9443,&#34;ĠìķłìŀĶ&#34;:9444,&#34;ĠìłĢì§Ģ&#34;:9445,&#34;ĠíĹ¬&#34;:9446,&#34;Ġê±°íĴĪ&#34;:9447,&#34;ĠìĹŃìŀĳ&#34;:9448,&#34;Ġëĭ¤ìĭľë³´ê¸°&#34;:9449,&#34;ëŃĩ&#34;:9450,&#34;Ġëª°ìķĦ&#34;:9451,&#34;Ġê¸°ëĮĢê°Ģ&#34;:9452,&#34;Ġê°ķìļĶ&#34;:9453,&#34;ìĻľìĿ´ëłĩê²Į&#34;:9454,&#34;ëĲĲëĭ¤&#34;:9455,&#34;ĠíĺĦìĭ¤ìĿĢ&#34;:9456,&#34;ìĥĿê°ģìľ¼ë¡ľ&#34;:9457,&#34;Ġë¦°&#34;:9458,&#34;ĠìĦłìłķ&#34;:9459,&#34;Ġê³¼ìłķìĿ´&#34;:9460,&#34;Ġë°©ê¸Ī&#34;:9461,&#34;êµ¬ë¨¼&#34;:9462,&#34;ĠìĿ¼ë³¸ìĿĺ&#34;:9463,&#34;ëĬĶê±°ëĭ¤&#34;:9464,&#34;Ġê¸´ë°ķ&#34;:9465,&#34;Īë°ľ&#34;:9466,&#34;ì£½ìĿ´ê³ł&#34;:9467,&#34;ĠëĨĢëŀĺ&#34;:9468,&#34;Ġì»¬&#34;:9469,&#34;Ġì»·&#34;:9470,&#34;Ġì»´íĵ¨íĦ°&#34;:9471,&#34;Ġìŀħìŀ¥&#34;:9472,&#34;Ġê³°&#34;:9473,&#34;íķłìĪĺëıĦ&#34;:9474,&#34;ĠëĤłìķĦ&#34;:9475,&#34;ĠíĺķìĤ¬&#34;:9476,&#34;ì¤ĺëıĦ&#34;:9477,&#34;Ġì´Īëĵ±íķĻêµĲ&#34;:9478,&#34;Ġìĭľë¦¬ì¦ĪìĿĺ&#34;:9479,&#34;ë²łìĬ¤íĬ¸&#34;:9480,&#34;ìĿ´ìķ¼ê¸°ê°Ģ&#34;:9481,&#34;Ġë°°ê²½ìĿ´&#34;:9482,&#34;Ġì£¼ê³łìĭ¶ëĭ¤&#34;:9483,&#34;Ġì¶©ë¶Ħíķĺëĭ¤&#34;:9484,&#34;Ġì»¤ìĦľ&#34;:9485,&#34;ìĹ°ì¶ľìĿ´&#34;:9486,&#34;ë»ĳ&#34;:9487,&#34;ê½Ŀ&#34;:9488,&#34;Ġìĵ¸ìĵ¸&#34;:9489,&#34;ĠìŀĲìĹ°ìĬ¤ëŁ½ê²Į&#34;:9490,&#34;íĭ°ë¹Ħë¡ľ&#34;:9491,&#34;ëłĪëĶĶ&#34;:9492,&#34;ĠìĺģíĻĶìĿ¸ê²ĥ&#34;:9493,&#34;ìµľìķħìĿĺìĺģíĻĶ&#34;:9494,&#34;Ġìĭ«ìĸ´íķĺëĬĶ&#34;:9495,&#34;ìłĲì¤Ģê²ĥ&#34;:9496,&#34;ì¶ľìĹ°ì§Ħ&#34;:9497,&#34;Ġì©Ķìĸ´&#34;:9498,&#34;Ġë¸ĶëŀĻì½Ķë¯¸ëĶĶ&#34;:9499,&#34;Ġìĭľë¦¬ì¦Īë¥¼&#34;:9500,&#34;ĠìĨĮìŀ¥íķĺê³ł&#34;:9501,&#34;Ġë»¥&#34;:9502,&#34;Ġì¯§&#34;:9503,&#34;ĠíķĺìĿ´íĭ´&#34;:9504,&#34;ëŀĺê³¤ë³¼&#34;:9505,&#34;!?&#34;:9506,&#34;ëĭ¤ìĨĮ&#34;:9507,&#34;ëĤĦ&#34;:9508,&#34;ĠìĺģíĻĶìĨį&#34;:9509,&#34;ĠìĺģíĻĶëŀĢ&#34;:9510,&#34;ìĭ¬ë¦¬&#34;:9511,&#34;ìĿĺëıĦ&#34;:9512,&#34;ìĿĢìĺģíĻĶ&#34;:9513,&#34;ëĤĺìĺ¬ëķĮ&#34;:9514,&#34;Ġë§Ļ&#34;:9515,&#34;ìĸ´ìĿ´ê°Ģ&#34;:9516,&#34;ë¡ľë´ĩ&#34;:9517,&#34;ë¡ľë²ĦíĬ¸&#34;:9518,&#34;ëłĢ&#34;:9519,&#34;ëį°ëłĲ&#34;:9520,&#34;íķ´ìĦĿ&#34;:9521,&#34;íķ´ì¤Ģëĭ¤&#34;:9522,&#34;ìĥĪë²½ìĹĲ&#34;:9523,&#34;ìĺģíĻĶë³´ë©´ìĦľ&#34;:9524,&#34;ìĬ¤íĮĮìĿ´&#34;:9525,&#34;ëŀ¬ëĭ¤&#34;:9526,&#34;ì¤į&#34;:9527,&#34;ìłķì¹ĺ&#34;:9528,&#34;Ġì¢ĭìķĹ&#34;:9529,&#34;ìŀĪìĹĪëĬĶëį°&#34;:9530,&#34;ìłĦìĿĢ&#34;:9531,&#34;ìĥģëıĦ&#34;:9532,&#34;ìĺ¤ìļ°&#34;:9533,&#34;Ġìĺ¬ëĵľ&#34;:9534,&#34;Ġë§Įìķ½&#34;:9535,&#34;ìĦ±íķľ&#34;:9536,&#34;ĠìĹ°ê¸°ë§Į&#34;:9537,&#34;Ġëª¨ë°©&#34;:9538,&#34;ĠìķĪê°ĢëĬĶ&#34;:9539,&#34;Ġì£¼ìĦ¸ìļĶ&#34;:9540,&#34;ĠëĵľëĶĶìĸ´&#34;:9541,&#34;ĠìĿ¸ì¢ħ&#34;:9542,&#34;Ġìµľê³łìĿ¸ëĵ¯&#34;:9543,&#34;ìľłì¾Įíķľ&#34;:9544,&#34;Ġë¹Ħíķ´ìĦľ&#34;:9545,&#34;Ġë¹Ħì¥¬ìĸ¼&#34;:9546,&#34;Ġë¯¸ì³Ĳ&#34;:9547,&#34;ë²Ħì¸ł&#34;:9548,&#34;Ġë§Įëĵ¤ìĹĪëĥĲ&#34;:9549,&#34;ëĲĺìļĶ&#34;:9550,&#34;ĠëģĿëĤ¨&#34;:9551,&#34;ìĺģêµŃ&#34;:9552,&#34;ë¦¬ëĵľ&#34;:9553,&#34;ë¦¬ë·°&#34;:9554,&#34;Ġìķłì´ĪìĹĲ&#34;:9555,&#34;OOOO&#34;:9556,&#34;ĠëĬĲëĤĢëĭ¤&#34;:9557,&#34;ĠìŀĪëĬĶê°Ģ&#34;:9558,&#34;ĠëĦĺì³Ĳ&#34;:9559,&#34;Ġë©ĭìł¸ìļĶ&#34;:9560,&#34;Ġê¸°ìĸµëĤľëĭ¤&#34;:9561,&#34;Ġìŀ¥ë©´ë§Į&#34;:9562,&#34;ĠíķľêµŃìĺģíĻĶìĿĺ&#34;:9563,&#34;Ġê´ľì°®ìķĺëįĺ&#34;:9564,&#34;Ġìĭ¬íķĺëĭ¤&#34;:9565,&#34;ê¼Ī&#34;:9566,&#34;ì§ĳì¤ĳ&#34;:9567,&#34;ĠìļĶë¦¬&#34;:9568,&#34;ĠìķĦìĿ´ëĶĶ&#34;:9569,&#34;Ġë§Įëĵ¤ìĸ´ì£¼&#34;:9570,&#34;Ġê³µíı¬ê°Ģ&#34;:9571,&#34;íĿ¬ëĬĶ&#34;:9572,&#34;Ġíİ¸ìĿ¸ëį°&#34;:9573,&#34;ìķĦìĿ´ëĵ¤ê³¼&#34;:9574,&#34;Ġê²ĮìĬ¤íĬ¸&#34;:9575,&#34;ë´ĲëıĦë´ĲëıĦ&#34;:9576,&#34;íĬ¹ìľłìĿĺ&#34;:9577,&#34;ê¸°ëĮĢìĿ´ìĥģ&#34;:9578,&#34;ëŁ¬ë¬¼&#34;:9579,&#34;ĠëĬĲê»´ì§Ģì§Ģ&#34;:9580,&#34;ì°½ìĭľìłĪ&#34;:9581,&#34;ĠìłľëĮĢë¡ľëĲľ&#34;:9582,&#34;ĠíĨµíķ´ìĦľ&#34;:9583,&#34;ĠìĿĺë¯¸ëıĦ&#34;:9584,&#34;ì¤ĺìķ¼ì§Ģ&#34;:9585,&#34;ë²ķíķľ&#34;:9586,&#34;ìĬ¬íĶĦê³ł&#34;:9587,&#34;ì§Ŀíīģ&#34;:9588,&#34;ì¶©ë¶ĦíŀĪ&#34;:9589,&#34;Ġíŀĺëĵ¤ìĹĪëĭ¤&#34;:9590,&#34;Ġì²¨ë¶ĢíĦ°&#34;:9591,&#34;ëªħìŀĳìĿ´ëĭ¤&#34;:9592,&#34;Ġìĸ´ë¦°ìĭľìłĪ&#34;:9593,&#34;Ġì£ł&#34;:9594,&#34;ìĺ¬ëł¤&#34;:9595,&#34;ĠìĦ¤ìłķìĿĢ&#34;:9596,&#34;Ġê°ĲëıĻìłģìĿ´ìĹĲìļĶ&#34;:9597,&#34;ĠìĿ¸ë¬¼ëĵ¤ìĿĺ&#34;:9598,&#34;ĠìĻłë§Įíķľ&#34;:9599,&#34;ìķĦëĭĮëį°&#34;:9600,&#34;ĠëĨĴìĿĢì§Ģ&#34;:9601,&#34;ĠëįĶìļ±ëįĶ&#34;:9602,&#34;ë¦¬ìĿĺë¦¬ìĿĺ&#34;:9603,&#34;ĠìĹĲë¡ľìĺģíĻĶ&#34;:9604,&#34;íĢĦ&#34;:9605,&#34;ë²Īë´ĲëıĦ&#34;:9606,&#34;Ġê¼¬ë§Ī&#34;:9607,&#34;Ġê»Ĳëĭ¤&#34;:9608,&#34;Ġë©´ìĿ´&#34;:9609,&#34;Ġê¹ľëĨĢ&#34;:9610,&#34;Ġëļľ&#34;:9611,&#34;Ġì°¬ìĤ¬ë¥¼&#34;:9612,&#34;Ġëºı&#34;:9613,&#34;ĠìķĪë¬´ìĦŃ&#34;:9614,&#34;ĠìĺĨìĹĲ&#34;:9615,&#34;ì¼ĢìĿ´ë¸ĶìĹĲìĦľ&#34;:9616,&#34;°ľê²¬&#34;:9617,&#34;íıīë²Ķíķľ&#34;:9618,&#34;ĠìĤ´ëĭ¤ìĤ´ëĭ¤&#34;:9619,&#34;ìĦ¸íı¬ìĨĮëħĢ&#34;:9620,&#34;ĠíĿ¬ëĮĢìĿĺ&#34;:9621,&#34;ì§Ģë¶Ģì§Ģ&#34;:9622,&#34;Ġíķ¸ëĵľ&#34;:9623,&#34;âĺħâĺĨâĺħâĺĨ&#34;:9624,&#34;el&#34;:9625,&#34;ff&#34;:9626,&#34;ĠH&#34;:9627,&#34;ģëĭĪëĭ¤&#34;:9628,&#34;ì§ĪëĿ¼&#34;:9629,&#34;ëĵ¦&#34;:9630,&#34;ë¦ħ&#34;:9631,&#34;ĠìĺģíĻĶìłģ&#34;:9632,&#34;ìĭ¹&#34;:9633,&#34;ëĤĺë³´ëĭ¤&#34;:9634,&#34;ĠìĿ´ìĸ´ì§ĢëĬĶ&#34;:9635,&#34;...;;&#34;:9636,&#34;Ġì§¤&#34;:9637,&#34;ìĺģíĻĶë³´ëĬĶ&#34;:9638,&#34;ìĺģíĻĶìĺĢëĭ¤&#34;:9639,&#34;ë³´ìĨĮ&#34;:9640,&#34;ìĬ¤íħĿ&#34;:9641,&#34;ëŀ¨&#34;:9642,&#34;ê±°ìĽĮ&#34;:9643,&#34;ìĤĲ&#34;:9644,&#34;ìŀĲëĵ¤ìĿĢ&#34;:9645,&#34;ìŀĲìĭłìĿĺ&#34;:9646,&#34;ìķ¼ìŀĲ&#34;:9647,&#34;ì£¼ë§Ĳ&#34;:9648,&#34;ìłģìĿĦ&#34;:9649,&#34;Ġì§Ģìĺ¥&#34;:9650,&#34;Ġíķľìĭľê°Ħ&#34;:9651,&#34;ëĵľë¦½&#34;:9652,&#34;ĠìĭľíĹĺ&#34;:9653,&#34;Ġì°Ķ&#34;:9654,&#34;ê²ĥìĿ¸ê°Ģ&#34;:9655,&#34;ì¹ĺê¸°&#34;:9656,&#34;Ġê¸°ì¡´&#34;:9657,&#34;Ġëª¨ëıħ&#34;:9658,&#34;ìĤ¬ê¸°&#34;:9659,&#34;ìĤ¬ì§Ħ&#34;:9660,&#34;Ġë¬´ìĿĺë¯¸&#34;:9661,&#34;ëŁ¬ìĽĢ&#34;:9662,&#34;ĠìķĪê²¨&#34;:9663,&#34;ĠìķĪì¢ĭìĿĢ&#34;:9664,&#34;Ġì¡±&#34;:9665,&#34;Ġê°ľì½ĺ&#34;:9666,&#34;Ġë³¸ì§Ģ&#34;:9667,&#34;ìĺĢëĦ¤ìļĶ&#34;:9668,&#34;Ġìĺ¤ë¡ľì§Ģ&#34;:9669,&#34;ĠìĤ¬ê³¼&#34;:9670,&#34;Ġì¤ĳìĿĺ&#34;:9671,&#34;ìĺģìĿĢ&#34;:9672,&#34;ëıĻìĿĦ&#34;:9673,&#34;Ġíķ´ìļĶ&#34;:9674,&#34;ì¡°íķľ&#34;:9675,&#34;íĤµ&#34;:9676,&#34;Ġìĭ¶ìĿĮ&#34;:9677,&#34;ìķłê¸°&#34;:9678,&#34;Ġì¢ĭìķĦíķĺìĭľëĬĶ&#34;:9679,&#34;ìľĦë¥¼&#34;:9680,&#34;¬ë¦½&#34;:9681,&#34;ĠìĭłìĿ¸&#34;:9682,&#34;ìĿ´ëŁ°ê±´&#34;:9683,&#34;ìķĬê³ł&#34;:9684,&#34;Ġëª¨ë¥´ê²Ł&#34;:9685,&#34;ĠìķĦê¹ĿìĬµëĭĪëĭ¤&#34;:9686,&#34;ĠíĬĢ&#34;:9687,&#34;Ġë§Īì§Ģë§īìĿĢ&#34;:9688,&#34;Ġì²ĺìĿĮìĹĲ&#34;:9689,&#34;ĠìĿ´ìķ¼ê¸°ìĿĺ&#34;:9690,&#34;ë¡łìĿĺ&#34;:9691,&#34;íĺĢìĦľ&#34;:9692,&#34;ĠìĦ¸ìĽĶ&#34;:9693,&#34;ĠíıīìĨĮ&#34;:9694,&#34;Ġë§Įëĵłê±´ì§Ģ&#34;:9695,&#34;Ġë³Ħë¡ľìĺĢëĭ¤&#34;:9696,&#34;ĠëĬĲëĤĮëıĦ&#34;:9697,&#34;ì¹´ëį°ë¯¸&#34;:9698,&#34;ìķĪë³¸&#34;:9699,&#34;ì§Ģë£¨íķ´ìĦľ&#34;:9700,&#34;ë©Ķë¦¬&#34;:9701,&#34;ĠëĤ¨ìŀĲëĬĶ&#34;:9702,&#34;Ġë»Ķíķĺëĭ¤&#34;:9703,&#34;Ġëª¨ëĵłê²ĥìĿ´&#34;:9704,&#34;ĠìĻ¸ë©´&#34;:9705,&#34;ĠìĹ´ìłķ&#34;:9706,&#34;ëıĪìľ¼ë¡ľ&#34;:9707,&#34;ë¸Įë£¨ìĬ¤&#34;:9708,&#34;Ġìŀ¬ë°ĭìĸ´ìļĶ&#34;:9709,&#34;ĠìĤ´ìķĦìķ¼&#34;:9710,&#34;ëĨĪìĿĺ&#34;:9711,&#34;ë¶Ģë¶ĦìĿĢ&#34;:9712,&#34;íĺľêµĲ&#34;:9713,&#34;Ġìĸ´ìĿ´ìĹĨìĿĮ&#34;:9714,&#34;Ġë¯¸êµŃìĿĺ&#34;:9715,&#34;ê²ĥê°ĻìķĦ&#34;:9716,&#34;ëĭĿë§¨&#34;:9717,&#34;Ġì»¤ëħķ&#34;:9718,&#34;Ġê°ĸê²Į&#34;:9719,&#34;Ġê°ĸì¶ĺ&#34;:9720,&#34;ĠìĹŃìĤ¬ìĹĲ&#34;:9721,&#34;Ġìĺ¤ê¸Ģìĺ¤ê¸Ģ&#34;:9722,&#34;Ġì¤ĺëıĦ&#34;:9723,&#34;Ġ2000&#34;:9724,&#34;Ġê·¸ëŁ´ëĵ¯&#34;:9725,&#34;ì´ĪëĶ©ëķĮ&#34;:9726,&#34;ĠìĨĮë¦Ħëģ¼ì¹ĺëĬĶ&#34;:9727,&#34;ë§ĲìĿ´íķĦìļĶìĹĨëĭ¤&#34;:9728,&#34;ìķĦìī½ëĭ¤&#34;:9729,&#34;ìĦ¹ìĬ¤&#34;:9730,&#34;Ġì¼ĢìĿ´ë¸ĶìĹĲìĦľ&#34;:9731,&#34;Ġë¬´ìĸ¸ê°Ģ&#34;:9732,&#34;ĠìķĪë´ĲìĦľ&#34;:9733,&#34;ê¹ģëĭĪëĭ¤&#34;:9734,&#34;cn&#34;:9735,&#34;¥¸&#34;:9736,&#34;ª»&#34;:9737,&#34;Ħĺ&#34;:9738,&#34;ìĿ´ë»Ĳ&#34;:9739,&#34;ìłĪë&#34;:9740,&#34;ì§ĢíĻĺ&#34;:9741,&#34;íķľíħĲ&#34;:9742,&#34;ëĤĺë¬´&#34;:9743,&#34;ëĤĺìłĢëĤĺ&#34;:9744,&#34;ê²ĮìĿ´&#34;:9745,&#34;ê²Įëĭ¤ê°Ģ&#34;:9746,&#34;ìĬ¬ë¦¬&#34;:9747,&#34;ë¦¬ìī¬&#34;:9748,&#34;ëłĺ&#34;:9749,&#34;ìĿ¸ìľ¼ë¡ľ&#34;:9750,&#34;ìĿ¸íĺľ&#34;:9751,&#34;ìĺģíĻĶëĦ¤ìļĶ&#34;:9752,&#34;ìĭľì¼°&#34;:9753,&#34;ìĹĨìĬµëĭĪëĭ¤&#34;:9754,&#34;ĠíķĺëĭĪ&#34;:9755,&#34;ì§Ħíĺ¸&#34;:9756,&#34;íĨ¡&#34;:9757,&#34;íķłìĪľ&#34;:9758,&#34;Ġë´¤ëĬĶì§Ģ&#34;:9759,&#34;ê²ĥëĵ¤ìĿ´&#34;:9760,&#34;ê²ĥì²ĺëŁ¼&#34;:9761,&#34;ìłľë¥¼&#34;:9762,&#34;Ġê¸°ìŀĲ&#34;:9763,&#34;ĠìĹ°ê¸°ìŀĲëĵ¤&#34;:9764,&#34;ëŀĺìļĶ&#34;:9765,&#34;ìĭłíĺľ&#34;:9766,&#34;ëŁ¬ëĭĪ&#34;:9767,&#34;ëŁ¬íĭ°ë¸Į&#34;:9768,&#34;ĠìķĪëĭ¤&#34;:9769,&#34;ĠìķĪê°Ĳ&#34;:9770,&#34;ĠìķĪëĤĺìĺ¤ëĬĶ&#34;:9771,&#34;~~!&#34;:9772,&#34;Ġë§Ĳìķĺëĭ¤&#34;:9773,&#34;ëħĦìĿĦ&#34;:9774,&#34;ĠìłľìĻķ&#34;:9775,&#34;íĥĢìĿ´&#34;:9776,&#34;Ġê²ĥê³¼&#34;:9777,&#34;Ġìĺ¤íĶĦëĭĿ&#34;:9778,&#34;ĠìĨĮíĴĪ&#34;:9779,&#34;ìĭ¬ìĿĢ&#34;:9780,&#34;Ġë§İëĦ¤&#34;:9781,&#34;ìĽĲëıĦ&#34;:9782,&#34;Ġê³µì¤ĳ&#34;:9783,&#34;ìŀ¬ë°ĮìĹĪìĿĮ&#34;:9784,&#34;ĠìĽĥê²¼ëĭ¤&#34;:9785,&#34;ëĳĲê·¼&#34;:9786,&#34;Ġê±°ìķ¼&#34;:9787,&#34;ë°Ķëĭ¤&#34;:9788,&#34;Ġë°ĶëŀĢëĭ¤&#34;:9789,&#34;êµ°ìĿ´&#34;:9790,&#34;!!!!!!!&#34;:9791,&#34;Ġíı¼&#34;:9792,&#34;íĮĲìľ¼ë¡ľ&#34;:9793,&#34;Ġìµľê³łìĿĺìĺģíĻĶ&#34;:9794,&#34;Ġì²ĺìĿĮìĿ´&#34;:9795,&#34;ĠëªħìŀĳìĿĢ&#34;:9796,&#34;Ġë¡ľë¹Ī&#34;:9797,&#34;Ġë¡ľë²ĦíĬ¸&#34;:9798,&#34;ìĻľìĿ´ë¦¬&#34;:9799,&#34;íħĮë¦¬&#34;:9800,&#34;ê²©ìĿ´&#34;:9801,&#34;Ġë§īìĥģ&#34;:9802,&#34;¬ëĿ¼ê¸°&#34;:9803,&#34;ĠìĿĮëª¨&#34;:9804,&#34;Ġêµ¬ì¡°&#34;:9805,&#34;Ġë§ŀì¶Ķ&#34;:9806,&#34;ì¼ĢíĮħ&#34;:9807,&#34;ĠìķĦìĿ´ëĵ¤ìĿĺ&#34;:9808,&#34;ĠìķĦìĿ´ëĵ¤ìĹĲê²Į&#34;:9809,&#34;Ġëª°ìŀħíķĺê²Į&#34;:9810,&#34;ì§ĢìķĬê²Į&#34;:9811,&#34;ì§ĢìķĬìķĦ&#34;:9812,&#34;ìĵ°ëłĪê¸°ëĭ¤&#34;:9813,&#34;ĠëĪĦëĤĺ&#34;:9814,&#34;ĠìĿ¸ìĥĿìĿĢ&#34;:9815,&#34;Ġë°ĽìķĦìķ¼&#34;:9816,&#34;Ġê·¹ìŀ¥ìĹĲ&#34;:9817,&#34;ëĦĺì¹ĺëĬĶ&#34;:9818,&#34;íĬ¹ë³Ħ&#34;:9819,&#34;Ġë²Ħë¬´&#34;:9820,&#34;ĠìĹĦì²ŃëĤĺê²Į&#34;:9821,&#34;ĠìĹīëļ±&#34;:9822,&#34;ĠìĨĲê¼½&#34;:9823,&#34;ĠíĮĮê³ł&#34;:9824,&#34;Ġë¬¼ìĸ´&#34;:9825,&#34;Ġê°Ģì¡±ìĿĦ&#34;:9826,&#34;Ġê°Ģì¡±ìķł&#34;:9827,&#34;Ġë¶Ģì¡±íķĺê³ł&#34;:9828,&#34;ĠìĤ¶ê³¼&#34;:9829,&#34;Ġìŀ¬ë°ĭìĿĮ&#34;:9830,&#34;ĠëĤĺë¦ĦëĮĢë¡ľ&#34;:9831,&#34;Ġê´Ģê°ĿìĿ´&#34;:9832,&#34;Ġì¶©ê²©ìłģìĿ´&#34;:9833,&#34;ë¡Ńëĭ¤&#34;:9834,&#34;Ġíģ´ë¦¬&#34;:9835,&#34;ĠìºĲìĬ¤íĮħìĿ´&#34;:9836,&#34;Ġë¹ĦêµĲíķĺë©´&#34;:9837,&#34;Ġì±ĦìĽĮ&#34;:9838,&#34;ĠìĪľìĪĺíķĺê³ł&#34;:9839,&#34;ìĵ¸ëį°&#34;:9840,&#34;ìŀĳê°Ģëĭĺ&#34;:9841,&#34;Ġìıł&#34;:9842,&#34;Ġë¹µìłĲ&#34;:9843,&#34;Ġì´ĪëĶ©ëķĮ&#34;:9844,&#34;Ġìī½ì§Ģ&#34;:9845,&#34;Ġë°ĶëĿ¼ë³´ëĬĶ&#34;:9846,&#34;ĠìĨĮë¦¬ë§Į&#34;:9847,&#34;Ġê·¸ëŁ´ìĭ¸&#34;:9848,&#34;íķĺê²łìĬµëĭĪëĭ¤&#34;:9849,&#34;ĠíĻįì½©ìĺģíĻĶ&#34;:9850,&#34;ë±Ģ&#34;:9851,&#34;ëĭ¬ëĿ¼ê³ł&#34;:9852,&#34;ìĹ¬ëŁ¬ë¶Ħ&#34;:9853,&#34;ĠìĦ¸ëł¨ëĲľ&#34;:9854,&#34;Ġìĺģíĸ¥ìĿĦ&#34;:9855,&#34;ĠíħĲëį°&#34;:9856,&#34;ëº&#34;:9857,&#34;ìĿ´ëŁ´&#34;:9858,&#34;ìķ¨&#34;:9859,&#34;ìĿĺìĻ¸ë¡ľ&#34;:9860,&#34;ë¡ľëĿ¼&#34;:9861,&#34;ë§Įìķ½&#34;:9862,&#34;ë¦¬ìĬ¨&#34;:9863,&#34;ĠìłĪë°ĺ&#34;:9864,&#34;ìķĦëĥĲ&#34;:9865,&#34;ìĿ¸íķ´&#34;:9866,&#34;ëį°ìļĶ&#34;:9867,&#34;Ġì§Ļ&#34;:9868,&#34;ĠëĭĪëĦ¤&#34;:9869,&#34;ĠëĤĺìķĦ&#34;:9870,&#34;Ġëĭ¤ë¦Ħ&#34;:9871,&#34;ìłĦíŀĪ&#34;:9872,&#34;Ġìłķì£¼íĸī&#34;:9873,&#34;ĠìŀĪìĸ´ëıĦ&#34;:9874,&#34;ìĺ¤ë²Ħ&#34;:9875,&#34;Ġëĵ¬&#34;:9876,&#34;Ġìĸ´ëĶĺ&#34;:9877,&#34;Ġìĺ¬ë¦¬ë&#34;:9878,&#34;Ġì§Ģê°Ģ&#34;:9879,&#34;ê·¸ëŁ´&#34;:9880,&#34;ì¡ĭ&#34;:9881,&#34;Ġê°Ģê¹Ŀ&#34;:9882,&#34;ìĦ±ê³µ&#34;:9883,&#34;íŀĪíŀĪ&#34;:9884,&#34;ìĸĢ&#34;:9885,&#34;Ġê¸°ë²ķ&#34;:9886,&#34;ìĿ¼ëķĮ&#34;:9887,&#34;ĠìĹ°ê¸°íķľ&#34;:9888,&#34;ĠìĹ°ê¸°ë¡ľ&#34;:9889,&#34;êµ¬êµ¬&#34;:9890,&#34;ĠìłĦëıĦ&#34;:9891,&#34;êµŃëĤ´&#34;:9892,&#34;ì¤ĳíķľ&#34;:9893,&#34;ë¶ĦìłķëıĦ&#34;:9894,&#34;ê²łê³ł&#34;:9895,&#34;ëł¥ìĹĲ&#34;:9896,&#34;ëħĦëıĻìķĪ&#34;:9897,&#34;ìĺĢêµ¬ëĤĺ&#34;:9898,&#34;ĠìĸĦ&#34;:9899,&#34;ĠëĤ¨ê¸´&#34;:9900,&#34;ĠìĿ´ëŁ°ìĭĿìľ¼ë¡ľ&#34;:9901,&#34;ìĽĲìĿĦ&#34;:9902,&#34;ìĪĢ&#34;:9903,&#34;Ġë²ĦìłĦ&#34;:9904,&#34;ĠëĵľëĿ¼ë§ĪëıĦ&#34;:9905,&#34;ë¦¬ëł¤&#34;:9906,&#34;ĠìĤ¬ëŀĳìĬ¤ëŁ½ê³ł&#34;:9907,&#34;íĹī&#34;:9908,&#34;ë°ĺëĭ´&#34;:9909,&#34;¬ë¦´&#34;:9910,&#34;ë°ľë¡ľ&#34;:9911,&#34;ìĿ´ëŁ°ìĺģíĻĶëĬĶ&#34;:9912,&#34;ĠìĿĺë¦¬&#34;:9913,&#34;Ġì¶Ķê°Ģ&#34;:9914,&#34;Ġê¸°ëĮĢíĸĪ&#34;:9915,&#34;ĠìĦ±íĺķ&#34;:9916,&#34;ë°°ìļ°ëĵ¤ëıĦ&#34;:9917,&#34;ìĤ¬ëŀĮëıĦ&#34;:9918,&#34;ĠìĹĲìĿ´&#34;:9919,&#34;ĠëĲĲëĭ¤&#34;:9920,&#34;íģ¬ê°Ģ&#34;:9921,&#34;Ġë§¤ëł¥ìłģ&#34;:9922,&#34;ì§ĢìķĬìĿĮ&#34;:9923,&#34;ĠìķĪëĲĺëĦ¤&#34;:9924,&#34;Ġìłģê·¹&#34;:9925,&#34;ĠìĥĿì¡´&#34;:9926,&#34;íĪŃ&#34;:9927,&#34;ĠëĪĪìľ¼ë¡ľ&#34;:9928,&#34;ĠìļķìĿĦ&#34;:9929,&#34;Ġìŀ¬ìķĻ&#34;:9930,&#34;ìłĲëıĦìķĦê¹Ŀëĭ¤&#34;:9931,&#34;Ġì¹ĺë£Į&#34;:9932,&#34;Ġê°Ħì§Ģ&#34;:9933,&#34;ëģĿëĤ´&#34;:9934,&#34;ĠìŀĲì²´ëıĦ&#34;:9935,&#34;Ġìķħë§Ī&#34;:9936,&#34;ĠëĤĺìĻĢëıĦ&#34;:9937,&#34;ê²ģê²Į&#34;:9938,&#34;ë¸Ķë£¨&#34;:9939,&#34;ìĽĲìŀĳìĹĲ&#34;:9940,&#34;ëŃĶì§Ģ&#34;:9941,&#34;Ġìĸ´ìĿ´ìĹĨëĭ¤&#34;:9942,&#34;ĠìĦ¸ìĥģìĹĲìĦľ&#34;:9943,&#34;ì±ħìŀĦ&#34;:9944,&#34;Ġì¶ĶìĸµìĿĦ&#34;:9945,&#34;ìĭ¸ìĿ´ì½Ķ&#34;:9946,&#34;ĠìĬ¤íĥĢëİĢ&#34;:9947,&#34;ĠìķĶìļ¸&#34;:9948,&#34;Ġëª°ìŀħëıĦëıĦ&#34;:9949,&#34;ìĹĶëĶ©ìĿ´&#34;:9950,&#34;íģ¬ë¥¼&#34;:9951,&#34;ĠìĥģíĻ©ìĿ´&#34;:9952,&#34;ìĭŃëĭĪëĭ¤&#34;:9953,&#34;Ġëĵ£ëĬĶ&#34;:9954,&#34;ëĤĺë¦ĦëĮĢë¡ľ&#34;:9955,&#34;ë³¼ë§Įíķ¨&#34;:9956,&#34;ĠìĹīë§Ŀì§Ħì°½&#34;:9957,&#34;ĠëĨĪëĵ¤&#34;:9958,&#34;ĠìĿ´ëĶ°ìľĦë¡ľ&#34;:9959,&#34;ĠíĹĪìĪłíķľ&#34;:9960,&#34;Ġìĥģì²ĺë¥¼&#34;:9961,&#34;ìłĲì£¼ê¸°ëıĦ&#34;:9962,&#34;íĿĶíķľ&#34;:9963,&#34;BSìĹĲìĦľ&#34;:9964,&#34;ĠíĿīëĤ´ëĤ´&#34;:9965,&#34;ĠìĦ¬ìĦ¸íķľ&#34;:9966,&#34;ĪĦëĿ¼&#34;:9967,&#34;ĠëĵľëĿ¼ë§ĪëĿ¼ê³ł&#34;:9968,&#34;ĠìĿĢê·¼íŀĪ&#34;:9969,&#34;ĠìĹ½ê¸°&#34;:9970,&#34;ê¹ľì§Ŀ&#34;:9971,&#34;Ġìĸ´ìĦ¤íĶĦê²Į&#34;:9972,&#34;ĠìķĪë´Ħ&#34;:9973,&#34;.(&#34;:9974,&#34;.?&#34;:9975,&#34;na&#34;:9976,&#34;ģìĹĲ&#34;:9977,&#34;..;&#34;:9978,&#34;ëĬĻ&#34;:9979,&#34;ìķµ&#34;:9980,&#34;ìķľ&#34;:9981,&#34;ìļ¬&#34;:9982,&#34;íķĺëįĶëĭĪ&#34;:9983,&#34;ê¸°ì¢ħìĺģ&#34;:9984,&#34;ìĿĦêº¼&#34;:9985,&#34;ĠìĿ´ì§Ģ&#34;:9986,&#34;ìĸ´ëł¤&#34;:9987,&#34;ë¡ľìĿĺ&#34;:9988,&#34;ë¦¬ì¦ĺ&#34;:9989,&#34;ëĵ¤ë¦°&#34;:9990,&#34;ìķĦì¤Įë§Ī&#34;:9991,&#34;ìĿ¸ëĵ¤ìĿĢ&#34;:9992,&#34;ìĺģíĻĶì¤ĳìĹĲ&#34;:9993,&#34;ë³´ìĿ¸ëĭ¤&#34;:9994,&#34;ìŀĲìľł&#34;:9995,&#34;ìĹĪìĿĦíħĲëį°&#34;:9996,&#34;Ġíķĺìłķìļ°&#34;:9997,&#34;ìķ¼ê²łëĭ¤&#34;:9998,&#34;ìĪĺê³ł&#34;:9999} . !head /gdrive/My Drive/nlpbook/bbpe/merges.txt . #version: 0.2 - Trained by `huggingface/tokenizers` Ġ ì Ġ ë ì Ŀ ë ĭ í ķ ê ° . . ìĿ ´ ëĭ ¤ . BPE 어휘집합과 바이그램 쌍/병합우선순위 일부를 출력했습니다. . BERT &#53664;&#53356;&#45208;&#51060;&#51200; &#44396;&#52629; . import os os.makedirs(&#39;/gdrive/My Drive/nlpbook/wordpiece&#39;, exist_ok= True) . from tokenizers import BertWordPieceTokenizer wordpiece_tokenizer = BertWordPieceTokenizer(lowercase = False) wordpiece_tokenizer.train( files=[&#39;/root/train.txt&#39;, &#39;/root/test.txt&#39;], vocab_size = 10000, ) wordpiece_tokenizer.save_model(&#39;/gdrive/My Drive/nlpbook/wordpiece&#39;) . [&#39;/gdrive/My Drive/nlpbook/wordpiece/vocab.txt&#39;] . 워드피스 방식입니다. BPE와 차이점은 단순히 빈도를 기준으로 병합하는 것이 아니라 병합했을 때 말뭉치의 우도를 높이는 쌍을 병합합니다. . !head /gdrive/My Drive/nlpbook/wordpiece/vocab.txt . [PAD] [UNK] [CLS] [SEP] [MASK] ! &#34; % &amp; &#39; . 워드피스 수행 결과의 일부 입니다. . &#53664;&#53360;&#54868;&#54616;&#44592; - GPT . from transformers import GPT2Tokenizer tokenizer_gpt = GPT2Tokenizer.from_pretrained(&#39;/gdrive/My Drive/nlpbook/bbpe&#39;) tokenizer_gpt.pad_token = &#39;[PAD]&#39; . file /gdrive/My Drive/nlpbook/bbpe/config.json not found . sentences = [ &quot;아 더빙.. 진짜 짜증나네요 목소리&quot;, &quot;흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나&quot;, &quot;별루 였다..&quot;, ] tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences] tokenized_sentences . [[&#39;ìķĦ&#39;, &#39;ĠëįĶë¹Ļ&#39;, &#39;..&#39;, &#39;Ġì§Ħì§ľ&#39;, &#39;Ġì§ľì¦ĿëĤĺ&#39;, &#39;ëĦ¤ìļĶ&#39;, &#39;Ġëª©ìĨĮë¦¬&#39;], [&#39;íĿł&#39;, &#39;...&#39;, &#39;íı¬ìĬ¤íĦ°&#39;, &#39;ë³´ê³ł&#39;, &#39;Ġì´ĪëĶ©&#39;, &#39;ìĺģíĻĶ&#39;, &#39;ì¤Ħ&#39;, &#39;....&#39;, &#39;ìĺ¤ë²Ħ&#39;, &#39;ìĹ°ê¸°&#39;, &#39;ì¡°ì°¨&#39;, &#39;Ġê°Ģë³į&#39;, &#39;ì§Ģ&#39;, &#39;ĠìķĬ&#39;, &#39;êµ¬ëĤĺ&#39;], [&#39;ë³Ħë£¨&#39;, &#39;Ġìĺ&#39;, &#39;Ģëĭ¤&#39;, &#39;..&#39;]] . GPT 토크나이저로 토큰화 해봤는데요. 알수 없는 언어들이 출력됩니다. . 이는 GPT 모델은 바이트 기준 BPE를 적용하기 때문입니다. . batch_inputs = tokenizer_gpt( sentences, padding = &#39;max_length&#39;, # 문장 최대 길이에 맞춰 패딩 max_length = 12, # 문장 토큰 기준 최대 길이 truncation = True, # 문장 잘림 허용 옵션 ) batch_inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;]) . 실제 모델 입력값 입니다. 결과는 input_ids, attention_mask 두개로 나옵니다. . batch_inputs[&#39;input_ids&#39;] . [[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0], [3693, 336, 2876, 758, 2883, 356, 806, 422, 9875, 875, 2960, 7292], [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]] . input_ids는 토큰화 결과를 가지고 각 토큰을 인덱스로 바꾼 것 입니다. . 여기서 모든 문장의 길이가 12로 맞춰줬는데 앞선 max_length 인자에 12를 넣었기 때문입니다. . [PAD] 토큰은 인덱스 0이며 더미 토큰입니다. 즉 위 값 중 0이 있으면 문장이 짧아 토큰 길이를 맞춰준 것으로 생각할 수 있습니다. . 문장 잘림을 허용하는 옵션 때문에 문장2는 토큰길이가 원래 15였는데 12안에 들어왔습니다. . batch_inputs[&#39;attention_mask&#39;] . [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]] . attention_mask는 일반 토큰이 자리한 곳(1)과 패딩 토큰이 자리한 곳(0)을 구분해주는 장치입니다. . &#53664;&#53360;&#54868;&#54616;&#44592; - BERT . from transformers import BertTokenizer tokenizer_bert = BertTokenizer.from_pretrained( &#39;/gdrive/My Drive/nlpbook/wordpiece&#39;, do_lower_case = False, ) . file /gdrive/My Drive/nlpbook/wordpiece/config.json not found . sentences = [ &quot;아 더빙.. 진짜 짜증나네요 목소리&quot;, &quot;흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나&quot;, &quot;별루 였다..&quot;, ] tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences] tokenized_sentences . [[&#39;아&#39;, &#39;더빙&#39;, &#39;.&#39;, &#39;.&#39;, &#39;진짜&#39;, &#39;짜증나&#39;, &#39;##네요&#39;, &#39;목소리&#39;], [&#39;흠&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;포스터&#39;, &#39;##보고&#39;, &#39;초딩&#39;, &#39;##영화&#39;, &#39;##줄&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;오버&#39;, &#39;##연기&#39;, &#39;##조차&#39;, &#39;가볍&#39;, &#39;##지&#39;, &#39;않&#39;, &#39;##구나&#39;], [&#39;별루&#39;, &#39;였다&#39;, &#39;.&#39;, &#39;.&#39;]] . 토큰 일부에 있는 ##은 해당 토큰이 어절의 시작이 아님을 나타냅니다. . batch_inputs = tokenizer_bert( sentences, padding = &#39;max_length&#39;, max_length = 12, truncation = True, ) batch_inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]) . 코드 적합시 input_ids, token_type_ids, attention_mask 총 3개의 출력물이 나옵니다. . batch_inputs[&#39;input_ids&#39;] . [[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0], [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1040, 16, 3], [2, 3274, 9507, 16, 16, 3, 0, 0, 0, 0, 0, 0]] . 모든 문장 시작시 2, 끝날 때 3이 붙은 것을 알 수 있습니다. . 여기서 2는 [CLS], 3은 [SEP]라는 토큰에 대응하는 인덱스입니다. . batch_inputs[&#39;attention_mask&#39;] . [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]] . attention_mask은 GPT와 마찬가지로 일반 토큰이 차지한 부분을 구분하는 역할을 합니다. . batch_inputs[&#39;token_type_ids&#39;] . [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] . token_type_ids는 세그먼트에 해당합니다. BERT 모델은 기본적으로 문서 2개를 입력받는데요. . 첫 번째 세그먼트(문서 혹은 문장)은 0, 두 번째 세그먼트는 1을 주어 둘을 구분합니다. . 이번 실습에서 문장을 하나씩 넣었으므로 모든 값이 0입니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/gpt/bert/token/2021/12/20/Do_natural_language1.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/gpt/bert/token/2021/12/20/Do_natural_language1.html",
            "date": " • Dec 20, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "[SSUDA] 캐글 이용자 2021 설문조사 결과 분석",
            "content": ". &#52880;&#44544;&#44284; &#50672;&#46041;&#54616;&#44592; . !pip install kaggle !pip install --upgrade --force-reinstall --no-deps kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Collecting kaggle Downloading kaggle-1.5.12.tar.gz (58 kB) |████████████████████████████████| 58 kB 2.5 MB/s Building wheels for collected packages: kaggle Building wheel for kaggle (setup.py) ... done Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=540a3a7d36ae6106f20d1f7c29b1afe562ca44be8bd818181fa99f5c13aeecb8 Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5 Successfully built kaggle Installing collected packages: kaggle Attempting uninstall: kaggle Found existing installation: kaggle 1.5.12 Uninstalling kaggle-1.5.12: Successfully uninstalled kaggle-1.5.12 Successfully installed kaggle-1.5.12 . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;ff1e945a67cd54bc7068e3afe4a03ad6&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c kaggle-survey-2021 . Downloading kaggle-survey-2021.zip to /content 0% 0.00/3.01M [00:00&lt;?, ?B/s] 100% 3.01M/3.01M [00:00&lt;00:00, 103MB/s] . !unzip kaggle-survey-2021.zip . Archive: kaggle-survey-2021.zip inflating: kaggle_survey_2021_responses.csv inflating: supplementary_data/kaggle_survey_2021_answer_choices.pdf inflating: supplementary_data/kaggle_survey_2021_methodology.pdf . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import gc # For Memory Optimization import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Not sure if I used this from wordcloud import WordCloud from scipy.stats import norm # Some more necessary libraries (These are for drawing the image on the bar charts) import matplotlib.font_manager as fm from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox import matplotlib.image as mpimg # To Avoid unnecessary warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) # Since there are many columns, I would like to view them all pd.set_option(&#39;display.max_rows&#39;, 100) pd.set_option(&#39;display.max_columns&#39;, 400) . df = pd.read_csv(&#39;kaggle_survey_2021_responses.csv&#39;) df = df.iloc[1:,:] # The first row was describing the columns. Better to look at the description from the Metadata file provided df.head(3).style.set_properties(**{&quot;background-color&quot;: &quot;#76c5d6&quot;,&quot;color&quot;: &quot;black&quot;, &quot;border-color&quot;: &quot;black&quot;}) . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 Q7_Part_4 Q7_Part_5 Q7_Part_6 Q7_Part_7 Q7_Part_8 Q7_Part_9 Q7_Part_10 Q7_Part_11 Q7_Part_12 Q7_OTHER Q8 Q9_Part_1 Q9_Part_2 Q9_Part_3 Q9_Part_4 Q9_Part_5 Q9_Part_6 Q9_Part_7 Q9_Part_8 Q9_Part_9 Q9_Part_10 Q9_Part_11 Q9_Part_12 Q9_OTHER Q10_Part_1 Q10_Part_2 Q10_Part_3 Q10_Part_4 Q10_Part_5 Q10_Part_6 Q10_Part_7 Q10_Part_8 Q10_Part_9 Q10_Part_10 Q10_Part_11 Q10_Part_12 Q10_Part_13 Q10_Part_14 Q10_Part_15 Q10_Part_16 Q10_OTHER Q11 Q12_Part_1 Q12_Part_2 Q12_Part_3 Q12_Part_4 Q12_Part_5 Q12_OTHER Q13 Q14_Part_1 Q14_Part_2 Q14_Part_3 Q14_Part_4 Q14_Part_5 Q14_Part_6 Q14_Part_7 Q14_Part_8 Q14_Part_9 Q14_Part_10 Q14_Part_11 Q14_OTHER Q15 Q16_Part_1 Q16_Part_2 Q16_Part_3 Q16_Part_4 Q16_Part_5 Q16_Part_6 Q16_Part_7 Q16_Part_8 Q16_Part_9 Q16_Part_10 Q16_Part_11 Q16_Part_12 Q16_Part_13 Q16_Part_14 Q16_Part_15 Q16_Part_16 Q16_Part_17 Q16_OTHER Q17_Part_1 Q17_Part_2 Q17_Part_3 Q17_Part_4 Q17_Part_5 Q17_Part_6 Q17_Part_7 Q17_Part_8 Q17_Part_9 Q17_Part_10 Q17_Part_11 Q17_OTHER Q18_Part_1 Q18_Part_2 Q18_Part_3 Q18_Part_4 Q18_Part_5 Q18_Part_6 Q18_OTHER Q19_Part_1 Q19_Part_2 Q19_Part_3 Q19_Part_4 Q19_Part_5 Q19_OTHER Q20 Q21 Q22 Q23 Q24_Part_1 Q24_Part_2 Q24_Part_3 Q24_Part_4 Q24_Part_5 Q24_Part_6 Q24_Part_7 Q24_OTHER Q25 Q26 Q27_A_Part_1 Q27_A_Part_2 Q27_A_Part_3 Q27_A_Part_4 Q27_A_Part_5 Q27_A_Part_6 Q27_A_Part_7 Q27_A_Part_8 Q27_A_Part_9 Q27_A_Part_10 Q27_A_Part_11 Q27_A_OTHER Q28 Q29_A_Part_1 Q29_A_Part_2 Q29_A_Part_3 Q29_A_Part_4 Q29_A_OTHER Q30_A_Part_1 Q30_A_Part_2 Q30_A_Part_3 Q30_A_Part_4 Q30_A_Part_5 Q30_A_Part_6 Q30_A_Part_7 Q30_A_OTHER Q31_A_Part_1 Q31_A_Part_2 Q31_A_Part_3 Q31_A_Part_4 Q31_A_Part_5 Q31_A_Part_6 Q31_A_Part_7 Q31_A_Part_8 Q31_A_Part_9 Q31_A_OTHER Q32_A_Part_1 Q32_A_Part_2 Q32_A_Part_3 Q32_A_Part_4 Q32_A_Part_5 Q32_A_Part_6 Q32_A_Part_7 Q32_A_Part_8 Q32_A_Part_9 Q32_A_Part_10 Q32_A_Part_11 Q32_A_Part_12 Q32_A_Part_13 Q32_A_Part_14 Q32_A_Part_15 Q32_A_Part_16 Q32_A_Part_17 Q32_A_Part_18 Q32_A_Part_19 Q32_A_Part_20 Q32_A_OTHER Q33 Q34_A_Part_1 Q34_A_Part_2 Q34_A_Part_3 Q34_A_Part_4 Q34_A_Part_5 Q34_A_Part_6 Q34_A_Part_7 Q34_A_Part_8 Q34_A_Part_9 Q34_A_Part_10 Q34_A_Part_11 Q34_A_Part_12 Q34_A_Part_13 Q34_A_Part_14 Q34_A_Part_15 Q34_A_Part_16 Q34_A_OTHER Q35 Q36_A_Part_1 Q36_A_Part_2 Q36_A_Part_3 Q36_A_Part_4 Q36_A_Part_5 Q36_A_Part_6 Q36_A_Part_7 Q36_A_OTHER Q37_A_Part_1 Q37_A_Part_2 Q37_A_Part_3 Q37_A_Part_4 Q37_A_Part_5 Q37_A_Part_6 Q37_A_Part_7 Q37_A_OTHER Q38_A_Part_1 Q38_A_Part_2 Q38_A_Part_3 Q38_A_Part_4 Q38_A_Part_5 Q38_A_Part_6 Q38_A_Part_7 Q38_A_Part_8 Q38_A_Part_9 Q38_A_Part_10 Q38_A_Part_11 Q38_A_OTHER Q39_Part_1 Q39_Part_2 Q39_Part_3 Q39_Part_4 Q39_Part_5 Q39_Part_6 Q39_Part_7 Q39_Part_8 Q39_Part_9 Q39_OTHER Q40_Part_1 Q40_Part_2 Q40_Part_3 Q40_Part_4 Q40_Part_5 Q40_Part_6 Q40_Part_7 Q40_Part_8 Q40_Part_9 Q40_Part_10 Q40_Part_11 Q40_OTHER Q41 Q42_Part_1 Q42_Part_2 Q42_Part_3 Q42_Part_4 Q42_Part_5 Q42_Part_6 Q42_Part_7 Q42_Part_8 Q42_Part_9 Q42_Part_10 Q42_Part_11 Q42_OTHER Q27_B_Part_1 Q27_B_Part_2 Q27_B_Part_3 Q27_B_Part_4 Q27_B_Part_5 Q27_B_Part_6 Q27_B_Part_7 Q27_B_Part_8 Q27_B_Part_9 Q27_B_Part_10 Q27_B_Part_11 Q27_B_OTHER Q29_B_Part_1 Q29_B_Part_2 Q29_B_Part_3 Q29_B_Part_4 Q29_B_OTHER Q30_B_Part_1 Q30_B_Part_2 Q30_B_Part_3 Q30_B_Part_4 Q30_B_Part_5 Q30_B_Part_6 Q30_B_Part_7 Q30_B_OTHER Q31_B_Part_1 Q31_B_Part_2 Q31_B_Part_3 Q31_B_Part_4 Q31_B_Part_5 Q31_B_Part_6 Q31_B_Part_7 Q31_B_Part_8 Q31_B_Part_9 Q31_B_OTHER Q32_B_Part_1 Q32_B_Part_2 Q32_B_Part_3 Q32_B_Part_4 Q32_B_Part_5 Q32_B_Part_6 Q32_B_Part_7 Q32_B_Part_8 Q32_B_Part_9 Q32_B_Part_10 Q32_B_Part_11 Q32_B_Part_12 Q32_B_Part_13 Q32_B_Part_14 Q32_B_Part_15 Q32_B_Part_16 Q32_B_Part_17 Q32_B_Part_18 Q32_B_Part_19 Q32_B_Part_20 Q32_B_OTHER Q34_B_Part_1 Q34_B_Part_2 Q34_B_Part_3 Q34_B_Part_4 Q34_B_Part_5 Q34_B_Part_6 Q34_B_Part_7 Q34_B_Part_8 Q34_B_Part_9 Q34_B_Part_10 Q34_B_Part_11 Q34_B_Part_12 Q34_B_Part_13 Q34_B_Part_14 Q34_B_Part_15 Q34_B_Part_16 Q34_B_OTHER Q36_B_Part_1 Q36_B_Part_2 Q36_B_Part_3 Q36_B_Part_4 Q36_B_Part_5 Q36_B_Part_6 Q36_B_Part_7 Q36_B_OTHER Q37_B_Part_1 Q37_B_Part_2 Q37_B_Part_3 Q37_B_Part_4 Q37_B_Part_5 Q37_B_Part_6 Q37_B_Part_7 Q37_B_OTHER Q38_B_Part_1 Q38_B_Part_2 Q38_B_Part_3 Q38_B_Part_4 Q38_B_Part_5 Q38_B_Part_6 Q38_B_Part_7 Q38_B_Part_8 Q38_B_Part_9 Q38_B_Part_10 Q38_B_Part_11 Q38_B_OTHER . 1 910 | 50-54 | Man | India | Bachelor’s degree | Other | 5-10 years | Python | R | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Python | nan | nan | nan | nan | nan | nan | nan | nan | Vim / Emacs | nan | nan | nan | nan | nan | Colab Notebooks | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | A laptop | nan | Google Cloud TPUs | nan | nan | nan | nan | 2-5 times | Matplotlib | Seaborn | nan | Ggplot / ggplot2 | Shiny | nan | nan | nan | nan | Leaflet / Folium | nan | nan | 5-10 years | Scikit-learn | TensorFlow | nan | nan | nan | nan | nan | nan | nan | nan | nan | Caret | nan | nan | nan | nan | nan | nan | Linear or Logistic Regression | Decision Trees or Random Forests | Gradient Boosting Machines (xgboost, lightgbm, etc) | Bayesian Approaches | nan | Dense Neural Networks (MLPs, etc) | Convolutional Neural Networks | nan | Recurrent Neural Networks | nan | nan | nan | General purpose image/video tools (PIL, cv2, skimage, etc) | nan | nan | nan | nan | nan | nan | Word embeddings/vectors (GLoVe, fastText, word2vec) | nan | nan | nan | nan | nan | Manufacturing/Fabrication | 50-249 employees | 3-4 | No (we do not use ML methods) | nan | nan | nan | nan | nan | nan | None of these activities are an important part of my role at work | nan | 25,000-29,999 | $100-$999 | nan | nan | Google Cloud Platform (GCP) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Google Cloud Compute Engine | nan | nan | nan | nan | nan | nan | Google Cloud Storage (GCS) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | PostgreSQL | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | GitHub | nan | Kaggle | nan | nan | nan | nan | Coursera | edX | Kaggle Learn Courses | DataCamp | nan | Udacity | Udemy | nan | nan | nan | nan | nan | Local development environments (RStudio, JupyterLab, etc.) | nan | Email newsletters (Data Elixir, O&#39;Reilly Data &amp; AI, etc) | nan | Kaggle (notebooks, forums, etc) | nan | YouTube (Kaggle YouTube, Cloud AI Adventures, etc) | Podcasts (Chai Time Data Science, O’Reilly Data Show, etc) | Blogs (Towards Data Science, Analytics Vidhya, etc) | Journal Publications (peer-reviewed journals, conference proceedings, etc) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2 784 | 50-54 | Man | Indonesia | Master’s degree | Program/Project Manager | 20+ years | nan | nan | SQL | C | C++ | Java | nan | nan | nan | nan | nan | nan | nan | Python | nan | nan | nan | nan | nan | nan | Notepad++ | nan | nan | nan | Jupyter Notebook | nan | nan | Kaggle Notebooks | Colab Notebooks | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc) | nan | nan | nan | nan | None | nan | Never | Matplotlib | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Under 1 year | Scikit-learn | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Linear or Logistic Regression | Decision Trees or Random Forests | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Manufacturing/Fabrication | 1000-9,999 employees | 1-2 | We are exploring ML methods (and may one day put a model into production) | nan | Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data | nan | nan | nan | nan | nan | nan | 60,000-69,999 | $0 ($USD) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Kaggle Learn Courses | nan | nan | nan | nan | nan | Cloud-certification programs (direct from AWS, Azure, GCP, or similar) | University Courses (resulting in a university degree) | nan | nan | Advanced statistical software (SPSS, SAS, etc.) | nan | nan | nan | nan | nan | nan | nan | nan | Journal Publications (peer-reviewed journals, conference proceedings, etc) | nan | nan | nan | nan | nan | Google Cloud Platform (GCP) | nan | Oracle Cloud | nan | nan | nan | nan | nan | nan | nan | nan | nan | Google Cloud Compute Engine | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | MySQL | nan | SQLite | Oracle Database | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Google Cloud SQL | nan | nan | nan | nan | nan | nan | nan | Google Data Studio | nan | nan | nan | nan | Qlik | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Automated model selection (e.g. auto-sklearn, xcessiv) | nan | nan | nan | nan | nan | Google Cloud AutoML | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | . 3 924 | 22-24 | Man | Pakistan | Master’s degree | Software Engineer | 1-3 years | Python | nan | nan | nan | C++ | Java | nan | nan | nan | nan | nan | nan | nan | Python | nan | nan | nan | nan | PyCharm | nan | nan | nan | nan | nan | Jupyter Notebook | nan | Other | Kaggle Notebooks | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | A laptop | nan | nan | nan | nan | nan | Other | Never | Matplotlib | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | I do not use machine learning methods | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Academics/Education | 1000-9,999 employees | 0 | I do not know | nan | nan | nan | nan | nan | nan | None of these activities are an important part of my role at work | nan | $0-999 | $0 ($USD) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | DataRobot | nan | nan | nan | nan | nan | nan | MySQL | nan | nan | nan | MongoDB | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | MySQL | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | nan | nan | nan | nan | nan | I do not share my work publicly | nan | nan | nan | nan | DataCamp | nan | nan | nan | nan | nan | nan | nan | nan | Basic statistical software (Microsoft Excel, Google Sheets, etc.) | nan | nan | nan | Kaggle (notebooks, forums, etc) | nan | YouTube (Kaggle YouTube, Cloud AI Adventures, etc) | nan | nan | nan | nan | nan | nan | Amazon Web Services (AWS) | nan | Google Cloud Platform (GCP) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Microsoft Azure Virtual Machines | Google Cloud Compute Engine | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Azure Machine Learning Studio | Google Cloud Vertex AI | DataRobot | nan | nan | nan | nan | nan | nan | MySQL | PostgreSQL | nan | nan | MongoDB | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Microsoft Power BI | nan | nan | nan | Tableau | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Automated model selection (e.g. auto-sklearn, xcessiv) | nan | nan | nan | nan | nan | nan | nan | nan | DataRobot AutoML | nan | nan | nan | nan | nan | nan | nan | nan | TensorBoard | nan | nan | nan | nan | nan | nan | nan | . print(&#39;Number of rows:&#39;, df.shape[0]) print(&#39;Number of columns:&#39;, df.shape[1]) . Number of rows: 25973 Number of columns: 369 . &#52395;&#48264;&#51704; &#51656;&#47928; : &#45208;&#51060; . df[&#39;Q1&#39;].value_counts() . 25-29 4931 18-21 4901 22-24 4694 30-34 3441 35-39 2504 40-44 1890 45-49 1375 50-54 964 55-59 592 60-69 553 70+ 128 Name: Q1, dtype: int64 . fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) # Method for image def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 # Creating a DataFrame to get the values and their counts (this was for my purpose) # new_df = pd.DataFrame(df[&#39;Q1&#39;].value_counts()) # I wanted to have the highest value in the middle, so i wrote the following two code lines age_bucket = [&#39;70+&#39;,&#39;55-59&#39;,&#39;45-49&#39;,&#39;35-39&#39;,&#39;22-24&#39;,&#39;25-29&#39;,&#39;18-21&#39;,&#39;30-34&#39;,&#39;40-44&#39;,&#39;50-54&#39;,&#39;60-69&#39;] #new_df.index age_bucket_cnt = [128,592,1375,2504,4694,4931,4901,3441,1890,964,553] #list(new_df.Q1.values) color = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;] # Deciding the color width = [0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.8, 0.8, 0.8, 0.8] # The Width alpha = [0.3, 0.45, 0.5, 0.6, 0.75, 1.0, 0.75, 0.6, 0.5, 0.45, 0.3] # The Opacity fontsize= [20, 20, 20, 20, 25, 35, 30, 20, 20, 20, 20] x_num = [0,1,2,3,4,5,6,7,8,9,10] for i in range(11): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Age Bucket of all Kagglers&quot;,x=5,y=5500, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.2, 5, 4700) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 확실히 대학생이나 취업 준비생이 많이 이용하는 느낌이다. . 다만 18-21세 연령대 이용률이 생각보다 높은 것이 신기했다. . &#46160;&#48264;&#51704; &#51656;&#47928;: &#49457;&#48324; . df[&#39;Q2&#39;].value_counts() . Man 20598 Woman 4890 Prefer not to say 355 Nonbinary 88 Prefer to self-describe 42 Name: Q2, dtype: int64 . Gender = [&#39;Man&#39;, &#39;Woman&#39;, &#39;Others&#39;] # Setting size in Chart based on # given values Gender_cnt = [20598, 4890, 485] # colors colors = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#FFFF00&#39;, &#39;#ADFF2F&#39;, &#39;#FFA500&#39;] # explosion explode = (0.05, 0.05, 0.2) plt.figure(figsize=[20,10]) # Pie Chart plt.pie(Gender_cnt, colors=colors, autopct=&#39;%1.1f%%&#39;, pctdistance=1.2, explode=explode,) # draw circle centre_circle = plt.Circle((0, 0), 0.70, fc=&#39;white&#39;) fig = plt.gcf() plt.legend(Gender, loc = &quot;upper right&quot;,title=&quot;Genders&quot;, prop={&#39;size&#39;: 15}) # Adding Circle in Pie chart fig.gca().add_artist(centre_circle) plt.rcParams[&#39;font.size&#39;] = 25 # Adding Title of chart plt.text(s=&quot;Gender Diversity in Kaggle&quot;,x=0,y=1.3, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) gc.collect() # Displaing Chart plt.show() . 남자가 약 80%, 여자가 약 18%이고 기타 이유(공개 희망 안함, 미 기제 등) 2% 입니다. . 확실히 남성이 주류인 분야인 것 같습니다. . &#49464;&#48264;&#51704; &#51656;&#47928;: &#44397;&#51201; . df[&#39;Q3&#39;].value_counts() . India 7434 United States of America 2650 Other 1270 Japan 921 China 814 Brazil 751 Russia 742 Nigeria 702 United Kingdom of Great Britain and Northern Ireland 550 Pakistan 530 Egypt 482 Germany 470 Spain 454 Indonesia 444 Turkey 416 France 401 South Korea 359 Taiwan 334 Canada 331 Bangladesh 317 Italy 311 Mexico 279 Viet Nam 277 Australia 264 Kenya 248 Colombia 225 Poland 219 Iran, Islamic Republic of... 195 Ukraine 186 Singapore 182 Argentina 182 Malaysia 156 Netherlands 153 South Africa 146 Morocco 140 Israel 138 Thailand 123 Portugal 119 Peru 117 United Arab Emirates 111 Tunisia 109 Philippines 108 Sri Lanka 106 Chile 102 Greece 102 Ghana 99 Saudi Arabia 89 Ireland 84 Sweden 81 Hong Kong (S.A.R.) 79 Nepal 75 Switzerland 71 I do not wish to disclose my location 69 Belgium 65 Czech Republic 63 Romania 61 Austria 51 Belarus 51 Ecuador 50 Denmark 48 Uganda 47 Norway 45 Kazakhstan 45 Algeria 44 Ethiopia 43 Iraq 43 Name: Q3, dtype: int64 . !pip install geopandas import geopandas as gpd # List of countries we are interested in lis_countries = [&quot;Algeria&quot;,&quot;Argentina&quot;,&quot;Australia&quot;,&quot;Austria&quot;,&quot;Bangladesh&quot;,&quot;Belarus&quot;,&quot;Belgium&quot;,&quot;Brazil&quot;,&quot;Canada&quot;,&quot;Chile&quot;,&quot;China&quot;,&quot;Colombia&quot;, &quot;Czechia&quot;,&quot;Denmark&quot;,&quot;Ecuador&quot;,&quot;Egypt&quot;,&quot;Ethiopia&quot;,&quot;France&quot;,&quot;Germany&quot;,&quot;Ghana&quot;,&quot;Greece&quot;,&quot;India&quot;,&quot;Indonesia&quot;,&quot;Iraq&quot;,&quot;Ireland&quot;, &quot;Israel&quot;,&quot;Italy&quot;,&quot;Japan&quot;,&quot;Kazakhstan&quot;,&quot;Kenya&quot;,&quot;Malaysia&quot;,&quot;Mexico&quot;,&quot;Morocco&quot;,&quot;Nepal&quot;,&quot;Netherlands&quot;,&quot;Nigeria&quot;,&quot;Norway&quot;,&quot;Pakistan&quot;, &quot;Peru&quot;,&quot;Philippines&quot;,&quot;Poland&quot;,&quot;Portugal&quot;,&quot;Romania&quot;,&quot;Russia&quot;,&quot;Saudi Arabia&quot;,&quot;South Africa&quot;,&quot;South Korea&quot;,&quot;Spain&quot;,&quot;Sri Lanka&quot;, &quot;Sweden&quot;,&quot;Switzerland&quot;,&quot;Taiwan&quot;,&quot;Thailand&quot;,&quot;Tunisia&quot;,&quot;Turkey&quot;,&quot;Uganda&quot;,&quot;Ukraine&quot;,&quot;United Arab Emirates&quot;,&quot;United Kingdom&quot;, &quot;United States of America&quot;,&quot;Vietnam&quot;] # Reading the geopandas data world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) country_data = lis_countries # Passing the list of countries here country_geo = list(world[&#39;name&#39;]) # The country list from the geopandas dataset # List of all the values of population of Kagglers from each country lis_pop = [44,182,264,51,317,51,65,751,331,102,814,225,63,48,50,482,43,401,470,99,102,7434,444,43,84,138,311,921,45,248,156,279,140,75,153, 702,45,530,117,108,219,119,61,742,89,146,359,454,106,81,71,334,123,109,416,47,186,111,550,2650,277] # Next we need to create a dataframe with lis_countries and lis_pop our_country_analysis = pd.DataFrame(lis_countries, columns=[&#39;Country&#39;]) our_country_analysis[&#39;KagglePopulation&#39;] = lis_pop # Next, we are going to visualize this... mapped = world.set_index(&#39;name&#39;).join(our_country_analysis.set_index(&#39;Country&#39;)).reset_index() to_be_mapped = &#39;KagglePopulation&#39; vmin, vmax = 0,10000 fig, ax = plt.subplots(1, figsize=(25,30)) mapped.dropna().plot(column=to_be_mapped, cmap=&#39;cividis&#39;, linewidth=0.8, ax=ax, edgecolors=&#39;1&#39;, alpha=0.7) ax.text(s=&quot;Kagglers All Around the Globe&quot;,x=0,y=100, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) ax.set_axis_off() sm = plt.cm.ScalarMappable(cmap=&#39;cividis&#39;, norm=plt.Normalize(vmin=vmin, vmax=vmax)) sm._A = [] gc.collect() cbar = fig.colorbar(sm, orientation=&#39;vertical&#39;, shrink= .25) . Requirement already satisfied: geopandas in /usr/local/lib/python3.7/dist-packages (0.10.2) Requirement already satisfied: fiona&gt;=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.20) Requirement already satisfied: shapely&gt;=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.0) Requirement already satisfied: pyproj&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (3.2.1) Requirement already satisfied: pandas&gt;=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (2021.10.8) Requirement already satisfied: six&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (1.15.0) Requirement already satisfied: attrs&gt;=17 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (21.2.0) Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (2.5.0) Requirement already satisfied: cligj&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (0.7.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (57.4.0) Requirement already satisfied: click-plugins&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (1.1.1) Requirement already satisfied: click&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (7.1.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2018.9) Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25.0-&gt;geopandas) (1.19.5) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2.8.2) . 포화도가 높을 수록 노란색에 가까워 지는 것을 알 수 있습니다. . 인도 사람들이 확실히 많이 이용하는 모습이군요. . 중간중간 하얗게 빈 나라들도 있습니다. . &#45348;&#48264;&#51704; &#51656;&#47928;: &#54617;&#47141; . df[&#39;Q4&#39;].value_counts() . Master’s degree 10132 Bachelor’s degree 9907 Doctoral degree 2795 Some college/university study without earning a bachelor’s degree 1735 I prefer not to answer 627 No formal education past high school 417 Professional doctorate 360 Name: Q4, dtype: int64 . fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) # Method for image def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 # I wanted to have the highest value in the middle, so i wrote the following two code lines age_bucket = [&#39;Professional Doctorate&#39;,&#39;High School&#39;,&#39;Bachelor’s degree&#39;,&#39;Master’s degree&#39;,&#39;Doctoral degree&#39;,&#39;Others&#39;,&#39;No Answer&#39;] age_bucket_cnt = [360,417,9907,10132,2795,1735,627] color = [&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;] # Deciding the color width = [0.8, 0.8, 0.9, 0.9, 0.9, 0.8, 0.8,] # The Width alpha = [0.5, 0.6, 0.75, 1.0, 0.75, 0.6, 0.5] # The Opacity fontsize= [12, 16, 18, 21, 16, 16, 16] x_num = [0,1,2,3,4,5,6] for i in range(7): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Educational Qualifications of all Kagglers&quot;,x=3,y=11000, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.25, 3, 9500) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 대부분의 캐글 이용자들은 학사 이상의 학위를 가지고 있습니다. . (Master&#39;s degree : 석사, Bachelor&#39;s degree : 학사, Doctoral degree : 박사 학위) . &#45796;&#49455;&#48264;&#51704; &#51656;&#47928;: &#51649;&#50629; . df[&#39;Q5&#39;].value_counts() . Student 6804 Data Scientist 3616 Software Engineer 2449 Other 2393 Data Analyst 2301 Currently not employed 1986 Research Scientist 1538 Machine Learning Engineer 1499 Business Analyst 968 Program/Project Manager 849 Data Engineer 668 Product Manager 319 Statistician 313 DBA/Database Engineer 171 Developer Relations/Advocacy 99 Name: Q5, dtype: int64 . # Method for image def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) # Creating a DataFrame to get the values and their counts (this was for my purpose) # new_df = pd.DataFrame(df[&#39;Q1&#39;].value_counts()) # I wanted to have the highest value in the middle, so i wrote the following two code lines age_bucket = [&#39;Developer n Relations n/Advocacy&#39;,&#39;Statistician&#39;,&#39;Data n Engineer&#39;,&#39;Business n Analyst&#39;,&#39;Research n Scientist&#39;,&#39;Data n Analyst&#39;,&#39;Software n Engineer&#39;,&#39;Student&#39;, &#39;Data n Scientist&#39;,&#39;Other&#39;,&#39;Unemployed&#39;,&#39;ML n Engineer&#39;,&#39;Project n Manager&#39;,&#39;Product n Manager&#39;,&#39;DB n Engineer&#39;] #new_df.index age_bucket_cnt = [99,313,668,968,1538,2301,2449,6804,3414,2393,1986,1499,849,319,171] #list(new_df.Q1.values) color = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#E6E6E6&#39;, &#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;, &#39;#E6E6E6&#39;] # Deciding the color width = [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8] # The Width alpha = [0.3, 0.45, 0.3, 0.45, 0.5, 0.6, 0.75, 1.0, 0.75, 0.6, 0.5, 0.45, 0.3, 0.3, 0.45] # The Opacity fontsize= [12, 12, 14, 14, 14, 14, 18, 20, 16, 14, 12, 14, 14, 12, 12] x_num = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14] for i in range(15): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Current Role of all Kagglers&quot;,x=7.5,y=7500, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.15, 7, 6500) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 작성자의 예측과 다르게 학생이 압도적으로 높은 수치가 나왔습니다. . (대부분이 ML 전문가나 데이터 분석가가 나올것이라고 생각한 것 같아요.) . 여기서 주목할 점이 Others 입니다. 꽤 상위권에 위치하는데요. . 타 분야 사람이 캐글 이용에 적극적인 것으로 생각할 수 있는데요. 데이터 분석이 많은 분야에서 응용될 수 있다는 것을 보여주는 것 같아요. . &#50668;&#49455;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#44221;&#47141; . df[&#39;Q6&#39;].value_counts() . 1-3 years 7874 &lt; 1 years 5881 3-5 years 4061 5-10 years 3099 10-20 years 2166 20+ years 1860 I have never written code 1032 Name: Q6, dtype: int64 . years_bin = [&#39;1-3years&#39;,&#39;&lt;1years&#39;,&#39;3-5years&#39;,&#39;5-10years&#39;,&#39;10-20years&#39;,&#39;20+years&#39;,&#39;Never Coded&#39;] years_cnt = [7874, 5881, 4061, 3099, 2166, 1860, 1032] fig = plt.figure(figsize=(20,10)) plt.barh(width=years_cnt, y=years_bin, height=0.7, color = [&#39;#189AB4&#39;, &#39;#189AB4&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;], alpha=0.8) ##################### For the Years of Experience ################################### s1 = [&#39;1-3years&#39;,&#39;&lt;1years&#39;,&#39;3-5years&#39;,&#39;5-10years&#39;,&#39;10-20years&#39;,&#39;20+years&#39;,&#39;Never Coded&#39;] x1 = [8874, 6881, 5061, 4099, 3366, 2860, 2432] y1 = [0,1,2,3,4,5,6] for i in range(7): plt.text(s = s1[i], x=x1[i], y=y1[i] ,fontsize=25,va=&#39;center&#39;,ha=&#39;right&#39;,alpha=0.8) plt.title(&quot;Average Years of Programming Experience of Kagglers&quot;, fontsize=42, pad=20, color=&#39;#189AB4&#39;) plt.axis(&#39;off&#39;) plt.gca().invert_yaxis() plt.show() . 캐글 내에 생각보다 코딩 경력이 오래된 사람이 많지 않습니다. . 젏은 플렛폼이라고도 생각할 수 있고, 초보자가 접근하기 어렵지 않다고도 생각할 수 있겠네요. . &#51068;&#44273;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#50616;&#50612; . df[&#39;Q7_Part_1&#39;].value_counts() . Python 21860 Name: Q7_Part_1, dtype: int64 . df[&#39;Q7_Part_2&#39;].value_counts() . R 5334 Name: Q7_Part_2, dtype: int64 . Tool = [&#39;Python&#39;, &#39;R&#39;] # Setting size in Chart based on # given values Tool_cnt = [21860, 5334] # colors colors = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;] # explosion explode = (0.05, 0.05) plt.figure(figsize=[20,10]) # Pie Chart plt.pie(Tool_cnt, colors=colors, autopct=&#39;%1.1f%%&#39;, pctdistance=1.2, explode=explode,) # draw circle centre_circle = plt.Circle((0, 0), 0.70, fc=&#39;white&#39;) fig = plt.gcf() plt.legend(Tool, loc = &quot;upper right&quot;,title=&quot;Programming Languages&quot;, prop={&#39;size&#39;: 15}) # Adding Circle in Pie chart fig.gca().add_artist(centre_circle) plt.rcParams[&#39;font.size&#39;] = 25 # Adding Title of chart plt.text(s=&quot;Which Programming Tool do they Prefer?&quot;,x=0,y=1.3, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) gc.collect() # Displaing Chart plt.show() . 파이썬과 R 이외에 다른 선택지도 있었고, 중복 선택이 허용된 문항이지만 작성자는 파이썬과 R만을 비교했습니다. . 파이썬이 80% 이상으로 압도적인 사용률을 보였는데요. . 앞서 조사한 결과에서 학생인 사람이 많고, 타 분야 전문가도 많기 때문에 쉬운 언어인 파이썬의 사용률이 높지 않을까 생각했어요. . &#50668;&#45919;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#50616;&#50612;2 . df[&#39;Q8&#39;] = df[&#39;Q8&#39;].apply(lambda x: &#39;Others&#39; if x not in [&#39;Python&#39;,&#39;R&#39;,&#39;SQL&#39;] else x) df[&#39;Q8&#39;].value_counts() . Python 20213 Others 2977 R 1445 SQL 1338 Name: Q8, dtype: int64 . Tool = [&#39;Python&#39;, &#39;R&#39;, &#39;SQL&#39;, &#39;Others&#39;] # Setting size in Chart based on # given values Tool_cnt = [20213, 1445, 1338, 2977] # colors colors = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#FFFF00&#39;, &#39;#ADFF2F&#39;] # explosion explode = (0.05, 0.05, 0.05, 0.05) plt.figure(figsize=[20,10]) # Pie Chart plt.pie(Tool_cnt, colors=colors, autopct=&#39;%1.1f%%&#39;, pctdistance=1.2, explode=explode,) # draw circle centre_circle = plt.Circle((0, 0), 0.70, fc=&#39;white&#39;) fig = plt.gcf() plt.legend(Tool, loc = &quot;upper right&quot;,title=&quot;Programming Languages&quot;, prop={&#39;size&#39;: 15}) # Adding Circle in Pie chart fig.gca().add_artist(centre_circle) plt.rcParams[&#39;font.size&#39;] = 25 # Adding Title of chart plt.text(s=&quot;What do they Recommend for Data Science?&quot;,x=0,y=1.3, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) gc.collect() # Displaing Chart plt.show() . 앞선 조사와 비슷한데, 차이점은 중복선택이 안된다는 점입니다. . 선택지가 꽤 많았는데도 파이썬이 압도적인 선택률을 보이네요. . &#50500;&#54857;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#54872;&#44221;(IDE) . df[&#39;Q9_Part_1&#39;].value_counts() . Jupyter (JupyterLab, Jupyter Notebooks, etc) 5488 Name: Q9_Part_1, dtype: int64 . df[&#39;Q9_Part_2&#39;].value_counts() . RStudio 4771 Name: Q9_Part_2, dtype: int64 . 이런식으로 값을 추출해서 적용한 것 같아요. . name = [&#39;JupyterLab&#39;,&#39;RStudio&#39;,&#39;Visual Studio&#39;,&#39;VS Code&#39;,&#39;PyCharm&#39;,&#39;Spyder&#39;,&#39;Notepad++&#39;,&#39;Sublime Text&#39;,&#39;Vim/Emacs&#39;,&#39;MATLAB&#39;,&#39;Jupyter Notebook&#39;,&#39;None&#39;,&#39;Other&#39;] value = [5488,4771,4110,10040,7468,3794,3937,2839,1646,2203,16233,526,1491] # Creating a dataframe to store this information df_nine_ = pd.DataFrame(name, columns=[&#39;IDE&#39;]) df_nine_[&#39;Values&#39;] = value df_nine_ = df_nine_.sort_values(by=&quot;Values&quot;, ascending=False) df_nine_ fig = plt.figure(figsize=(20,10)) plt.barh(width=list(df_nine_[&#39;Values&#39;].unique()), y=list(df_nine_[&#39;IDE&#39;].unique()), height=0.7, color = [&#39;#189AB4&#39;, &#39;#189AB4&#39;, &#39;#189AB4&#39;, &#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;], alpha=0.8) ##################### For the Years of Experience ################################### s1 = list(df_nine_[&#39;IDE&#39;].unique()) x1 = [19833,12040,9468,7788,6471,6810,6437,5294,5539,4003,3946,2691,1726] y1 = [0,1,2,3,4,5,6,7,8,9,10,11,12] for i in range(13): plt.text(s = s1[i], x=x1[i], y=y1[i] , fontsize=25,va=&#39;center&#39;,ha=&#39;right&#39;,alpha=0.8) plt.title(&quot;Preferred IDE of Kagglers&quot;, fontsize=42, pad=20, color=&#39;#189AB4&#39;) plt.axis(&#39;off&#39;) plt.gca().invert_yaxis() gc.collect() plt.show() . 주피터 노트북이 사용자 친화적이라고 코멘트를 합니다. 시프트+엔터시 결과물이 바로 나와 편리하다는 근거와 함께. . VS CODE는 다른 언어(C) 할때 저도 사용했는데, 깃허브와 연동이 좋아서 사용이 편리합니다. 역시 많은 사용자가 이용하는 것 같아요. . 파이참도 저는 써보진 않았지만 높은 순위를 기록합니다. . R을 사용하는 사람 비율 대비 R스튜디오도 많이 쓰는 모습을 보이는데, 대부분에 R 사용자가 R스튜디오를 사용한다고 생각됩니다. . &#50676;&#48264;&#51704; &#51656;&#47928;: &#51452; &#49324;&#50857; &#45432;&#53944;&#48513; . df[&#39;Q10_Part_1&#39;].value_counts() . Kaggle Notebooks 9507 Name: Q10_Part_1, dtype: int64 . df[&#39;Q10_Part_2&#39;].value_counts() . Colab Notebooks 9792 Name: Q10_Part_2, dtype: int64 . 코랩 노트북, 캐글 노트북 이용자 이외는 Other로 생각한 것 같습니다. . def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 # Visualizing the Hosted Notebooks. (Hidden Input) fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) age_bucket = [&#39;None&#39;,&#39;Colab Notebook&#39;,&#39;Kaggle Notebook&#39;] age_bucket_cnt = [7174,9792,9507] color = [&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;] # Deciding the color width = [0.9, 0.9, 0.9] # The Width alpha = [0.55, 1.0, 0.75] # The Opacity fontsize= [25, 45, 30] x_num = [0,1,2] for i in range(3): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Preferred Hosted Notebooks&quot;,x=1,y=11000, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.3, 1, 9000) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 코랩 노트북과 캐글 노트북의 사용자 수가 비슷합니다. . 코랩 노트북은 점유율 1위로, GPU 사용이 일부 가능하고 구글 드라이브와 연동이 잘된다는 점을 큰 장점으로 소개합니다. . 물론 캐글 이용자 조사이기 때문에 캐글 데이터와 캐글 노트북 간 호완성, 접근성이 좋아서 캐글 노트북 사용자가 다소 많이 집계됬습니다. . 다만 캐글 노트북 만에 분명한 장점이 있겠죠? 한번 어느 환경인지 기회될때 탐색하는 것도 좋을 것 같아요. . 또 특이한 점은 두 노트북 이외 각자의 PC환경을 사용하는 사람도 꽤 많다는 것입니다. . &#50676;&#54620;&#48264;&#51704; &#51656;&#47928;: &#44032;&#49549;&#44592; &#50976;&#47924; . df[&#39;Q12_Part_1&#39;].value_counts() . NVIDIA GPUs 8036 Name: Q12_Part_1, dtype: int64 . df[&#39;Q12_Part_2&#39;].value_counts() . Google Cloud TPUs 3451 Name: Q12_Part_2, dtype: int64 . df[&#39;Q12_Part_3&#39;].value_counts() . AWS Trainium Chips 414 Name: Q12_Part_3, dtype: int64 . df[&#39;Q12_Part_4&#39;].value_counts() . AWS Inferentia Chips 416 Name: Q12_Part_4, dtype: int64 . df[&#39;Q12_Part_5&#39;].value_counts() . None 13234 Name: Q12_Part_5, dtype: int64 . df[&#39;Q12_OTHER&#39;].value_counts() . Other 867 Name: Q12_OTHER, dtype: int64 . name = [&quot;None&quot;,&quot;NVIDIA GPUs&quot;,&quot;Google Cloud TPUs&quot;,&quot;Other&quot;,&quot;AWS Inferentia Chips&quot;,&quot;AWS Trainium Chips&quot;] count = [13234,8036,3451,867,416,414] # Visualizing using a barh: fig = plt.figure(figsize=(20,10)) plt.barh(width=count, y=name, height=0.7, color = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#189AB4&#39;, &#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;], alpha=0.8) ##################### For the Years of Experience ################################### s1 = name x1 = [14234,10236,6651,2067,3916,3714] y1 = [0,1,2,3,4,5] for i in range(6): plt.text(s = s1[i], x=x1[i], y=y1[i] , fontsize=25,va=&#39;center&#39;,ha=&#39;right&#39;,alpha=0.8) plt.title(&quot;Specialized Hardware&quot;, fontsize=42, pad=20, color=&#39;#189AB4&#39;) plt.axis(&#39;off&#39;) plt.gca().invert_yaxis() gc.collect() plt.show() . GPU나 TPU를 사용하지 않는 캐글 사용자가 상당히 많이 있네요. . &#45712;&#45184;&#51216; . 대회참가를 위한 데이터 공부가 아니라 설문조사를 시각화 하는 공부였습니다. . 이쁘게 시각화 하기 위해서 작성자가 다양하게 노력한 모습을 확인했습니다. . 또한 설문조사가 캐글 이용자 관련 설문조사라서 결과에 대해 더 흥미롭게 확인 한 것 같아요. . 가볍게 공부하기 좋은 데이터 셋인것 같습니다. . 대회 출처 : https://www.kaggle.com/c/kaggle-survey-2021 . 코드 출처 : https://www.kaggle.com/vivek468/what-s-up-kaggle-kaggle-survey-2021 .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/visualization/survey/2021/12/19/kagglessu8.html",
            "relUrl": "/ssuda/jupyter/kaggle/visualization/survey/2021/12/19/kagglessu8.html",
            "date": " • Dec 19, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "[SSUDA] 캐글 제품 분류",
            "content": ". from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, 100) pd.set_option(&#39;display.max_rows&#39;, 100) from sklearn.preprocessing import LabelEncoder, OneHotEncoder from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV from sklearn.feature_selection import SelectFromModel from sklearn.metrics import accuracy_score, confusion_matrix, classification_report import xgboost as xg from collections import Counter !pip install kneed # kneed is not installed in kaggle. uncomment the above line. from kneed import KneeLocator import warnings warnings.filterwarnings(&quot;ignore&quot;) . Collecting kneed Downloading kneed-0.7.0-py2.py3-none-any.whl (9.4 kB) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from kneed) (1.4.1) Requirement already satisfied: numpy&gt;=1.14.2 in /usr/local/lib/python3.7/dist-packages (from kneed) (1.19.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from kneed) (3.2.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (3.0.6) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (0.11.0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;kneed) (1.15.0) Installing collected packages: kneed Successfully installed kneed-0.7.0 . path = &#39;/content/drive/MyDrive/otto_group/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sampleSubmission.csv&#39;) train.head() . id feat_1 feat_2 feat_3 feat_4 feat_5 feat_6 feat_7 feat_8 feat_9 feat_10 feat_11 feat_12 feat_13 feat_14 feat_15 feat_16 feat_17 feat_18 feat_19 feat_20 feat_21 feat_22 feat_23 feat_24 feat_25 feat_26 feat_27 feat_28 feat_29 feat_30 feat_31 feat_32 feat_33 feat_34 feat_35 feat_36 feat_37 feat_38 feat_39 feat_40 feat_41 feat_42 feat_43 feat_44 feat_45 feat_46 feat_47 feat_48 feat_49 feat_50 feat_51 feat_52 feat_53 feat_54 feat_55 feat_56 feat_57 feat_58 feat_59 feat_60 feat_61 feat_62 feat_63 feat_64 feat_65 feat_66 feat_67 feat_68 feat_69 feat_70 feat_71 feat_72 feat_73 feat_74 feat_75 feat_76 feat_77 feat_78 feat_79 feat_80 feat_81 feat_82 feat_83 feat_84 feat_85 feat_86 feat_87 feat_88 feat_89 feat_90 feat_91 feat_92 feat_93 target . 0 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 4 | 1 | 1 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | 0 | 11 | 0 | 1 | 1 | 0 | 1 | 0 | 7 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 1 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 2 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 3 4 | 1 | 0 | 0 | 1 | 6 | 1 | 5 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 2 | 2 | 0 | 0 | 0 | 58 | 0 | 10 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 2 | 0 | 1 | 2 | 1 | 3 | 0 | 0 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 5 | 0 | 0 | 4 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 2 | 0 | 22 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 4 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | Class_1 | . &#45936;&#51060;&#53552; &#53456;&#49353; . train.columns . Index([&#39;id&#39;, &#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, &#39;feat_6&#39;, &#39;feat_7&#39;, &#39;feat_8&#39;, &#39;feat_9&#39;, &#39;feat_10&#39;, &#39;feat_11&#39;, &#39;feat_12&#39;, &#39;feat_13&#39;, &#39;feat_14&#39;, &#39;feat_15&#39;, &#39;feat_16&#39;, &#39;feat_17&#39;, &#39;feat_18&#39;, &#39;feat_19&#39;, &#39;feat_20&#39;, &#39;feat_21&#39;, &#39;feat_22&#39;, &#39;feat_23&#39;, &#39;feat_24&#39;, &#39;feat_25&#39;, &#39;feat_26&#39;, &#39;feat_27&#39;, &#39;feat_28&#39;, &#39;feat_29&#39;, &#39;feat_30&#39;, &#39;feat_31&#39;, &#39;feat_32&#39;, &#39;feat_33&#39;, &#39;feat_34&#39;, &#39;feat_35&#39;, &#39;feat_36&#39;, &#39;feat_37&#39;, &#39;feat_38&#39;, &#39;feat_39&#39;, &#39;feat_40&#39;, &#39;feat_41&#39;, &#39;feat_42&#39;, &#39;feat_43&#39;, &#39;feat_44&#39;, &#39;feat_45&#39;, &#39;feat_46&#39;, &#39;feat_47&#39;, &#39;feat_48&#39;, &#39;feat_49&#39;, &#39;feat_50&#39;, &#39;feat_51&#39;, &#39;feat_52&#39;, &#39;feat_53&#39;, &#39;feat_54&#39;, &#39;feat_55&#39;, &#39;feat_56&#39;, &#39;feat_57&#39;, &#39;feat_58&#39;, &#39;feat_59&#39;, &#39;feat_60&#39;, &#39;feat_61&#39;, &#39;feat_62&#39;, &#39;feat_63&#39;, &#39;feat_64&#39;, &#39;feat_65&#39;, &#39;feat_66&#39;, &#39;feat_67&#39;, &#39;feat_68&#39;, &#39;feat_69&#39;, &#39;feat_70&#39;, &#39;feat_71&#39;, &#39;feat_72&#39;, &#39;feat_73&#39;, &#39;feat_74&#39;, &#39;feat_75&#39;, &#39;feat_76&#39;, &#39;feat_77&#39;, &#39;feat_78&#39;, &#39;feat_79&#39;, &#39;feat_80&#39;, &#39;feat_81&#39;, &#39;feat_82&#39;, &#39;feat_83&#39;, &#39;feat_84&#39;, &#39;feat_85&#39;, &#39;feat_86&#39;, &#39;feat_87&#39;, &#39;feat_88&#39;, &#39;feat_89&#39;, &#39;feat_90&#39;, &#39;feat_91&#39;, &#39;feat_92&#39;, &#39;feat_93&#39;, &#39;target&#39;], dtype=&#39;object&#39;) . 컬럼수는 93개 입니다. . train[&#39;target&#39;].unique() . array([&#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;, &#39;Class_6&#39;, &#39;Class_7&#39;, &#39;Class_8&#39;, &#39;Class_9&#39;], dtype=object) . Y 변수의 클레스 종류가 9개 입니다. . sample_submission.head() . id Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 0 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 5 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 저번이랑 비슷하게 제출 파일 형식은 각 클레스 별로 분류 될 확률을 기제하면 되겠네요. . sum((train.isnull()).sum()) . 0 . 결측값이 있는지 확인했습니다. info 함수로 확인하기에는 피처가 너무 커서 직관적으로 확인하기 힘듭니다. . from sklearn.preprocessing import LabelEncoder le=LabelEncoder() train[&#39;target&#39;]=le.fit_transform(train[&#39;target&#39;]) plt.figure(figsize=(12,5)) sns.countplot(train[&#39;target&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0cca07b90&gt; . 라벨 인코더를 통해 클레스 이름을 간단하게( Class_1 =&gt; 0) 바꿨습니다. . 클레스 개수가 각각 몇개있는지 파악했는데요. 균등하진 않아보입니다. . &#47784;&#45944; 1 . from sklearn.model_selection import train_test_split list_models=[] list_scores=[] y = train[&#39;target&#39;] x = train.drop([&#39;target&#39;, &#39;id&#39;],axis=1) x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2) . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score lr=LogisticRegression(max_iter=100000) lr.fit(x_train,y_train) pred_1=lr.predict(x_test) score_1=accuracy_score(y_test,pred_1) list_models.append(&#39;logistic regression&#39;) list_scores.append(score_1) . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_1,ax=axes[0]) sns.countplot(y_test,ax=axes[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0cbd85e10&gt; . 간단한 로지스틱 회귀로, 왼쪽이 예측값, 오른쪽이 실제값입니다. 실제 비율이 제일 높은 1번은 더 많이 예측하는 모습을 보입니다. . 비율이 높은 편인 5번, 7번, 8번은 실제 값과 예측 값이 비슷합니다. . 하지만 나머지 값들은 실제 값에 있는 비율 만큼 예측 값에서 비슷한 개수로 추정해주지 못했습니다. . 물론 이 현상만으로 비율을 일관적으로 예측할 수는 없습니다.(8번은 예측/실제 값 개수 비슷, 2번은 실제 값에 비해 예측값이 너무 적음) . 다만 불균형한 테스터 셋을 분류하는 문제에서 다음과 같은 문제가 있다는걸 인지해야겠습니다. . 개수가 많은 클레스를 예측하는 확률은 높아지고, 개수가 적은 클레스를 예측하는 확률은 낮아진다는 점 입니다. . from sklearn.ensemble import RandomForestClassifier rfc=RandomForestClassifier() rfc.fit(x_train,y_train) pred_2=rfc.predict(x_test) score_2=accuracy_score(y_test,pred_2) list_scores.append(score_2) list_models.append(&#39;random forest classifier&#39;) . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_2,ax=axes[0]) sns.countplot(y_test,ax=axes[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0c14ea1d0&gt; . 렌덤 포레스트 분류기법입니다. 앞서 말한것과 비슷한 일이 벌어집니다. . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_1,ax=axes[0]) axes[0].legend(title=&#39;predictions by logistic regression&#39;) sns.countplot(pred_2,ax=axes[1]) axes[1].legend(title=&#39;predictions by random forest&#39;) . No handles with labels found to put in legend. No handles with labels found to put in legend. . &lt;matplotlib.legend.Legend at 0x7fc0c1429ad0&gt; . 두 모델이 비슷한 현상을 보인다는 걸 다시한번 보여준 것 같습니다. . from sklearn.svm import SVC svm=SVC() svm.fit(x_train,y_train) pred_3=svm.predict(x_test) score_3=accuracy_score(y_test,pred_3) list_scores.append(score_3) list_models.append(&#39;support vector machines&#39;) . from xgboost import XGBClassifier xgb=XGBClassifier() xgb.fit(x_train,y_train) pred_4=xgb.predict(x_test) score_4=accuracy_score(y_test,pred_4) list_models.append(&#39;xgboost classifier&#39;) list_scores.append(score_4) . plt.figure(figsize=(12,5)) plt.bar(list_models,list_scores,width=0.3) plt.xlabel(&#39;classifictions models&#39;) plt.ylabel(&#39;accuracy scores&#39;) plt.show() . SVM, XGB 모델도 적용시켜보았습니다. . 랜덤 포레스트 분류 모델이 성능이 가장 괜찮아 보입니다. . &#47784;&#45944; 2 . !pip install &quot;autogluon.tabular[all]==0.1.1b20210312&quot; . Collecting autogluon.tabular[all]==0.1.1b20210312 Downloading autogluon.tabular-0.1.1b20210312-py3-none-any.whl (234 kB) |████████████████████████████████| 234 kB 4.2 MB/s Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.19.5) Collecting scikit-learn&lt;0.25,&gt;=0.22.0 Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB) |████████████████████████████████| 22.3 MB 1.6 MB/s Collecting autogluon.features==0.1.1b20210312 Downloading autogluon.features-0.1.1b20210312-py3-none-any.whl (48 kB) |████████████████████████████████| 48 kB 4.4 MB/s Requirement already satisfied: pandas&lt;2.0,&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.1.5) Collecting scipy==1.5.4 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) |████████████████████████████████| 25.9 MB 1.8 MB/s Requirement already satisfied: networkx&lt;3.0,&gt;=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (2.6.3) Collecting autogluon.core==0.1.1b20210312 Downloading autogluon.core-0.1.1b20210312-py3-none-any.whl (312 kB) |████████████████████████████████| 312 kB 50.1 MB/s Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (3.6.4) Requirement already satisfied: psutil&lt;=5.7.0,&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (5.4.8) Requirement already satisfied: torch&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.10.0+cu111) Requirement already satisfied: fastai&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.0.61) Collecting lightgbm&lt;4.0,&gt;=3.0 Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB) |████████████████████████████████| 2.0 MB 49.3 MB/s Collecting catboost&lt;0.25,&gt;=0.23.0 Downloading catboost-0.24.4-cp37-none-manylinux1_x86_64.whl (65.7 MB) |████████████████████████████████| 65.7 MB 46 kB/s Collecting xgboost&lt;1.4,&gt;=1.3.2 Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB) |████████████████████████████████| 157.5 MB 63 kB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.23.0) Collecting dill==0.3.3 Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB) |████████████████████████████████| 81 kB 9.6 MB/s Requirement already satisfied: autograd&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3) Collecting paramiko&gt;=2.4 Downloading paramiko-2.8.0-py2.py3-none-any.whl (206 kB) |████████████████████████████████| 206 kB 50.7 MB/s Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.29.24) Requirement already satisfied: tqdm&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.62.3) Requirement already satisfied: tornado&gt;=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (5.1.1) Collecting boto3 Downloading boto3-1.20.14-py3-none-any.whl (131 kB) |████████████████████████████████| 131 kB 49.8 MB/s Collecting ConfigSpace==0.4.18 Downloading ConfigSpace-0.4.18.tar.gz (950 kB) |████████████████████████████████| 950 kB 49.4 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.2.2) Collecting graphviz&lt;0.9.0,&gt;=0.8.1 Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB) Collecting distributed&gt;=2.6.0 Downloading distributed-2021.11.2-py3-none-any.whl (802 kB) |████████████████████████████████| 802 kB 50.6 MB/s Requirement already satisfied: dask&gt;=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.12.0) Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace==0.4.18-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.6) Requirement already satisfied: future&gt;=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd&gt;=1.3-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.16.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.15.0) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.4.1) Collecting dask&gt;=2.6.0 Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB) |████████████████████████████████| 1.0 MB 40.4 MB/s Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.4.0) Requirement already satisfied: toolz&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.2) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.11.3) Requirement already satisfied: click&gt;=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.1.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (57.4.0) Requirement already satisfied: msgpack&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.2) Requirement already satisfied: tblib&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.7.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.13) Requirement already satisfied: zict&gt;=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.0) Collecting cloudpickle&gt;=1.5.0 Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from dask&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (21.3) Collecting partd&gt;=0.3.10 Downloading partd-1.2.0-py3-none-any.whl (19 kB) Collecting fsspec&gt;=0.6.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 54.6 MB/s Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.1+cu111) Requirement already satisfied: spacy&gt;=2.0.18 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.2.4) Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.352.0) Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.7.3) Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.6.3) Requirement already satisfied: fastprogress&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.0) Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.1.2) Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.2) Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&lt;4.0,&gt;=3.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.37.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&lt;2.0,&gt;=1.0.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&lt;2.0,&gt;=1.0.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.8.2) Collecting cryptography&gt;=2.5 Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB) |████████████████████████████████| 3.6 MB 43.1 MB/s Collecting pynacl&gt;=1.0.1 Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB) |████████████████████████████████| 961 kB 45.2 MB/s Collecting bcrypt&gt;=3.1.3 Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB) |████████████████████████████████| 63 kB 2.3 MB/s Requirement already satisfied: cffi&gt;=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.15.0) Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.1-&gt;bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.21) Collecting locket Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&lt;0.25,&gt;=0.22.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&lt;0.25,&gt;=0.22.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.1.0) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.4.0) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.5) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.8.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.6) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.6) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.1.3) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.4.1) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.8.2) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.10.0.2) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.6.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.4) Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict&gt;=0.1.3-&gt;distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.1) Collecting jmespath&lt;1.0.0,&gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting botocore&lt;1.24.0,&gt;=1.23.14 Downloading botocore-1.23.14-py3-none-any.whl (8.2 MB) |████████████████████████████████| 8.2 MB 37.3 MB/s Collecting s3transfer&lt;0.6.0,&gt;=0.5.0 Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 7.5 MB/s Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 49.6 MB/s Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.3) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (21.2.0) Requirement already satisfied: more-itertools&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (8.11.0) Requirement already satisfied: atomicwrites&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.4.0) Requirement already satisfied: pluggy&lt;0.8,&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.7.1) Requirement already satisfied: py&gt;=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.11.0) Building wheels for collected packages: ConfigSpace Building wheel for ConfigSpace (PEP 517) ... done Created wheel for ConfigSpace: filename=ConfigSpace-0.4.18-cp37-cp37m-linux_x86_64.whl size=2880650 sha256=7b9c24d3da86378fe64cff390f09a143606ba3ac7a45f5b37fa2827e1aed4124 Stored in directory: /root/.cache/pip/wheels/36/f7/0f/36f368c419ea1a8024fc3d6c078c3111dfef43fa1d14cfebe0 Successfully built ConfigSpace Installing collected packages: urllib3, locket, jmespath, partd, fsspec, cloudpickle, botocore, scipy, s3transfer, pynacl, dask, cryptography, bcrypt, scikit-learn, paramiko, graphviz, distributed, dill, ConfigSpace, boto3, autogluon.core, autogluon.features, xgboost, lightgbm, catboost, autogluon.tabular Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 Attempting uninstall: cloudpickle Found existing installation: cloudpickle 1.3.0 Uninstalling cloudpickle-1.3.0: Successfully uninstalled cloudpickle-1.3.0 Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: dask Found existing installation: dask 2.12.0 Uninstalling dask-2.12.0: Successfully uninstalled dask-2.12.0 Attempting uninstall: scikit-learn Found existing installation: scikit-learn 1.0.1 Uninstalling scikit-learn-1.0.1: Successfully uninstalled scikit-learn-1.0.1 Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 Attempting uninstall: distributed Found existing installation: distributed 1.25.3 Uninstalling distributed-1.25.3: Successfully uninstalled distributed-1.25.3 Attempting uninstall: dill Found existing installation: dill 0.3.4 Uninstalling dill-0.3.4: Successfully uninstalled dill-0.3.4 Attempting uninstall: xgboost Found existing installation: xgboost 0.90 Uninstalling xgboost-0.90: Successfully uninstalled xgboost-0.90 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. multiprocess 0.70.12.2 requires dill&gt;=0.3.4, but you have dill 0.3.3 which is incompatible. gym 0.17.3 requires cloudpickle&lt;1.7.0,&gt;=1.2.0, but you have cloudpickle 2.0.0 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible. Successfully installed ConfigSpace-0.4.18 autogluon.core-0.1.1b20210312 autogluon.features-0.1.1b20210312 autogluon.tabular-0.1.1b20210312 bcrypt-3.2.0 boto3-1.20.14 botocore-1.23.14 catboost-0.24.4 cloudpickle-2.0.0 cryptography-36.0.0 dask-2021.11.2 dill-0.3.3 distributed-2021.11.2 fsspec-2021.11.1 graphviz-0.8.4 jmespath-0.10.0 lightgbm-3.3.1 locket-0.2.1 paramiko-2.8.0 partd-1.2.0 pynacl-1.4.0 s3transfer-0.5.0 scikit-learn-0.24.2 scipy-1.5.4 urllib3-1.25.11 xgboost-1.3.3 . from autogluon.tabular import TabularDataset, TabularPredictor from autogluon.tabular.models.knn.knn_rapids_model import KNNRapidsModel from autogluon.tabular.models.lr.lr_rapids_model import LinearRapidsModel path = &#39;/content/drive/MyDrive/otto_group/&#39; train = TabularDataset(path + &#39;train.csv&#39;) test = TabularDataset(path + &#39;test.csv&#39;) label = &#39;target&#39; . Loaded data from: /content/drive/MyDrive/otto_group/train.csv | Columns = 95 / 95 | Rows = 61878 -&gt; 61878 Loaded data from: /content/drive/MyDrive/otto_group/test.csv | Columns = 94 / 94 | Rows = 144368 -&gt; 144368 . !pip install cuml . Collecting cuml Downloading cuml-0.6.1.post1.tar.gz (1.1 kB) Building wheels for collected packages: cuml Building wheel for cuml (setup.py) ... error ERROR: Failed building wheel for cuml Running setup.py clean for cuml Failed to build cuml Installing collected packages: cuml Running setup.py install for cuml ... error ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c &#39;import io, os, sys, setuptools, tokenize; sys.argv[0] = &#39;&#34;&#39;&#34;&#39;/tmp/pip-install-d9q8bg1e/cuml_e5625faa4a144d1cb1dbda39971d1a35/setup.py&#39;&#34;&#39;&#34;&#39;; __file__=&#39;&#34;&#39;&#34;&#39;/tmp/pip-install-d9q8bg1e/cuml_e5625faa4a144d1cb1dbda39971d1a35/setup.py&#39;&#34;&#39;&#34;&#39;;f = getattr(tokenize, &#39;&#34;&#39;&#34;&#39;open&#39;&#34;&#39;&#34;&#39;, open)(__file__) if os.path.exists(__file__) else io.StringIO(&#39;&#34;&#39;&#34;&#39;from setuptools import setup; setup()&#39;&#34;&#39;&#34;&#39;);code = f.read().replace(&#39;&#34;&#39;&#34;&#39; r n&#39;&#34;&#39;&#34;&#39;, &#39;&#34;&#39;&#34;&#39; n&#39;&#34;&#39;&#34;&#39;);f.close();exec(compile(code, __file__, &#39;&#34;&#39;&#34;&#39;exec&#39;&#34;&#39;&#34;&#39;))&#39; install --record /tmp/pip-record-yfs4_fyl/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/cuml Check the logs for full command output. . predictor = TabularPredictor( label=label, eval_metric=&#39;log_loss&#39;, learner_kwargs={&#39;ignored_columns&#39;: [&#39;id&#39;]} ).fit( train, presets=&#39;best_quality&#39;, hyperparameters={ KNNRapidsModel: {}, LinearRapidsModel: {}, &#39;RF&#39;: {}, &#39;XGB&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;CAT&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;GBM&#39;: [{}, {&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, &#39;GBMLarge&#39;], &#39;NN&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;FASTAI&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, }, ) . No path specified. Models will be saved in: &#34;AutogluonModels/ag-20211127_122603/&#34; Presets specified: [&#39;best_quality&#39;] Beginning AutoGluon training ... AutoGluon will save models to &#34;AutogluonModels/ag-20211127_122603/&#34; AutoGluon Version: 0.1.1b20210312 Train Data Rows: 61878 Train Data Columns: 94 Preprocessing data ... AutoGluon infers your prediction problem is: &#39;multiclass&#39; (because dtype of label-column == object). 9 unique label values: [&#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;, &#39;Class_6&#39;, &#39;Class_7&#39;, &#39;Class_8&#39;, &#39;Class_9&#39;] If &#39;multiclass&#39; is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: [&#39;binary&#39;, &#39;multiclass&#39;, &#39;regression&#39;]) Train Data Class Count: 9 Using Feature Generators to preprocess the data ... Dropping user-specified ignored columns: [&#39;id&#39;] Fitting AutoMLPipelineFeatureGenerator... Available Memory: 12407.99 MB Train Data (Original) Memory Usage: 46.04 MB (0.4% of available memory) Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features. Stage 1 Generators: Fitting AsTypeFeatureGenerator... Stage 2 Generators: Fitting FillNaFeatureGenerator... Stage 3 Generators: Fitting IdentityFeatureGenerator... Stage 4 Generators: Fitting DropUniqueFeatureGenerator... Types of features in original data (raw dtype, special dtypes): (&#39;int&#39;, []) : 93 | [&#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, ...] Types of features in processed data (raw dtype, special dtypes): (&#39;int&#39;, []) : 93 | [&#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, ...] 0.5s = Fit runtime 93 features in original data used to generate 93 features in processed data. Train Data (Processed) Memory Usage: 46.04 MB (0.4% of available memory) Data preprocessing and feature engineering runtime = 0.7s ... AutoGluon will gauge predictive performance using evaluation metric: &#39;log_loss&#39; This metric expects predicted probabilities rather than predicted class labels, so you&#39;ll need to use predict_proba() instead of predict() To change this, specify the eval_metric argument of fit() Custom Model Type Detected: &lt;class &#39;autogluon.tabular.models.knn.knn_rapids_model.KNNRapidsModel&#39;&gt; Custom Model Type Detected: &lt;class &#39;autogluon.tabular.models.lr.lr_rapids_model.LinearRapidsModel&#39;&gt; . ModuleNotFoundError Traceback (most recent call last) /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/try_import.py in try_import_rapids_cuml() 162 try: --&gt; 163 import cuml 164 except ImportError: ModuleNotFoundError: No module named &#39;cuml&#39; During handling of the above exception, another exception occurred: ImportError Traceback (most recent call last) &lt;ipython-input-12-488de9012a7d&gt; in &lt;module&gt;() 14 &#39;GBM&#39;: [{}, {&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, &#39;GBMLarge&#39;], 15 &#39;NN&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &gt; 16 &#39;FASTAI&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, 17 }, 18 ) /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/decorators.py in _call(*args, **kwargs) 27 def _call(*args, **kwargs): 28 gargs, gkwargs = g(*other_args, *args, **kwargs) &gt; 29 return f(*gargs, **gkwargs) 30 return _call 31 return _unpack_inner /usr/local/lib/python3.7/dist-packages/autogluon/tabular/predictor/predictor.py in fit(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, **kwargs) 689 self._learner.fit(X=train_data, X_val=tuning_data, X_unlabeled=unlabeled_data, 690 holdout_frac=holdout_frac, num_bag_folds=num_bag_folds, num_bag_sets=num_bag_sets, num_stack_levels=num_stack_levels, --&gt; 691 hyperparameters=hyperparameters, core_kwargs=core_kwargs, time_limit=time_limit, verbosity=verbosity) 692 self._set_post_fit_vars() 693 /usr/local/lib/python3.7/dist-packages/autogluon/tabular/learner/abstract_learner.py in fit(self, X, X_val, **kwargs) 124 raise AssertionError(&#39;Learner is already fit.&#39;) 125 self._validate_fit_input(X=X, X_val=X_val, **kwargs) --&gt; 126 return self._fit(X=X, X_val=X_val, **kwargs) 127 128 def _fit(self, X: DataFrame, X_val: DataFrame = None, scheduler_options=None, hyperparameter_tune=False, /usr/local/lib/python3.7/dist-packages/autogluon/tabular/learner/default_learner.py in _fit(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, verbosity, **trainer_fit_kwargs) 93 94 self.save() &gt; 95 trainer.fit(X, y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, holdout_frac=holdout_frac, time_limit=time_limit_trainer, **trainer_fit_kwargs) 96 self.save_trainer(trainer=trainer) 97 time_end = time.time() /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/auto_trainer.py in fit(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, feature_prune, holdout_frac, num_stack_levels, core_kwargs, time_limit, **kwargs) 50 self._train_multi_and_ensemble(X, y, X_val, y_val, X_unlabeled=X_unlabeled, hyperparameters=hyperparameters, 51 feature_prune=feature_prune, &gt; 52 num_stack_levels=num_stack_levels, time_limit=time_limit, core_kwargs=core_kwargs) 53 54 def get_models_distillation(self, hyperparameters, **kwargs): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in _train_multi_and_ensemble(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, **kwargs) 1290 self._num_cols_train = len(list(X.columns)) 1291 model_names_fit = self.train_multi_levels(X, y, hyperparameters=hyperparameters, X_val=X_val, y_val=y_val, -&gt; 1292 X_unlabeled=X_unlabeled, level_start=1, level_end=num_stack_levels+1, time_limit=time_limit, **kwargs) 1293 if len(self.get_model_names()) == 0: 1294 raise ValueError(&#39;AutoGluon did not successfully train any models&#39;) /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in train_multi_levels(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, feature_prune, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack) 259 models=hyperparameters, level=level, base_model_names=base_model_names, 260 feature_prune=feature_prune, --&gt; 261 core_kwargs=core_kwargs_level, aux_kwargs=aux_kwargs_level, name_suffix=name_suffix, 262 ) 263 model_names_fit += base_model_names + aux_models /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in stack_new_level(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, feature_prune, core_kwargs, aux_kwargs, name_suffix) 285 aux_kwargs[&#39;name_suffix&#39;] = aux_kwargs.get(&#39;name_suffix&#39;, &#39;&#39;) + name_suffix 286 core_models = self.stack_new_level_core(X=X, y=y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, models=models, --&gt; 287 level=level, base_model_names=base_model_names, feature_prune=feature_prune, **core_kwargs) 288 289 if self.bagged_mode: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in stack_new_level_core(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, **kwargs) 342 )) 343 --&gt; 344 models, model_args_fit = get_models_func(hyperparameters=models, **get_models_kwargs) 345 if model_args_fit: 346 hyperparameter_tune_kwargs = { /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/auto_trainer.py in get_models(self, hyperparameters, **kwargs) 26 return get_preset_models(path=path, problem_type=problem_type, eval_metric=eval_metric, 27 num_classes=num_classes, hyperparameters=hyperparameters, invalid_model_names=invalid_model_names, &gt; 28 feature_metadata=feature_metadata, silent=silent, **kwargs) 29 30 def fit(self, X, y, hyperparameters, X_val=None, y_val=None, X_unlabeled=None, feature_prune=False, holdout_frac=0.1, num_stack_levels=0, core_kwargs: dict = None, time_limit=None, **kwargs): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/model_presets/presets.py in get_preset_models(path, problem_type, eval_metric, hyperparameters, feature_metadata, num_classes, level, ensemble_type, ensemble_kwargs, ag_args_fit, ag_args, ag_args_ensemble, name_suffix, default_priorities, invalid_model_names, excluded_model_types, hyperparameter_preprocess_func, hyperparameter_preprocess_kwargs, silent) 189 model = model_factory(model_cfg, path=path, problem_type=problem_type, eval_metric=eval_metric, 190 num_classes=num_classes, name_suffix=name_suffix, ensemble_type=ensemble_type, ensemble_kwargs=ensemble_kwargs, --&gt; 191 invalid_name_set=invalid_name_set, level=level, feature_metadata=feature_metadata) 192 invalid_name_set.add(model.name) 193 if &#39;hyperparameter_tune_kwargs&#39; in model_cfg[AG_ARGS]: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/model_presets/presets.py in model_factory(model, path, problem_type, eval_metric, num_classes, name_suffix, ensemble_type, ensemble_kwargs, invalid_name_set, level, feature_metadata) 296 model_params.pop(AG_ARGS, None) 297 model_params.pop(AG_ARGS_ENSEMBLE, None) --&gt; 298 model_init = model_type(path=path, name=name, problem_type=problem_type, eval_metric=eval_metric, num_classes=num_classes, hyperparameters=model_params, feature_metadata=feature_metadata) 299 300 if ensemble_kwargs is not None: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/knn/knn_model.py in __init__(self, **kwargs) 25 def __init__(self, **kwargs): 26 super().__init__(**kwargs) &gt; 27 self._model_type = self._get_model_type() 28 29 def _get_model_type(self): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/knn/knn_rapids_model.py in _get_model_type(self) 26 &#34;&#34;&#34; 27 def _get_model_type(self): &gt; 28 try_import_rapids_cuml() 29 from cuml.neighbors import KNeighborsClassifier, KNeighborsRegressor 30 if self.problem_type == REGRESSION: /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/try_import.py in try_import_rapids_cuml() 163 import cuml 164 except ImportError: --&gt; 165 raise ImportError(&#34;`import cuml` failed. n&#34; 166 &#34;Ensure that you have a GPU and CUDA installation, and then install RAPIDS. n&#34; 167 &#34;You will likely need to create a fresh conda environment based off of a RAPIDS install, and then install AutoGluon on it. n&#34; ImportError: `import cuml` failed. Ensure that you have a GPU and CUDA installation, and then install RAPIDS. You will likely need to create a fresh conda environment based off of a RAPIDS install, and then install AutoGluon on it. RAPIDS is highly experimental within AutoGluon, and we recommend to only use RAPIDS if you are an advanced user / developer. Please refer to RAPIDS install instructions for more information: https://rapids.ai/start.html#get-rapids NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &#34;Open Examples&#34; button below. . submission = test[[&#39;id&#39;]] test_pred_proba = predictor.predict_proba(test) submission = pd.concat([submission, test_pred_proba], axis=1) submission.to_csv(&#39;submission.csv&#39;, index=False) submission.head() . 오류가 지속적으로 나서 실행을 못했습니다. (autogluon 모델) . 자동으로 분석해주는 모델인것 같고 실제로 이 모델 점수 상위 1%를 기록했다고 합니다. . &#47784;&#45944; 3 . from patsy import dmatrices from sklearn.neural_network import MLPClassifier columns = train.columns[1:-1] X = train[columns] y = np.ravel(train[&#39;target&#39;]) model = MLPClassifier(solver=&#39;lbfgs&#39;, alpha=1e-5, hidden_layer_sizes = (30, 10), random_state = 0, verbose = True) model.fit(X, y) . MLPClassifier(alpha=1e-05, hidden_layer_sizes=(30, 10), random_state=0, solver=&#39;lbfgs&#39;, verbose=True) . pred = model.predict(X) print(model.score(X, y)) print(sum(pred == y) / len(y)) . 0.8057629529073338 0.8057629529073338 . Xtest = test[test.columns[1:]] test_prob = model.predict_proba(Xtest) solution = pd.DataFrame(test_prob, columns=[&#39;Class_1&#39;,&#39;Class_2&#39;,&#39;Class_3&#39;,&#39;Class_4&#39;,&#39;Class_5&#39;,&#39;Class_6&#39;,&#39;Class_7&#39;,&#39;Class_8&#39;,&#39;Class_9&#39;]) solution[&#39;id&#39;] = test[&#39;id&#39;] cols = solution.columns.tolist() cols = cols[-1:] + cols[:-1] solution = solution[cols] solution.to_csv(&#39;otto_prediction.csv&#39;, index = False) .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/labelencoder/randomforest/xgboost/svm/mlp/classifier/2021/11/27/kagglessu7.html",
            "relUrl": "/ssuda/jupyter/kaggle/labelencoder/randomforest/xgboost/svm/mlp/classifier/2021/11/27/kagglessu7.html",
            "date": " • Nov 27, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "[SSUDA] 신용카드 사용자 연체 예측",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, 100) import warnings warnings.filterwarnings(&quot;ignore&quot;) from lightgbm import LGBMClassifier from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import OneHotEncoder import random train = pd.read_csv(&quot;/content/drive/MyDrive/carddata/train.csv&quot;) test = pd.read_csv(&#39;/content/drive/MyDrive/carddata/test.csv&#39;) sample_submission = pd.read_csv(&#39;/content/drive/MyDrive/carddata/sample_submission.csv&#39;) train.head() . index gender car reality child_num income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email occyp_type family_size begin_month credit . 0 0 | F | N | N | 0 | 202500.0 | Commercial associate | Higher education | Married | Municipal apartment | -13899 | -4709 | 1 | 0 | 0 | 0 | NaN | 2.0 | -6.0 | 1.0 | . 1 1 | F | N | Y | 1 | 247500.0 | Commercial associate | Secondary / secondary special | Civil marriage | House / apartment | -11380 | -1540 | 1 | 0 | 0 | 1 | Laborers | 3.0 | -5.0 | 1.0 | . 2 2 | M | Y | Y | 0 | 450000.0 | Working | Higher education | Married | House / apartment | -19087 | -4434 | 1 | 0 | 1 | 0 | Managers | 2.0 | -22.0 | 2.0 | . 3 3 | F | N | Y | 0 | 202500.0 | Commercial associate | Secondary / secondary special | Married | House / apartment | -15088 | -2092 | 1 | 0 | 1 | 0 | Sales staff | 2.0 | -37.0 | 0.0 | . 4 4 | F | Y | Y | 0 | 157500.0 | State servant | Higher education | Married | House / apartment | -15037 | -2105 | 1 | 0 | 0 | 0 | Managers | 2.0 | -26.0 | 2.0 | . &#44592;&#48376; &#48320;&#49688; &#49444;&#47749; . gender : 성별(F/M), car : 차량 소유 유무(Y/N), reality : 부동산 소유 유무(Y/N), child_num : 자녀 수 . income_total : 연간 소득, income_type : 소득 분류(5개로 분리), edu_type : 교육 수준(5개로 분리) . family_type : 결혼 여부(5개로 분리), house_type : 생활 방식(6개로 분리), DAYS_BIRTH : 출생일(수집일부터 음수로 계산) . DAYS_EMPLOYED : 업무 시작일(수집일부터 음수로 계산, 업무 안하는 사람은 365243 값 부여), FLAG_MOBIL : 핸드폰 소유 여부 . work_phone : 업무용 전화 소유 여부, phone : 가정용 전화 소유 여부, email : 이메일 소유 여부 . occyp_type : 직업 유형, family_size: 가족 규모, begin_month : 신용카드 발급 월(수집일로부터 음수 계산) . 반응변수 =&gt; credit : 사용자의 신용카드 대금 연체를 기준으로 한 신용도. 낮을수록 높은 신용임. . train.describe() . index child_num income_total DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email family_size begin_month credit . count 26457.000000 | 26457.000000 | 2.645700e+04 | 26457.000000 | 26457.000000 | 26457.0 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | . mean 13228.000000 | 0.428658 | 1.873065e+05 | -15958.053899 | 59068.750728 | 1.0 | 0.224742 | 0.294251 | 0.091280 | 2.196848 | -26.123294 | 1.519560 | . std 7637.622372 | 0.747326 | 1.018784e+05 | 4201.589022 | 137475.427503 | 0.0 | 0.417420 | 0.455714 | 0.288013 | 0.916717 | 16.559550 | 0.702283 | . min 0.000000 | 0.000000 | 2.700000e+04 | -25152.000000 | -15713.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | -60.000000 | 0.000000 | . 25% 6614.000000 | 0.000000 | 1.215000e+05 | -19431.000000 | -3153.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -39.000000 | 1.000000 | . 50% 13228.000000 | 0.000000 | 1.575000e+05 | -15547.000000 | -1539.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -24.000000 | 2.000000 | . 75% 19842.000000 | 1.000000 | 2.250000e+05 | -12446.000000 | -407.000000 | 1.0 | 0.000000 | 1.000000 | 0.000000 | 3.000000 | -12.000000 | 2.000000 | . max 26456.000000 | 19.000000 | 1.575000e+06 | -7705.000000 | 365243.000000 | 1.0 | 1.000000 | 1.000000 | 1.000000 | 20.000000 | 0.000000 | 2.000000 | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 26457 entries, 0 to 26456 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 index 26457 non-null int64 1 gender 26457 non-null object 2 car 26457 non-null object 3 reality 26457 non-null object 4 child_num 26457 non-null int64 5 income_total 26457 non-null float64 6 income_type 26457 non-null object 7 edu_type 26457 non-null object 8 family_type 26457 non-null object 9 house_type 26457 non-null object 10 DAYS_BIRTH 26457 non-null int64 11 DAYS_EMPLOYED 26457 non-null int64 12 FLAG_MOBIL 26457 non-null int64 13 work_phone 26457 non-null int64 14 phone 26457 non-null int64 15 email 26457 non-null int64 16 occyp_type 18286 non-null object 17 family_size 26457 non-null float64 18 begin_month 26457 non-null float64 19 credit 26457 non-null float64 dtypes: float64(4), int64(8), object(8) memory usage: 4.0+ MB . 유일하게 occyp_type(직업유형) 변수가 null 값이 존재합니다. . NAN으로 채워넣겠습니다. . train.fillna(&#39;NAN&#39;, inplace=True) test.fillna(&#39;NAN&#39;, inplace=True) . plt.subplots(figsize = (8,8)) plt.pie(train[&#39;credit&#39;].value_counts(), labels = train[&#39;credit&#39;].value_counts().index, autopct=&quot;%.2f%%&quot;, shadow = True, startangle = 90) plt.title(&#39;credit ratio&#39;, size=20) plt.show() . matplotlib 패키지 내 pie 차트를 이용해 반응변수의 비율을 확인했습니다. . 신용등급이 떨어지는 2번의 비율이 상당히 크군요. . &#48276;&#51452;&#54805; &#48320;&#49688;&#47484; &#49888;&#50857;&#46321;&#44553;&#48324;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . train_0 = train[train[&#39;credit&#39;]==0.0] train_1 = train[train[&#39;credit&#39;]==1.0] train_2 = train[train[&#39;credit&#39;]==2.0] def cat_plot(column): f, ax = plt.subplots(1, 3, figsize=(16, 6)) sns.countplot(x = column, data = train_0, ax = ax[0], order = train_0[column].value_counts().index) ax[0].tick_params(labelsize=12) ax[0].set_title(&#39;credit = 0&#39;) ax[0].set_ylabel(&#39;count&#39;) ax[0].tick_params(rotation=50) sns.countplot(x = column, data = train_1, ax = ax[1], order = train_1[column].value_counts().index) ax[1].tick_params(labelsize=12) ax[1].set_title(&#39;credit = 1&#39;) ax[1].set_ylabel(&#39;count&#39;) ax[1].tick_params(rotation=50) sns.countplot(x = column, data = train_2, ax = ax[2], order = train_2[column].value_counts().index) ax[2].tick_params(labelsize=12) ax[2].set_title(&#39;credit = 2&#39;) ax[2].set_ylabel(&#39;count&#39;) ax[2].tick_params(rotation=50) plt.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() cat_plot(&quot;gender&quot;) . train 데이터를 신용등급에 따라 분류한 뒤 설명변수와에 관계를 그래프로 보는 함수를 만들었습니다. . 성별에 대해서 살펴봤는데, 절대적으로 여성이 그냥 많은 것 같습니다. . 더불어 성별에 따른 신용등급 차이는 모두 비슷한 비율에 그래프인 것으로 보아 확인하기 힘듭니다. . cat_plot(&#39;car&#39;) . 우선 차량보유를 하지 않은 사람이 모든 비율에서 많습니다. . 다만 신용 등급과에 연관성은 그래프로 봤을땐 크게 없는 것 같네요. . cat_plot(&#39;reality&#39;) . 모든 신용 등급에서 부동산을 소유한 사람들이 많았습니다. . 딱히 신용 등급에 따른 차이가 존재하지 않는 것 같네요. . cat_plot(&#39;income_type&#39;) . 소득 종류 변수도 신용 등급 별로 차이가 두드러지진 않습니다. . 다만 학생은 신용등급 0에 없는 점이 눈에 띄네요. . cat_plot(&#39;edu_type&#39;) . 교육 수준 변수 또한 신용 등급별로 차이가 있어보이진 않네요. . cat_plot(&#39;family_type&#39;) . 가족 구성 변수에 따른 신용등급 변수도 차이가 없는 것 같아요. . 전반적으로 결혼한 사람이 많은 것이 눈에 띄네요. . cat_plot(&#39;house_type&#39;) . house_type 변수 또한 큰 의미가 없는 변수인 것 같습니다. 대부분 House / apartment 타입이기 때문에 의미가 더더욱 없습니다. . cat_plot(&#39;FLAG_MOBIL&#39;) . 여기에 나온 모든 사람은 스마트폰을 보유하고 있습니다. . cat_plot(&#39;work_phone&#39;) . 신용 등급 그룹 별 가정 전화 비율이 차이가 없습니다. 가정용 전화기 보유률이 떨어지는게 눈에 띄네요. . cat_plot(&#39;email&#39;) . 이메일 변수 또한 유의미하지 않아 보입니다. . f, ax = plt.subplots(1, 3, figsize=(16, 6)) sns.countplot(y = &#39;occyp_type&#39;, data = train_0, order = train_0[&#39;occyp_type&#39;].value_counts().index, ax=ax[0]) sns.countplot(y = &#39;occyp_type&#39;, data = train_1, order = train_1[&#39;occyp_type&#39;].value_counts().index, ax=ax[1]) sns.countplot(y = &#39;occyp_type&#39;, data = train_2, order = train_2[&#39;occyp_type&#39;].value_counts().index, ax=ax[2]) plt.subplots_adjust(wspace=0.5, hspace=0.3) plt.show() . 직업 유형 변수를 신용 등급별로 비교했습니다. . 전반적인 경향은 비슷하지만, 세세한 차이가 조금 있어보입니다. . &#50672;&#49549;&#54805; &#48320;&#49688;&#47484; &#49888;&#50857;&#46321;&#44553;&#48324;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . def num_plot(column): fig, axes = plt.subplots(1, 3, figsize=(16, 6)) sns.distplot(train_0[column], ax = axes[0]) axes[0].tick_params(labelsize=12) axes[0].set_title(&#39;credit = 0&#39;) axes[0].set_ylabel(&#39;count&#39;) sns.distplot(train_1[column], ax = axes[1]) axes[1].tick_params(labelsize=12) axes[1].set_title(&#39;credit = 1&#39;) axes[1].set_ylabel(&#39;count&#39;) sns.distplot(train_2[column], ax = axes[2]) axes[2].tick_params(labelsize=12) axes[2].set_title(&#39;credit = 2&#39;) axes[2].set_ylabel(&#39;count&#39;) plt.subplots_adjust(wspace=0.3, hspace=0.3) num_plot(&quot;child_num&quot;) . 자녀 수 변수입니다. 신용 등급별로 큰 차이는 없어보입니다. . 다만 신용등급 2에 자녀가 아주 많은 소수의 변수가 존재하는 걸 알 수 있습니다. . num_plot(&quot;family_size&quot;) . 가족 수 변수도 자식 수 변수와 마찬가지 결과를 보이는 것 같아요. . num_plot(&quot;income_total&quot;) . 신용등급에 따른 월간 소득 차이는 크게 없어 보입니다. (??) . sns.distplot(train_0[&#39;income_total&#39;],label=&#39;0.0&#39;, hist=False) sns.distplot(train_1[&#39;income_total&#39;],label=&#39;0.1&#39;, hist=False) sns.distplot(train_2[&#39;income_total&#39;],label=&#39;0.2&#39;, hist=False) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f8be19fa9d0&gt; . 정확히 확인하기 위해 그래프를 겹첬는데요. 조금 차이는 있으나 많이 비슷한 것을 볼 수 있습니다. . num_plot(&quot;DAYS_BIRTH&quot;) . 숫자의 절대값이 작을 수록 젊은 사람 변수 입니다. 그래프가 전반적으로 비슷해 보입니다. . train_0[&#39;Month&#39;] = abs(train_0[&#39;begin_month&#39;]) train_1[&#39;Month&#39;] = abs(train_1[&#39;begin_month&#39;]) train_2[&#39;Month&#39;] = abs(train_2[&#39;begin_month&#39;]) train_0 = train_0.astype({&#39;Month&#39;: &#39;int&#39;}) train_1 = train_1.astype({&#39;Month&#39;: &#39;int&#39;}) train_2 = train_2.astype({&#39;Month&#39;: &#39;int&#39;}) train_0[&#39;Month&#39;].head() num_plot(&quot;Month&quot;) . 카드 생성일 변수를 양수로 바꿔서 분석했습니다. . 전반적으로 흐름은 비슷해보이는데, 카드 발급 초기에서 약 70프로 정도는 신용등급 1을, 약 30프로는 0을 부여하는 것 같습니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . object_col = [] for col in train.columns: if train[col].dtype == &#39;object&#39;: object_col.append(col) enc = OneHotEncoder() enc.fit(train.loc[:,object_col]) train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), columns=enc.get_feature_names(object_col)) train.drop(object_col, axis=1, inplace=True) train = pd.concat([train, train_onehot_df], axis=1) test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), columns=enc.get_feature_names(object_col)) test.drop(object_col, axis=1, inplace=True) test = pd.concat([test, test_onehot_df], axis=1) . 범주형 변수는 모두 원-핫 인코딩을 해줍니다. . sample_submission . index 0 1 2 . 0 26457 | 0 | 0 | 0 | . 1 26458 | 0 | 0 | 0 | . 2 26459 | 0 | 0 | 0 | . 3 26460 | 0 | 0 | 0 | . 4 26461 | 0 | 0 | 0 | . ... ... | ... | ... | ... | . 9995 36452 | 0 | 0 | 0 | . 9996 36453 | 0 | 0 | 0 | . 9997 36454 | 0 | 0 | 0 | . 9998 36455 | 0 | 0 | 0 | . 9999 36456 | 0 | 0 | 0 | . 10000 rows × 4 columns . 이 대회는 0, 1, 2의 확률이 어떻게 되는지 예측하는 모델입니다. . skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) folds=[] for train_idx, valid_idx in skf.split(train, train[&#39;credit&#39;]): folds.append((train_idx, valid_idx)) random.seed(42) lgb_models={} for fold in range(5): print(f&#39;===================================={fold+1}============================================&#39;) train_idx, valid_idx = folds[fold] X_train, X_valid, y_train, y_valid = train.drop([&#39;credit&#39;],axis=1).iloc[train_idx].values, train.drop([&#39;credit&#39;],axis=1).iloc[valid_idx].values, train[&#39;credit&#39;][train_idx].values, train[&#39;credit&#39;][valid_idx].values lgb = LGBMClassifier(n_estimators=1000) lgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=30, verbose=100) lgb_models[fold]=lgb print(f&#39;================================================================================ n n&#39;) . ====================================1============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676692 valid_1&#39;s multi_logloss: 0.766702 [200] training&#39;s multi_logloss: 0.596634 valid_1&#39;s multi_logloss: 0.755074 [300] training&#39;s multi_logloss: 0.53456 valid_1&#39;s multi_logloss: 0.751863 [400] training&#39;s multi_logloss: 0.482683 valid_1&#39;s multi_logloss: 0.750901 Early stopping, best iteration is: [385] training&#39;s multi_logloss: 0.489523 valid_1&#39;s multi_logloss: 0.750597 ================================================================================ ====================================2============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.673988 valid_1&#39;s multi_logloss: 0.778812 [200] training&#39;s multi_logloss: 0.593911 valid_1&#39;s multi_logloss: 0.766056 [300] training&#39;s multi_logloss: 0.532019 valid_1&#39;s multi_logloss: 0.762532 Early stopping, best iteration is: [358] training&#39;s multi_logloss: 0.500235 valid_1&#39;s multi_logloss: 0.761024 ================================================================================ ====================================3============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676709 valid_1&#39;s multi_logloss: 0.771762 [200] training&#39;s multi_logloss: 0.593522 valid_1&#39;s multi_logloss: 0.758924 Early stopping, best iteration is: [236] training&#39;s multi_logloss: 0.57026 valid_1&#39;s multi_logloss: 0.758105 ================================================================================ ====================================4============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.675515 valid_1&#39;s multi_logloss: 0.7694 [200] training&#39;s multi_logloss: 0.597206 valid_1&#39;s multi_logloss: 0.758117 [300] training&#39;s multi_logloss: 0.533343 valid_1&#39;s multi_logloss: 0.753141 Early stopping, best iteration is: [308] training&#39;s multi_logloss: 0.528916 valid_1&#39;s multi_logloss: 0.752857 ================================================================================ ====================================5============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676696 valid_1&#39;s multi_logloss: 0.767947 [200] training&#39;s multi_logloss: 0.595696 valid_1&#39;s multi_logloss: 0.757343 [300] training&#39;s multi_logloss: 0.531936 valid_1&#39;s multi_logloss: 0.753206 Early stopping, best iteration is: [346] training&#39;s multi_logloss: 0.50629 valid_1&#39;s multi_logloss: 0.752064 ================================================================================ . sample_submission.iloc[:,1:]=0 for fold in range(5): sample_submission.iloc[:,1:] += lgb_models[fold].predict_proba(test)/5 sample_submission.to_csv(&#39;ssu6_submission.csv&#39;, index=False) sample_submission.head() . index 0 1 2 . 0 26457 | 0.018329 | 0.187203 | 0.794468 | . 1 26458 | 0.061934 | 0.121026 | 0.817041 | . 2 26459 | 0.027629 | 0.203945 | 0.768427 | . 3 26460 | 0.067723 | 0.199497 | 0.732780 | . 4 26461 | 0.079370 | 0.229451 | 0.691179 | .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/visualization/onehotencoder/lgm/classifier/2021/11/14/kagglessu6.html",
            "relUrl": "/ssuda/jupyter/kaggle/visualization/onehotencoder/lgm/classifier/2021/11/14/kagglessu6.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "[SSUDA] 따릉이 데이터 예측 코드",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) train = pd.read_csv(&quot;/content/drive/MyDrive/bicycle/train.csv&quot;) test = pd.read_csv(&#39;/content/drive/MyDrive/bicycle/test.csv&#39;) sample_submission = pd.read_csv(&#39;/content/drive/MyDrive/bicycle/sample_submission.csv&#39;) train.head() . date_time wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability number_of_rentals . 0 2018-04-01 | 207.500 | 4.000 | 0.000 | 3.050 | 75.000 | 12.600 | 21.000 | 30.000 | 22994 | . 1 2018-04-02 | 208.317 | 2.950 | 0.000 | 3.278 | 69.833 | 12.812 | 19.000 | 19.500 | 28139 | . 2 2018-04-03 | 213.516 | 2.911 | 0.000 | 2.690 | 74.879 | 10.312 | 15.316 | 19.113 | 26817 | . 3 2018-04-04 | 143.836 | 3.692 | 0.425 | 3.138 | 71.849 | 8.312 | 12.368 | 43.493 | 26034 | . 4 2018-04-05 | 95.905 | 4.000 | 0.723 | 3.186 | 73.784 | 5.875 | 10.421 | 63.378 | 2833 | . &#48320;&#49688; &#53456;&#49353; . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 273 entries, 0 to 272 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 date_time 273 non-null object 1 wind_direction 273 non-null float64 2 sky_condition 273 non-null float64 3 precipitation_form 273 non-null float64 4 wind_speed 273 non-null float64 5 humidity 273 non-null float64 6 low_temp 273 non-null float64 7 high_temp 273 non-null float64 8 Precipitation_Probability 273 non-null float64 9 number_of_rentals 273 non-null int64 dtypes: float64(8), int64(1), object(1) memory usage: 21.5+ KB . number_of_rentals : 따릉이 대여량(Y값), date_time : 날짜, wind_direction : 풍향 . sky_condition : 하늘 상태(1 : 맑음, 3 : 구름 많음, 4 : 흐림, 하루에 8번 측정한 값 평균) . precipitation_form : 강수 형태(0 : 맑음, 1 : 비, 마찬가지로 하루에 8번 측정한 값 평균) . wind_speed : 풍속, humidity : 습도, low_temp : 최저기온, high_temp : 최고기온, precipitation_Probability : 강수확률 . 결측값은 없습니다. . import matplotlib.pyplot as plt plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train[&#39;number_of_rentals&#39;].values)) plt.show() . 반응변수의 이상치은 관찰되지 않는 것으로 보입니다. . train[&#39;date_time&#39;] = pd.to_datetime(train[&#39;date_time&#39;]) test[&#39;date_time&#39;] = pd.to_datetime(test[&#39;date_time&#39;]) train[&#39;day&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).day test[&#39;day&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).day train[&#39;month&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).month test[&#39;month&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).month train[&#39;year&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).year test[&#39;year&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).year train[&#39;weekday&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).weekday test[&#39;weekday&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).weekday . date_time이 날짜 변수이기 때문에 데이터 형식을 datetime으로 바꾸어줍니다. . 그 후 datetime 데이터 형식으로 얻을 수 있는 이점, 날/달/연/주말 변수를 추출합니다. . train[&#39;wind_direction&#39;].hist() train[&#39;wind_direction&#39;].max() . 321.622 . wind_direction은 바람 방향 변수입니다. 아마 방향을 360도로 생각해서 만든 변수인 것 같습니다. . 다만 바람 방향과 따릉이 대여량은 상관 없을 것 같습니다. . 물론, 서울 자전거 도로가 한강 기준으로 많이 구성되어 있어 도로가 동-서 기준으로 많이 있긴 합니다. . 하지만 바람 방향이 오늘은 이쪽이니 자전거를 타자라는 생각을 하진 않을 것 같습니다. 바람 세기가 더 중요하죠. . 또 바람 방향 변수는 하루에도 계속 바뀌기 때문에 평균적인 방향인 것 같은데, 만약 바람이 주로 0에서 20, 340에서 360 각도로 불었을때 평균치는 약 180입니다. . (방향이 동쪽에서 위 아래로만 움직인다면 10에서 350으로 쉽게 바뀔 수 있습니다.) . 이 수치가 과연 유의미할지 개인적으로 의문이 들어서, 이 변수는 빼는 것이 좋아보입니다. . train[&#39;precipitation_form&#39;].corr(train[&#39;Precipitation_Probability&#39;]) . 0.9106089542607185 . train[&#39;precipitation_form&#39;].corr(train[&#39;sky_condition&#39;]) . 0.6738137525457335 . 비가 오는 상황을 예측하는 두 변수 precipitation_form와 Precipitation_Probability간 상관관계는 당연히 높습니다. . 다만 Precipitation_Probability는 강우 확률 예측 변수 입니다. . 때문에 일일 강우 단기예측 기록인 precipitation_form 변수가 하루 비가 오는 날을 더 잘 표현할 것으로 생각됩니다. . 비슷한 부분을 설명하는 두 변수이기 때문에 precipitation_form 변수만 사용하겠습니다. . precipitation_form 변수는 하늘 상태를 나타내는 sky_condition 변수와도 상관관계가 높지만 극단적이진 않습니다. . 날씨가 흐린것 자체가 따릉이 대여량에 부정적인 영향을 준다고 생각하기 때문에 sky_condition 변수는 사용하겠습니다. . import matplotlib.pyplot as plt plt.figure(figsize=(20, 10)) plt.bar(train[&#39;date_time&#39;][train[&#39;year&#39;] == 2018], train[&#39;number_of_rentals&#39;][train[&#39;year&#39;] == 2018], width=0.6, color=&#39;grey&#39;) . &lt;BarContainer object of 91 artists&gt; . train[&#39;day&#39;][train[&#39;month&#39;] == 5] += 30 train[&#39;day&#39;][train[&#39;month&#39;] == 6] += 61 test[&#39;day&#39;][test[&#39;month&#39;] == 5] += 30 test[&#39;day&#39;][test[&#39;month&#39;] == 6] += 61 . 따릉이 대여량을 2018년 기준으로 날짜순으로 확인했습니다. . 4~6월 데이터인 만큼, 날이 점점 따뜻해지는 영향으로 변동이 심하긴 하지만 증가하는 추세가 보이는 것 같습니다. . (중간중간 값이 급격히 작아지는 것은 아마 비가 오는날인거 같습니다.) . 그래서 날짜 변수를 쓰는것 보다, 누적된 날짜가 몇일인지를 기록하는 변수를 쓰는게 좋을 것 같습니다. . (4월 15일 =&gt; 15일, 5월 2일 =&gt; 30일 + 2일 = 32일, 6월 10일 =&gt; 30일 + 31일 + 10일 = 71일) . 이렇게 되면 달 변수 또한 쓰지 않는게 좋을 것 같습니다. 만든 변수가 달 변수가 설명할 부분까지 설명하기 때문이죠. . import seaborn as sns def barplots(variable): plot = train.groupby(variable)[&#39;number_of_rentals&#39;].mean() sns.barplot(plot.index,plot.values) barplots(&#39;year&#39;) . 연도별 따릉이 이용자수를 나타내는 그래프 입니다. . 시간이 지날수록 따릉이 이용자수가 늘어나는 것을 확인할 수 있습니다. 그러므로 연도 변수는 매우 중요한 변수임을 알 수 있겠죠. . barplots(&#39;weekday&#39;) . 요일별 따릉이 이용자수를 나타내는 그래프 입니다. weekday 변수는 0은 월요일, 6은 일요일을 나타내는 요일 변수입니다. . 직관적으로 확인했을때 일요일에 따릉이 이용자수가 유의미하게 적은 것이 눈에 띕니다. . train_label = train[&#39;number_of_rentals&#39;] train.drop([&#39;date_time&#39;,&#39;wind_direction&#39;, &#39;Precipitation_Probability&#39;, &#39;month&#39;, &#39;number_of_rentals&#39;], axis = 1, inplace= True) test.drop([&#39;date_time&#39;,&#39;wind_direction&#39;, &#39;Precipitation_Probability&#39;, &#39;month&#39;], axis = 1, inplace= True) . 앞서 설명한 변수들을 제거합니다. . &#47784;&#45944; &#51201;&#54633; . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;number_of_rentals&#39;] = rf.predict(test) sample_submission.to_csv(&#39;bicycle_final_4.csv&#39;,encoding=&#39;UTF-8&#39;,index=False) . from xgboost import XGBRegressor xgb = XGBRegressor() xgb.fit(train,train_label) sample_submission[&#39;number_of_rentals&#39;] = xgb.predict(test) sample_submission.to_csv(&#39;bicycle_final_7.csv&#39;,encoding=&#39;UTF-8&#39;,index=False) . [11:30:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . 간단한 랜덤 포레스트 모델을 사용했습니다. 다른 모델을 사용하거나 하이퍼 파라미터를 조정하면 점수가 더 오를수도 있겠죠? .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/dacon/datetime/xgboost/regression/2021/11/04/kagglessu5_plus.html",
            "relUrl": "/ssuda/jupyter/dacon/datetime/xgboost/regression/2021/11/04/kagglessu5_plus.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "[SSUDA] 판매량 예측 데이터 분석",
            "content": ". &#52880;&#44544;&#44284; &#50672;&#46041;&#54616;&#44592; . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;23e68db36970b65937516103c630ba75&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c competitive-data-science-predict-future-sales . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sample_submission.csv.zip to /content 0% 0.00/468k [00:00&lt;?, ?B/s] 100% 468k/468k [00:00&lt;00:00, 69.0MB/s] Downloading sales_train.csv.zip to /content 38% 5.00M/13.3M [00:00&lt;00:01, 5.79MB/s] 100% 13.3M/13.3M [00:00&lt;00:00, 14.4MB/s] Downloading item_categories.csv to /content 0% 0.00/3.49k [00:00&lt;?, ?B/s] 100% 3.49k/3.49k [00:00&lt;00:00, 2.51MB/s] Downloading shops.csv to /content 0% 0.00/2.91k [00:00&lt;?, ?B/s] 100% 2.91k/2.91k [00:00&lt;00:00, 10.6MB/s] Downloading test.csv.zip to /content 0% 0.00/1.02M [00:00&lt;?, ?B/s] 100% 1.02M/1.02M [00:00&lt;00:00, 156MB/s] Downloading items.csv.zip to /content 0% 0.00/368k [00:00&lt;?, ?B/s] 100% 368k/368k [00:00&lt;00:00, 117MB/s] . !unzip items.csv.zip !unzip sales_train.csv.zip !unzip sample_submission.csv.zip !unzip test.csv.zip . Archive: items.csv.zip inflating: items.csv Archive: sales_train.csv.zip inflating: sales_train.csv Archive: sample_submission.csv.zip inflating: sample_submission.csv Archive: test.csv.zip inflating: test.csv . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from matplotlib import pylab as plt import matplotlib.dates as mdates plt.rcParams[&#39;figure.figsize&#39;] = (15.0, 8.0) import seaborn as sns . train = pd.read_csv(&#39;./sales_train.csv&#39;) print (&#39;number of shops: &#39;, train[&#39;shop_id&#39;].max()) print (&#39;number of items: &#39;, train[&#39;item_id&#39;].max()) num_month = train[&#39;date_block_num&#39;].max() print (&#39;number of month: &#39;, num_month) print (&#39;size of train: &#39;, train.shape) train.head() . number of shops: 59 number of items: 22169 number of month: 33 size of train: (2935849, 6) . date date_block_num shop_id item_id item_price item_cnt_day . 0 02.01.2013 | 0 | 59 | 22154 | 999.00 | 1.0 | . 1 03.01.2013 | 0 | 25 | 2552 | 899.00 | 1.0 | . 2 05.01.2013 | 0 | 25 | 2552 | 899.00 | -1.0 | . 3 06.01.2013 | 0 | 25 | 2554 | 1709.05 | 1.0 | . 4 15.01.2013 | 0 | 25 | 2555 | 1099.00 | 1.0 | . 변수 설명 . date : 날짜 변수, date_block_num : 달 변수(2013년 1월 =&gt; 0, 2015년 10월 =&gt; 33) . shop_id, item_id : 상점/제품의 고유번호 변수 . item_price : 제품의 가격 변수, item_cnt_dat : 그 날 제품이 팔린 개수 . (여기서 item_cnt_dat 변수가 음수인 것은 물건이 반품된 것을 의미하는 것 같습니다.) . test = pd.read_csv(&#39;./test.csv&#39;) test.head() . ID shop_id item_id . 0 0 | 5 | 5037 | . 1 1 | 5 | 5320 | . 2 2 | 5 | 5233 | . 3 3 | 5 | 5232 | . 4 4 | 5 | 5268 | . sub = pd.read_csv(&#39;./sample_submission.csv&#39;) sub.head() . ID item_cnt_month . 0 0 | 0.5 | . 1 1 | 0.5 | . 2 2 | 0.5 | . 3 3 | 0.5 | . 4 4 | 0.5 | . 2015년 11월 데이터를 예측하는 캐글 대회입니다. . date_block_num 변수는 34가 되겠죠. . items = pd.read_csv(&#39;./items.csv&#39;) print (&#39;number of categories: &#39;, items[&#39;item_category_id&#39;].max()) # the maximun number of category id items.head() . number of categories: 83 . item_name item_id item_category_id . 0 ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.) D | 0 | 40 | . 1 !ABBYY FineReader 12 Professional Edition Full... | 1 | 76 | . 2 ***В ЛУЧАХ СЛАВЫ (UNV) D | 2 | 40 | . 3 ***ГОЛУБАЯ ВОЛНА (Univ) D | 3 | 40 | . 4 ***КОРОБКА (СТЕКЛО) D | 4 | 40 | . train_clean = train.drop(labels = [&#39;date&#39;, &#39;item_price&#39;], axis = 1) train_clean.head() . date_block_num shop_id item_id item_cnt_day . 0 0 | 59 | 22154 | 1.0 | . 1 0 | 25 | 2552 | 1.0 | . 2 0 | 25 | 2552 | -1.0 | . 3 0 | 25 | 2554 | 1.0 | . 4 0 | 25 | 2555 | 1.0 | . 날짜는 대체하는 date_block_num 변수가 있기 때문에 빼줍니다. . 또 제품 가격 변수 또한 빼줍니다. . train_clean = train_clean.groupby([&quot;item_id&quot;,&quot;shop_id&quot;,&quot;date_block_num&quot;]).sum().reset_index() train_clean = train_clean.rename(index=str, columns = {&quot;item_cnt_day&quot;:&quot;item_cnt_month&quot;}) train_clean = train_clean[[&quot;item_id&quot;,&quot;shop_id&quot;,&quot;date_block_num&quot;,&quot;item_cnt_month&quot;]] train_clean . item_id shop_id date_block_num item_cnt_month . 0 0 | 54 | 20 | 1.0 | . 1 1 | 55 | 15 | 2.0 | . 2 1 | 55 | 18 | 1.0 | . 3 1 | 55 | 19 | 1.0 | . 4 1 | 55 | 20 | 1.0 | . ... ... | ... | ... | ... | . 1609119 22168 | 12 | 8 | 1.0 | . 1609120 22168 | 16 | 1 | 1.0 | . 1609121 22168 | 42 | 1 | 1.0 | . 1609122 22168 | 43 | 2 | 1.0 | . 1609123 22169 | 25 | 14 | 1.0 | . 1609124 rows × 4 columns . 같은 달별로(= date_block_num 변수가 같은 값으로) 묶어줍니다. . 테스트 데이터에서 예측하고자 하는 값의 범위가 달 단위이기 때문입니다. . 변수 이름 또한 그에 맞게 item_cnt_month로 바꿨습니다. . &#49884;&#44228;&#50676; &#45936;&#51060;&#53552; &#50672;&#49845;&#54616;&#44592; . check = train_clean[[&quot;shop_id&quot;,&quot;item_id&quot;,&quot;date_block_num&quot;,&quot;item_cnt_month&quot;]] check = check.loc[check[&#39;shop_id&#39;] == 5] check = check.loc[check[&#39;item_id&#39;] == 5037] check . shop_id item_id date_block_num item_cnt_month . 400439 5 | 5037 | 20 | 1.0 | . 400440 5 | 5037 | 22 | 1.0 | . 400441 5 | 5037 | 23 | 2.0 | . 400442 5 | 5037 | 24 | 2.0 | . 400443 5 | 5037 | 28 | 1.0 | . 400444 5 | 5037 | 29 | 1.0 | . 400445 5 | 5037 | 30 | 1.0 | . 400446 5 | 5037 | 31 | 3.0 | . 400447 5 | 5037 | 32 | 1.0 | . 특정 shop_id와 item_id 값을 가지는 값만 모았습니다. . 시계열 분석을 처음하기 때문에 1차로 소량의 데이터를 다루었습니다. . 이렇게 데이터 분석을 공부하면 보다 직관적으로 LSTM 모델을 학습할 수 있을 것 같습니다. . plt.figure(figsize=(10,4)) plt.title(&#39;Check - Sales of Item 5037 at Shop 5&#39;) plt.xlabel(&#39;Month&#39;) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) plt.plot(check[&quot;date_block_num&quot;],check[&quot;item_cnt_month&quot;]); . 단순히 Y값에 대해 그림을 그려보았습니다. . month_list=[i for i in range(num_month+1)] # num_month = train[&#39;date_block_num&#39;].max(), 최고값 shop = [] for i in range(num_month+1): shop.append(5) item = [] for i in range(num_month+1): item.append(5037) months_full = pd.DataFrame({&#39;shop_id&#39;:shop, &#39;item_id&#39;:item,&#39;date_block_num&#39;:month_list}) months_full.head(10) . shop_id item_id date_block_num . 0 5 | 5037 | 0 | . 1 5 | 5037 | 1 | . 2 5 | 5037 | 2 | . 3 5 | 5037 | 3 | . 4 5 | 5037 | 4 | . 5 5 | 5037 | 5 | . 6 5 | 5037 | 6 | . 7 5 | 5037 | 7 | . 8 5 | 5037 | 8 | . 9 5 | 5037 | 9 | . 빈 데이터를 없애기 위해 처음부터 데이터프레임을 세팅하는 모습입니다. . shop = [] for i in range(num_month+1): shop.append(5) . 다만 이 코드 보다는 [5]*(num_month+1) 식으로 리스트를 구성하는게 더 깔끔한 것 같습니다. . sales_33month = pd.merge(check, months_full, how=&#39;right&#39;, on=[&#39;shop_id&#39;,&#39;item_id&#39;,&#39;date_block_num&#39;]) sales_33month = sales_33month.sort_values(by=[&#39;date_block_num&#39;]) sales_33month.fillna(0.00,inplace=True) plt.figure(figsize=(10,4)) plt.title(&#39;Check - Sales of Item 5037 at Shop 5 for whole period&#39;) plt.xlabel(&#39;Month&#39;) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) plt.plot(sales_33month[&quot;date_block_num&quot;],sales_33month[&quot;item_cnt_month&quot;]); . 물품 구매가 없는 데이터까지 0 값을 넣어서 그림을 그렸습니다. . for i in range(1,6): sales_33month[&quot;T_&quot; + str(i)] = sales_33month.item_cnt_month.shift(i) sales_33month.fillna(0.0, inplace=True) df = sales_33month[[&#39;shop_id&#39;,&#39;item_id&#39;,&#39;date_block_num&#39;,&#39;T_1&#39;,&#39;T_2&#39;,&#39;T_3&#39;,&#39;T_4&#39;,&#39;T_5&#39;, &#39;item_cnt_month&#39;]].reset_index() df = df.drop(labels = [&#39;index&#39;], axis = 1) df . shop_id item_id date_block_num T_1 T_2 T_3 T_4 T_5 item_cnt_month . 0 5 | 5037 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 5 | 5037 | 1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 5 | 5037 | 2 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 5 | 5037 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 5 | 5037 | 4 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 5 | 5037 | 5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 6 5 | 5037 | 6 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 7 5 | 5037 | 7 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 8 5 | 5037 | 8 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 9 5 | 5037 | 9 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 10 5 | 5037 | 10 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 11 5 | 5037 | 11 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 12 5 | 5037 | 12 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 13 5 | 5037 | 13 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 14 5 | 5037 | 14 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 15 5 | 5037 | 15 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 16 5 | 5037 | 16 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 17 5 | 5037 | 17 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 18 5 | 5037 | 18 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 19 5 | 5037 | 19 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 20 5 | 5037 | 20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 21 5 | 5037 | 21 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 22 5 | 5037 | 22 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 23 5 | 5037 | 23 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 2.0 | . 24 5 | 5037 | 24 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | 2.0 | . 25 5 | 5037 | 25 | 2.0 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 26 5 | 5037 | 26 | 0.0 | 2.0 | 2.0 | 1.0 | 0.0 | 0.0 | . 27 5 | 5037 | 27 | 0.0 | 0.0 | 2.0 | 2.0 | 1.0 | 0.0 | . 28 5 | 5037 | 28 | 0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 1.0 | . 29 5 | 5037 | 29 | 1.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | . 30 5 | 5037 | 30 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 31 5 | 5037 | 31 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 3.0 | . 32 5 | 5037 | 32 | 3.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | . 33 5 | 5037 | 33 | 1.0 | 3.0 | 1.0 | 1.0 | 1.0 | 0.0 | . 시계열 분석을 기초부터 뜯어본 것 같습니다. . T1 ~ T5에 의미는 최근 5달간 이전 Y값의 기록입니다. 예를 들면 T1은 한달 전 Y값을 나타냅니다. . 시간의 흐름에 따라 예측값이 영향을 받기 때문에 이러한 방식이 지금 이 데이터에서 적절합니다. . LSTM &#47784;&#45944; &#49324;&#50857; . train_df = df[:-3] val_df = df[-3:] x_train,y_train = train_df.drop([&quot;item_cnt_month&quot;],axis=1),train_df.item_cnt_month x_val,y_val = val_df.drop([&quot;item_cnt_month&quot;],axis=1),val_df.item_cnt_month . 맨 마지막 3개 데이터를 test 데이터로 사용합니다. . from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM model_lstm = Sequential() model_lstm.add(LSTM(15, input_shape=(1,8))) model_lstm.add(Dense(1)) model_lstm.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . from sklearn.preprocessing import StandardScaler,MinMaxScaler scaler = StandardScaler() scaler = MinMaxScaler(feature_range=(-1, 1)) x_train_scaled = scaler.fit_transform(x_train) x_valid_scaled = scaler.fit_transform(x_val) . x_train_reshaped = x_train_scaled.reshape((x_train_scaled.shape[0], 1, x_train_scaled.shape[1])) x_val_resaped = x_valid_scaled.reshape((x_valid_scaled.shape[0], 1, x_valid_scaled.shape[1])) history = model_lstm.fit(x_train_reshaped, y_train, validation_data=(x_val_resaped, y_val),epochs=70, batch_size=12, verbose=2, shuffle=False) y_pre = model_lstm.predict(x_val_resaped) . Epoch 1/70 3/3 - 2s - loss: 0.4119 - accuracy: 0.7742 - val_loss: 3.6385 - val_accuracy: 0.3333 Epoch 2/70 3/3 - 0s - loss: 0.3959 - accuracy: 0.7742 - val_loss: 3.5825 - val_accuracy: 0.3333 Epoch 3/70 3/3 - 0s - loss: 0.3818 - accuracy: 0.7742 - val_loss: 3.5290 - val_accuracy: 0.3333 Epoch 4/70 3/3 - 0s - loss: 0.3689 - accuracy: 0.7742 - val_loss: 3.4781 - val_accuracy: 0.3333 Epoch 5/70 3/3 - 0s - loss: 0.3571 - accuracy: 0.7742 - val_loss: 3.4296 - val_accuracy: 0.3333 Epoch 6/70 3/3 - 0s - loss: 0.3464 - accuracy: 0.7742 - val_loss: 3.3839 - val_accuracy: 0.3333 Epoch 7/70 3/3 - 0s - loss: 0.3368 - accuracy: 0.7742 - val_loss: 3.3409 - val_accuracy: 0.3333 Epoch 8/70 3/3 - 0s - loss: 0.3281 - accuracy: 0.7742 - val_loss: 3.3008 - val_accuracy: 0.3333 Epoch 9/70 3/3 - 0s - loss: 0.3203 - accuracy: 0.7742 - val_loss: 3.2637 - val_accuracy: 0.3333 Epoch 10/70 3/3 - 0s - loss: 0.3132 - accuracy: 0.7742 - val_loss: 3.2296 - val_accuracy: 0.3333 Epoch 11/70 3/3 - 0s - loss: 0.3069 - accuracy: 0.7742 - val_loss: 3.1984 - val_accuracy: 0.3333 Epoch 12/70 3/3 - 0s - loss: 0.3012 - accuracy: 0.7742 - val_loss: 3.1702 - val_accuracy: 0.3333 Epoch 13/70 3/3 - 0s - loss: 0.2960 - accuracy: 0.7742 - val_loss: 3.1451 - val_accuracy: 0.3333 Epoch 14/70 3/3 - 0s - loss: 0.2913 - accuracy: 0.7742 - val_loss: 3.1228 - val_accuracy: 0.3333 Epoch 15/70 3/3 - 0s - loss: 0.2869 - accuracy: 0.7742 - val_loss: 3.1035 - val_accuracy: 0.3333 Epoch 16/70 3/3 - 0s - loss: 0.2829 - accuracy: 0.7742 - val_loss: 3.0871 - val_accuracy: 0.3333 Epoch 17/70 3/3 - 0s - loss: 0.2791 - accuracy: 0.7742 - val_loss: 3.0733 - val_accuracy: 0.3333 Epoch 18/70 3/3 - 0s - loss: 0.2755 - accuracy: 0.7742 - val_loss: 3.0623 - val_accuracy: 0.3333 Epoch 19/70 3/3 - 0s - loss: 0.2720 - accuracy: 0.7742 - val_loss: 3.0537 - val_accuracy: 0.3333 Epoch 20/70 3/3 - 0s - loss: 0.2687 - accuracy: 0.7742 - val_loss: 3.0476 - val_accuracy: 0.3333 Epoch 21/70 3/3 - 0s - loss: 0.2654 - accuracy: 0.7742 - val_loss: 3.0437 - val_accuracy: 0.3333 Epoch 22/70 3/3 - 0s - loss: 0.2622 - accuracy: 0.7742 - val_loss: 3.0419 - val_accuracy: 0.3333 Epoch 23/70 3/3 - 0s - loss: 0.2590 - accuracy: 0.7742 - val_loss: 3.0421 - val_accuracy: 0.3333 Epoch 24/70 3/3 - 0s - loss: 0.2558 - accuracy: 0.8065 - val_loss: 3.0440 - val_accuracy: 0.3333 Epoch 25/70 3/3 - 0s - loss: 0.2527 - accuracy: 0.8065 - val_loss: 3.0477 - val_accuracy: 0.3333 Epoch 26/70 3/3 - 0s - loss: 0.2495 - accuracy: 0.8387 - val_loss: 3.0528 - val_accuracy: 0.3333 Epoch 27/70 3/3 - 0s - loss: 0.2463 - accuracy: 0.8387 - val_loss: 3.0592 - val_accuracy: 0.3333 Epoch 28/70 3/3 - 0s - loss: 0.2432 - accuracy: 0.8387 - val_loss: 3.0669 - val_accuracy: 0.3333 Epoch 29/70 3/3 - 0s - loss: 0.2401 - accuracy: 0.8387 - val_loss: 3.0756 - val_accuracy: 0.3333 Epoch 30/70 3/3 - 0s - loss: 0.2370 - accuracy: 0.8387 - val_loss: 3.0853 - val_accuracy: 0.3333 Epoch 31/70 3/3 - 0s - loss: 0.2339 - accuracy: 0.8387 - val_loss: 3.0958 - val_accuracy: 0.3333 Epoch 32/70 3/3 - 0s - loss: 0.2308 - accuracy: 0.8387 - val_loss: 3.1070 - val_accuracy: 0.3333 Epoch 33/70 3/3 - 0s - loss: 0.2278 - accuracy: 0.8065 - val_loss: 3.1187 - val_accuracy: 0.6667 Epoch 34/70 3/3 - 0s - loss: 0.2248 - accuracy: 0.8065 - val_loss: 3.1310 - val_accuracy: 0.6667 Epoch 35/70 3/3 - 0s - loss: 0.2219 - accuracy: 0.8065 - val_loss: 3.1436 - val_accuracy: 0.6667 Epoch 36/70 3/3 - 0s - loss: 0.2190 - accuracy: 0.7742 - val_loss: 3.1565 - val_accuracy: 0.6667 Epoch 37/70 3/3 - 0s - loss: 0.2162 - accuracy: 0.7742 - val_loss: 3.1696 - val_accuracy: 0.3333 Epoch 38/70 3/3 - 0s - loss: 0.2134 - accuracy: 0.7742 - val_loss: 3.1829 - val_accuracy: 0.3333 Epoch 39/70 3/3 - 0s - loss: 0.2107 - accuracy: 0.8065 - val_loss: 3.1963 - val_accuracy: 0.3333 Epoch 40/70 3/3 - 0s - loss: 0.2081 - accuracy: 0.8065 - val_loss: 3.2096 - val_accuracy: 0.3333 Epoch 41/70 3/3 - 0s - loss: 0.2056 - accuracy: 0.8065 - val_loss: 3.2229 - val_accuracy: 0.3333 Epoch 42/70 3/3 - 0s - loss: 0.2031 - accuracy: 0.8065 - val_loss: 3.2361 - val_accuracy: 0.3333 Epoch 43/70 3/3 - 0s - loss: 0.2008 - accuracy: 0.8065 - val_loss: 3.2492 - val_accuracy: 0.3333 Epoch 44/70 3/3 - 0s - loss: 0.1985 - accuracy: 0.8065 - val_loss: 3.2621 - val_accuracy: 0.3333 Epoch 45/70 3/3 - 0s - loss: 0.1963 - accuracy: 0.8065 - val_loss: 3.2748 - val_accuracy: 0.3333 Epoch 46/70 3/3 - 0s - loss: 0.1941 - accuracy: 0.8065 - val_loss: 3.2872 - val_accuracy: 0.3333 Epoch 47/70 3/3 - 0s - loss: 0.1921 - accuracy: 0.8065 - val_loss: 3.2994 - val_accuracy: 0.3333 Epoch 48/70 3/3 - 0s - loss: 0.1901 - accuracy: 0.8065 - val_loss: 3.3113 - val_accuracy: 0.3333 Epoch 49/70 3/3 - 0s - loss: 0.1882 - accuracy: 0.8065 - val_loss: 3.3229 - val_accuracy: 0.3333 Epoch 50/70 3/3 - 0s - loss: 0.1864 - accuracy: 0.8065 - val_loss: 3.3342 - val_accuracy: 0.3333 Epoch 51/70 3/3 - 0s - loss: 0.1847 - accuracy: 0.8065 - val_loss: 3.3451 - val_accuracy: 0.3333 Epoch 52/70 3/3 - 0s - loss: 0.1830 - accuracy: 0.8065 - val_loss: 3.3558 - val_accuracy: 0.3333 Epoch 53/70 3/3 - 0s - loss: 0.1814 - accuracy: 0.8065 - val_loss: 3.3661 - val_accuracy: 0.3333 Epoch 54/70 3/3 - 0s - loss: 0.1799 - accuracy: 0.8065 - val_loss: 3.3760 - val_accuracy: 0.3333 Epoch 55/70 3/3 - 0s - loss: 0.1785 - accuracy: 0.8065 - val_loss: 3.3855 - val_accuracy: 0.3333 Epoch 56/70 3/3 - 0s - loss: 0.1771 - accuracy: 0.8065 - val_loss: 3.3947 - val_accuracy: 0.3333 Epoch 57/70 3/3 - 0s - loss: 0.1757 - accuracy: 0.8065 - val_loss: 3.4036 - val_accuracy: 0.3333 Epoch 58/70 3/3 - 0s - loss: 0.1745 - accuracy: 0.8065 - val_loss: 3.4120 - val_accuracy: 0.3333 Epoch 59/70 3/3 - 0s - loss: 0.1732 - accuracy: 0.8065 - val_loss: 3.4201 - val_accuracy: 0.3333 Epoch 60/70 3/3 - 0s - loss: 0.1720 - accuracy: 0.8065 - val_loss: 3.4278 - val_accuracy: 0.3333 Epoch 61/70 3/3 - 0s - loss: 0.1709 - accuracy: 0.8065 - val_loss: 3.4351 - val_accuracy: 0.3333 Epoch 62/70 3/3 - 0s - loss: 0.1698 - accuracy: 0.8065 - val_loss: 3.4420 - val_accuracy: 0.3333 Epoch 63/70 3/3 - 0s - loss: 0.1687 - accuracy: 0.8065 - val_loss: 3.4485 - val_accuracy: 0.3333 Epoch 64/70 3/3 - 0s - loss: 0.1677 - accuracy: 0.8065 - val_loss: 3.4547 - val_accuracy: 0.3333 Epoch 65/70 3/3 - 0s - loss: 0.1667 - accuracy: 0.8065 - val_loss: 3.4605 - val_accuracy: 0.3333 Epoch 66/70 3/3 - 0s - loss: 0.1658 - accuracy: 0.8065 - val_loss: 3.4659 - val_accuracy: 0.3333 Epoch 67/70 3/3 - 0s - loss: 0.1648 - accuracy: 0.8065 - val_loss: 3.4710 - val_accuracy: 0.3333 Epoch 68/70 3/3 - 0s - loss: 0.1639 - accuracy: 0.8065 - val_loss: 3.4758 - val_accuracy: 0.3333 Epoch 69/70 3/3 - 0s - loss: 0.1631 - accuracy: 0.8065 - val_loss: 3.4802 - val_accuracy: 0.3333 Epoch 70/70 3/3 - 0s - loss: 0.1622 - accuracy: 0.8065 - val_loss: 3.4844 - val_accuracy: 0.3333 . fig, ax = plt.subplots() ax.plot(x_val[&#39;date_block_num&#39;], y_val, label=&#39;Actual&#39;) ax.plot(x_val[&#39;date_block_num&#39;], y_pre, label=&#39;Predicted&#39;) plt.title(&#39;LSTM Prediction vs Actual Sales for last 3 months&#39;) plt.xlabel(&#39;Month&#39;) plt.xticks(x_val[&#39;date_block_num&#39;]) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) ax.legend() plt.show() . LSTM 모델을 적용시킨 모습입니다. . 잘 맞췄다면 잘 맞췄다고도 말 할수 있고 아쉽다면 아쉽다고 할 수 있는 결과인 것 같습니다. . &#45936;&#51060;&#53552; &#53456;&#49353; . sales_data = pd.read_csv(&#39;./sales_train.csv&#39;) item_cat = pd.read_csv(&#39;./item_categories.csv&#39;) items = pd.read_csv(&#39;./items.csv&#39;) shops = pd.read_csv(&#39;./shops.csv&#39;) sample_submission = pd.read_csv(&#39;./sample_submission.csv&#39;) test_data = pd.read_csv(&#39;./test.csv&#39;) . def basic_eda(df): print(&quot;-TOP 5 RECORDS--&quot;) print(df.head(5)) print(&quot;-INFO--&quot;) print(df.info()) print(&quot;-Describe-&quot;) print(df.describe()) print(&quot;-Columns--&quot;) print(df.columns) print(&quot;-Data Types--&quot;) print(df.dtypes) print(&quot;-Missing Values-&quot;) print(df.isnull().sum()) print(&quot;-NULL values-&quot;) print(df.isna().sum()) print(&quot;--Shape Of Data-&quot;) print(df.shape) print(&quot;=============================Sales Data=============================&quot;) basic_eda(sales_data) print(&quot;=============================Test data=============================&quot;) basic_eda(test_data) print(&quot;=============================Item Categories=============================&quot;) basic_eda(item_cat) print(&quot;=============================Items=============================&quot;) basic_eda(items) print(&quot;=============================Shops=============================&quot;) basic_eda(shops) print(&quot;=============================Sample Submission=============================&quot;) basic_eda(sample_submission) . =============================Sales Data============================= -TOP 5 RECORDS-- date date_block_num shop_id item_id item_price item_cnt_day 0 02.01.2013 0 59 22154 999.00 1.0 1 03.01.2013 0 25 2552 899.00 1.0 2 05.01.2013 0 25 2552 899.00 -1.0 3 06.01.2013 0 25 2554 1709.05 1.0 4 15.01.2013 0 25 2555 1099.00 1.0 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2935849 entries, 0 to 2935848 Data columns (total 6 columns): # Column Dtype -- 0 date object 1 date_block_num int64 2 shop_id int64 3 item_id int64 4 item_price float64 5 item_cnt_day float64 dtypes: float64(2), int64(3), object(1) memory usage: 134.4+ MB None -Describe- date_block_num shop_id item_id item_price item_cnt_day count 2.935849e+06 2.935849e+06 2.935849e+06 2.935849e+06 2.935849e+06 mean 1.456991e+01 3.300173e+01 1.019723e+04 8.908532e+02 1.242641e+00 std 9.422988e+00 1.622697e+01 6.324297e+03 1.729800e+03 2.618834e+00 min 0.000000e+00 0.000000e+00 0.000000e+00 -1.000000e+00 -2.200000e+01 25% 7.000000e+00 2.200000e+01 4.476000e+03 2.490000e+02 1.000000e+00 50% 1.400000e+01 3.100000e+01 9.343000e+03 3.990000e+02 1.000000e+00 75% 2.300000e+01 4.700000e+01 1.568400e+04 9.990000e+02 1.000000e+00 max 3.300000e+01 5.900000e+01 2.216900e+04 3.079800e+05 2.169000e+03 -Columns-- Index([&#39;date&#39;, &#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;, &#39;item_price&#39;, &#39;item_cnt_day&#39;], dtype=&#39;object&#39;) -Data Types-- date object date_block_num int64 shop_id int64 item_id int64 item_price float64 item_cnt_day float64 dtype: object -Missing Values- date 0 date_block_num 0 shop_id 0 item_id 0 item_price 0 item_cnt_day 0 dtype: int64 -NULL values- date 0 date_block_num 0 shop_id 0 item_id 0 item_price 0 item_cnt_day 0 dtype: int64 --Shape Of Data- (2935849, 6) =============================Test data============================= -TOP 5 RECORDS-- ID shop_id item_id 0 0 5 5037 1 1 5 5320 2 2 5 5233 3 3 5 5232 4 4 5 5268 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214200 entries, 0 to 214199 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 ID 214200 non-null int64 1 shop_id 214200 non-null int64 2 item_id 214200 non-null int64 dtypes: int64(3) memory usage: 4.9 MB None -Describe- ID shop_id item_id count 214200.000000 214200.000000 214200.000000 mean 107099.500000 31.642857 11019.398627 std 61834.358168 17.561933 6252.644590 min 0.000000 2.000000 30.000000 25% 53549.750000 16.000000 5381.500000 50% 107099.500000 34.500000 11203.000000 75% 160649.250000 47.000000 16071.500000 max 214199.000000 59.000000 22167.000000 -Columns-- Index([&#39;ID&#39;, &#39;shop_id&#39;, &#39;item_id&#39;], dtype=&#39;object&#39;) -Data Types-- ID int64 shop_id int64 item_id int64 dtype: object -Missing Values- ID 0 shop_id 0 item_id 0 dtype: int64 -NULL values- ID 0 shop_id 0 item_id 0 dtype: int64 --Shape Of Data- (214200, 3) =============================Item Categories============================= -TOP 5 RECORDS-- item_category_name item_category_id 0 PC - Гарнитуры/Наушники 0 1 Аксессуары - PS2 1 2 Аксессуары - PS3 2 3 Аксессуары - PS4 3 4 Аксессуары - PSP 4 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 84 entries, 0 to 83 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 item_category_name 84 non-null object 1 item_category_id 84 non-null int64 dtypes: int64(1), object(1) memory usage: 1.4+ KB None -Describe- item_category_id count 84.000000 mean 41.500000 std 24.392622 min 0.000000 25% 20.750000 50% 41.500000 75% 62.250000 max 83.000000 -Columns-- Index([&#39;item_category_name&#39;, &#39;item_category_id&#39;], dtype=&#39;object&#39;) -Data Types-- item_category_name object item_category_id int64 dtype: object -Missing Values- item_category_name 0 item_category_id 0 dtype: int64 -NULL values- item_category_name 0 item_category_id 0 dtype: int64 --Shape Of Data- (84, 2) =============================Items============================= -TOP 5 RECORDS-- item_name item_id item_category_id 0 ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.) D 0 40 1 !ABBYY FineReader 12 Professional Edition Full... 1 76 2 ***В ЛУЧАХ СЛАВЫ (UNV) D 2 40 3 ***ГОЛУБАЯ ВОЛНА (Univ) D 3 40 4 ***КОРОБКА (СТЕКЛО) D 4 40 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22170 entries, 0 to 22169 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 item_name 22170 non-null object 1 item_id 22170 non-null int64 2 item_category_id 22170 non-null int64 dtypes: int64(2), object(1) memory usage: 519.7+ KB None -Describe- item_id item_category_id count 22170.00000 22170.000000 mean 11084.50000 46.290753 std 6400.07207 15.941486 min 0.00000 0.000000 25% 5542.25000 37.000000 50% 11084.50000 40.000000 75% 16626.75000 58.000000 max 22169.00000 83.000000 -Columns-- Index([&#39;item_name&#39;, &#39;item_id&#39;, &#39;item_category_id&#39;], dtype=&#39;object&#39;) -Data Types-- item_name object item_id int64 item_category_id int64 dtype: object -Missing Values- item_name 0 item_id 0 item_category_id 0 dtype: int64 -NULL values- item_name 0 item_id 0 item_category_id 0 dtype: int64 --Shape Of Data- (22170, 3) =============================Shops============================= -TOP 5 RECORDS-- shop_name shop_id 0 !Якутск Орджоникидзе, 56 фран 0 1 !Якутск ТЦ &#34;Центральный&#34; фран 1 2 Адыгея ТЦ &#34;Мега&#34; 2 3 Балашиха ТРК &#34;Октябрь-Киномир&#34; 3 4 Волжский ТЦ &#34;Волга Молл&#34; 4 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 60 entries, 0 to 59 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 shop_name 60 non-null object 1 shop_id 60 non-null int64 dtypes: int64(1), object(1) memory usage: 1.1+ KB None -Describe- shop_id count 60.000000 mean 29.500000 std 17.464249 min 0.000000 25% 14.750000 50% 29.500000 75% 44.250000 max 59.000000 -Columns-- Index([&#39;shop_name&#39;, &#39;shop_id&#39;], dtype=&#39;object&#39;) -Data Types-- shop_name object shop_id int64 dtype: object -Missing Values- shop_name 0 shop_id 0 dtype: int64 -NULL values- shop_name 0 shop_id 0 dtype: int64 --Shape Of Data- (60, 2) =============================Sample Submission============================= -TOP 5 RECORDS-- ID item_cnt_month 0 0 0.5 1 1 0.5 2 2 0.5 3 3 0.5 4 4 0.5 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214200 entries, 0 to 214199 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 ID 214200 non-null int64 1 item_cnt_month 214200 non-null float64 dtypes: float64(1), int64(1) memory usage: 3.3 MB None -Describe- ID item_cnt_month count 214200.000000 214200.0 mean 107099.500000 0.5 std 61834.358168 0.0 min 0.000000 0.5 25% 53549.750000 0.5 50% 107099.500000 0.5 75% 160649.250000 0.5 max 214199.000000 0.5 -Columns-- Index([&#39;ID&#39;, &#39;item_cnt_month&#39;], dtype=&#39;object&#39;) -Data Types-- ID int64 item_cnt_month float64 dtype: object -Missing Values- ID 0 item_cnt_month 0 dtype: int64 -NULL values- ID 0 item_cnt_month 0 dtype: int64 --Shape Of Data- (214200, 2) . 앞 코드와 다른 사람 코드입니다. . 여기서 train 데이터 프레임을 이 사람은 sales_data 이름으로 했네요. . 사실 데이터 탐색하는 함수를 잘 만들어 놓은것 같아서 향후 다른 데이터 분석시 복사를 위해 가져왔습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . sales_data[&#39;date&#39;] = pd.to_datetime(sales_data[&#39;date&#39;],format = &#39;%d.%m.%Y&#39;) dataset = sales_data.pivot_table(index = [&#39;shop_id&#39;,&#39;item_id&#39;], values = [&#39;item_cnt_day&#39;],columns = [&#39;date_block_num&#39;],fill_value = 0,aggfunc=&#39;sum&#39;) dataset.reset_index(inplace = True) dataset.head() . shop_id item_id item_cnt_day . date_block_num 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 . 0 0 | 30 | 0 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 31 | 0 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 32 | 6 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 33 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 35 | 1 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 판다스 내 피벗 테이블을 사용하는 모습입니다. group_by 함수를 확장한 것으로 생각할 수 있습니다. . 피벗 테이블은 우선 index로 데이터를 구분 짓습니다. 여기서 shop_id, item_id가 모두 같은 값을 가진 행끼리 그룹을 짓습니다. . 다음으로 columns로 한번 더 데이터를 구분 짓습니다. 같은 상점, 같은 제품을 달별로 나누었습니다. . values는 실제 적용되는 값을 의미합니다. 여기서는 item_cnt_day 변수를 사용했습니다. . 상점, 제품, 달이 같은 데이터 별로 구분했을때 여러개의 item_cnt_day 값을 더해주는 함수(aggfunc=&#39;sum&#39;)를 사용합니다. . 빈 값도 충분히 존재할 가능성이 있는데, 그 경우 거래 기록이 존재하지 않았다는 의미이므로 0값을 채웁니다.(fill_value = 0) . dataset = pd.merge(test_data,dataset,on = [&#39;item_id&#39;,&#39;shop_id&#39;],how = &#39;left&#39;) dataset.fillna(0,inplace = True) dataset.head() . /usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right) warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) . ID shop_id item_id (item_cnt_day, 0) (item_cnt_day, 1) (item_cnt_day, 2) (item_cnt_day, 3) (item_cnt_day, 4) (item_cnt_day, 5) (item_cnt_day, 6) (item_cnt_day, 7) (item_cnt_day, 8) (item_cnt_day, 9) (item_cnt_day, 10) (item_cnt_day, 11) (item_cnt_day, 12) (item_cnt_day, 13) (item_cnt_day, 14) (item_cnt_day, 15) (item_cnt_day, 16) (item_cnt_day, 17) (item_cnt_day, 18) (item_cnt_day, 19) (item_cnt_day, 20) (item_cnt_day, 21) (item_cnt_day, 22) (item_cnt_day, 23) (item_cnt_day, 24) (item_cnt_day, 25) (item_cnt_day, 26) (item_cnt_day, 27) (item_cnt_day, 28) (item_cnt_day, 29) (item_cnt_day, 30) (item_cnt_day, 31) (item_cnt_day, 32) (item_cnt_day, 33) . 0 0 | 5 | 5037 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 2.0 | 2.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 1.0 | 3.0 | 1.0 | 0.0 | . 1 1 | 5 | 5320 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 2 | 5 | 5233 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 2.0 | 0.0 | 1.0 | 3.0 | 1.0 | . 3 3 | 5 | 5232 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 4 4 | 5 | 5268 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 피벗 테이블을 사용해 같은 상점, 제품을 달 별로 거래기록이 몇건 있었는가를 나타내는 데이터 프레임입니다. . 이를 활용해 test 데이터 프레임과 병합한다면 테스트 데이터에 있는 상점, 제품의 이전 달별 거래기록을 전부 알 수 있습니다. . 이때 만약 병합이 안된 데이터가 있다면(이전 거래기록이 없는 데이터이겠죠?) 0으로 값을 넣어줍니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . dataset.drop([&#39;shop_id&#39;,&#39;item_id&#39;,&#39;ID&#39;],inplace = True, axis = 1) dataset.head() X_train = np.expand_dims(dataset.values[:,:-1],axis = 2) y_train = dataset.values[:,-1:] X_test = np.expand_dims(dataset.values[:,1:],axis = 2) print(X_train.shape,y_train.shape,X_test.shape) . (214200, 33, 1) (214200, 1) (214200, 33, 1) . 데이터를 모델링 하기 위해 상점, 제품 데이터를 지우고, train과 test 데이터 셋을 만들었습니다. . X_train : 0번째 달부터 32번째 달까지 거래 기록 데이터 . y_train : 33번째 달 거래 기록 데이터 . X_test : 1번째 달부터 33번째 달까지 거래 기록 데이터(train과 test간 데이터 형식을 맞추기 위해) . 우리가 예측해야할 y_test는 34번째 달 거래 기록 데이터, 즉 2015년 10월 거래 기록 데이터 입니다. . from keras.models import Sequential from keras.layers import LSTM,Dense,Dropout my_model = Sequential() my_model.add(LSTM(units = 64,input_shape = (33,1))) my_model.add(Dropout(0.4)) my_model.add(Dense(1)) my_model.compile(loss = &#39;mse&#39;,optimizer = &#39;adam&#39;, metrics = [&#39;mean_squared_error&#39;]) my_model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_1 (LSTM) (None, 64) 16896 _________________________________________________________________ dropout (Dropout) (None, 64) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ . my_model.fit(X_train,y_train,batch_size = 4096,epochs = 10) . Epoch 1/10 53/53 [==============================] - 27s 471ms/step - loss: 30.6011 - mean_squared_error: 30.6011 Epoch 2/10 53/53 [==============================] - 25s 466ms/step - loss: 30.2430 - mean_squared_error: 30.2430 Epoch 3/10 53/53 [==============================] - 24s 462ms/step - loss: 30.0014 - mean_squared_error: 30.0014 Epoch 4/10 53/53 [==============================] - 25s 481ms/step - loss: 29.8476 - mean_squared_error: 29.8476 Epoch 5/10 53/53 [==============================] - 26s 482ms/step - loss: 29.7404 - mean_squared_error: 29.7404 Epoch 6/10 53/53 [==============================] - 26s 487ms/step - loss: 29.7396 - mean_squared_error: 29.7396 Epoch 7/10 53/53 [==============================] - 25s 480ms/step - loss: 29.7369 - mean_squared_error: 29.7369 Epoch 8/10 53/53 [==============================] - 25s 473ms/step - loss: 29.6503 - mean_squared_error: 29.6503 Epoch 9/10 53/53 [==============================] - 25s 472ms/step - loss: 29.6353 - mean_squared_error: 29.6353 Epoch 10/10 53/53 [==============================] - 25s 468ms/step - loss: 29.5096 - mean_squared_error: 29.5096 . &lt;keras.callbacks.History at 0x7f2b3e51ff90&gt; . 모델을 LSTM(시계열 분석) 방법을 사용해서 분석합니다. 사실 LSTM 모델을 처음 사용했는데요. . 이번주에 다소 시간이 부족해 LSTM 모델의 사용방법이나 원리 등은 아직 파악하지 못했네요. (다른 사람 발표를 경청하겠습니다.) . submission_pfs = my_model.predict(X_test) submission_pfs = submission_pfs.clip(0,20) submission = pd.DataFrame({&#39;ID&#39;:test_data[&#39;ID&#39;],&#39;item_cnt_month&#39;:submission_pfs.ravel()}) submission.to_csv(&#39;./submission.csv&#39;,index = False) submission . ID item_cnt_month . 0 0 | 0.396485 | . 1 1 | 0.103207 | . 2 2 | 0.743674 | . 3 3 | 0.135947 | . 4 4 | 0.103207 | . ... ... | ... | . 214195 214195 | 0.331131 | . 214196 214196 | 0.103207 | . 214197 214197 | 0.097571 | . 214198 214198 | 0.103207 | . 214199 214199 | 0.069235 | . 214200 rows × 2 columns . 데이터를 모델에 적용시켜 예측값을 찾은 뒤, 제출 형식에 맞게 데이터 프레임 형식을 조정했습니다. . 이때 clip 함수는 이상치 조정 함수입니다. . clip(최솟값, 최댓값) 구조로 범위를 벗어나면 범위 내로 값을 조정시켜줍니다. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; data = {&#39;col_0&#39;: [9, -3, 0, -1, 5], &#39;col_1&#39;: [-2, -7, 6, 8, -5]} df = pd.DataFrame(data) df . col_0 col_1 . 0 9 | -2 | . 1 -3 | -7 | . 2 0 | 6 | . 3 -1 | 8 | . 4 5 | -5 | . df.clip(-4, 6) . col_0 col_1 . 0 6 | -2 | . 1 -3 | -4 | . 2 0 | 6 | . 3 -1 | 6 | . 4 5 | -4 | . 예시를 보면 보다 직관적으로 이해가 가능할 것 같습니다. . 이 함수는 범용성이 넓으니 다른 데이터 분석에 자주 쓰일 수 있어 따로 정리했네요. . !kaggle competitions submit -c competitive-data-science-predict-future-sales -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 3.55M/3.55M [00:04&lt;00:00, 769kB/s] Successfully submitted to Predict Future Sales . 캐글에 파일을 자동 제출하는 코드입니다. . 스코어는 약 1.02로 만 2천명 중 6천등 정도를 기록합니다. . &#45712;&#45184;&#51216; . 우선 공부하기 좋은 데이터를 찾아 줘서 고맙습니다. . 시계열 자료가 현실에서 상당히 많아 꼭 공부해보고 싶은 분야였는데, 이번 기회에 분석하게 되서 너무 좋습니다. . 개인적으로 공부하고 싶은 분야가 이미지 분류같은 것 보다는 자연어 처리, 시계열 분석 등 현실 세계를 설명할 수 있는 것 입니다. . 이번엔 시간이 다소 부족해서 자주쓰는 시계열 모델인 LSTM 모델의 탐구가 부족했습니다. . 다른 사람 발표 경청하고, 시간이 있을때 LSTM 모델을 열심히 공부해보고 싶네요. . 감사합니다. . &lt;/div&gt;",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/time%20series/lstm/datetime/clip/2021/10/28/kagglessu4.html",
            "relUrl": "/ssuda/jupyter/kaggle/time%20series/lstm/datetime/clip/2021/10/28/kagglessu4.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "[시뮬레이션] 중간고사 범위 복습",
            "content": ". &#53076;&#47017;&#50640;&#49436; R &#49324;&#50857;&#48277; by &#54805;&#46973; . https://colab.research.google.com/notebook#create=true&amp;language=r . 뒷부분에 language=r 만 붙여주면 정상적으로 코랩 R버전이 실행됩니다. . for (i in 1:10){ print(i) } . [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 . 6.6&#51208; &#48372;&#54744;&#44552; &#52397;&#44396; &#47928;&#51228; . 기존 보험 가입자 n0(사용값 1명), 기존 자본금 a0 (사용값 25000), 기간 365. 보함 가입자는 기간 1당 보험금 C(사용값 11000)을 각각 지불합니다. . 이때 자본금이 음이 되지 않을 확률을 모의실험으로 구하는 문제 입니다. . 일어날 사건은 보험금청구, 신규고객 가입, 기존계약해지 인데요. . 보험금 청구는 도착률 알파(사용값 10)인 포아송 과정, 이때 청구 금액은 지수분포 (사용 람다값 1/1000)을 따릅니다. . 포아송 과정이란 사건 발생 시간 분포가 평균 1/알파인 지수분포 입니다. . 이 모의실험을 300번 실시해봅니다. . (신규고객 가입, 기존계약 해지는 무시합니다.) . n.sim &lt;- 300 # 모의실험 실행 횟수 n0 &lt;- 1; a0 &lt;- 25000; T &lt;- 365; c &lt;- 11000 # 가입자, 자본금, 기간, 단위기간당 보험금 초기값 부여 alpha &lt;- 10; nu &lt;- 0; mu &lt;- 0 # 알파값, 신규계약과 기존계약 해지는 무시합니다. generate.Y &lt;- function() rexp(1, rate = 1/1000) # 청구금액 만드는 함수를 생성합니다. I &lt;- numeric(length = n.sim) # 자본금이 음이되는지 여부를 실험마다 기록하는 변수 입니다. for (i in 1:n.sim){ # 실험 n.sim(300)번 실행 t &lt;- 0; a &lt;- a0; n &lt;- n0 # 시점, 자본금, 고객 수 초기값 부여 total.rate &lt;- nu + n * mu + n * alpha # 사건 발생 람다값 부여. 여기서 유효한 값은 n * alpha(보험금 청구) 입니다. tE &lt;- rexp(1, rate = total.rate) # 첫 사건 발생 시간 repeat{ if (tE &gt; T) { # 주어진 기간을 초과했을 경우 I[i] &lt;- 1 # 중간에 중단되지 않고 주어진 기간(365)를 무사히 초과했기 때문에 이번 실험은 성공임을 기록해줍니다. break # 반복분 끝내기. 다음 모의 실험이 실행되겠죠. } if (tE &lt;= T){ # 주어진 기간 내. a &lt;- a + n * c * (tE - t) # 보험금 수금. 여기서 tE는 새 사건 발생 시간, t는 과거 사건 발생시간. t &lt;- tE # 시점을 새 사건 발생시간에 맞춰줍니다. J &lt;- sample(1:3, 1, prob = c(nu, n*mu, n*alpha)) # 이번 사건은 어떤사건인지 정해줍니다. 하지만 여기선 무조건 J는 3이됩니다. if (J == 1) n &lt;- n + 1 # 신규고객 가입 if (J == 2) n &lt;- n - 1 # 기존고객 해지 if (J == 3){ Y &lt;- generate.Y(); # 보험금 청구 금액 찾기 if (Y &gt; a){ # 현재 자본금보다 보험 청구 금액이 많으면 = 자본금이 음수가 됨. I[i] &lt;- 0 # 이번 실험은 실패임을 기록 break # 반복문 끝내기 } else a &lt;- a - Y # 자본금이 음수가 되는 일이 벌어지지 않으면 자본금에서 돈을 쓰면 되겠죠. } tE &lt;- t + rexp(1,rate=total.rate) # 다음 사건이 일어날 시점을 탐색합니다. } } } mean(I) cat(&#39;자본금이 남아있을 확률 95%신뢰구간 [&#39;,mean(I)- 1.96*sd(I) /sqrt(n.sim),&#39;,&#39;,mean(I) + 1.96*sd(I)/sqrt(n.sim), &#39;] n&#39;) . 0.916666666666667 자본금이 남아있을 확률 95%신뢰구간 [ 0.8853385 , 0.9479949 ] . 자본금이 음수가 되지 않을 확률이 90%정도 됩니다. 신뢰구간도 구할 수 있군요. . 강의는 여기까지 가르쳤는데요. 시험문제는 이를 응용하는 문제가 나올 수 있습니다. . 제일 쉬운 예시로 고객의 가입과 탈퇴가 포함된 함수를 만드는 문제가 나올수도 있겠습니다. . 신규고객 가입을 람다가 1인 포아송 과정으로, 기존 고객 탈퇴를 람다가 0.1인 포아송 과정으로 하겠습니다. . 그리고 기존 고객의 초기 수를 10으로 하겠습니다. . n.sim &lt;- 100 n0 &lt;- 10; a0 &lt;- 25000; T &lt;- 365; c &lt;- 11000 alpha &lt;- 10; nu &lt;- 1; mu &lt;- 0.1 # 이부분만 바꿔주면 됨 generate.Y &lt;- function() rexp(1, rate = 1/1000) I &lt;- numeric(length = n.sim) for (i in 1:n.sim){ t &lt;- 0; a &lt;- a0; n &lt;- n0 total.rate &lt;- nu + n * mu + n * alpha tE &lt;- rexp(1, rate = total.rate) repeat{ if (tE &gt; T) { I[i] &lt;- 1 break } if (tE &lt;= T){ a &lt;- a + n * c * (tE - t) t &lt;- tE J &lt;- sample(1:3, 1, prob = c(nu, n*mu, n*alpha)) if (J == 1) n &lt;- n + 1 if (J == 2){ n &lt;- n - 1 if (n == 0){ #보험금이 0이된 경우. I[i] &lt;- 0 break } } if (J == 3){ Y &lt;- generate.Y(); if (Y &gt; a){ I[i] &lt;- 0 break } else a &lt;- a - Y } tE &lt;- t + rexp(1,rate=total.rate) } } } mean(I) cat(&#39;자본금이 남아있을 확률 95%신뢰구간 [&#39;,mean(I)- 1.96*sd(I) /sqrt(n.sim),&#39;,&#39;,mean(I) + 1.96*sd(I)/sqrt(n.sim), &#39;] n&#39;) . 0.36 자본금이 남아있을 확률 95%신뢰구간 [ 0.265446 , 0.454554 ] . 이미 세 사건이 일어날걸 가정하고 함수를 다 만들어나서 단순히 nu와 mu값만 넣어주면 됩니다. . 다만 보험 가입자가 0명이 될 경우도 있는데 그 경우 또한 실패로 하겠습니다. . 확실히 가입자가 늘어나고 탈퇴를 할 수 있는 등 불확실성이 커지니 자본금이 음이 될 확률이 줄어들었습니다. . 6.8&#51208; &#51452;&#49885; &#50741;&#49496; &#54665;&#49324; &#51204;&#47029; . t시점에 주식 가격 S(t) = S(0) * exp(x1 + x2 .. + xt) 이라고 가정합니다. 이때 xi는 iid인 정규분포 변수입니다. . 알파 &lt; 평균 + 분산 / 2 (정규분포 평균, 분산) 일때 좋은 전략이 다음과 같이 알려져있습니다. . P(m) = S(N-m) 이라고 할때(만기를 m 앞둔 시점에서의 주가) 다음 조건이 만족하면 옵션을 행사합니다. . P(m) &gt; K(옵션권한가격 = 초기가격) 쉽게 얘기해 주식 가격이 초기가격보다 높아야합니다. 당연하죠. | P(m) &gt; K + f(i) 을 i = 1,2, .. , m 구간에서 모두 만족해야합니다. f(i)는 식이 복잡해 생략합니다. | 또 다른 전략은 끝나는 시점까지 기다렸다가 최종 시점 주식 가격이 K보다 클때만 사는 전략입니다. . 두 전략중 어떤 전략이 좋을지 모의실험 1000회를 통해 알아봅시다. . n.sim &lt;- 1000 # 실험횟수 N &lt;- 20; K &lt;- 100; S.zero &lt;- 100; mu &lt;- -0.05 # N : 기간, K, S.zero : 초기 금액(사실 어느값을 써도 비슷함) sg &lt;- 0.3; alp &lt;- mu + 0.5*sg^2 # mu = -0.05, 시그마 = 0.3 E &lt;- numeric(length = n.sim) E2 &lt;- numeric(length = n.sim) for (i in 1:n.sim){ S &lt;- S.zero * exp(cumsum(rnorm(N,mu,sg))) # 주식 가격을 구해놓음. E2[i] &lt;- max(S[N] - K, 0) # 최종시점 주식 가격이 K보다 크면 그만큼 이득, 아니면 이득 0. P &lt;- numeric(length=N+1) # P[0]은 R에서 쓸수 없음. 그래서 길이 자체를 N+1로 해줌. P[N+1] &lt;- S.zero #초기값 부여 m &lt;- N - 1 # m 초기값 부여. 미리 값 1을 뺀 모양새.(위에서 P[N+1] 초기값을 부여했기 때문에) flag &lt;- FALSE repeat{ m.plus &lt;- m + 1 # P에서는 m값을 1을 올려서 해줌. P[m.plus] &lt;- S[N-m] # 값 넣어줌. if(P[m.plus] &gt; K) flag &lt;- TRUE # 1번조건 만족 표현 if(flag &amp; m &gt; 0){ # 조건1만족 + m이 0아닐때(m이 0일때는 1번조건만 따짐.) b &lt;- ((1:m)*mu - log(K/P[m.plus])) / (sg*sqrt(1:m)) op &lt;- P[m.plus] * exp((1:m)*alp)* pnorm(sg*sqrt(1:m) + b) - K * pnorm(b) # 복잡한 식 f(i), 벡터 형태로 되어있음. flag &lt;- all(P[m.plus] &gt; K + op) # all은 모든 조건이 true일때만 true를 보내줌. } if(flag) break else m &lt;- m - 1 # 조건을 모두 만족하면 즉시 옵션 행사. if(m &lt; 0) break # 시점이 모두 끝났으면 종료. } if (flag) E[i] &lt;- P[m.plus] - K else E[i] &lt;- 0 # 조건 만족시 이득본 만큼 기록. } cat(&#39;전략1 95%신뢰구간 [&#39;,mean(E) - 1.96*sd(E)/sqrt(n.sim), &#39;,&#39;,mean(E) + 1.96*sd(E)/sqrt(n.sim), &#39;] n&#39;) cat(&#39;전략2 95%신뢰구간 [&#39;,mean(E2) - 1.96*sd(E2)/sqrt(n.sim), &#39;,&#39;,mean(E2) + 1.96*sd(E2)/sqrt(n.sim), &#39;] n&#39;) . 전략1 95%신뢰구간 [ 35.36105 , 45.49005 ] 전략2 95%신뢰구간 [ 28.57401 , 44.68403 ] . 여러번 실행을 해보면 전략1과 전략2의 이득 평균이 비슷합니다. . 다만 전략2의 신뢰구간이 큰 것은 그만큼 전략2가 불안정한 전략임을 알 수 있습니다. . &#48512;&#53944;&#49828;&#53944;&#47129; &#51060;&#47200; . 뽑힌 표본들을 새로운 분포로 가정하여 반복추출(복원추출)을 통해 모수를 추정하는 방법 입니다. . 표본들이 실제 분포와 비슷할 수록 모수 추정이 더 정확해집니다. . 이 방법의 장점은 중심극한정리 등 분포가정을 하지 않아도 된다는 점입니다. . 다음은 부트스트렙 예시로, 표본들이 있을때 Var(s^2)의 추정량을 구하는 문제입니다. . x &lt;- c(5,4,9,6,21,17,11,20,7,10,21,15,13,16,8) n &lt;- 15 B &lt;- 400 # 400번 실시 f.var &lt;- function(x) var(sample(x, n, rep = T)) b.var &lt;- replicate(B, f.var(x)) var(b.var) # estimate of var(s^2) hist(b.var) . 57.6508361857024",
            "url": "https://ksy1526.github.io/myblog/school/jupyter/simulation/r/bootstrap/2021/10/21/%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98_%EA%B3%BC%EB%AA%A9_%EB%B3%B5%EC%8A%B5.html",
            "relUrl": "/school/jupyter/simulation/r/bootstrap/2021/10/21/%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98_%EA%B3%BC%EB%AA%A9_%EB%B3%B5%EC%8A%B5.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "[SSUDA] 택시 데이터 분석",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle (3).json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;23e68db36970b65937516103c630ba75&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c nyc-taxi-trip-duration . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) train.zip: Skipping, found more recently modified local copy (use --force to force download) test.zip: Skipping, found more recently modified local copy (use --force to force download) sample_submission.zip: Skipping, found more recently modified local copy (use --force to force download) . !unzip train.zip !unzip test.zip !unzip sample_submission.zip . Archive: train.zip replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: train.csv Archive: test.zip replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: test.csv Archive: sample_submission.zip replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: sample_submission.csv . 압축되어 있는 데이터라서 압축 풀어줍니다. . %matplotlib inline import pandas as pd from datetime import datetime import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge,BayesianRidge from sklearn.cluster import MiniBatchKMeans from sklearn.metrics import mean_squared_error from math import radians, cos, sin, asin, sqrt import seaborn as sns import matplotlib import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [16, 10] . train = pd.read_csv(&#39;./train.csv&#39;) test = pd.read_csv(&#39;./test.csv&#39;) . &#45936;&#51060;&#53552; &#53456;&#49353; . train.head() . id vendor_id pickup_datetime dropoff_datetime passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude store_and_fwd_flag trip_duration . 0 id2875421 | 2 | 2016-03-14 17:24:55 | 2016-03-14 17:32:30 | 1 | -73.982155 | 40.767937 | -73.964630 | 40.765602 | N | 455 | . 1 id2377394 | 1 | 2016-06-12 00:43:35 | 2016-06-12 00:54:38 | 1 | -73.980415 | 40.738564 | -73.999481 | 40.731152 | N | 663 | . 2 id3858529 | 2 | 2016-01-19 11:35:24 | 2016-01-19 12:10:48 | 1 | -73.979027 | 40.763939 | -74.005333 | 40.710087 | N | 2124 | . 3 id3504673 | 2 | 2016-04-06 19:32:31 | 2016-04-06 19:39:40 | 1 | -74.010040 | 40.719971 | -74.012268 | 40.706718 | N | 429 | . 4 id2181028 | 2 | 2016-03-26 13:30:55 | 2016-03-26 13:38:10 | 1 | -73.973053 | 40.793209 | -73.972923 | 40.782520 | N | 435 | . train.describe() . vendor_id passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude trip_duration . count 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | . mean 1.534950e+00 | 1.664530e+00 | -7.397349e+01 | 4.075092e+01 | -7.397342e+01 | 4.075180e+01 | 9.594923e+02 | . std 4.987772e-01 | 1.314242e+00 | 7.090186e-02 | 3.288119e-02 | 7.064327e-02 | 3.589056e-02 | 5.237432e+03 | . min 1.000000e+00 | 0.000000e+00 | -1.219333e+02 | 3.435970e+01 | -1.219333e+02 | 3.218114e+01 | 1.000000e+00 | . 25% 1.000000e+00 | 1.000000e+00 | -7.399187e+01 | 4.073735e+01 | -7.399133e+01 | 4.073588e+01 | 3.970000e+02 | . 50% 2.000000e+00 | 1.000000e+00 | -7.398174e+01 | 4.075410e+01 | -7.397975e+01 | 4.075452e+01 | 6.620000e+02 | . 75% 2.000000e+00 | 2.000000e+00 | -7.396733e+01 | 4.076836e+01 | -7.396301e+01 | 4.076981e+01 | 1.075000e+03 | . max 2.000000e+00 | 9.000000e+00 | -6.133553e+01 | 5.188108e+01 | -6.133553e+01 | 4.392103e+01 | 3.526282e+06 | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1458644 entries, 0 to 1458643 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 id 1458644 non-null object 1 vendor_id 1458644 non-null int64 2 pickup_datetime 1458644 non-null object 3 dropoff_datetime 1458644 non-null object 4 passenger_count 1458644 non-null int64 5 pickup_longitude 1458644 non-null float64 6 pickup_latitude 1458644 non-null float64 7 dropoff_longitude 1458644 non-null float64 8 dropoff_latitude 1458644 non-null float64 9 store_and_fwd_flag 1458644 non-null object 10 trip_duration 1458644 non-null int64 dtypes: float64(4), int64(3), object(4) memory usage: 122.4+ MB . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 625134 entries, 0 to 625133 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 id 625134 non-null object 1 vendor_id 625134 non-null int64 2 pickup_datetime 625134 non-null object 3 passenger_count 625134 non-null int64 4 pickup_longitude 625134 non-null float64 5 pickup_latitude 625134 non-null float64 6 dropoff_longitude 625134 non-null float64 7 dropoff_latitude 625134 non-null float64 8 store_and_fwd_flag 625134 non-null object dtypes: float64(4), int64(2), object(3) memory usage: 42.9+ MB . dropoff_datetime 변수가 test에는 없습니다. 도착 시간을 맞추는 예제이기 때문에 그렇습니다. . &#48152;&#51025;&#48320;&#49688; &#44288;&#52272; . plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train.trip_duration.values)) plt.xlabel(&#39;index&#39;, fontsize=12) plt.ylabel(&#39;trip duration&#39;, fontsize=12) plt.show() . 반응변수의 이상치가 많아보입니다. 제거하겠습니다. . m = np.mean(train[&#39;trip_duration&#39;]) s = np.std(train[&#39;trip_duration&#39;]) train = train[train[&#39;trip_duration&#39;] &lt;= m + 2*s] train = train[train[&#39;trip_duration&#39;] &gt;= m - 2*s] plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train.trip_duration.values)) plt.xlabel(&#39;index&#39;, fontsize=12) plt.ylabel(&#39;trip duration&#39;, fontsize=12) plt.show() . 이상치는 대부분 제거된 것 같습니다. 다만 일부 데이터가 큰 값을 갖는거 같아요. . plt.hist(train[&#39;trip_duration&#39;].values, bins=100) plt.xlabel(&#39;trip_duration&#39;) plt.ylabel(&#39;number of train records&#39;) plt.show() . 히스토그램으로 확인하니 그렇습니다. 우측 꼬리가 긴 모양으로 로그변환이 필요해보입니다. . train[&#39;log_trip_duration&#39;] = np.log(train[&#39;trip_duration&#39;].values + 1) plt.hist(train[&#39;log_trip_duration&#39;].values, bins=100) plt.xlabel(&#39;log(trip_duration)&#39;) plt.ylabel(&#39;number of train records&#39;) plt.show() sns.distplot(train[&quot;log_trip_duration&quot;], bins =100) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f99495dd450&gt; . 확실히 그래프 모양이 괜찮아졌습니다. distplot 함수를 통해 그리기도 하였네요 . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . train = train[train[&#39;pickup_longitude&#39;] &lt;= -73.75] train = train[train[&#39;pickup_longitude&#39;] &gt;= -74.03] train = train[train[&#39;pickup_latitude&#39;] &lt;= 40.85] train = train[train[&#39;pickup_latitude&#39;] &gt;= 40.63] train = train[train[&#39;dropoff_longitude&#39;] &lt;= -73.75] train = train[train[&#39;dropoff_longitude&#39;] &gt;= -74.03] train = train[train[&#39;dropoff_latitude&#39;] &lt;= 40.85] train = train[train[&#39;dropoff_latitude&#39;] &gt;= 40.63] . 뉴욕의 위도는 (-74.03, -73.75) 경도는 (40.63, 40.85) 사이 입니다. . 이 값을 벗어나는 위도/경도 데이터를 제거하겠습니다. . train[&#39;pickup_datetime&#39;] = pd.to_datetime(train.pickup_datetime) test[&#39;pickup_datetime&#39;] = pd.to_datetime(test.pickup_datetime) train.loc[:, &#39;pickup_date&#39;] = train[&#39;pickup_datetime&#39;].dt.date test.loc[:, &#39;pickup_date&#39;] = test[&#39;pickup_datetime&#39;].dt.date train[&#39;dropoff_datetime&#39;] = pd.to_datetime(train.dropoff_datetime) #Not in Test . to_datetime 함수로 datetime 변수로 바궈주었습니다. . plt.plot(train.groupby(&#39;pickup_date&#39;).count()[[&#39;id&#39;]], &#39;o-&#39;, label=&#39;train&#39;) plt.plot(test.groupby(&#39;pickup_date&#39;).count()[[&#39;id&#39;]], &#39;o-&#39;, label=&#39;test&#39;) plt.title(&#39;Trips over Time.&#39;) plt.legend(loc=0) plt.ylabel(&#39;Trips&#39;) plt.show() . 트레인과 테스트 데이터를 같이 그리니 유사한 측면을 발견하기가 쉬운것 같아요. . 1월 하순경 이동횟수가 급격하게 감소한것이 관찰됩니다. 또 5월 하순경 감소세가 또 관찰됩니다. . 계절적으로 추운것도 있겠지만 작성자는 다른 요인이 있지 않을까 생각하네요. . import warnings warnings.filterwarnings(&quot;ignore&quot;) plot_vendor = train.groupby(&#39;vendor_id&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=800) plt.ylim(ymax=840) sns.barplot(plot_vendor.index,plot_vendor.values) plt.title(&#39;Time per Vendor&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) . No handles with labels found to put in legend. . Text(0, 0.5, &#39;Time in Seconds&#39;) . 범위를 800~840으로 두어서 그렇지 두 vendor 간 큰 차이를 보이진 않습니다. . snwflag = train.groupby(&#39;store_and_fwd_flag&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=0) plt.ylim(ymax=1100) plt.title(&#39;Time per store_and_fwd_flag&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) sns.barplot(snwflag.index,snwflag.values) . No handles with labels found to put in legend. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f993c2c18d0&gt; . 공급업체에 보내기 전 기록이 잘 저장되었는지 나타내는 변수로 꽤 많이 차이가 납니다. . 작성자는 일부 직원이 이동시간을 정확히 기록하지 못해 발생하는 왜곡이라고 말합니다. . pc = train.groupby(&#39;passenger_count&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=0) plt.ylim(ymax=1100) plt.title(&#39;Time per store_and_fwd_flag&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) sns.barplot(pc.index,pc.values) . No handles with labels found to put in legend. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f993c2b0550&gt; . 승객 수는 뚜렷한 여행을 주지 못합니다. . 승객을 아무도 태우지 않았는데 4분정도 이동한 것은 직원의 실수로 보입니다. . train.groupby(&#39;passenger_count&#39;).size() . passenger_count 0 52 1 1018715 2 206864 3 58989 4 27957 5 76912 6 47639 dtype: int64 . &#50948;&#52824; &#45936;&#51060;&#53552; . city_long_border = (-74.03, -73.75) city_lat_border = (40.63, 40.85) fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True) ax[0].scatter(train[&#39;pickup_longitude&#39;].values[:100000], train[&#39;pickup_latitude&#39;].values[:100000], color=&#39;blue&#39;, s=1, label=&#39;train&#39;, alpha=0.1) ax[1].scatter(test[&#39;pickup_longitude&#39;].values[:100000], test[&#39;pickup_latitude&#39;].values[:100000], color=&#39;green&#39;, s=1, label=&#39;test&#39;, alpha=0.1) fig.suptitle(&#39;Train and test area complete overlap.&#39;) ax[0].legend(loc=0) ax[0].set_ylabel(&#39;latitude&#39;) ax[0].set_xlabel(&#39;longitude&#39;) ax[1].set_xlabel(&#39;longitude&#39;) ax[1].legend(loc=0) plt.ylim(city_lat_border) plt.xlim(city_long_border) plt.show() . 자세한 코드 관찰은 위치 데이터 분석을 할때 다시 확인하겠습니다. . train, test 간 위치 데이터가 매우 유사함을 알 수 있습니다. . def haversine_array(lat1, lng1, lat2, lng2): lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) AVG_EARTH_RADIUS = 6371 # in km lat = lat2 - lat1 lng = lng2 - lng1 d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) return h def dummy_manhattan_distance(lat1, lng1, lat2, lng2): a = haversine_array(lat1, lng1, lat1, lng2) b = haversine_array(lat1, lng1, lat2, lng1) return a + b def bearing_array(lat1, lng1, lat2, lng2): AVG_EARTH_RADIUS = 6371 # in km lng_delta_rad = np.radians(lng2 - lng1) lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) y = np.sin(lng_delta_rad) * np.cos(lat2) x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) return np.degrees(np.arctan2(y, x)) . train.loc[:, &#39;distance_haversine&#39;] = haversine_array(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;distance_haversine&#39;] = haversine_array(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) train.loc[:, &#39;distance_dummy_manhattan&#39;] = dummy_manhattan_distance(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;distance_dummy_manhattan&#39;] = dummy_manhattan_distance(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) train.loc[:, &#39;direction&#39;] = bearing_array(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;direction&#39;] = bearing_array(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) . 위도/경도를 활용하여 다양한 관측값을 나타내는 함수입니다. . 이해하기에 조금 벅차서 일단 다양한 변수를 추가해줄수 있구나 하고 넘어갔네요. . coords = np.vstack((train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]].values, train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]].values)) sample_ind = np.random.permutation(len(coords))[:500000] kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind]) train.loc[:, &#39;pickup_cluster&#39;] = kmeans.predict(train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]]) train.loc[:, &#39;dropoff_cluster&#39;] = kmeans.predict(train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]]) test.loc[:, &#39;pickup_cluster&#39;] = kmeans.predict(test[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]]) test.loc[:, &#39;dropoff_cluster&#39;] = kmeans.predict(test[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]]) . np.vstack는 데이터를 묶어주는 함수입니다. . 위도, 경도 데이터를 클러스트로 묶어주었습니다. . fig, ax = plt.subplots(ncols=1, nrows=1) ax.scatter(train.pickup_longitude.values[:500000], train.pickup_latitude.values[:500000], s=10, lw=0, c=train.pickup_cluster[:500000].values, cmap=&#39;autumn&#39;, alpha=0.2) ax.set_xlim(city_long_border) ax.set_ylim(city_lat_border) ax.set_xlabel(&#39;Longitude&#39;) ax.set_ylabel(&#39;Latitude&#39;) plt.show() . 군집화가 잘된 것을 시각적으로 확인하였습니다. . &#45216;&#51676; &#45936;&#51060;&#53552; . train[&#39;Month&#39;] = train[&#39;pickup_datetime&#39;].dt.month test[&#39;Month&#39;] = test[&#39;pickup_datetime&#39;].dt.month train[&#39;DayofMonth&#39;] = train[&#39;pickup_datetime&#39;].dt.day test[&#39;DayofMonth&#39;] = test[&#39;pickup_datetime&#39;].dt.day train[&#39;Hour&#39;] = train[&#39;pickup_datetime&#39;].dt.hour test[&#39;Hour&#39;] = test[&#39;pickup_datetime&#39;].dt.hour train[&#39;dayofweek&#39;] = train[&#39;pickup_datetime&#39;].dt.dayofweek test[&#39;dayofweek&#39;] = test[&#39;pickup_datetime&#39;].dt.dayofweek . 픽업된 시간으로 다양한 파생 날짜/시간 데이터를 생성한 모습입니다. datetime 변수이기에 가능한 모습입니다. . 여기서 dayofweek 변수는 요일변수로 0을 일요일로 생각하여 6을 토요일까지 쓰는 변수입니다. . train.loc[:, &#39;avg_speed_h&#39;] = 1000 * train[&#39;distance_haversine&#39;] / train[&#39;trip_duration&#39;] train.loc[:, &#39;avg_speed_m&#39;] = 1000 * train[&#39;distance_dummy_manhattan&#39;] / train[&#39;trip_duration&#39;] fig, ax = plt.subplots(ncols=3, sharey=True) ax[0].plot(train.groupby(&#39;Hour&#39;).mean()[&#39;avg_speed_h&#39;], &#39;bo-&#39;, lw=2, alpha=0.7) ax[1].plot(train.groupby(&#39;dayofweek&#39;).mean()[&#39;avg_speed_h&#39;], &#39;go-&#39;, lw=2, alpha=0.7) ax[2].plot(train.groupby(&#39;Month&#39;).mean()[&#39;avg_speed_h&#39;], &#39;ro-&#39;, lw=2, alpha=0.7) ax[0].set_xlabel(&#39;Hour of Day&#39;) ax[1].set_xlabel(&#39;Day of Week&#39;) ax[2].set_xlabel(&#39;Month of Year&#39;) ax[0].set_ylabel(&#39;Average Speed&#39;) fig.suptitle(&#39;Average Traffic Speed by Date-part&#39;) plt.show() . 정확히 이해하진 못했지만 distance_haversine가 위치 변수를 보고 만든 거리 변수입니다. . 그렇기 때문에 거리 / 시간 = 평균속도 변수를 만들었습니다. 이 평균속도를 시각/요일/달 별로 얼마나 다른지 시각화했습니다. . 물론 분모인 시간이 반응변수 이기 때문에 분석에 사용할수는 없습니다. . 보통 오전 5시~9시, 오후 5시(17시) ~ 7시(19시) 사이가 가장 도로가 혼잡해 속도가 떨어집니다. . 예상과 어느정도 일치하면서도 출/퇴근 이외 근무시간도 속도가 출/퇴근 시간과 비슷하게 떨어집니다. . 또 금토일의 평균속도가 상대적으로 빠르며 달별로는 겨울의 평균속도가 빠릅니다. . &#50896;&#54635;&#51064;&#53076;&#46377; . vendor_train = pd.get_dummies(train[&#39;vendor_id&#39;], prefix=&#39;vi&#39;, prefix_sep=&#39;_&#39;) vendor_test = pd.get_dummies(test[&#39;vendor_id&#39;], prefix=&#39;vi&#39;, prefix_sep=&#39;_&#39;) passenger_count_train = pd.get_dummies(train[&#39;passenger_count&#39;], prefix=&#39;pc&#39;, prefix_sep=&#39;_&#39;) passenger_count_test = pd.get_dummies(test[&#39;passenger_count&#39;], prefix=&#39;pc&#39;, prefix_sep=&#39;_&#39;) store_and_fwd_flag_train = pd.get_dummies(train[&#39;store_and_fwd_flag&#39;], prefix=&#39;sf&#39;, prefix_sep=&#39;_&#39;) store_and_fwd_flag_test = pd.get_dummies(test[&#39;store_and_fwd_flag&#39;], prefix=&#39;sf&#39;, prefix_sep=&#39;_&#39;) cluster_pickup_train = pd.get_dummies(train[&#39;pickup_cluster&#39;], prefix=&#39;p&#39;, prefix_sep=&#39;_&#39;) cluster_pickup_test = pd.get_dummies(test[&#39;pickup_cluster&#39;], prefix=&#39;p&#39;, prefix_sep=&#39;_&#39;) cluster_dropoff_train = pd.get_dummies(train[&#39;dropoff_cluster&#39;], prefix=&#39;d&#39;, prefix_sep=&#39;_&#39;) cluster_dropoff_test = pd.get_dummies(test[&#39;dropoff_cluster&#39;], prefix=&#39;d&#39;, prefix_sep=&#39;_&#39;) month_train = pd.get_dummies(train[&#39;Month&#39;], prefix=&#39;m&#39;, prefix_sep=&#39;_&#39;) month_test = pd.get_dummies(test[&#39;Month&#39;], prefix=&#39;m&#39;, prefix_sep=&#39;_&#39;) dom_train = pd.get_dummies(train[&#39;DayofMonth&#39;], prefix=&#39;dom&#39;, prefix_sep=&#39;_&#39;) dom_test = pd.get_dummies(test[&#39;DayofMonth&#39;], prefix=&#39;dom&#39;, prefix_sep=&#39;_&#39;) hour_train = pd.get_dummies(train[&#39;Hour&#39;], prefix=&#39;h&#39;, prefix_sep=&#39;_&#39;) hour_test = pd.get_dummies(test[&#39;Hour&#39;], prefix=&#39;h&#39;, prefix_sep=&#39;_&#39;) dow_train = pd.get_dummies(train[&#39;dayofweek&#39;], prefix=&#39;dow&#39;, prefix_sep=&#39;_&#39;) dow_test = pd.get_dummies(test[&#39;dayofweek&#39;], prefix=&#39;dow&#39;, prefix_sep=&#39;_&#39;) . 범주형 변수들을 전부 원핫인코딩을 했습니다. . prefix 와 prefix_sep 으로 원핫인코딩 변수 이름도 설정할수 있네요. . passenger_count_test = passenger_count_test.drop(&#39;pc_9&#39;, axis = 1) . 다만 9명이 탑승한 2건은 표본이 너무 적어 과적합될수도 있고 직관적으로도 말이 안되서 열을 삭제합니다. . train = train.drop([&#39;id&#39;,&#39;vendor_id&#39;,&#39;passenger_count&#39;,&#39;store_and_fwd_flag&#39;,&#39;Month&#39;,&#39;DayofMonth&#39;,&#39;Hour&#39;,&#39;dayofweek&#39;,&#39;pickup_datetime&#39;, &#39;pickup_date&#39;,&#39;pickup_longitude&#39;,&#39;pickup_latitude&#39;,&#39;dropoff_longitude&#39;,&#39;dropoff_latitude&#39;],axis = 1) Test_id = test[&#39;id&#39;] test = test.drop([&#39;id&#39;,&#39;vendor_id&#39;,&#39;passenger_count&#39;,&#39;store_and_fwd_flag&#39;,&#39;Month&#39;,&#39;DayofMonth&#39;,&#39;Hour&#39;,&#39;dayofweek&#39;, &#39;pickup_datetime&#39;, &#39;pickup_date&#39;, &#39;pickup_longitude&#39;,&#39;pickup_latitude&#39;,&#39;dropoff_longitude&#39;,&#39;dropoff_latitude&#39;], axis = 1) train = train.drop([&#39;dropoff_datetime&#39;,&#39;avg_speed_h&#39;,&#39;avg_speed_m&#39;,&#39;trip_duration&#39;], axis = 1) . 원핫인코딩 된 변수들, 시각화를 위해 만들었던 변수들, 변환한 변수들, id 등 필요없는 변수를 제거합니다. . Train_Master = pd.concat([train, vendor_train, passenger_count_train, store_and_fwd_flag_train, cluster_pickup_train, cluster_dropoff_train, month_train, dom_train, hour_test, dow_train ], axis=1) Test_master = pd.concat([test, vendor_test, passenger_count_test, store_and_fwd_flag_test, cluster_pickup_test, cluster_dropoff_test, month_test, dom_test, hour_test, dow_test], axis=1) Train_Master.shape,Test_master.shape . ((1446345, 285), (625134, 284)) . 원핫인코딩했던 변수들을 합쳐줍니다. . &#47784;&#45944; &#51201;&#54633; . X_train = Train_Master.drop([&#39;log_trip_duration&#39;], axis=1) Y_train = Train_Master[&quot;log_trip_duration&quot;] Y_train = Y_train.reset_index().drop(&#39;index&#39;,axis = 1) . 이 코드 이후로 모델적합을 해야하는데 코랩에서 계속 램이 부족하다고 하네요. . 데이터도 크고, 열 개수도 원핫인코딩으로 늘려서 그런거 같습니다. . XGB였다가 LGB로 바꾸고, 노말모델로 하고 어떻게 해도 계속 램이 부족해서 실행이 안되네요. . 코드를 리뷰하는 목적이고 요즘 시간이 넉넉하지 못해서 여기까지 하겠습니다. . from lightgbm import LGBMRegressor model = LGBMRegressor() model.fit(X_train, Y_train) . pred = model.predict(Test_master) pred = np.exp(pred) submission = pd.concat([Test_id, pd.DataFrame(pred)], axis=1) submission.columns = [&#39;id&#39;,&#39;trip_duration&#39;] submission[&#39;trip_duration&#39;] = submission.apply(lambda x : 1 if (x[&#39;trip_duration&#39;] &lt;= 0) else x[&#39;trip_duration&#39;], axis = 1) submission.to_csv(&quot;./submission.csv&quot;, index=False) . !kaggle competitions submit -c nyc-taxi-trip-duration -f submission.csv -m &quot;Message&quot; . &#45712;&#45184;&#51216; . 우선 스스로 고른 데이터인데 위도, 경도를 이용한 데이터여서 조금 어려웠습니다. . 주에 하나씩 코드 리뷰를 하는데 열심히 하면 위치 데이터를 이해하는데 큰 도움이 되겠지만 당장 필요한 기술이 아니라 넘어갔네요. . 한것만 보면 크게 고생한거 같진 않지만, 너무 복잡한 코드들이 많아서 어느정도 했다가 어려워서 처음부터 다시 세번정도 한거 같습니다. . 그래도 남의 코드를 보면서 참 많은걸 배우네요. 시간이 생각보다 많이 들긴 했는데, 그 만큼 배워가는게 있는거 같아요. . 여기에 못담고 지운 코드들 중에도 배운 코드가 많아요. 예를 들어 판다스 옵션을 건드는 코드? . 데이콘 대회에도 도움이 될 거 같습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/datetime/scale/location/regression/2021/10/07/kagglestudy3.html",
            "relUrl": "/ssuda/jupyter/kaggle/datetime/scale/location/regression/2021/10/07/kagglestudy3.html",
            "date": " • Oct 7, 2021"
        }
        
    
  
    
        ,"post46": {
            "title": "[머신러닝 가이드] 6-1 주성분 분석(PCA)",
            "content": ". &#48531;&#44867; &#45936;&#51060;&#53552; . from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;] irisDF = pd.DataFrame(iris.data, columns = columns) irisDF[&#39;target&#39;] = iris.target irisDF.head(3) . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . markers = [&#39;^&#39;,&#39;s&#39;,&#39;o&#39;] for i, marker in enumerate(markers): x_axis_data = irisDF[irisDF[&#39;target&#39;] == i][&#39;sepal_length&#39;] y_axis_data = irisDF[irisDF[&#39;target&#39;] == i][&#39;sepal_width&#39;] plt.scatter(x_axis_data, y_axis_data, marker = marker, label = iris.target_names[i]) plt.legend() plt.xlabel(&#39;sepal length&#39;) plt.ylabel(&#39;sepal width&#39;) plt.show() . 길이를 x축 너비를 y축으로, 도형으로 붓꽃 데이터를 구분했습니다. . 파란색 데이터는 y축값 3이상, x축값 6이하인 곳에 일정하게 분포돼 있습니다. . 노란색과 초록색 데이터는 이 두 특성으로 구분하기 힘듭니다. . from sklearn.preprocessing import StandardScaler iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:,:-1]) . 타겟 값을 제외한 모든 특성을 표준 정규 분포를 따르게 변환했습니다. . PCA방법은 특성의 스케일에 영향을 받기 때문에 동일한 스케일로 변환하는 것이 필수입니다. . from sklearn.decomposition import PCA pca = PCA(n_components = 2) pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) print(iris_pca.shape) . (150, 2) . 4차원 데이터를 2차원 PCA 데이터로 변환하였습니다. . pca_columns = [&#39;pca_component_1&#39;, &#39;pca_component_2&#39;] irisDF_pca = pd.DataFrame(iris_pca, columns = pca_columns) irisDF_pca[&#39;target&#39;] = iris.target irisDF_pca.head(3) . pca_component_1 pca_component_2 target . 0 -2.264703 | 0.480027 | 0 | . 1 -2.080961 | -0.674134 | 0 | . 2 -2.364229 | -0.341908 | 0 | . 만들어진 PCA 특성 값으로 데이터 프레임을 만들었습니다. . markers = [&#39;^&#39;,&#39;s&#39;,&#39;o&#39;] for i, marker in enumerate(markers): x_axis_data = irisDF_pca[irisDF[&#39;target&#39;] == i][&#39;pca_component_1&#39;] y_axis_data = irisDF_pca[irisDF[&#39;target&#39;] == i][&#39;pca_component_2&#39;] plt.scatter(x_axis_data, y_axis_data, marker = marker, label = iris.target_names[i]) plt.legend() plt.xlabel(&#39;pca_component_1&#39;) plt.ylabel(&#39;pca_component_2&#39;) plt.show() . 두 개의 pca 특성 값으로 노란색과 초록색 데이터 까지 분류가 가능해집니다. . 사실 두 개의 pca 특성 값에 네 개의 특성값이 섞여있다고 볼 수 있는데요. . 삼차원 이상에 데이터는 시각화 하기 힘들기 때문에 이렇게 시각화 할 수 있는것이 pca분석의 장점이라고 할 수 있습니다. . print(pca.explained_variance_ratio_) . [0.72962445 0.22850762] . explained_varianceratio 값은 변환 된 특성이 얼마나 변동을 설명하는 가를 보여쥽니다. . 두 개의 pca 특성이 약 95% 정도에 변동을 설명하고 있습니다. . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np rcf = RandomForestClassifier(random_state = 156) scores = cross_val_score(rcf, iris.data, iris.target, scoring = &#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도 :&#39;, scores) print(&#39;평균 정확도 :&#39;, np.mean(scores)) . 개별 정확도 : [0.98 0.94 0.96] 평균 정확도 : 0.96 . 기존 4차원 데이터를 랜덤포레스트 기법을 이용해서 검정했습니다. . 평균 정확도는 약 96%가 나옵니다. . pca_x = irisDF_pca[[&#39;pca_component_1&#39;,&#39;pca_component_2&#39;]] scores_pca = cross_val_score(rcf, pca_x, iris.target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도 :&#39;, scores_pca) print(&#39;평균 정확도 :&#39;, np.mean(scores_pca)) . 개별 정확도 : [0.88 0.88 0.88] 평균 정확도 : 0.88 . PCA기법으로 변환한 데이터를 통해 분석한 결과, 평균 정확도는 약 88%가 나옵니다. . 성능이 다소 감소했다고도 볼 수 있습니다. . 하지만 특성 수가 절반이 된 걸 생각해보면 원본 데이터의 특성을 상당부분 잘 유지하고 있다고도 볼 수 있습니다. . &#49888;&#50857;&#52852;&#46300; &#44256;&#44061; &#45936;&#51060;&#53552; &#49464;&#53944; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd df = pd.read_excel(&#39;/content/drive/MyDrive/credit_card.xls&#39;, header = 1, sheet_name=&#39;Data&#39;).iloc[0:,1:] print(df.shape) df.head(3) . (30000, 24) . LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | 3913 | 3102 | 689 | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | 2682 | 1725 | 2682 | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 29239 | 14027 | 13559 | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 24개의 특성과 3만개의 데이터가 있습니다. . df.rename(columns={&#39;PAY_0&#39;:&#39;PAY_1&#39;, &#39;default payment next month&#39;:&#39;default&#39;}, inplace=True) y_target = df[&#39;default&#39;] x_features = df.drop(&#39;default&#39;, axis = 1) . pay_0 다음 pay_2 칼럼이 있어서 pay_1로 이름 변경했습니다. . default.. 칼럼도 길어서 짧게 바꿨습니다. . import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline corr = x_features.corr() plt.figure(figsize = (14,14)) sns.heatmap(corr, annot=True, fmt = &#39;.1g&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f76d75da490&gt; . 상관계수 행렬을 관찰해본 결과 PAY 변수끼리, 또 BILL 변수 끼리 상관계수가 매우 높은 것을 알 수 있습니다. . 다중공선성 등 상당부분 문제가 있기 때문에 PCA 방법으로 조정해보겠습니다. . from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler cols_bill = [&#39;BILL_AMT&#39;+str(i) for i in range(1,7)] print(&#39;대상 속성명:&#39;, cols_bill) scaler = StandardScaler() df_cols_scaled = scaler.fit_transform(x_features[cols_bill]) pca = PCA(n_components = 2) pca.fit(df_cols_scaled) print(&#39;변동성:&#39;, pca.explained_variance_ratio_) . 대상 속성명: [&#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;] 변동성: [0.90555253 0.0509867 ] . 단 두 개의 pca 특성으로 변동성을 95프로이상 설명할 수 있습니다. . import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score rcf = RandomForestClassifier(n_estimators = 300, random_state = 156) scores = cross_val_score(rcf, x_features, y_target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도:&#39;, scores) print(&#39;평균 정확도:&#39;, np.mean(scores)) . 개별 정확도: [0.8083 0.8196 0.8232] 평균 정확도: 0.8170333333333333 . 원본 데이터를 그대로 적용했을 때 정확도 입니다. . scaler = StandardScaler() df_scaled = scaler.fit_transform(x_features) pca = PCA(n_components = 6) df_pca = pca.fit_transform(df_scaled) scores_pca = cross_val_score(rcf, df_pca, y_target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도:&#39;, scores_pca) print(&#39;평균 정확도:&#39;, np.mean(scores_pca)) . 개별 정확도: [0.7924 0.7969 0.8012] 평균 정확도: 0.7968333333333334 . 전체 23개의 속성중 6개 속성만 이용했음에도 정확도가 원본 데이터 대비 크게 떨어지지 않습니다. . 이 기법은 최근 컴퓨터 비전 분야에 많이 쓰입니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/pca/scale/randomforest/2021/10/06/PythonMachine6_1.html",
            "relUrl": "/book/jupyter/guide/pca/scale/randomforest/2021/10/06/PythonMachine6_1.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post47": {
            "title": "[머신러닝 가이드] 5-4 실전분석(자전거 대여 수요 예측)",
            "content": ". &#52880;&#44544;&#50640;&#49436; &#45936;&#51060;&#53552; &#51649;&#51217; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;0c820de52cea65ec11954012ef8b00d2&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ # Permission Warning이 발생하지 않도록 해줍니다. !chmod 600 ~/.kaggle/kaggle.json . ! kaggle competitions download -c bike-sharing-demand . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sampleSubmission.csv to /content 0% 0.00/140k [00:00&lt;?, ?B/s] 100% 140k/140k [00:00&lt;00:00, 50.8MB/s] Downloading test.csv to /content 0% 0.00/316k [00:00&lt;?, ?B/s] 100% 316k/316k [00:00&lt;00:00, 44.9MB/s] Downloading train.csv to /content 0% 0.00/633k [00:00&lt;?, ?B/s] 100% 633k/633k [00:00&lt;00:00, 40.4MB/s] . &#45936;&#51060;&#53552; &#46168;&#47084;&#48372;&#44592; &#48143; &#44032;&#44277; . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;, category = RuntimeWarning) bike_df = pd.read_csv(&#39;./train.csv&#39;) print(bike_df.shape) bike_df.head() . (10886, 12) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . 변수는 11개, 10886개 데이터가 있습니다. datetime변수는 가공이 필요합니다. . bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB . 결측값은 없습니다. . bike_df[&#39;datetime&#39;] = bike_df.datetime.apply(pd.to_datetime) bike_df[&#39;year&#39;] = bike_df.datetime.apply(lambda x : x.year) bike_df[&#39;month&#39;] = bike_df.datetime.apply(lambda x : x.month) bike_df[&#39;day&#39;] = bike_df.datetime.apply(lambda x : x.day) bike_df[&#39;hour&#39;] = bike_df.datetime.apply(lambda x : x.hour) bike_df.head(3) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | 2011 | 1 | 1 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | 2011 | 1 | 1 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | 2011 | 1 | 1 | 2 | . pd.to_datetime 함수를 통해 데이터 타임을 datetime으로 바꿨습니다. . datetime 데이터 타입은 year, month 등등으로 구분할 수 있습니다. . 이를 활용하여 년, 달, 날, 시간 변수로 각각 생성하였습니다. . drop_columns = [&#39;datetime&#39;, &#39;casual&#39;, &#39;registered&#39;] bike_df.drop(drop_columns, axis = 1, inplace = True) . datetime 변수는 분해를 했기 때문에 원본 변수가 필요 없어졌습니다. . casual + registered = count 변수 이므로 두 변수 모두 제외하겠습니다. . from sklearn.metrics import mean_squared_error, mean_absolute_error def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle def rmse(y, pred): return np.sqrt(mean_squared_error(y, pred)) def evaluate_regr(y, pred): rmsle_val = rmsle(y, pred) rmse_val = rmse(y, pred) mae_val = mean_absolute_error(y, pred) print(&#39;rmsle :&#39;, np.round(rmsle_val, 4), &#39;rmse :&#39;, np.round(rmse_val, 4), &#39;mse :&#39;, np.round(mae_val, 4)) . 이번 분석의 성능 평가 방법은 rmsle 이기 때문에 이를 구현했습니다. . &#52395;&#48264;&#51704; &#48516;&#49437; . from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso y_target = bike_df[&#39;count&#39;] x_features = bike_df.drop([&#39;count&#39;], axis = 1, inplace = False) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target, test_size = 0.3, random_state = 0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) evaluate_regr(y_test, pred) . rmsle : 1.1647 rmse : 140.8996 mse : 105.9244 . 실제 타겟 값이 대여 횟수임으로 지금 rmse 값은 매우 크다고 볼 수 있습니다. . def get_top_error_data(y_test, pred, n_tops = 5): result_df = pd.DataFrame(y_test.values, columns=[&#39;real_count&#39;]) result_df[&#39;predicted_count&#39;] = np.round(pred) result_df[&#39;diff&#39;] = np.abs(result_df[&#39;real_count&#39;] - result_df[&#39;predicted_count&#39;]) print(result_df.sort_values(&#39;diff&#39;, ascending= False)[:n_tops]) get_top_error_data(y_test, pred) . real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 . 실제값과 예측값이 가장 차이가 큰 5개 데이터를 출력했습니다. . 상당히 차이가 많이 나는걸 볼 수 있는데요. . 타겟값의 분포가 치우쳐 있는지 확인을 해볼 필요가 있겠습니다. . y_target.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c8fd090&gt; . 오른쪽 꼬리가 매우 두터운 형태임을 알 수 있습니다. . 이런 형태일 때 가장 자주 쓰이는 로그변환을 적용해보겠습니다. . y_log_transform = np.log1p(y_target) y_log_transform.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c792690&gt; . 정규분포와는 다소 차이가 있지만 변환 전보다 왜곡 정도가 많이 개선됐습니다. . y_target_log = np.log1p(y_target) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target_log, test_size= 0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) y_test_exp = np.expm1(y_test) pred_exp = np.expm1(pred) evaluate_regr(y_test_exp, pred_exp) . rmsle : 1.0168 rmse : 162.5943 mse : 109.2862 . mse 값은 전보다 개선 되었지만 rmse 값은 더 증가하였습니다. . 무슨 이유일까요? . &#46160;&#48264;&#51704; &#48516;&#49437; . coef = pd.Series(lr_reg.coef_, index=x_features.columns) coef_sort = coef.sort_values(ascending = False) sns.barplot(x=coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c1f0990&gt; . 다른 값에 비해 year값이 높습니다. . year값은 년도인데 년도가 이렇게 큰 영향을 미치는 것을 일반적인 사실로 받아들이기 힘듭니다. . 이유를 추정해보자면 연도 변수의 값이 큰 점을 들 수 있습니다.(2011,2012) . 비슷한 이유로 범주형 변수로 변환할 필요가 있는 변수들을 원핫인코딩방식으로 변환하겠습니다. . x_features_ohe = pd.get_dummies(x_features, columns = [&#39;year&#39;, &#39;month&#39;,&#39;day&#39;,&#39;hour&#39;,&#39;holiday&#39;, &#39;workingday&#39;, &#39;season&#39;, &#39;weather&#39;]) x_features_ohe.shape . (10886, 73) . 원핫 인코딩 결과 열 개수가 73개로 크게 늘어났습니다. . x_train, x_test, y_train, y_test = train_test_split(x_features_ohe, y_target_log, test_size= 0.3, random_state=0) def get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = False): model.fit(x_train, y_train) pred = model.predict(x_test) if is_expm1: y_test = np.expm1(y_test) pred = np.expm1(pred) print(model.__class__.__name__) evaluate_regr(y_test, pred) lr_reg = LinearRegression() ridge_reg = Ridge(alpha = 10) lasso_reg = Lasso(alpha = 0.01) for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = True) . LinearRegression rmsle : 0.5896 rmse : 97.6878 mse : 63.3821 Ridge rmsle : 0.5901 rmse : 98.5286 mse : 63.8934 Lasso rmsle : 0.6348 rmse : 113.2188 mse : 72.8027 . 원핫 인코딩을 적용한 후 결과가 눈에 띄게 좋아졌습니다. . coef = pd.Series(lr_reg.coef_, index=x_features_ohe.columns) coef_sort = coef.sort_values(ascending = False)[:20] sns.barplot(x = coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c772bd0&gt; . 회귀계수가 높은 피처 20개를 출력해보았습니다. . &#49464;&#48264;&#51704; &#48516;&#49437; . from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor rf_reg = RandomForestRegressor(n_estimators = 500) gbm_reg = GradientBoostingRegressor(n_estimators = 500) xgb_reg = XGBRegressor(n_estimaters = 500) lgbm_reg = LGBMRegressor(n_estimaters = 500) for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: get_model_predict(model, x_train.values, x_test.values, y_train.values, y_test.values, is_expm1=True) . RandomForestRegressor rmsle : 0.3549 rmse : 50.2976 mse : 31.1562 GradientBoostingRegressor rmsle : 0.3299 rmse : 53.3352 mse : 32.7448 [16:01:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor rmsle : 0.4828 rmse : 95.6137 mse : 59.2047 LGBMRegressor rmsle : 0.3315 rmse : 51.3807 mse : 31.8325 . 부스팅 모델을 사용하면 더 좋은 성능을 보일 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/datetime/scale/randomforest/boost/regression/2021/10/02/PythonMachine5_4.html",
            "relUrl": "/book/jupyter/guide/datetime/scale/randomforest/boost/regression/2021/10/02/PythonMachine5_4.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post48": {
            "title": "[SSUDA] 자전거 수요 예측 모델",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle (1).json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;0c820de52cea65ec11954012ef8b00d2&#34;}&#39;} . kaggle.json 파일 선택합니다. . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c bike-sharing-demand . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading train.csv to /content 0% 0.00/633k [00:00&lt;?, ?B/s] 100% 633k/633k [00:00&lt;00:00, 42.8MB/s] Downloading sampleSubmission.csv to /content 0% 0.00/140k [00:00&lt;?, ?B/s] 100% 140k/140k [00:00&lt;00:00, 44.0MB/s] Downloading test.csv to /content 0% 0.00/316k [00:00&lt;?, ?B/s] 100% 316k/316k [00:00&lt;00:00, 41.9MB/s] . 캐글에서 복사한 코드에 느낌표만 붙여줍니다. . import sklearn import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import numpy as np from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge ,Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score from sklearn.feature_selection import VarianceThreshold from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures from sklearn.metrics import mean_squared_log_error as msle import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn.ensemble import GradientBoostingRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import GridSearchCV from xgboost import XGBRegressor %matplotlib inline . train=pd.read_csv(&#39;./train.csv&#39;) test=pd.read_csv(&#39;./test.csv&#39;) . &#45936;&#51060;&#53552; &#46168;&#47084;&#48372;&#44592; . train.head() . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . test.head() . datetime season holiday workingday weather temp atemp humidity windspeed . 0 2011-01-20 00:00:00 | 1 | 0 | 1 | 1 | 10.66 | 11.365 | 56 | 26.0027 | . 1 2011-01-20 01:00:00 | 1 | 0 | 1 | 1 | 10.66 | 13.635 | 56 | 0.0000 | . 2 2011-01-20 02:00:00 | 1 | 0 | 1 | 1 | 10.66 | 13.635 | 56 | 0.0000 | . 3 2011-01-20 03:00:00 | 1 | 0 | 1 | 1 | 10.66 | 12.880 | 56 | 11.0014 | . 4 2011-01-20 04:00:00 | 1 | 0 | 1 | 1 | 10.66 | 12.880 | 56 | 11.0014 | . 변수가 3개 차이 나는데, casual + registered = count 변수 입니다. . 테스트 데이터에서는 count를 맞추는것이 목적입니다. . train.drop([&#39;casual&#39;,&#39;registered&#39;],1,inplace=True) . 글쓴이는 이런 이유로 쿨하게 두 변수를 날렸습니다. . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 count 10886 non-null int64 dtypes: float64(3), int64(6), object(1) memory usage: 850.6+ KB . 데이터 10886개, count를 제외한 변수 개수 9개, 결측값은 없습니다. . train.describe() . season holiday workingday weather temp atemp humidity windspeed count . count 10886.000000 | 10886.000000 | 10886.000000 | 10886.000000 | 10886.00000 | 10886.000000 | 10886.000000 | 10886.000000 | 10886.000000 | . mean 2.506614 | 0.028569 | 0.680875 | 1.418427 | 20.23086 | 23.655084 | 61.886460 | 12.799395 | 191.574132 | . std 1.116174 | 0.166599 | 0.466159 | 0.633839 | 7.79159 | 8.474601 | 19.245033 | 8.164537 | 181.144454 | . min 1.000000 | 0.000000 | 0.000000 | 1.000000 | 0.82000 | 0.760000 | 0.000000 | 0.000000 | 1.000000 | . 25% 2.000000 | 0.000000 | 0.000000 | 1.000000 | 13.94000 | 16.665000 | 47.000000 | 7.001500 | 42.000000 | . 50% 3.000000 | 0.000000 | 1.000000 | 1.000000 | 20.50000 | 24.240000 | 62.000000 | 12.998000 | 145.000000 | . 75% 4.000000 | 0.000000 | 1.000000 | 2.000000 | 26.24000 | 31.060000 | 77.000000 | 16.997900 | 284.000000 | . max 4.000000 | 1.000000 | 1.000000 | 4.000000 | 41.00000 | 45.455000 | 100.000000 | 56.996900 | 977.000000 | . 최솟값 또는 최댓값에 이상한 값은 없습니다. . 또 값을 관찰해보면 season, holiday, workingday, weather 변수는 범주형 변수인 것을 알 수 있습니다. . 그리고 datetime 변수는 형태가 특수하여 이 표에 표현되지 않습니다. . datetime &#48320;&#49688;&#50640; &#45824;&#54644; . train[&#39;datetime&#39;] . 0 2011-01-01 00:00:00 1 2011-01-01 01:00:00 2 2011-01-01 02:00:00 3 2011-01-01 03:00:00 4 2011-01-01 04:00:00 ... 10881 2012-12-19 19:00:00 10882 2012-12-19 20:00:00 10883 2012-12-19 21:00:00 10884 2012-12-19 22:00:00 10885 2012-12-19 23:00:00 Name: datetime, Length: 10886, dtype: object . datetime 변수는 날짜 + 시간 변수 입니다. . 데이터 타입은 현재 object 타입인데요. 변경해보겠습니다. . train[&#39;datetime&#39;] = pd.to_datetime(train[&#39;datetime&#39;]) test[&#39;datetime&#39;] = pd.to_datetime(test[&#39;datetime&#39;]) train[&#39;Month&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).month test[&#39;Month&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).month train[&#39;Year&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).year test[&#39;Year&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).year train[&#39;WeekDay&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).weekday test[&#39;WeekDay&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).weekday train[&#39;Hour&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).hour test[&#39;Hour&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).hour train.head(3) . datetime season holiday workingday weather temp atemp humidity windspeed count Month Year WeekDay Hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 16 | 1 | 2011 | 5 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 40 | 1 | 2011 | 5 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 32 | 1 | 2011 | 5 | 2 | . 판다스에 to_* 함수를 통해 데이터 타입을 바꿀 수 있습니다. . 여기서는 to_datetime 함수로 &#39;datetime&#39; 데이터 형으로 바꿨습니다. . 이 데이터 형에 특징은 DatetimeIndex함수를 이용하여 연도/달/일 등을 쉽게 추출할 수 있다는 것 입니다. . &#45936;&#51060;&#53552; &#49548;&#44060; . season =&gt; 계절 변수, 1 : 봄, 2 : 여름, 3 : 가을, 4 : 겨울 . holiday =&gt; 휴일 변수, 날짜가 휴일이면 1 아니면 0 . workingday =&gt; 근무일 변수, 날짜가 주말도 휴일도 아니라면 1 . weather =&gt; 날씨 변수, 맑음이 1 폭우가 4. 흐릴수록 값이 점차 증가. . temp =&gt; 온도, atemp =&gt; 체감온도, humidity =&gt; 습도, windspeed =&gt; 풍속 . categorical_cols=[&#39;season&#39;,&#39;holiday&#39;,&#39;workingday&#39;,&#39;weather&#39;] numerical_cols=[&#39;temp&#39;,&#39;atemp&#39;,&#39;humidity&#39;,&#39;windspeed&#39;] label=&#39;count&#39; . 변수를 범주형 변수와 연속형 변수로 나눴습니다. . &#49884;&#44036; &#45936;&#51060;&#53552; &#44288;&#52769; . def encodetime(train,test,col,label): d3=train[[col,label]].groupby(col).mean() d3.sort_values(by=&#39;count&#39;,ascending=False) plt.scatter(x=d3.index,y=d3[&#39;count&#39;]) d3=d3.sort_values(by=&#39;count&#39;) d3[&#39;w&#39;]=np.arange(train[col].nunique()) #순서 dic=dict(zip(d3.index,d3[&#39;w&#39;])) train[col]=train[col].map(dic) test[col]=test[col].map(dic) . .nunique() =&gt; 유니크한 범주 개수 출력 . dict(zip()) =&gt; 두 시리즈 변수를 튜플로 묶고 딕셔러니 자료형으로 변환 . map =&gt; 주로 함수를 입력하는데, 여기서는 딕셔너리 키 값을 받을때 value 값을 반환해주는 함수로 사용 . 즉 이 함수는 col 변수로 변수가 입력되면 그 변수의 plot를 출력해주고 0~n 까지 레이블 인코딩을 해줍니다. . encodetime(train,test,&#39;Year&#39;,label) . 연도별로 차이가 있습니다. . encodetime(train,test,&#39;Month&#39;,label) . 날씨가 따뜻한 여름 부근에 확실히 값이 큽니다. . encodetime(train,test,&#39;Hour&#39;,label) . 야간시간에 값이 떨어집니다. . encodetime(train,test,&#39;WeekDay&#39;,label) . 0부터 월요일이므로 값이 많이 떨어지는 6은 일요일입니다. . &#48276;&#51452;&#54805; &#51088;&#47308; &#49884;&#44033;&#54868; . def boxplot(x,y,**kwargs): sns.boxplot(x=x,y=y) x=plt.xticks(rotation=90) f=pd.melt(train,id_vars=[&#39;count&#39;],value_vars=categorical_cols) g=sns.FacetGrid(f,col=&#39;variable&#39;,col_wrap=2,sharex=False) g.map(boxplot,&#39;value&#39;,&#39;count&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd4625d22d0&gt; . boxplot 모형입니다. count변수의 이상치가 변수에 상관없이 존재합니다. . 파이썬 문법 : 함수 인자 내 **kwargs에 대해서. . 단순하게 함수 인자 내 *이나 **이 보일 경우 C언어에서 사용하는 포인터 개념이 아닙니다. . 몇개의 인자를 보낼지 모를때 사용되며, **같은 경우 딕셔너리 형태일때 사용합니다. . 다만 이 코드는 너무 복잡해서 지금 실력에서 어떻게 해석을 못하겠습니다. . sns.pairplot(train[[*numerical_cols,&#39;count&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x7fd460faa110&gt; . *numerical_cols은 리스트를 해체한다고 이해하면 편할 것 같아요. . pairplot함수를 통해 연속형 변수 간에 산점도를 한 눈에 볼 수 있습니다. . f, ax = plt.subplots(figsize=(15, 15)) corr = train[[*numerical_cols,&#39;count&#39;]].corr() sns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd461781f50&gt; . heatmap 함수는 아까 본 산점도를 상관계수 버전으로 보여줍니다. . square는 셀을 정사각형으로 출력하는 것, annot은 셀 안에 숫자를 출력해주는 것을 의미합니다. . 여기서는 체감온도와 실제 온도 간 상관계수가 엄청 높은 것이 눈에 띄네요. . f, ax = plt.subplots(figsize=(15, 15)) corr = train.corr() sns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd4602b2650&gt; . 조금 더 확장해서 모든 변수간 상관계수를 살펴보았습니다. . &#48152;&#51025;&#48320;&#49688; &#48320;&#54872; . sns.displot(train[label] , kde=True, height=8.27, aspect=11.7/8.27) sns.displot(np.log(train[label]) , kde=True, height=8.27, aspect=11.7/8.27) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd46015a090&gt; . count 변수를 로그변환 해보았는데요. . 로그변환 전 우측 꼬리가 긴 그래프였는데 변환 후 상대적으로 더 정규분포에 가까워졌습니다. . 다만 왼쪽 꼬리가 다소 길어져 변환 정도를 조절할 필요가 있겠습니다. . def trans(x,l1=0.3,l2=0): if l1!=0: return ((x+l2)**l1-1)/l1 else: return np.log(x+l2) def rev_trans(x,l1=0.3,l2=0): return (x*l1+1)**(1/l1)-l2 z=train[label].apply(trans) sns.displot(z , kde=True, height=8.27, aspect=11.7/8.27) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd45feda3d0&gt; . l2가 0일때 이는 람다가 l1인 box-cox 변환입니다. . 이 변환은 정규분포가 아닌 값을 정규분포형태로 변환합니다. . 그림을 확인해보면 이전 대비 그래프가 확연히 정규분포 형태에 가까운 것을 알 수 있습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . x=train.drop([&#39;count&#39;,&#39;datetime&#39;,&#39;atemp&#39;],1) xtest=test.drop([&#39;datetime&#39;,&#39;atemp&#39;],1) y=train[&#39;count&#39;] xt,xv,yt,yv=train_test_split(x,y,test_size=0.2,random_state=101) . 변수이름을 조금 대충 만들었네요. . 보면서 생각난 점은 데이터 분석 프로젝트롤 여러명이서 할 경우 변수명 통일이 상당히 중요하다는 점 입니다. . def redun(x): return x def mk_model_RF(xt1,xv1,yt,yv,md=None,func1=redun,func2=redun,mss=2,n_est=100,al=0): ytt=yt.apply(func1) yvt=yv.apply(func2) model=RandomForestRegressor(max_depth=md, random_state=0,min_samples_split=mss,n_estimators=n_est,ccp_alpha=al) model.fit(xt1,ytt) ypt=np.apply_along_axis(func2,arr=model.predict(xt1),axis=0) ypv=np.apply_along_axis(func2,arr=model.predict(xv1),axis=0) print(&#39;training r2:&#39;,r2_score(yt,ypt)) print(&#39;Validation r2:&#39;,r2_score(yv,ypv)) print(&#39;training rmsle:&#39;,np.sqrt(msle(yt,ypt))) print(&#39;validation rmsle:&#39;,np.sqrt(msle(yv,ypv))) return model . 함수를 조금 복잡하게 만들었는데 함수 한 개만 소개하겠습니다. . np.apply_along_axis 함수는 인자가 (함수, 어레이, 행/열 여부) 입니다. . apply와 비슷한 역할의 함수인데 넘파이 함수라서 실행속도가 엄청 빠릅니다. . mk_model_RF(xt,xv,yt,yv) . training r2: 0.9922090269471024 Validation r2: 0.9338288545809785 training rmsle: 0.16871805329660905 validation rmsle: 0.379570233664069 . RandomForestRegressor(bootstrap=True, ccp_alpha=0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=0, verbose=0, warm_start=False) . 랜덤 포레스트 방법이네요. . count 변수에 box-cox변환을 하지 않은 결과입니다. . mk_model_RF(xt,xv,yt,yv,func1=trans,func2=rev_trans,n_est=400,md=20) . training r2: 0.9916973216662878 Validation r2: 0.9338712883052291 training rmsle: 0.12704646646266962 validation rmsle: 0.34592422107196 . RandomForestRegressor(bootstrap=True, ccp_alpha=0, criterion=&#39;mse&#39;, max_depth=20, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None, oob_score=False, random_state=0, verbose=0, warm_start=False) . box-cox 변환을 한 결과가 조금 더 좋은 것 같습니다. . def mk_model_xgb(xt,xv,yt,yv,func1=redun,func2=redun,lr=1,min_child_weight =25,colsample_bytree = 0.8,md=None): model =XGBRegressor( colsample_bytree = colsample_bytree, learning_rate = lr,min_child_weight =min_child_weight, max_depth=md ) ytt=yt.apply(func1) model.fit(xt,ytt) ypt=np.apply_along_axis(func2,arr=model.predict(xt),axis=0) ypv=np.apply_along_axis(func2,arr=model.predict(xv),axis=0) print(&#39;training r2:&#39;,r2_score(yt,ypt)) print(&#39;Validation r2:&#39;,r2_score(yv,ypv)) print(&#39;training rmsle:&#39;,np.sqrt(msle(yt,ypt))) print(&#39;validation rmsle:&#39;,np.sqrt(msle(yv,ypv))) return model . mk_model_xgb(xt,xv,yt,yv,func1=trans,func2=rev_trans,lr=0.2,min_child_weight =20,colsample_bytree = 0.8,md=20) . [14:54:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. training r2: 0.9945096180099459 Validation r2: 0.9504273424389956 training rmsle: 0.10444065614264901 validation rmsle: 0.3133324307194445 . XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8, gamma=0, importance_type=&#39;gain&#39;, learning_rate=0.2, max_delta_step=0, max_depth=20, min_child_weight=20, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective=&#39;reg:linear&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) . xgboost 사용 결과 값이 더 좋게 나옵니다. box-cox 변환을 한 결과입니다. . model=XGBRegressor(colsample_bytree = 0.8, learning_rate = 0.2,min_child_weight =20, max_depth=20).fit(x,y.apply(trans)) . [14:55:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . 그래서 box-cox변환 후 xgboost를 사용해 모델을 적합시켰습니다. . &#48516;&#49437; &#44208;&#44284; &#51228;&#52636; . yp=np.apply_along_axis(rev_trans,arr=model.predict(xtest),axis=0) . plt.hist(yp) . (array([2543., 1497., 1057., 606., 367., 184., 125., 69., 34., 11.]), array([7.3024821e-01, 9.8740685e+01, 1.9675111e+02, 2.9476157e+02, 3.9277197e+02, 4.9078241e+02, 5.8879285e+02, 6.8680328e+02, 7.8481372e+02, 8.8282416e+02, 9.8083459e+02], dtype=float32), &lt;a list of 10 Patch objects&gt;) . test[&#39;count&#39;]=yp test[[&#39;datetime&#39;, &#39;count&#39;]].to_csv(&#39;./submission.csv&#39;, index=False) !kaggle competitions submit -c bike-sharing-demand -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 188k/188k [00:01&lt;00:00, 106kB/s] Successfully submitted to Bike Sharing Demand . 별도에 작업 없이 캐글과 연동하여 바로 제출할 수 있습니다. . &#45712;&#45184;&#51216; . 우선 이번 데이터 분석은 저번보다 열심히 한 것 같네요. . 중점으로 두었던 것은 제가 시각화 부분이 많이 부족해서 이 부분 공부해보려고 이번 코드 골랐습니다. . 잊고 있었던 box-cox 정규분포 변환에 대해 다시 떠올리게 되었던 것도 큰 수확인것 같네요. . 이 사람이 코드 설명을 크게 한 것이 없어 찾아보느라도 고생한 것 같습니다. . 결과는 0.41정도로 상위권은 아니지만 노력 대비 어느정도 성과가 있습니다. . 이번 코드 리뷰를 통해서 많이 배운것 같습니다. . 대회 출처 : https://www.kaggle.com/c/bike-sharing-demand . 코드 출처 : https://www.kaggle.com/muhammedmamdouhsalah/bike-sharing .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/randomforest/scale/boxcox/xgboost/regression/2021/10/01/kagglessu3.html",
            "relUrl": "/ssuda/jupyter/kaggle/randomforest/scale/boxcox/xgboost/regression/2021/10/01/kagglessu3.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post49": {
            "title": "[머신러닝 가이드] 5-3 다양한 회귀",
            "content": ". &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression cancer = load_breast_cancer() . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split scaler = StandardScaler() data_scaled = scaler.fit_transform(cancer.data) x_train, x_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size = 0.3, random_state = 0) . 평균이 0, 분산이 1인 정규분포 형태로 형 변환을 했습니다. . 로지스틱 회귀기법은 선형 회귀 방식에 응용으로 데이터의 정규분포도에 영향을 많이 받습니다. . from sklearn.metrics import accuracy_score, roc_auc_score import numpy as np lr_clf = LogisticRegression() lr_clf.fit(x_train, y_train) lr_preds = lr_clf.predict(x_test) print(&#39;정확도 :&#39;, np.round(accuracy_score(y_test, lr_preds), 4)) print(&#39;roc 커브 :&#39;, np.round(roc_auc_score(y_test, lr_preds), 4)) . 정확도 : 0.9766 roc 커브 : 0.9716 . from sklearn.model_selection import GridSearchCV params = {&#39;penalty&#39; : [&#39;l2&#39;, &#39;l1&#39;], &#39;C&#39; : [0.01, 0.1, 1, 5, 10]} grid_clf = GridSearchCV(lr_clf, param_grid = params, scoring = &#39;accuracy&#39;, cv = 3) grid_clf.fit(data_scaled, cancer.target) print(&#39;최적 파라미터 : &#39;, grid_clf.best_params_, &#39;최적 평균 정확도&#39;, grid_clf.best_score_) . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터 : {&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;} 최적 평균 정확도 0.975392184164114 . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터는 l2 규제로(릿지 회귀) c가(알파의 역수) 1일때 입니다. . &#53944;&#47532; &#44592;&#48152; &#54924;&#44480; &#47784;&#45944; . from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor import pandas as pd import numpy as np boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) rf = RandomForestRegressor(random_state = 0, n_estimators = 1000) neg_mse_scores = cross_val_score(rf, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse score : &#39;, np.round(neg_mse_scores, 4)) print(&#39;rmse score : &#39;, np.round(rmse_scores, 4)) print(&#39;평균 rmse score : &#39;, np.round(avg_rmse, 4)) . mse score : [ -7.933 -13.0584 -20.5278 -46.3057 -18.8008] rmse score : [2.8166 3.6136 4.5308 6.8048 4.336 ] 평균 rmse score : 4.4204 . 랜덤 포레스트 회귀 입니다. 평균 rmse 값은 4.42로 꽤 좋은 수치 입니다. . def get_model_cv_prediction(model, x_data, y_target): neg_mse_scores = cross_val_score(model, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(model.__class__.__name__) print(&#39;평균 rmse : &#39;, np.round(avg_rmse, 4)) . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor dt_reg = DecisionTreeRegressor(random_state = 0, max_depth = 4) rf_reg = RandomForestRegressor(random_state = 0, n_estimators = 1000) gb_reg = GradientBoostingRegressor(random_state = 0, n_estimators = 1000) xgb_reg = XGBRegressor(random_state = 0, n_estimators = 1000) lgb_reg = LGBMRegressor(random_state = 0, n_estimators = 1000) models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] for model in models: get_model_cv_prediction(model, x_data, y_target) . DecisionTreeRegressor 평균 rmse : 6.2377 RandomForestRegressor 평균 rmse : 4.4204 GradientBoostingRegressor 평균 rmse : 4.2692 [13:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor 평균 rmse : 4.0889 LGBMRegressor 평균 rmse : 4.6464 . 여러 모델을 테스트 해보았습니다. . xgb부스팅 모델의 성능이 가장 우수하게 나왔습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/logistic/randomforest/boost/regression/2021/09/30/PythonMachine5_3.html",
            "relUrl": "/book/jupyter/guide/logistic/randomforest/boost/regression/2021/09/30/PythonMachine5_3.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post50": {
            "title": "[머신러닝 가이드] 5-2 선형회귀응용",
            "content": ". &#45796;&#54637; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures import numpy as np x = np.arange(4).reshape(2,2) # 행 부터 숫자 채워짐 print(&#39;일차 단항식 계수 피처: n&#39;, x) poly = PolynomialFeatures(degree = 2) poly.fit(x) poly_ftr = poly.transform(x) print(&#39;변환된 2차 다항식 계수 피처: n&#39;, poly_ftr) . 일차 단항식 계수 피처: [[0 1] [2 3]] 변환된 2차 다항식 계수 피처: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] . 2차 다항계수는 [1, x1, x2, x1^2, x1x2, x2^2] 로 구성되어 있습니다. . def polynomial_func(x): y = 1 + 2 * x[:,0] + 3 * x[:,0] **2 + 4 * x[:,1] **3 return y y = polynomial_func(x) . from sklearn.linear_model import LinearRegression poly_ftr = PolynomialFeatures(degree = 3).fit_transform(x) print(&#39;3차 다항식 계수 feature: n&#39;, poly_ftr) model = LinearRegression() model.fit(poly_ftr, y) print(&#39;회귀 계수 n&#39;, np.round(model.coef_,2)) print(&#39;회귀 shape&#39;, model.coef_.shape) . 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] 회귀 shape (10,) . poly함수로 다항식 계수를 생성한 뒤 단순 선형 회귀 함수에 대입해줍니다. . 원하는 값인 [1,2,0,3,0,0,0,0,0,4] 와 다소 차이가 있긴 합니다. . &#47551;&#51648; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline import numpy as np model = Pipeline([(&#39;poly&#39;, PolynomialFeatures(degree=3)), (&#39;linear&#39;, LinearRegression())]) model = model.fit(x,y) print(&#39;회귀 계수 n&#39;, np.round(model.named_steps[&#39;linear&#39;].coef_,2)) . 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] . 파이프 라인 함수로 다항식으로에 변환과 선형 회귀를 한번에 한 모습입니다. . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score from sklearn.datasets import load_boston import pandas as pd boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) ridge = Ridge(alpha = 10) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-11.42 -24.29 -28.14 -74.6 -28.52] rmse scores [3.38 4.93 5.31 8.64 5.34] 평균 rmse score: 5.52 . 단순 선형회귀 모델 rmse 평균값이 5.84로 릿지 회귀가 더 좋은 퍼포먼스를 보입니다. . alphas = [0,0.1,1,10,100] for alpha in alphas: ridge = Ridge(alpha=alpha) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 5.8287 alpha 값 0.1 일때 평균 rmse : 5.7885 alpha 값 1 일때 평균 rmse : 5.6526 alpha 값 10 일때 평균 rmse : 5.5182 alpha 값 100 일때 평균 rmse : 5.3296 . alpha 값이 100일때가 가장 값이 좋습니다. . import matplotlib.pyplot as plt import seaborn as sns fig, axs = plt.subplots(figsize= (18,6), nrows = 1, ncols = 5) coeff_df = pd.DataFrame() for pos, alpha in enumerate(alphas): ridge = Ridge(alpha = alpha) ridge.fit(x_data, y_target) coeff = pd.Series(data=ridge.coef_, index = x_data.columns) colname = &#39;alpha:&#39;+str(alpha) coeff_df[colname] = coeff coeff = coeff.sort_values(ascending = False) axs[pos].set_title(colname) axs[pos].set_xlim(-3, 6) sns.barplot(x=coeff.values, y = coeff.index, ax = axs[pos]) plt.show() . 알파 값이 커지면(=규제가 세지면) 회귀계수 값이 전반적으로 작아집니다. . 다만 릿지 회귀에 경우 회귀 계수를 0으로 만들지는 않습니다. . &#46972;&#50136; &#54924;&#44480; . from sklearn.linear_model import Lasso, ElasticNet def get_linear_reg_eval(model_name, params = None, x_data_n = None, y_target_n = None, verbose= True, return_coeff = True): coeff_df = pd.DataFrame() if verbose : print(model_name) for param in params: if model_name ==&#39;Ridge&#39; : model = Ridge(alpha = param) elif model_name ==&#39;Lasso&#39; : model = Lasso(alpha = param) elif model_name ==&#39;ElasticNet&#39; : model = ElasticNet(alpha = param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, x_data_n, y_target_n, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores)) print(&#39;alpha &#39;, param, &#39;일때 평균 rmse:&#39;, np.round(avg_rmse,2)) model.fit(x_data_n, y_target_n) if return_coeff: coeff = pd.Series(data=model.coef_, index = x_data_n.columns) colname = &#39;alpha:&#39;+str(param) coeff_df[colname] = coeff return coeff_df . lasso_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;Lasso&#39;,params=lasso_alphas, x_data_n = x_data, y_target_n= y_target) . Lasso alpha 0.07 일때 평균 rmse: 5.61 alpha 0.1 일때 평균 rmse: 5.62 alpha 0.5 일때 평균 rmse: 5.67 alpha 1 일때 평균 rmse: 5.78 alpha 3 일때 평균 rmse: 6.19 . 알파 값이 0.07일때 최고 성능을 보여줍니다. . 앞서 한 릿지보다는 성능이 떨어지지만, 단순 선형 회귀 모델보다 값이 크므로 쓰임새가 있습니다. . sort_column = &#39;alpha:&#39;+str(lasso_alphas[0]) coeff_lasso_df.sort_values(by = sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.789725 | 3.703202 | 2.498212 | 0.949811 | 0.000000 | . CHAS 1.434343 | 0.955190 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.270936 | 0.274707 | 0.277451 | 0.264206 | 0.061864 | . ZN 0.049059 | 0.049211 | 0.049544 | 0.049165 | 0.037231 | . B 0.010248 | 0.010249 | 0.009469 | 0.008247 | 0.006510 | . NOX -0.000000 | -0.000000 | -0.000000 | -0.000000 | 0.000000 | . AGE -0.011706 | -0.010037 | 0.003604 | 0.020910 | 0.042495 | . TAX -0.014290 | -0.014570 | -0.015442 | -0.015212 | -0.008602 | . INDUS -0.042120 | -0.036619 | -0.005253 | -0.000000 | -0.000000 | . CRIM -0.098193 | -0.097894 | -0.083289 | -0.063437 | -0.000000 | . LSTAT -0.560431 | -0.568769 | -0.656290 | -0.761115 | -0.807679 | . PTRATIO -0.765107 | -0.770654 | -0.758752 | -0.722966 | -0.265072 | . DIS -1.176583 | -1.160538 | -0.936605 | -0.668790 | -0.000000 | . 계수가 0인것이 보입니다. 알파값이 커질수록 회귀 계수가 0인 것이 늘어납니다. . &#50648;&#46972;&#49828;&#54001; &#54924;&#44480; . 다음은 엘라스틱 회귀 입니다. 쉽게 라쏘회귀 + 릿지 회귀로 볼 수 있습니다. . 라쏘 회귀에 경우 서로 상관관계가 높은 피처가 있으면 중요 피처를 제외하고 모두 회귀계수를 0으로 만듭니다. . 이를 다소 완화해주기 위한 목적으로 만들어졌습니다. 다만 수행시간이 다소 깁니다. . 여기서 알파는 알파1 + 알파 2 이며, l1_ratio는 말 그대로 l1규제(라쏘) 비율입니다. . elastic_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;ElasticNet&#39;, params=elastic_alphas, x_data_n= x_data, y_target_n= y_target) . ElasticNet alpha 0.07 일때 평균 rmse: 5.54 alpha 0.1 일때 평균 rmse: 5.53 alpha 0.5 일때 평균 rmse: 5.47 alpha 1 일때 평균 rmse: 5.6 alpha 3 일때 평균 rmse: 6.07 . 알파값이 0.5일때 가장 좋은 예측 성능을 보여줍니다. . sort_column = &#39;alpha:&#39;+str(elastic_alphas[0]) coeff_lasso_df.sort_values(by= sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.574162 | 3.414154 | 1.918419 | 0.938789 | 0.000000 | . CHAS 1.330724 | 0.979706 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.278880 | 0.283443 | 0.300761 | 0.289299 | 0.146846 | . ZN 0.050107 | 0.050617 | 0.052878 | 0.052136 | 0.038268 | . B 0.010122 | 0.010067 | 0.009114 | 0.008320 | 0.007020 | . AGE -0.010116 | -0.008276 | 0.007760 | 0.020348 | 0.043446 | . TAX -0.014522 | -0.014814 | -0.016046 | -0.016218 | -0.011417 | . INDUS -0.044855 | -0.042719 | -0.023252 | -0.000000 | -0.000000 | . CRIM -0.099468 | -0.099213 | -0.089070 | -0.073577 | -0.019058 | . NOX -0.175072 | -0.000000 | -0.000000 | -0.000000 | -0.000000 | . LSTAT -0.574822 | -0.587702 | -0.693861 | -0.760457 | -0.800368 | . PTRATIO -0.779498 | -0.784725 | -0.790969 | -0.738672 | -0.423065 | . DIS -1.189438 | -1.173647 | -0.975902 | -0.725174 | -0.031208 | . 라쏘모델에 비해 회귀계수를 0으로 만드는 개수가 다소 줄었습니다. . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#51012; &#50948;&#54620; &#45936;&#51060;&#53552; &#48320;&#54872; . 선형 회귀에서 중요한 것 중 하나가 데이터 분포도의 정규화 입니다. . 특히 타깃값의 분포가 정규분포가 아닌 왜곡(skew)된 분포는 예측 성능에 부정적입니다. . 따라서 선형 회귀 모델을 적용하기 전 먼저 데이터 스케일링/정규화 작업을 수행해주어야 합니다. . from sklearn.preprocessing import StandardScaler, MinMaxScaler def get_scaled_data(method=&#39;None&#39;, p_degree = None, input_data = None): if method == &#39;Standard&#39;: scaled_data = StandardScaler().fit_transform(input_data) elif method == &#39;MinMax&#39;: scaled_data = MinMaxScaler().fit_transform(input_data) if method == &#39;Log&#39;: scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data . alphas = [0.1, 1, 10, 100] scale_methods=[(None, None), (&#39;Standard&#39;, None), (&#39;Standard&#39;,2), (&#39;MinMax&#39;,None), (&#39;MinMax&#39;, 2), (&#39;Log&#39;, None)] for scale_method in scale_methods: x_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=x_data) print(&#39; n 변환유형:&#39;, scale_method[0], &#39;, Polynomial Degree:&#39;, scale_method[1]) get_linear_reg_eval(&#39;Ridge&#39;, params = alphas, x_data_n=x_data_scaled, y_target_n= y_target, verbose=False, return_coeff = False) . 변환유형: None , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: MinMax , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: MinMax , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: Log , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 4.77 alpha 1 일때 평균 rmse: 4.68 alpha 10 일때 평균 rmse: 4.84 alpha 100 일때 평균 rmse: 6.24 . log 변환이 다른 변환에 비해 성능이 뛰어난 걸 볼 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/poly/ridge/lasso/regression/2021/09/25/PythonMachine5_2.html",
            "relUrl": "/book/jupyter/guide/poly/ridge/lasso/regression/2021/09/25/PythonMachine5_2.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post51": {
            "title": "[SSUDA] 심장병 데이터 분석",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd data = pd.read_csv(&quot;/content/drive/MyDrive/heart.csv&quot;) . Verson 1. &#49900;&#54540;&#54620; &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; &#47784;&#54805; . &#45936;&#51060;&#53552; &#51060;&#54644; . df = data.copy() df.head() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . 디폴트 값은 5입니다. . df.columns . Index([&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalachh&#39;, &#39;exng&#39;, &#39;oldpeak&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;, &#39;output&#39;], dtype=&#39;object&#39;) . df.columns.values.tolist() . [&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalachh&#39;, &#39;exng&#39;, &#39;oldpeak&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;, &#39;output&#39;] . 컬럼은 이런 방식으로 확인할 수 있습니다. . 밑에 DataFrame.columns.values.tolist() 함수는 컬럼 추출 중 가장 런타임이 빠르다고 합니다. . print(&#39;Shape is&#39;,df.shape) . Shape is (303, 14) . 303개 데이터, 14개 특성값이 있습니다. . df.isnull().sum() . age 0 sex 0 cp 0 trtbps 0 chol 0 fbs 0 restecg 0 thalachh 0 exng 0 oldpeak 0 slp 0 caa 0 thall 0 output 0 dtype: int64 . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trtbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalachh 303 non-null int64 8 exng 303 non-null int64 9 oldpeak 303 non-null float64 10 slp 303 non-null int64 11 caa 303 non-null int64 12 thall 303 non-null int64 13 output 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . 글쓴이는 윗방식으로 null값 유무를 체크했습니다. . 그러나 df.info() 방식이 여러가지 정보를 같이 줘 더 효율적입니다. . df.describe() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | . mean 54.366337 | 0.683168 | 0.966997 | 131.623762 | 246.264026 | 0.148515 | 0.528053 | 149.646865 | 0.326733 | 1.039604 | 1.399340 | 0.729373 | 2.313531 | 0.544554 | . std 9.082101 | 0.466011 | 1.032052 | 17.538143 | 51.830751 | 0.356198 | 0.525860 | 22.905161 | 0.469794 | 1.161075 | 0.616226 | 1.022606 | 0.612277 | 0.498835 | . min 29.000000 | 0.000000 | 0.000000 | 94.000000 | 126.000000 | 0.000000 | 0.000000 | 71.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 47.500000 | 0.000000 | 0.000000 | 120.000000 | 211.000000 | 0.000000 | 0.000000 | 133.500000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 55.000000 | 1.000000 | 1.000000 | 130.000000 | 240.000000 | 0.000000 | 1.000000 | 153.000000 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 61.000000 | 1.000000 | 2.000000 | 140.000000 | 274.500000 | 0.000000 | 1.000000 | 166.000000 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 77.000000 | 1.000000 | 3.000000 | 200.000000 | 564.000000 | 1.000000 | 2.000000 | 202.000000 | 1.000000 | 6.200000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . 데이터를 보면 어느정도 스케일링이 필요하다는 것을 알 수 있습니다. . &#53945;&#49457; &#49828;&#52992;&#51068;&#47553; . df[&#39;age&#39;] = df[&#39;age&#39;]/max(df[&#39;age&#39;]) df[&#39;cp&#39;] = df[&#39;cp&#39;]/max(df[&#39;cp&#39;]) df[&#39;trtbps&#39;] = df[&#39;trtbps&#39;]/max(df[&#39;trtbps&#39;]) df[&#39;chol&#39;] = df[&#39;chol&#39;]/max(df[&#39;chol&#39;]) df[&#39;thalachh&#39;] = df[&#39;thalachh&#39;]/max(df[&#39;thalachh&#39;]) . df.describe() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | . mean 0.706056 | 0.683168 | 0.322332 | 0.658119 | 0.436638 | 0.148515 | 0.528053 | 0.740826 | 0.326733 | 1.039604 | 1.399340 | 0.729373 | 2.313531 | 0.544554 | . std 0.117949 | 0.466011 | 0.344017 | 0.087691 | 0.091898 | 0.356198 | 0.525860 | 0.113392 | 0.469794 | 1.161075 | 0.616226 | 1.022606 | 0.612277 | 0.498835 | . min 0.376623 | 0.000000 | 0.000000 | 0.470000 | 0.223404 | 0.000000 | 0.000000 | 0.351485 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 0.616883 | 0.000000 | 0.000000 | 0.600000 | 0.374113 | 0.000000 | 0.000000 | 0.660891 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 0.714286 | 1.000000 | 0.333333 | 0.650000 | 0.425532 | 0.000000 | 1.000000 | 0.757426 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 0.792208 | 1.000000 | 0.666667 | 0.700000 | 0.486702 | 0.000000 | 1.000000 | 0.821782 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 2.000000 | 1.000000 | 1.000000 | 6.200000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . 이전과 달리 특성 스케일이 확실히 비슷해졌습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . from sklearn.model_selection import train_test_split #splitting data into training data and testing data X_train, X_test, y_train, y_test = train_test_split( df.drop([&#39;output&#39;], axis=1), df.output, test_size= 0.2, # 20% test data &amp; 80% train data random_state=0, stratify=df.output ) . stratify 속성 =&gt; y값의 공평한 분배를 위해 사용하는 속성입니다. . from sklearn.linear_model import LogisticRegression clf = LogisticRegression() clf.fit(X_train, y_train) from sklearn.metrics import accuracy_score Y_pred = clf.predict(X_test) acc=accuracy_score(y_test, Y_pred) print(&#39;Accuracy is&#39;,round(acc,2)*100,&#39;%&#39;) . Accuracy is 89.0 % . 로지스틱 회귀 모형을 별다른 튜닝 없이 사용했습니다. . 정확도 측면에서만 보면 캐글에 있는 다른 코드와 별반 다르지 않습니다. . Verson 2. &#49900;&#54540;&#54620; &#46373;&#47084;&#45789; &#47784;&#54805; . &#45936;&#51060;&#53552; &#51060;&#54644;2 . df = data.copy() df.output.value_counts() . 1 165 0 138 Name: output, dtype: int64 . 이전 모델에서 생략(?)된 부분인거 같은데 1과 0 값의 비율이 조금 차이가 있습니다. . df.corr().abs()[&#39;output&#39;].sort_values(ascending = False) . output 1.000000 exng 0.436757 cp 0.433798 oldpeak 0.430696 thalachh 0.421741 caa 0.391724 slp 0.345877 thall 0.344029 sex 0.280937 age 0.225439 trtbps 0.144931 restecg 0.137230 chol 0.085239 fbs 0.028046 Name: output, dtype: float64 . Y값과의 상관계수가 어느정도 되는지 확인해보았습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553;2 . from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score X = df.drop(&#39;output&#39;, axis = 1) y = df[&#39;output&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) X_train.shape . (242, 13) . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 여기서는 StandardScaler를 사용해 스케일링을 했습니다. . 평균 0, 분산 1로 조정합니다. 이 스케일링은 이상치가 있을때 잘 작용하지 않을 수 있습니다. . from tensorflow import keras model = keras.Sequential( [ keras.layers.Dense( 256, activation=&quot;relu&quot;, input_shape=[13] ), keras.layers.Dense(515, activation=&quot;relu&quot;), keras.layers.Dropout(0.3), keras.layers.Dense(50, activation=&quot;relu&quot;), keras.layers.Dropout(0.3), keras.layers.Dense(1, activation=&quot;sigmoid&quot;), ] ) model.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_20 (Dense) (None, 256) 3584 _________________________________________________________________ dense_21 (Dense) (None, 515) 132355 _________________________________________________________________ dropout_10 (Dropout) (None, 515) 0 _________________________________________________________________ dense_22 (Dense) (None, 50) 25800 _________________________________________________________________ dropout_11 (Dropout) (None, 50) 0 _________________________________________________________________ dense_23 (Dense) (None, 1) 51 ================================================================= Total params: 161,790 Trainable params: 161,790 Non-trainable params: 0 _________________________________________________________________ . 활성화 함수로 제일 많이 사용하는 relu와 sigmoid함수를 사용했습니다. . relu함수 : 입력이 양수일 경우 그대로 반환, 음수일경우 0으로 만듭니다. . sigmoid함수 : 1 / (1 + e^z) 함수. 값을 0에서 1 사이로 변환합니다. . 첫번째 구간에 아웃풋 값을 256개 주었는데, 변수값이 13개임으로 모수가 14개입니다. . 그래서 256*14 = 3584개 파라미터가 나오게 된 것입니다. . 중간에 있는 드롭아웃은 일정 비율만큼 뉴런을 랜덤하게 꺼서 과대적합을 막는 역할을 합니다. . model.compile(optimizer = &#39;Adam&#39;, loss = &#39;binary_crossentropy&#39;, metrics = [&#39;binary_accuracy&#39;]) early_stopping = keras.callbacks.EarlyStopping( patience = 20, min_delta = 0.001, restore_best_weights =True ) history = model.fit( X_train, y_train, validation_data=(X_test, y_test), batch_size=15, epochs=50, callbacks = [early_stopping], verbose=1, ) . Epoch 1/50 17/17 [==============================] - 1s 15ms/step - loss: 0.5500 - binary_accuracy: 0.7190 - val_loss: 0.3814 - val_binary_accuracy: 0.8852 Epoch 2/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3823 - binary_accuracy: 0.8347 - val_loss: 0.3797 - val_binary_accuracy: 0.8852 Epoch 3/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3354 - binary_accuracy: 0.8719 - val_loss: 0.4391 - val_binary_accuracy: 0.8197 Epoch 4/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3017 - binary_accuracy: 0.8802 - val_loss: 0.4147 - val_binary_accuracy: 0.8689 Epoch 5/50 17/17 [==============================] - 0s 6ms/step - loss: 0.2589 - binary_accuracy: 0.9091 - val_loss: 0.4388 - val_binary_accuracy: 0.8689 Epoch 6/50 17/17 [==============================] - 0s 6ms/step - loss: 0.2579 - binary_accuracy: 0.9256 - val_loss: 0.4795 - val_binary_accuracy: 0.8525 Epoch 7/50 17/17 [==============================] - 0s 7ms/step - loss: 0.2019 - binary_accuracy: 0.9256 - val_loss: 0.4895 - val_binary_accuracy: 0.8689 Epoch 8/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1889 - binary_accuracy: 0.9298 - val_loss: 0.5359 - val_binary_accuracy: 0.8361 Epoch 9/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1887 - binary_accuracy: 0.9215 - val_loss: 0.5324 - val_binary_accuracy: 0.8525 Epoch 10/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1578 - binary_accuracy: 0.9545 - val_loss: 0.5441 - val_binary_accuracy: 0.8689 Epoch 11/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1686 - binary_accuracy: 0.9215 - val_loss: 0.6338 - val_binary_accuracy: 0.8689 Epoch 12/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1448 - binary_accuracy: 0.9504 - val_loss: 0.6872 - val_binary_accuracy: 0.8197 Epoch 13/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1065 - binary_accuracy: 0.9628 - val_loss: 0.7682 - val_binary_accuracy: 0.8197 Epoch 14/50 17/17 [==============================] - 0s 7ms/step - loss: 0.0879 - binary_accuracy: 0.9835 - val_loss: 0.8583 - val_binary_accuracy: 0.8197 Epoch 15/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0877 - binary_accuracy: 0.9711 - val_loss: 0.9300 - val_binary_accuracy: 0.8361 Epoch 16/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0688 - binary_accuracy: 0.9835 - val_loss: 0.9281 - val_binary_accuracy: 0.8361 Epoch 17/50 17/17 [==============================] - 0s 8ms/step - loss: 0.0615 - binary_accuracy: 0.9835 - val_loss: 0.9688 - val_binary_accuracy: 0.8361 Epoch 18/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0496 - binary_accuracy: 0.9835 - val_loss: 1.0818 - val_binary_accuracy: 0.8197 Epoch 19/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0915 - binary_accuracy: 0.9628 - val_loss: 1.3326 - val_binary_accuracy: 0.8525 Epoch 20/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0953 - binary_accuracy: 0.9669 - val_loss: 1.1602 - val_binary_accuracy: 0.8525 Epoch 21/50 17/17 [==============================] - 0s 7ms/step - loss: 0.0366 - binary_accuracy: 0.9959 - val_loss: 1.1617 - val_binary_accuracy: 0.8525 Epoch 22/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0407 - binary_accuracy: 0.9876 - val_loss: 1.2300 - val_binary_accuracy: 0.8361 . model.evaluate(X_test, y_test) . 2/2 [==============================] - 0s 7ms/step - loss: 0.3797 - binary_accuracy: 0.8852 . [0.3796648383140564, 0.8852459192276001] . predictions =(model.predict(X_test)&gt;0.5).astype(&quot;int32&quot;) from sklearn.metrics import classification_report, confusion_matrix, accuracy_score accuracy_score(y_test, predictions) . 0.8852459016393442 . 아까 결과와 비슷한 수치를 보입니다. . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.00 0.00 0.00 29 1 0.52 1.00 0.69 32 accuracy 0.52 61 macro avg 0.26 0.50 0.34 61 weighted avg 0.28 0.52 0.36 61 . classification_report 함수가 상당히 유용한 걸 알 수있습니다. . 한번에 정밀도, 재현율, f1-score 값 까지 보여줍니다. . &#45712;&#45184;&#51216; . 분류에 기본적인 로지스틱 회귀모형과 단순한 딥러닝 코드를 따라해봤습니다. . 특히 딥러닝 부분에 경우 정말 기본적인 것밖에 몰라 코드 해석에 시간이 많이 걸렸네요. . 여러가지로 코드를 만져가며 느낀점은 이번 데이터에 경우 스케일링이 많이 중요한 것 같습니다. . 스케일링 종류에 따라서 정확도 값이 크게 변하는 것을 관찰했습니다. . 특히 트리기반 부스팅 모델이 아니라 더 그런 것 같습니다. . 너무 복잡한 모델을 급하게 이해하기 보다, 이해할 수 있는 모델을 관찰하며 데이터 분석은 어떤 과정으로 하는가를 살펴봤습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/logistic/scale/keras/regression/2021/09/24/kagglessu2.html",
            "relUrl": "/ssuda/jupyter/kaggle/logistic/scale/keras/regression/2021/09/24/kagglessu2.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post52": {
            "title": "[머신러닝 가이드] 5-1 단순선형회귀",
            "content": ". &#44221;&#49324;&#54616;&#44053;&#48277; . import numpy as np import matplotlib.pyplot as plt %matplotlib inline np.random.seed(8) x = 2 * np.random.randn(100,1) y = 6 + 4 * x + np.random.randn(100,1) plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x7f6bcf5bb850&gt; . y = 4x + 6 근사 . np.random.randn =&gt; 표준정규분포에서 값 생성. 100,1 은 값 행렬 형식 선언 입니다. . def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y-y_pred)) / N return cost . 편차 제곱 평균을 계산해주는 함수. . np.square =&gt; 제곱 해주는 함수 . def get_weight_updates(w1, w0, x, y, learning_rate = 0.01): N = len(y) # w1, w0 동일한 행렬 크기를 갖는 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) #np.dot 행렬의 곱 y_pred = np.dot(x, w1.T) + w0 diff = y - y_pred w0_factors = np.ones((N, 1)) w1_update = -(2/N) * learning_rate * (np.dot(x.T, diff)) w0_update = -(2/N) * learning_rate * (np.dot(w0_factors.T, diff)) return w1_update, w0_update . 편미분한 w1, w0값을 이용해서 w0, w1값을 지속적으로 업데이트 해줍니다 . np.zeros_like(w1) =&gt; w1값과 같은 형태에 값은 0인 행렬 생성 . np.dot(,) =&gt; 행렬 연산 . def gradient_descent_steps(x,y, iters = 10000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, x, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . 위 두 함수를 통해 w1, w0 값을 지속적으로 업데이트 하여 최적에 값에 도달하게 합니다. . w1, w0 = gradient_descent_steps(x,y, 1000) print(&#39;w1 :&#39;, np.round(w1[0,0],4), &#39;w0 :&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱평균:&#39;, np.round(get_cost(y, y_pred),4)) . w1 : 3.9974 w0 : 5.9649 편차제곱평균: 1.1967 . plt.scatter(x,y) plt.plot(x,y_pred) . [&lt;matplotlib.lines.Line2D at 0x7f6bcf10f110&gt;] . 경사하강법을 이용해 회귀선이 잘 만들어졌습니다. . 다만 데이터에 개수가 100개보다 훨씬 많아지면 전체데이터로 계수를 업데이트 하지 못합니다. . 그 때문에 실전에서는 대부분 (미니배치)확률적 경사 하강법을 이용합니다. . 이 방식은 전체 데이터가 아닌 일부 데이터로 계수를 업데이트 하기 때문에 속도가 상대적으로 빠릅니다. . 이를 구현해보겠습니다. . def stochastic_gradient_descent_steps(x,y,batch_size = 10, iters = 1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index = 0 for ind in range(iters): np.random.seed(ind) stochastic_random_index = np.random.permutation(x.shape[0]) sample_x = x[stochastic_random_index[0:batch_size]] sample_y = y[stochastic_random_index[0:batch_size]] w1_update, w0_update = get_weight_updates(w1, w0, sample_x, sample_y) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . np.random.permutation(x.shape[0]) =&gt; 주어진 데이터를 셔플해서 출력함 . 앞 함수와 바뀐 부분은 x, y를 샘플링해서 넣는다는 점 입니다. . w1, w0 = stochastic_gradient_descent_steps(x,y,iters=1000) print(&#39;w1:&#39;, np.round(w1[0,0],3), &#39;w0:&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱 평균:&#39;, np.round(get_cost(y,y_pred),4)) . w1: 4.006 w0: 5.9135 편차제곱 평균: 1.1996 . 편차제곱 평균 값이 전체 x,y를 투입했을때와 큰 차이가 없습니다. . 그러므로 계산 속도가 훨씬 빠른 미니배치 경사하강법을 많이 사용합니다. . &#45800;&#49692; &#49440;&#54805; &#54924;&#44480;(&#48372;&#49828;&#53556; &#51452;&#53469; &#44032;&#44201;) . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;보스턴 데이터 세트 크기:&#39;, bostonDF.shape) bostonDF.head() . 보스턴 데이터 세트 크기: (506, 14) . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . 사이킷런에 내장되어있는 보스턴 주택 데이터를 불러왔습니다. . bostonDF.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 CRIM 506 non-null float64 1 ZN 506 non-null float64 2 INDUS 506 non-null float64 3 CHAS 506 non-null float64 4 NOX 506 non-null float64 5 RM 506 non-null float64 6 AGE 506 non-null float64 7 DIS 506 non-null float64 8 RAD 506 non-null float64 9 TAX 506 non-null float64 10 PTRATIO 506 non-null float64 11 B 506 non-null float64 12 LSTAT 506 non-null float64 13 PRICE 506 non-null float64 dtypes: float64(14) memory usage: 55.5 KB . 결측값은 없으며 모든 피처가 float 형 입니다. . fig, axs = plt.subplots(figsize=(16,8), ncols = 4, nrows = 2) lm_features = [&#39;RM&#39;,&#39;ZN&#39;, &#39;INDUS&#39;,&#39;NOX&#39;,&#39;AGE&#39;,&#39;PTRATIO&#39;,&#39;LSTAT&#39;,&#39;RAD&#39;] for i, feature in enumerate(lm_features): row = int(i/4) col = i%4 sns.regplot(x=feature, y=&#39;PRICE&#39;, data=bostonDF, ax=axs[row][col]) . sns.regplot(x,y) =&gt; x,y 산점도와 함께 회귀직선을 그려줌. . plt.subplots(ncols = , nrows= ) 여러개의 그림을 그릴 수 있게 해줌. . RM과 LSTAT 변수가 가장 PRICE 변수와 연관성이 있어보입니다. . from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) x_train, x_test, y_train, y_test = train_test_split(x_data, y_target, test_size = 0.3, random_state = 156) lr = LinearRegression() lr.fit(x_train, y_train) y_preds = lr.predict(x_test) mse = mean_squared_error(y_test, y_preds) rmse = np.sqrt(mse) print(&#39;mse :&#39;, np.round(mse,4), &#39;, rmse :&#39;, np.round(rmse, 4)) print(&#39;결정계수:&#39;, np.round(r2_score(y_test, y_preds), 4)) . mse : 17.2969 , rmse : 4.159 결정계수: 0.7572 . 모델을 어느정도 설명해 준 모습입니다. . print(&#39;절편 값:&#39;,lr.intercept_) print(&#39;회귀 계수값:&#39;, np.round(lr.coef_,1)) . 절편 값: 40.995595172164755 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] . coeff = pd.Series(data=np.round(lr.coef_, 1), index = x_data.columns) coeff.sort_values(ascending=False) . RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 B 0.0 TAX -0.0 AGE 0.0 INDUS 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 . 변수 이름과 추정 회귀 계수를 맵핑 시킨 모습입니다. . NOX 변수의 계수 값이 크게 작아보입니다. . from sklearn.model_selection import cross_val_score neg_mse_scores = cross_val_score(lr, x_data, y_target, scoring=&#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-12.46 -26.05 -33.07 -80.76 -33.31] rmse scores [3.53 5.1 5.75 8.99 5.77] 평균 rmse score: 5.83 . 5개의 폴드 세트를 이용한 교차검증 입니다. . scoring = &#39;neg_mean_squared_error&#39; 같은 경우 보통 모델 평가를 위한 값이 커야 좋은 값인데, mse 값은 작아야 좋습니다. . 그러므로 음수를 붙여서 보정해준다고 생각하면 좋습니다. . 다음에는 다항회귀, 릿지/라쏘 회귀 부분을 공부하겠습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/math/regression/2021/09/20/PythonMachine5_1.html",
            "relUrl": "/book/jupyter/guide/math/regression/2021/09/20/PythonMachine5_1.html",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post53": {
            "title": "[SSUDA] 주택 가격 예측",
            "content": ". https://www.kaggle.com/c/house-prices-advanced-regression-techniques . 캐글에 있는 주택 가격 예측 데이터 분석입니다. . 부스팅 모델들이 튜닝하는데 시간이 걸리기 때문에 좀 더 간단한 선형 회귀 모델을 사용하겠습니다. . 분류 관련 공부를 조금 해본 경험으로, 회귀에 기본인 선형 회귀모델을 이번 데이터를 이용해 공부해보겠습니다. . 이번 분석에 핵심 포인트는 숫자 변수 대부분이 치우쳐 있으므로 숫자 변수를 log_transform하는 것입니다. . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; &#48143; &#46168;&#47084;&#48372;&#44592; . import pandas as pd import numpy as np import seaborn as sns import matplotlib import matplotlib.pyplot as plt from scipy.stats import skew from scipy.stats.stats import pearsonr %config InlineBackend.figure_format = &#39;retina&#39; #set &#39;png&#39; here when working on notebook %matplotlib inline . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . train = pd.read_csv(&quot;/content/drive/MyDrive/house/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/house/test.csv&quot;) . train.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating ... CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | ... | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | ... | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | ... | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | ... | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | ... | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 81 columns): # Column Non-Null Count Dtype -- -- 0 Id 1460 non-null int64 1 MSSubClass 1460 non-null int64 2 MSZoning 1460 non-null object 3 LotFrontage 1201 non-null float64 4 LotArea 1460 non-null int64 5 Street 1460 non-null object 6 Alley 91 non-null object 7 LotShape 1460 non-null object 8 LandContour 1460 non-null object 9 Utilities 1460 non-null object 10 LotConfig 1460 non-null object 11 LandSlope 1460 non-null object 12 Neighborhood 1460 non-null object 13 Condition1 1460 non-null object 14 Condition2 1460 non-null object 15 BldgType 1460 non-null object 16 HouseStyle 1460 non-null object 17 OverallQual 1460 non-null int64 18 OverallCond 1460 non-null int64 19 YearBuilt 1460 non-null int64 20 YearRemodAdd 1460 non-null int64 21 RoofStyle 1460 non-null object 22 RoofMatl 1460 non-null object 23 Exterior1st 1460 non-null object 24 Exterior2nd 1460 non-null object 25 MasVnrType 1452 non-null object 26 MasVnrArea 1452 non-null float64 27 ExterQual 1460 non-null object 28 ExterCond 1460 non-null object 29 Foundation 1460 non-null object 30 BsmtQual 1423 non-null object 31 BsmtCond 1423 non-null object 32 BsmtExposure 1422 non-null object 33 BsmtFinType1 1423 non-null object 34 BsmtFinSF1 1460 non-null int64 35 BsmtFinType2 1422 non-null object 36 BsmtFinSF2 1460 non-null int64 37 BsmtUnfSF 1460 non-null int64 38 TotalBsmtSF 1460 non-null int64 39 Heating 1460 non-null object 40 HeatingQC 1460 non-null object 41 CentralAir 1460 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1460 non-null int64 44 2ndFlrSF 1460 non-null int64 45 LowQualFinSF 1460 non-null int64 46 GrLivArea 1460 non-null int64 47 BsmtFullBath 1460 non-null int64 48 BsmtHalfBath 1460 non-null int64 49 FullBath 1460 non-null int64 50 HalfBath 1460 non-null int64 51 BedroomAbvGr 1460 non-null int64 52 KitchenAbvGr 1460 non-null int64 53 KitchenQual 1460 non-null object 54 TotRmsAbvGrd 1460 non-null int64 55 Functional 1460 non-null object 56 Fireplaces 1460 non-null int64 57 FireplaceQu 770 non-null object 58 GarageType 1379 non-null object 59 GarageYrBlt 1379 non-null float64 60 GarageFinish 1379 non-null object 61 GarageCars 1460 non-null int64 62 GarageArea 1460 non-null int64 63 GarageQual 1379 non-null object 64 GarageCond 1379 non-null object 65 PavedDrive 1460 non-null object 66 WoodDeckSF 1460 non-null int64 67 OpenPorchSF 1460 non-null int64 68 EnclosedPorch 1460 non-null int64 69 3SsnPorch 1460 non-null int64 70 ScreenPorch 1460 non-null int64 71 PoolArea 1460 non-null int64 72 PoolQC 7 non-null object 73 Fence 281 non-null object 74 MiscFeature 54 non-null object 75 MiscVal 1460 non-null int64 76 MoSold 1460 non-null int64 77 YrSold 1460 non-null int64 78 SaleType 1460 non-null object 79 SaleCondition 1460 non-null object 80 SalePrice 1460 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 924.0+ KB . all_data = pd.concat((train.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;], test.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;])) . id(고유번호)와 설명변수를 뺀 나머지 변수들을 전처리를 위해 all_data 변수로 합쳐주었습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 이 코드의 데이터 전처리는 화려하지 않습니다. 기본에 충실합니다. . 다음 3가지로 요약할 수 있습니다. . 로그(기능 + 1)를 사용하여 오른쪽으로 꼬리가 긴 그래프를 변환합니다. 그러면 어느정도 정규화됩니다. | 범주형 형상에 대한 더미 변수 생성 | 숫자 결측값(NaN)을 각 열의 평균으로 바꾸기 | 설명변수를 로그변환 해보기 . matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 6.0) prices = pd.DataFrame({&quot;price&quot;:train[&quot;SalePrice&quot;], &quot;log(price + 1)&quot;:np.log1p(train[&quot;SalePrice&quot;])}) prices.hist() . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1089890&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1052b10&gt;]], dtype=object) . 로그변환 전 우측 꼬리가 두터운 느낌이였는데 잘 정규화 된 모습입니다. . all_data.dtypes . MSSubClass int64 MSZoning object LotFrontage float64 LotArea int64 Street object ... MiscVal int64 MoSold int64 YrSold int64 SaleType object SaleCondition object Length: 79, dtype: object . train[&quot;SalePrice&quot;] = np.log1p(train[&quot;SalePrice&quot;]) numeric_feats = all_data.dtypes[all_data.dtypes != &quot;object&quot;].index . all_data.dtypes =&gt; 데이터 타입 나열. 여기서 인덱스는 변수이름이기 때문에 이런 방식으로 쉽게 추출. . skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) skewed_feats = skewed_feats[skewed_feats &gt; 0.75] skewed_feats = skewed_feats.index all_data[skewed_feats] = np.log1p(all_data[skewed_feats]) . shew = 왜도 값을 나타네는 함수. 왜도란 그래프가 비 대칭적인 모양인 것 . shew값이 큰 양수값이면 오른쪽으로 긴 꼬리를 가지는 분포를 가집니다. . 그러므로 shew값을 기준으로 로그변환을 할 변수를 찾을 수 있습니다. . 참고로 apply 함수는 파이썬 데이터 프레임에 적용하는 함수인데, 원하는 함수를 적용하고 싶을때 사용합니다. . 이때 apply 기본인자는 axis = 0이므로 열을 기준으로 함수를 적용합니다. . all_data = pd.get_dummies(all_data) all_data.head(5) . MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold MSZoning_C (all) MSZoning_FV MSZoning_RH MSZoning_RL ... GarageFinish_Unf GarageQual_Ex GarageQual_Fa GarageQual_Gd GarageQual_Po GarageQual_TA GarageCond_Ex GarageCond_Fa GarageCond_Gd GarageCond_Po GarageCond_TA PavedDrive_N PavedDrive_P PavedDrive_Y PoolQC_Ex PoolQC_Fa PoolQC_Gd Fence_GdPrv Fence_GdWo Fence_MnPrv Fence_MnWw MiscFeature_Gar2 MiscFeature_Othr MiscFeature_Shed MiscFeature_TenC SaleType_COD SaleType_CWD SaleType_Con SaleType_ConLD SaleType_ConLI SaleType_ConLw SaleType_New SaleType_Oth SaleType_WD SaleCondition_Abnorml SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Family SaleCondition_Normal SaleCondition_Partial . 0 4.110874 | 4.189655 | 9.042040 | 7 | 5 | 2003 | 2003 | 5.283204 | 6.561031 | 0.0 | 5.017280 | 6.753438 | 6.753438 | 6.751101 | 0.0 | 7.444833 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 8 | 0 | 2003.0 | 2.0 | 548.0 | 0.000000 | 4.127134 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 1 3.044522 | 4.394449 | 9.169623 | 6 | 8 | 1976 | 1976 | 0.000000 | 6.886532 | 0.0 | 5.652489 | 7.141245 | 7.141245 | 0.000000 | 0.0 | 7.141245 | 0.0 | 0.693147 | 2 | 0 | 3 | 0.693147 | 6 | 1 | 1976.0 | 2.0 | 460.0 | 5.700444 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 5 | 2007 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 4.110874 | 4.234107 | 9.328212 | 7 | 5 | 2001 | 2002 | 5.093750 | 6.188264 | 0.0 | 6.075346 | 6.825460 | 6.825460 | 6.765039 | 0.0 | 7.488294 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 6 | 1 | 2001.0 | 2.0 | 608.0 | 0.000000 | 3.761200 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 9 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 3 4.262680 | 4.110874 | 9.164401 | 7 | 5 | 1915 | 1970 | 0.000000 | 5.379897 | 0.0 | 6.293419 | 6.629363 | 6.869014 | 6.629363 | 0.0 | 7.448916 | 1.0 | 0.000000 | 1 | 0 | 3 | 0.693147 | 7 | 1 | 1998.0 | 3.0 | 642.0 | 0.000000 | 3.583519 | 5.609472 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2006 | 0 | 0 | 0 | 1 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 4 4.110874 | 4.442651 | 9.565284 | 8 | 5 | 2000 | 2000 | 5.860786 | 6.486161 | 0.0 | 6.196444 | 7.044033 | 7.044033 | 6.960348 | 0.0 | 7.695758 | 1.0 | 0.000000 | 2 | 1 | 4 | 0.693147 | 9 | 1 | 2000.0 | 3.0 | 836.0 | 5.262690 | 4.442651 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 12 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 288 columns . get_dummies 함수로 모든 object형 값이 원핫인코딩 됐습니다. . 저번에 프로젝트 할 때 변수를 하나하나 입력했던 것이 생각나는데 더 편한 방식을 알게 되었습니다. . all_data = all_data.fillna(all_data.mean()) . 결측값이 있을때 각 열의 평균값으로 대체하는 일반적인 방식입니다. . 윗 코드와 마찬가지로 저번 프로젝트에서 열마다 함수를 돌려 사용했는데 더 편한 방식을 알게 됐습니다. . X_train = all_data[:train.shape[0]] X_test = all_data[train.shape[0]:] y = train.SalePrice . 저번 프로젝트에서 트레인, 테스트 데이터에 각각 전처리를 적용했습니다. . 하지만 이 방법처럼 all_data로 묶고 한번에 전처리 하는 방식이 깔끔한 것 같습니다. . &#47551;&#51648; &#47784;&#45944; . 선형 회귀 모델 적합을 하겠습니다. . 이때 라쏘, 릿지 방법을 모두 사용해서 최적의 rmse 값을 찾겠습니다. . from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso from sklearn.model_selection import cross_val_score def rmse_cv(model): rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)) return(rmse) . cross_val_score 함수는 교차 검증 후 정확도를 리스트로 보여줍니다. . 여기서 cv = 5 이기 때문에 5-fold로 교차검증 하게 됩니다. . model_ridge = Ridge() . 릿지 모델의 주요 파라미터는 알파입니다. . 알파값이 높아지면 규제가 심해지고 과적합을 방지해줍니다. . 다만 너무 많이 높아지면 과소적합이 되기 때문에 적절한 값을 찾아야합니다. . alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas] . 다양한 알파값을 릿지 함수에 적용시켰습니다. . 여기서 [값 for alpha in alphas] 는 for루프를 리스트 내에서 돌리는 것 입니다. . cv_ridge = pd.Series(cv_ridge, index = alphas) cv_ridge.plot(title = &quot;Validation - Just Do It&quot;) plt.xlabel(&quot;alpha&quot;) plt.ylabel(&quot;rmse&quot;) . Text(0, 0.5, &#39;rmse&#39;) . 시리즈에 plot를 하면 그래프가 생깁니다. . 이때 x축은 인덱스, y축은 본 값이 들어갑니다. . 알파값이 10일때 rmse값이 최소로, 알파는 10을 쓰는 것이 좋겠습니다. . 보통 규제하는 변수와 예측도를 측정하는 값간에 그래프는 U자형태가 잘 나옵니다. . 그 이유는 규제가 약할때와 쌜 때 각각 과소적합, 과적합이 일어나 예측도를 측정하는 값이 커지기 때문입니다. . cv_ridge.min() . 0.1273373466867076 . 최적의 rmse값은 0.1273입니다. . &#46972;&#50136; &#47784;&#45944; . 이번엔 라쏘 모델입니다. . 라쏘 모델은 릿지 모델과 다르게 영향력이 작은 변수의 계수를 0으로 만듭니다. . 변수 선택 과정까지 한번에 할 수 있다는 것이 장점입니다. . model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y) . LassoCV 함수로 여러가지 알파값을 동시에 검정할 수 있습니다. . model_lasso.alpha_ . 0.0005 . rmse_cv(model_lasso).mean() . 0.12256735885048142 . 라쏘 모델이 rmse 값이 훨씬 낮아서 좋습니다. . 라쏘 모델을 사용하겠습니다. . coef = pd.Series(model_lasso.coef_, index = X_train.columns) . 회귀 모델.coef_ =&gt; 계수를 컬럼순으로 보여줍니다. . print(&quot;Lasso picked &quot; + str(sum(coef != 0)) + &quot; variables and eliminated the other &quot; + str(sum(coef == 0)) + &quot; variables&quot;) . Lasso picked 110 variables and eliminated the other 178 variables . 110개 변수는 선택되었고 178개 변수는 계수가 0, 즉 선택하지 않은 변수들입니다. . coef . MSSubClass -0.007480 LotFrontage 0.000000 LotArea 0.071826 OverallQual 0.053160 OverallCond 0.043027 ... SaleCondition_AdjLand 0.000000 SaleCondition_Alloca -0.000000 SaleCondition_Family -0.007925 SaleCondition_Normal 0.019666 SaleCondition_Partial 0.000000 Length: 288, dtype: float64 . imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)]) matplotlib.rcParams[&#39;figure.figsize&#39;] = (8.0, 10.0) imp_coef.plot(kind = &quot;barh&quot;) plt.title(&quot;Coefficients in the Lasso Model&quot;) . Text(0.5, 1.0, &#39;Coefficients in the Lasso Model&#39;) . sort_values() 함수는 범주형 변수의 히스토그램을 아는데 유용한 함수입니다. . 여기선 정렬기능으로 사용했는데, 정렬기능으로도 충분히 우수한 것을 보여줬습니다. . 정렬된 값 상위 10개, 하위 10개를 시각화했는데, 이 변수들이 핵심 변수입니다. . 왜냐하면 계수의 절대값이 큰 값이기 때문입니다. . 양의 값으로 가장 큰 GrLivArea변수는 면적으로 주택가격에 당연히 큰 영향을 끼칩니다. . matplotlib.rcParams[&#39;figure.figsize&#39;] = (6.0, 6.0) preds = pd.DataFrame({&quot;preds&quot;:model_lasso.predict(X_train), &quot;true&quot;:y}) preds[&quot;residuals&quot;] = preds[&quot;true&quot;] - preds[&quot;preds&quot;] preds.plot(x = &quot;preds&quot;, y = &quot;residuals&quot;,kind = &quot;scatter&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3bd2529490&gt; . 잔차 그림도 큰 이상이 없습니다. . model_lasso = Lasso(alpha = 0.0005).fit(X_train, y) pred = model_lasso.predict(X_test) pred2 = np.exp(pred) - 1 X_test[&#39;SalePrice&#39;] = pred2 X_test[&#39;Id&#39;] = test[&#39;Id&#39;] final = X_test[[&#39;Id&#39;,&#39;SalePrice&#39;]] final.to_csv(&#39;/content/drive/MyDrive/houselasso2.csv&#39;,encoding=&#39;UTF-8&#39;, index=False) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . 알파값 0.0005인 라쏘 모델로 모델을 적합시키고 그 모델로 예측 파일을 만들었습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/ridge/lasso/regression/2021/09/15/kagglessu1.html",
            "relUrl": "/ssuda/jupyter/kaggle/ridge/lasso/regression/2021/09/15/kagglessu1.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post54": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ksy1526.github.io/myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post55": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ksy1526.github.io/myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "안녕하세요. 김성연이라고 합니다. . 저는 현재 통계학과 재학중이며 머신러닝/딥러닝 관련 공부하고 있습니다. . 이 블로그는 제가 하는 공부를 기록하는 곳인데요. 방문하시는 분에게도 도움이 되었으면 좋겠습니다. . 구체적으로 캐글 내 인기 코드를 제 시각에 맞추서 리뷰하거나, 머신러닝/딥러닝 책 예제 코드를 따라하며 공부하고 있습니다. . SSUDA는 ‘우리들의 데이터 수다’ 모임으로 교내 소모임입니다. 주로 캐글 코드 리뷰를 업로드 합니다. . 머신러닝 가이드는 ‘파이썬 머신러닝 완벽가이드’ 교제 내 예제 코드를 따라한 것을 업로드 합니다. (https://book.naver.com/bookdb/book_detail.nhn?bid=16238302) . Do it 자연어는 ‘Do it! BERT와 GPT로 배우는 자연어 처리’ 교제 내 예제 코드를 따라한 것을 업로드 합니다. (https://book.naver.com/bookdb/book_detail.nhn?bid=16238302) . 시계열분석은 ‘실전 시계열 분석’ 교제 내 예제 코드를 따라한 것을 업로드 합니다. (https://book.naver.com/bookdb/book_detail.naver?bid=18563552) . DACON, Kaggle은 제가 데이콘이나 캐글 대회에 실제로 참가할때 사용한 코드를 업로드 합니다. . 혹시 코드가 이상하거나 질문 있으시면, 댓글이나 제 메일(zan05134@naver.com) 으로 연락 부탁드립니다. . 감사합니다. .",
          "url": "https://ksy1526.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  

  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ksy1526.github.io/myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}