{
  
    
        "post0": {
            "title": "Regularized Linear Models",
            "content": ". https://www.kaggle.com/c/house-prices-advanced-regression-techniques . 캐글에 있는 주택 가격 예측 데이터 분석입니다. . 부스팅 모델들이 튜닝하는데 시간이 걸리기 때문에 좀 더 간단한 선형 회귀 모델을 사용하겠습니다. . 분류 관련 공부를 조금 해본 경험으로, 회귀에 기본인 선형 회귀모델을 이번 데이터를 이용해 공부해보겠습니다. . 이번 분석에 핵심 포인트는 숫자 변수 대부분이 치우쳐 있으므로 숫자 변수를 log_transform하는 것입니다. . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; &#48143; &#46168;&#47084;&#48372;&#44592; . import pandas as pd import numpy as np import seaborn as sns import matplotlib import matplotlib.pyplot as plt from scipy.stats import skew from scipy.stats.stats import pearsonr %config InlineBackend.figure_format = &#39;retina&#39; #set &#39;png&#39; here when working on notebook %matplotlib inline . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . train = pd.read_csv(&quot;/content/drive/MyDrive/house/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/house/test.csv&quot;) . train.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating ... CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | ... | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | ... | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | ... | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | ... | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | ... | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 81 columns): # Column Non-Null Count Dtype -- -- 0 Id 1460 non-null int64 1 MSSubClass 1460 non-null int64 2 MSZoning 1460 non-null object 3 LotFrontage 1201 non-null float64 4 LotArea 1460 non-null int64 5 Street 1460 non-null object 6 Alley 91 non-null object 7 LotShape 1460 non-null object 8 LandContour 1460 non-null object 9 Utilities 1460 non-null object 10 LotConfig 1460 non-null object 11 LandSlope 1460 non-null object 12 Neighborhood 1460 non-null object 13 Condition1 1460 non-null object 14 Condition2 1460 non-null object 15 BldgType 1460 non-null object 16 HouseStyle 1460 non-null object 17 OverallQual 1460 non-null int64 18 OverallCond 1460 non-null int64 19 YearBuilt 1460 non-null int64 20 YearRemodAdd 1460 non-null int64 21 RoofStyle 1460 non-null object 22 RoofMatl 1460 non-null object 23 Exterior1st 1460 non-null object 24 Exterior2nd 1460 non-null object 25 MasVnrType 1452 non-null object 26 MasVnrArea 1452 non-null float64 27 ExterQual 1460 non-null object 28 ExterCond 1460 non-null object 29 Foundation 1460 non-null object 30 BsmtQual 1423 non-null object 31 BsmtCond 1423 non-null object 32 BsmtExposure 1422 non-null object 33 BsmtFinType1 1423 non-null object 34 BsmtFinSF1 1460 non-null int64 35 BsmtFinType2 1422 non-null object 36 BsmtFinSF2 1460 non-null int64 37 BsmtUnfSF 1460 non-null int64 38 TotalBsmtSF 1460 non-null int64 39 Heating 1460 non-null object 40 HeatingQC 1460 non-null object 41 CentralAir 1460 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1460 non-null int64 44 2ndFlrSF 1460 non-null int64 45 LowQualFinSF 1460 non-null int64 46 GrLivArea 1460 non-null int64 47 BsmtFullBath 1460 non-null int64 48 BsmtHalfBath 1460 non-null int64 49 FullBath 1460 non-null int64 50 HalfBath 1460 non-null int64 51 BedroomAbvGr 1460 non-null int64 52 KitchenAbvGr 1460 non-null int64 53 KitchenQual 1460 non-null object 54 TotRmsAbvGrd 1460 non-null int64 55 Functional 1460 non-null object 56 Fireplaces 1460 non-null int64 57 FireplaceQu 770 non-null object 58 GarageType 1379 non-null object 59 GarageYrBlt 1379 non-null float64 60 GarageFinish 1379 non-null object 61 GarageCars 1460 non-null int64 62 GarageArea 1460 non-null int64 63 GarageQual 1379 non-null object 64 GarageCond 1379 non-null object 65 PavedDrive 1460 non-null object 66 WoodDeckSF 1460 non-null int64 67 OpenPorchSF 1460 non-null int64 68 EnclosedPorch 1460 non-null int64 69 3SsnPorch 1460 non-null int64 70 ScreenPorch 1460 non-null int64 71 PoolArea 1460 non-null int64 72 PoolQC 7 non-null object 73 Fence 281 non-null object 74 MiscFeature 54 non-null object 75 MiscVal 1460 non-null int64 76 MoSold 1460 non-null int64 77 YrSold 1460 non-null int64 78 SaleType 1460 non-null object 79 SaleCondition 1460 non-null object 80 SalePrice 1460 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 924.0+ KB . all_data = pd.concat((train.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;], test.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;])) . id(고유번호)와 설명변수를 뺀 나머지 변수들을 전처리를 위해 all_data 변수로 합쳐주었습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 이 코드의 데이터 전처리는 화려하지 않습니다. 기본에 충실합니다. . 다음 3가지로 요약할 수 있습니다. . 로그(기능 + 1)를 사용하여 오른쪽으로 꼬리가 긴 그래프를 변환합니다. 그러면 어느정도 정규화됩니다. | 범주형 형상에 대한 더미 변수 생성 | 숫자 결측값(NaN)을 각 열의 평균으로 바꾸기 | 설명변수를 로그변환 해보기 . matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 6.0) prices = pd.DataFrame({&quot;price&quot;:train[&quot;SalePrice&quot;], &quot;log(price + 1)&quot;:np.log1p(train[&quot;SalePrice&quot;])}) prices.hist() . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1089890&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1052b10&gt;]], dtype=object) . 로그변환 전 우측 꼬리가 두터운 느낌이였는데 잘 정규화 된 모습입니다. . all_data.dtypes . MSSubClass int64 MSZoning object LotFrontage float64 LotArea int64 Street object ... MiscVal int64 MoSold int64 YrSold int64 SaleType object SaleCondition object Length: 79, dtype: object . train[&quot;SalePrice&quot;] = np.log1p(train[&quot;SalePrice&quot;]) numeric_feats = all_data.dtypes[all_data.dtypes != &quot;object&quot;].index . all_data.dtypes =&gt; 데이터 타입 나열. 여기서 인덱스는 변수이름이기 때문에 이런 방식으로 쉽게 추출. . skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) skewed_feats = skewed_feats[skewed_feats &gt; 0.75] skewed_feats = skewed_feats.index all_data[skewed_feats] = np.log1p(all_data[skewed_feats]) . shew = 왜도 값을 나타네는 함수. 왜도란 그래프가 비 대칭적인 모양인 것 . shew값이 큰 양수값이면 오른쪽으로 긴 꼬리를 가지는 분포를 가집니다. . 그러므로 shew값을 기준으로 로그변환을 할 변수를 찾을 수 있습니다. . 참고로 apply 함수는 파이썬 데이터 프레임에 적용하는 함수인데, 원하는 함수를 적용하고 싶을때 사용합니다. . 이때 apply 기본인자는 axis = 0이므로 열을 기준으로 함수를 적용합니다. . all_data = pd.get_dummies(all_data) all_data.head(5) . MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold MSZoning_C (all) MSZoning_FV MSZoning_RH MSZoning_RL ... GarageFinish_Unf GarageQual_Ex GarageQual_Fa GarageQual_Gd GarageQual_Po GarageQual_TA GarageCond_Ex GarageCond_Fa GarageCond_Gd GarageCond_Po GarageCond_TA PavedDrive_N PavedDrive_P PavedDrive_Y PoolQC_Ex PoolQC_Fa PoolQC_Gd Fence_GdPrv Fence_GdWo Fence_MnPrv Fence_MnWw MiscFeature_Gar2 MiscFeature_Othr MiscFeature_Shed MiscFeature_TenC SaleType_COD SaleType_CWD SaleType_Con SaleType_ConLD SaleType_ConLI SaleType_ConLw SaleType_New SaleType_Oth SaleType_WD SaleCondition_Abnorml SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Family SaleCondition_Normal SaleCondition_Partial . 0 4.110874 | 4.189655 | 9.042040 | 7 | 5 | 2003 | 2003 | 5.283204 | 6.561031 | 0.0 | 5.017280 | 6.753438 | 6.753438 | 6.751101 | 0.0 | 7.444833 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 8 | 0 | 2003.0 | 2.0 | 548.0 | 0.000000 | 4.127134 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 1 3.044522 | 4.394449 | 9.169623 | 6 | 8 | 1976 | 1976 | 0.000000 | 6.886532 | 0.0 | 5.652489 | 7.141245 | 7.141245 | 0.000000 | 0.0 | 7.141245 | 0.0 | 0.693147 | 2 | 0 | 3 | 0.693147 | 6 | 1 | 1976.0 | 2.0 | 460.0 | 5.700444 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 5 | 2007 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 4.110874 | 4.234107 | 9.328212 | 7 | 5 | 2001 | 2002 | 5.093750 | 6.188264 | 0.0 | 6.075346 | 6.825460 | 6.825460 | 6.765039 | 0.0 | 7.488294 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 6 | 1 | 2001.0 | 2.0 | 608.0 | 0.000000 | 3.761200 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 9 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 3 4.262680 | 4.110874 | 9.164401 | 7 | 5 | 1915 | 1970 | 0.000000 | 5.379897 | 0.0 | 6.293419 | 6.629363 | 6.869014 | 6.629363 | 0.0 | 7.448916 | 1.0 | 0.000000 | 1 | 0 | 3 | 0.693147 | 7 | 1 | 1998.0 | 3.0 | 642.0 | 0.000000 | 3.583519 | 5.609472 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2006 | 0 | 0 | 0 | 1 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 4 4.110874 | 4.442651 | 9.565284 | 8 | 5 | 2000 | 2000 | 5.860786 | 6.486161 | 0.0 | 6.196444 | 7.044033 | 7.044033 | 6.960348 | 0.0 | 7.695758 | 1.0 | 0.000000 | 2 | 1 | 4 | 0.693147 | 9 | 1 | 2000.0 | 3.0 | 836.0 | 5.262690 | 4.442651 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 12 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 288 columns . get_dummies 함수로 모든 object형 값이 원핫인코딩 됐습니다. . 저번에 프로젝트 할 때 변수를 하나하나 입력했던 것이 생각나는데 더 편한 방식을 알게 되었습니다. . all_data = all_data.fillna(all_data.mean()) . 결측값이 있을때 각 열의 평균값으로 대체하는 일반적인 방식입니다. . 윗 코드와 마찬가지로 저번 프로젝트에서 열마다 함수를 돌려 사용했는데 더 편한 방식을 알게 됐습니다. . X_train = all_data[:train.shape[0]] X_test = all_data[train.shape[0]:] y = train.SalePrice . 저번 프로젝트에서 트레인, 테스트 데이터에 각각 전처리를 적용했습니다. . 하지만 이 방법처럼 all_data로 묶고 한번에 전처리 하는 방식이 깔끔한 것 같습니다. . &#47551;&#51648; &#47784;&#45944; . 선형 회귀 모델 적합을 하겠습니다. . 이때 라쏘, 릿지 방법을 모두 사용해서 최적의 rmse 값을 찾겠습니다. . from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso from sklearn.model_selection import cross_val_score def rmse_cv(model): rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)) return(rmse) . cross_val_score 함수는 교차 검증 후 정확도를 리스트로 보여줍니다. . 여기서 cv = 5 이기 때문에 5-fold로 교차검증 하게 됩니다. . model_ridge = Ridge() . 릿지 모델의 주요 파라미터는 알파입니다. . 알파값이 높아지면 규제가 심해지고 과적합을 방지해줍니다. . 다만 너무 많이 높아지면 과소적합이 되기 때문에 적절한 값을 찾아야합니다. . alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas] . 다양한 알파값을 릿지 함수에 적용시켰습니다. . 여기서 [값 for alpha in alphas] 는 for루프를 리스트 내에서 돌리는 것 입니다. . cv_ridge = pd.Series(cv_ridge, index = alphas) cv_ridge.plot(title = &quot;Validation - Just Do It&quot;) plt.xlabel(&quot;alpha&quot;) plt.ylabel(&quot;rmse&quot;) . Text(0, 0.5, &#39;rmse&#39;) . 시리즈에 plot를 하면 그래프가 생깁니다. . 이때 x축은 인덱스, y축은 본 값이 들어갑니다. . 알파값이 10일때 rmse값이 최소로, 알파는 10을 쓰는 것이 좋겠습니다. . 보통 규제하는 변수와 예측도를 측정하는 값간에 그래프는 U자형태가 잘 나옵니다. . 그 이유는 규제가 약할때와 쌜 때 각각 과소적합, 과적합이 일어나 예측도를 측정하는 값이 커지기 때문입니다. . cv_ridge.min() . 0.1273373466867076 . 최적의 rmse값은 0.1273입니다. . &#46972;&#50136; &#47784;&#45944; . 이번엔 라쏘 모델입니다. . 라쏘 모델은 릿지 모델과 다르게 영향력이 작은 변수의 계수를 0으로 만듭니다. . 변수 선택 과정까지 한번에 할 수 있다는 것이 장점입니다. . model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y) . LassoCV 함수로 여러가지 알파값을 동시에 검정할 수 있습니다. . model_lasso.alpha_ . 0.0005 . rmse_cv(model_lasso).mean() . 0.12256735885048142 . 라쏘 모델이 rmse 값이 훨씬 낮아서 좋습니다. . 라쏘 모델을 사용하겠습니다. . coef = pd.Series(model_lasso.coef_, index = X_train.columns) . 회귀 모델.coef_ =&gt; 계수를 컬럼순으로 보여줍니다. . print(&quot;Lasso picked &quot; + str(sum(coef != 0)) + &quot; variables and eliminated the other &quot; + str(sum(coef == 0)) + &quot; variables&quot;) . Lasso picked 110 variables and eliminated the other 178 variables . 110개 변수는 선택되었고 178개 변수는 계수가 0, 즉 선택하지 않은 변수들입니다. . coef . MSSubClass -0.007480 LotFrontage 0.000000 LotArea 0.071826 OverallQual 0.053160 OverallCond 0.043027 ... SaleCondition_AdjLand 0.000000 SaleCondition_Alloca -0.000000 SaleCondition_Family -0.007925 SaleCondition_Normal 0.019666 SaleCondition_Partial 0.000000 Length: 288, dtype: float64 . imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)]) matplotlib.rcParams[&#39;figure.figsize&#39;] = (8.0, 10.0) imp_coef.plot(kind = &quot;barh&quot;) plt.title(&quot;Coefficients in the Lasso Model&quot;) . Text(0.5, 1.0, &#39;Coefficients in the Lasso Model&#39;) . sort_values() 함수는 범주형 변수의 히스토그램을 아는데 유용한 함수입니다. . 여기선 정렬기능으로 사용했는데, 정렬기능으로도 충분히 우수한 것을 보여줬습니다. . 정렬된 값 상위 10개, 하위 10개를 시각화했는데, 이 변수들이 핵심 변수입니다. . 왜냐하면 계수의 절대값이 큰 값이기 때문입니다. . 양의 값으로 가장 큰 GrLivArea변수는 면적으로 주택가격에 당연히 큰 영향을 끼칩니다. . matplotlib.rcParams[&#39;figure.figsize&#39;] = (6.0, 6.0) preds = pd.DataFrame({&quot;preds&quot;:model_lasso.predict(X_train), &quot;true&quot;:y}) preds[&quot;residuals&quot;] = preds[&quot;true&quot;] - preds[&quot;preds&quot;] preds.plot(x = &quot;preds&quot;, y = &quot;residuals&quot;,kind = &quot;scatter&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3bd2529490&gt; . 잔차 그림도 큰 이상이 없습니다. . model_lasso = Lasso(alpha = 0.0005).fit(X_train, y) pred = model_lasso.predict(X_test) pred2 = np.exp(pred) - 1 X_test[&#39;SalePrice&#39;] = pred2 X_test[&#39;Id&#39;] = test[&#39;Id&#39;] final = X_test[[&#39;Id&#39;,&#39;SalePrice&#39;]] final.to_csv(&#39;/content/drive/MyDrive/houselasso2.csv&#39;,encoding=&#39;UTF-8&#39;, index=False) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . 알파값 0.0005인 라쏘 모델로 모델을 적합시키고 그 모델로 예측 파일을 만들었습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/18/kagglessu1.html",
            "relUrl": "/2021/09/18/kagglessu1.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "파이썬 머신러닝 완벽 가이드 5장-실습(자전거 대여 수요 예측)",
            "content": ". &#52880;&#44544;&#50640;&#49436; &#45936;&#51060;&#53552; &#51649;&#51217; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;0c820de52cea65ec11954012ef8b00d2&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ # Permission Warning이 발생하지 않도록 해줍니다. !chmod 600 ~/.kaggle/kaggle.json . ! kaggle competitions download -c bike-sharing-demand . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sampleSubmission.csv to /content 0% 0.00/140k [00:00&lt;?, ?B/s] 100% 140k/140k [00:00&lt;00:00, 50.8MB/s] Downloading test.csv to /content 0% 0.00/316k [00:00&lt;?, ?B/s] 100% 316k/316k [00:00&lt;00:00, 44.9MB/s] Downloading train.csv to /content 0% 0.00/633k [00:00&lt;?, ?B/s] 100% 633k/633k [00:00&lt;00:00, 40.4MB/s] . &#45936;&#51060;&#53552; &#46168;&#47084;&#48372;&#44592; &#48143; &#44032;&#44277; . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;, category = RuntimeWarning) bike_df = pd.read_csv(&#39;./train.csv&#39;) print(bike_df.shape) bike_df.head() . (10886, 12) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . 변수는 11개, 10886개 데이터가 있습니다. datetime변수는 가공이 필요합니다. . bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB . 결측값은 없습니다. . bike_df[&#39;datetime&#39;] = bike_df.datetime.apply(pd.to_datetime) bike_df[&#39;year&#39;] = bike_df.datetime.apply(lambda x : x.year) bike_df[&#39;month&#39;] = bike_df.datetime.apply(lambda x : x.month) bike_df[&#39;day&#39;] = bike_df.datetime.apply(lambda x : x.day) bike_df[&#39;hour&#39;] = bike_df.datetime.apply(lambda x : x.hour) bike_df.head(3) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | 2011 | 1 | 1 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | 2011 | 1 | 1 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | 2011 | 1 | 1 | 2 | . pd.to_datetime 함수를 통해 데이터 타임을 datetime으로 바꿨습니다. . datetime 데이터 타입은 year, month 등등으로 구분할 수 있습니다. . 이를 활용하여 년, 달, 날, 시간 변수로 각각 생성하였습니다. . drop_columns = [&#39;datetime&#39;, &#39;casual&#39;, &#39;registered&#39;] bike_df.drop(drop_columns, axis = 1, inplace = True) . datetime 변수는 분해를 했기 때문에 원본 변수가 필요 없어졌습니다. . casual + registered = count 변수 이므로 두 변수 모두 제외하겠습니다. . from sklearn.metrics import mean_squared_error, mean_absolute_error def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle def rmse(y, pred): return np.sqrt(mean_squared_error(y, pred)) def evaluate_regr(y, pred): rmsle_val = rmsle(y, pred) rmse_val = rmse(y, pred) mae_val = mean_absolute_error(y, pred) print(&#39;rmsle :&#39;, np.round(rmsle_val, 4), &#39;rmse :&#39;, np.round(rmse_val, 4), &#39;mse :&#39;, np.round(mae_val, 4)) . 이번 분석의 성능 평가 방법은 rmsle 이기 때문에 이를 구현했습니다. . &#52395;&#48264;&#51704; &#48516;&#49437; . from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso y_target = bike_df[&#39;count&#39;] x_features = bike_df.drop([&#39;count&#39;], axis = 1, inplace = False) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target, test_size = 0.3, random_state = 0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) evaluate_regr(y_test, pred) . rmsle : 1.1647 rmse : 140.8996 mse : 105.9244 . 실제 타겟 값이 대여 횟수임으로 지금 rmse 값은 매우 크다고 볼 수 있습니다. . def get_top_error_data(y_test, pred, n_tops = 5): result_df = pd.DataFrame(y_test.values, columns=[&#39;real_count&#39;]) result_df[&#39;predicted_count&#39;] = np.round(pred) result_df[&#39;diff&#39;] = np.abs(result_df[&#39;real_count&#39;] - result_df[&#39;predicted_count&#39;]) print(result_df.sort_values(&#39;diff&#39;, ascending= False)[:n_tops]) get_top_error_data(y_test, pred) . real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 . 실제값과 예측값이 가장 차이가 큰 5개 데이터를 출력했습니다. . 상당히 차이가 많이 나는걸 볼 수 있는데요. . 타겟값의 분포가 치우쳐 있는지 확인을 해볼 필요가 있겠습니다. . y_target.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c8fd090&gt; . 오른쪽 꼬리가 매우 두터운 형태임을 알 수 있습니다. . 이런 형태일 때 가장 자주 쓰이는 로그변환을 적용해보겠습니다. . y_log_transform = np.log1p(y_target) y_log_transform.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c792690&gt; . 정규분포와는 다소 차이가 있지만 변환 전보다 왜곡 정도가 많이 개선됐습니다. . y_target_log = np.log1p(y_target) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target_log, test_size= 0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) y_test_exp = np.expm1(y_test) pred_exp = np.expm1(pred) evaluate_regr(y_test_exp, pred_exp) . rmsle : 1.0168 rmse : 162.5943 mse : 109.2862 . mse 값은 전보다 개선 되었지만 rmse 값은 더 증가하였습니다. . 무슨 이유일까요? . &#46160;&#48264;&#51704; &#48516;&#49437; . coef = pd.Series(lr_reg.coef_, index=x_features.columns) coef_sort = coef.sort_values(ascending = False) sns.barplot(x=coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c1f0990&gt; . 다른 값에 비해 year값이 높습니다. . year값은 년도인데 년도가 이렇게 큰 영향을 미치는 것을 일반적인 사실로 받아들이기 힘듭니다. . 이유를 추정해보자면 연도 변수의 값이 큰 점을 들 수 있습니다.(2011,2012) . 비슷한 이유로 범주형 변수로 변환할 필요가 있는 변수들을 원핫인코딩방식으로 변환하겠습니다. . x_features_ohe = pd.get_dummies(x_features, columns = [&#39;year&#39;, &#39;month&#39;,&#39;day&#39;,&#39;hour&#39;,&#39;holiday&#39;, &#39;workingday&#39;, &#39;season&#39;, &#39;weather&#39;]) x_features_ohe.shape . (10886, 73) . 원핫 인코딩 결과 열 개수가 73개로 크게 늘어났습니다. . x_train, x_test, y_train, y_test = train_test_split(x_features_ohe, y_target_log, test_size= 0.3, random_state=0) def get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = False): model.fit(x_train, y_train) pred = model.predict(x_test) if is_expm1: y_test = np.expm1(y_test) pred = np.expm1(pred) print(model.__class__.__name__) evaluate_regr(y_test, pred) lr_reg = LinearRegression() ridge_reg = Ridge(alpha = 10) lasso_reg = Lasso(alpha = 0.01) for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = True) . LinearRegression rmsle : 0.5896 rmse : 97.6878 mse : 63.3821 Ridge rmsle : 0.5901 rmse : 98.5286 mse : 63.8934 Lasso rmsle : 0.6348 rmse : 113.2188 mse : 72.8027 . 원핫 인코딩을 적용한 후 결과가 눈에 띄게 좋아졌습니다. . coef = pd.Series(lr_reg.coef_, index=x_features_ohe.columns) coef_sort = coef.sort_values(ascending = False)[:20] sns.barplot(x = coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c772bd0&gt; . 회귀계수가 높은 피처 20개를 출력해보았습니다. . &#49464;&#48264;&#51704; &#48516;&#49437; . from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor rf_reg = RandomForestRegressor(n_estimators = 500) gbm_reg = GradientBoostingRegressor(n_estimators = 500) xgb_reg = XGBRegressor(n_estimaters = 500) lgbm_reg = LGBMRegressor(n_estimaters = 500) for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: get_model_predict(model, x_train.values, x_test.values, y_train.values, y_test.values, is_expm1=True) . RandomForestRegressor rmsle : 0.3549 rmse : 50.2976 mse : 31.1562 GradientBoostingRegressor rmsle : 0.3299 rmse : 53.3352 mse : 32.7448 [16:01:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor rmsle : 0.4828 rmse : 95.6137 mse : 59.2047 LGBMRegressor rmsle : 0.3315 rmse : 51.3807 mse : 31.8325 . 부스팅 모델을 사용하면 더 좋은 성능을 보일 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/18/PythonMachine5_4.html",
            "relUrl": "/2021/09/18/PythonMachine5_4.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "파이썬 머신러닝 완벽가이드 5장-3",
            "content": ". &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression cancer = load_breast_cancer() . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split scaler = StandardScaler() data_scaled = scaler.fit_transform(cancer.data) x_train, x_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size = 0.3, random_state = 0) . 평균이 0, 분산이 1인 정규분포 형태로 형 변환을 했습니다. . 로지스틱 회귀기법은 선형 회귀 방식에 응용으로 데이터의 정규분포도에 영향을 많이 받습니다. . from sklearn.metrics import accuracy_score, roc_auc_score import numpy as np lr_clf = LogisticRegression() lr_clf.fit(x_train, y_train) lr_preds = lr_clf.predict(x_test) print(&#39;정확도 :&#39;, np.round(accuracy_score(y_test, lr_preds), 4)) print(&#39;roc 커브 :&#39;, np.round(roc_auc_score(y_test, lr_preds), 4)) . 정확도 : 0.9766 roc 커브 : 0.9716 . from sklearn.model_selection import GridSearchCV params = {&#39;penalty&#39; : [&#39;l2&#39;, &#39;l1&#39;], &#39;C&#39; : [0.01, 0.1, 1, 5, 10]} grid_clf = GridSearchCV(lr_clf, param_grid = params, scoring = &#39;accuracy&#39;, cv = 3) grid_clf.fit(data_scaled, cancer.target) print(&#39;최적 파라미터 : &#39;, grid_clf.best_params_, &#39;최적 평균 정확도&#39;, grid_clf.best_score_) . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터 : {&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;} 최적 평균 정확도 0.975392184164114 . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터는 l2 규제로(릿지 회귀) c가(알파의 역수) 1일때 입니다. . &#53944;&#47532; &#44592;&#48152; &#54924;&#44480; &#47784;&#45944; . from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor import pandas as pd import numpy as np boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) rf = RandomForestRegressor(random_state = 0, n_estimators = 1000) neg_mse_scores = cross_val_score(rf, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse score : &#39;, np.round(neg_mse_scores, 4)) print(&#39;rmse score : &#39;, np.round(rmse_scores, 4)) print(&#39;평균 rmse score : &#39;, np.round(avg_rmse, 4)) . mse score : [ -7.933 -13.0584 -20.5278 -46.3057 -18.8008] rmse score : [2.8166 3.6136 4.5308 6.8048 4.336 ] 평균 rmse score : 4.4204 . 랜덤 포레스트 회귀 입니다. 평균 rmse 값은 4.42로 꽤 좋은 수치 입니다. . def get_model_cv_prediction(model, x_data, y_target): neg_mse_scores = cross_val_score(model, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(model.__class__.__name__) print(&#39;평균 rmse : &#39;, np.round(avg_rmse, 4)) . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor dt_reg = DecisionTreeRegressor(random_state = 0, max_depth = 4) rf_reg = RandomForestRegressor(random_state = 0, n_estimators = 1000) gb_reg = GradientBoostingRegressor(random_state = 0, n_estimators = 1000) xgb_reg = XGBRegressor(random_state = 0, n_estimators = 1000) lgb_reg = LGBMRegressor(random_state = 0, n_estimators = 1000) models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] for model in models: get_model_cv_prediction(model, x_data, y_target) . DecisionTreeRegressor 평균 rmse : 6.2377 RandomForestRegressor 평균 rmse : 4.4204 GradientBoostingRegressor 평균 rmse : 4.2692 [13:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor 평균 rmse : 4.0889 LGBMRegressor 평균 rmse : 4.6464 . 여러 모델을 테스트 해보았습니다. . xgb부스팅 모델의 성능이 가장 우수하게 나왔습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/18/PythonMachine5_3.html",
            "relUrl": "/2021/09/18/PythonMachine5_3.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "파이썬 머신러닝 완벽가이드 5장-2",
            "content": ". &#45796;&#54637; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures import numpy as np x = np.arange(4).reshape(2,2) # 행 부터 숫자 채워짐 print(&#39;일차 단항식 계수 피처: n&#39;, x) poly = PolynomialFeatures(degree = 2) poly.fit(x) poly_ftr = poly.transform(x) print(&#39;변환된 2차 다항식 계수 피처: n&#39;, poly_ftr) . 일차 단항식 계수 피처: [[0 1] [2 3]] 변환된 2차 다항식 계수 피처: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] . 2차 다항계수는 [1, x1, x2, x1^2, x1x2, x2^2] 로 구성되어 있습니다. . def polynomial_func(x): y = 1 + 2 * x[:,0] + 3 * x[:,0] **2 + 4 * x[:,1] **3 return y y = polynomial_func(x) . from sklearn.linear_model import LinearRegression poly_ftr = PolynomialFeatures(degree = 3).fit_transform(x) print(&#39;3차 다항식 계수 feature: n&#39;, poly_ftr) model = LinearRegression() model.fit(poly_ftr, y) print(&#39;회귀 계수 n&#39;, np.round(model.coef_,2)) print(&#39;회귀 shape&#39;, model.coef_.shape) . 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] 회귀 shape (10,) . poly함수로 다항식 계수를 생성한 뒤 단순 선형 회귀 함수에 대입해줍니다. . 원하는 값인 [1,2,0,3,0,0,0,0,0,4] 와 다소 차이가 있긴 합니다. . &#47551;&#51648; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline import numpy as np model = Pipeline([(&#39;poly&#39;, PolynomialFeatures(degree=3)), (&#39;linear&#39;, LinearRegression())]) model = model.fit(x,y) print(&#39;회귀 계수 n&#39;, np.round(model.named_steps[&#39;linear&#39;].coef_,2)) . 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] . 파이프 라인 함수로 다항식으로에 변환과 선형 회귀를 한번에 한 모습입니다. . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score from sklearn.datasets import load_boston import pandas as pd boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) ridge = Ridge(alpha = 10) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-11.42 -24.29 -28.14 -74.6 -28.52] rmse scores [3.38 4.93 5.31 8.64 5.34] 평균 rmse score: 5.52 . 단순 선형회귀 모델 rmse 평균값이 5.84로 릿지 회귀가 더 좋은 퍼포먼스를 보입니다. . alphas = [0,0.1,1,10,100] for alpha in alphas: ridge = Ridge(alpha=alpha) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 5.8287 alpha 값 0.1 일때 평균 rmse : 5.7885 alpha 값 1 일때 평균 rmse : 5.6526 alpha 값 10 일때 평균 rmse : 5.5182 alpha 값 100 일때 평균 rmse : 5.3296 . alpha 값이 100일때가 가장 값이 좋습니다. . import matplotlib.pyplot as plt import seaborn as sns fig, axs = plt.subplots(figsize= (18,6), nrows = 1, ncols = 5) coeff_df = pd.DataFrame() for pos, alpha in enumerate(alphas): ridge = Ridge(alpha = alpha) ridge.fit(x_data, y_target) coeff = pd.Series(data=ridge.coef_, index = x_data.columns) colname = &#39;alpha:&#39;+str(alpha) coeff_df[colname] = coeff coeff = coeff.sort_values(ascending = False) axs[pos].set_title(colname) axs[pos].set_xlim(-3, 6) sns.barplot(x=coeff.values, y = coeff.index, ax = axs[pos]) plt.show() . 알파 값이 커지면(=규제가 세지면) 회귀계수 값이 전반적으로 작아집니다. . 다만 릿지 회귀에 경우 회귀 계수를 0으로 만들지는 않습니다. . &#46972;&#50136; &#54924;&#44480; . from sklearn.linear_model import Lasso, ElasticNet def get_linear_reg_eval(model_name, params = None, x_data_n = None, y_target_n = None, verbose= True, return_coeff = True): coeff_df = pd.DataFrame() if verbose : print(model_name) for param in params: if model_name ==&#39;Ridge&#39; : model = Ridge(alpha = param) elif model_name ==&#39;Lasso&#39; : model = Lasso(alpha = param) elif model_name ==&#39;ElasticNet&#39; : model = ElasticNet(alpha = param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, x_data_n, y_target_n, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores)) print(&#39;alpha &#39;, param, &#39;일때 평균 rmse:&#39;, np.round(avg_rmse,2)) model.fit(x_data_n, y_target_n) if return_coeff: coeff = pd.Series(data=model.coef_, index = x_data_n.columns) colname = &#39;alpha:&#39;+str(param) coeff_df[colname] = coeff return coeff_df . lasso_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;Lasso&#39;,params=lasso_alphas, x_data_n = x_data, y_target_n= y_target) . Lasso alpha 0.07 일때 평균 rmse: 5.61 alpha 0.1 일때 평균 rmse: 5.62 alpha 0.5 일때 평균 rmse: 5.67 alpha 1 일때 평균 rmse: 5.78 alpha 3 일때 평균 rmse: 6.19 . 알파 값이 0.07일때 최고 성능을 보여줍니다. . 앞서 한 릿지보다는 성능이 떨어지지만, 단순 선형 회귀 모델보다 값이 크므로 쓰임새가 있습니다. . sort_column = &#39;alpha:&#39;+str(lasso_alphas[0]) coeff_lasso_df.sort_values(by = sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.789725 | 3.703202 | 2.498212 | 0.949811 | 0.000000 | . CHAS 1.434343 | 0.955190 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.270936 | 0.274707 | 0.277451 | 0.264206 | 0.061864 | . ZN 0.049059 | 0.049211 | 0.049544 | 0.049165 | 0.037231 | . B 0.010248 | 0.010249 | 0.009469 | 0.008247 | 0.006510 | . NOX -0.000000 | -0.000000 | -0.000000 | -0.000000 | 0.000000 | . AGE -0.011706 | -0.010037 | 0.003604 | 0.020910 | 0.042495 | . TAX -0.014290 | -0.014570 | -0.015442 | -0.015212 | -0.008602 | . INDUS -0.042120 | -0.036619 | -0.005253 | -0.000000 | -0.000000 | . CRIM -0.098193 | -0.097894 | -0.083289 | -0.063437 | -0.000000 | . LSTAT -0.560431 | -0.568769 | -0.656290 | -0.761115 | -0.807679 | . PTRATIO -0.765107 | -0.770654 | -0.758752 | -0.722966 | -0.265072 | . DIS -1.176583 | -1.160538 | -0.936605 | -0.668790 | -0.000000 | . 계수가 0인것이 보입니다. 알파값이 커질수록 회귀 계수가 0인 것이 늘어납니다. . &#50648;&#46972;&#49828;&#54001; &#54924;&#44480; . 다음은 엘라스틱 회귀 입니다. 쉽게 라쏘회귀 + 릿지 회귀로 볼 수 있습니다. . 라쏘 회귀에 경우 서로 상관관계가 높은 피처가 있으면 중요 피처를 제외하고 모두 회귀계수를 0으로 만듭니다. . 이를 다소 완화해주기 위한 목적으로 만들어졌습니다. 다만 수행시간이 다소 깁니다. . 여기서 알파는 알파1 + 알파 2 이며, l1_ratio는 말 그대로 l1규제(라쏘) 비율입니다. . elastic_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;ElasticNet&#39;, params=elastic_alphas, x_data_n= x_data, y_target_n= y_target) . ElasticNet alpha 0.07 일때 평균 rmse: 5.54 alpha 0.1 일때 평균 rmse: 5.53 alpha 0.5 일때 평균 rmse: 5.47 alpha 1 일때 평균 rmse: 5.6 alpha 3 일때 평균 rmse: 6.07 . 알파값이 0.5일때 가장 좋은 예측 성능을 보여줍니다. . sort_column = &#39;alpha:&#39;+str(elastic_alphas[0]) coeff_lasso_df.sort_values(by= sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.574162 | 3.414154 | 1.918419 | 0.938789 | 0.000000 | . CHAS 1.330724 | 0.979706 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.278880 | 0.283443 | 0.300761 | 0.289299 | 0.146846 | . ZN 0.050107 | 0.050617 | 0.052878 | 0.052136 | 0.038268 | . B 0.010122 | 0.010067 | 0.009114 | 0.008320 | 0.007020 | . AGE -0.010116 | -0.008276 | 0.007760 | 0.020348 | 0.043446 | . TAX -0.014522 | -0.014814 | -0.016046 | -0.016218 | -0.011417 | . INDUS -0.044855 | -0.042719 | -0.023252 | -0.000000 | -0.000000 | . CRIM -0.099468 | -0.099213 | -0.089070 | -0.073577 | -0.019058 | . NOX -0.175072 | -0.000000 | -0.000000 | -0.000000 | -0.000000 | . LSTAT -0.574822 | -0.587702 | -0.693861 | -0.760457 | -0.800368 | . PTRATIO -0.779498 | -0.784725 | -0.790969 | -0.738672 | -0.423065 | . DIS -1.189438 | -1.173647 | -0.975902 | -0.725174 | -0.031208 | . 라쏘모델에 비해 회귀계수를 0으로 만드는 개수가 다소 줄었습니다. . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#51012; &#50948;&#54620; &#45936;&#51060;&#53552; &#48320;&#54872; . 선형 회귀에서 중요한 것 중 하나가 데이터 분포도의 정규화 입니다. . 특히 타깃값의 분포가 정규분포가 아닌 왜곡(skew)된 분포는 예측 성능에 부정적입니다. . 따라서 선형 회귀 모델을 적용하기 전 먼저 데이터 스케일링/정규화 작업을 수행해주어야 합니다. . from sklearn.preprocessing import StandardScaler, MinMaxScaler def get_scaled_data(method=&#39;None&#39;, p_degree = None, input_data = None): if method == &#39;Standard&#39;: scaled_data = StandardScaler().fit_transform(input_data) elif method == &#39;MinMax&#39;: scaled_data = MinMaxScaler().fit_transform(input_data) if method == &#39;Log&#39;: scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data . alphas = [0.1, 1, 10, 100] scale_methods=[(None, None), (&#39;Standard&#39;, None), (&#39;Standard&#39;,2), (&#39;MinMax&#39;,None), (&#39;MinMax&#39;, 2), (&#39;Log&#39;, None)] for scale_method in scale_methods: x_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=x_data) print(&#39; n 변환유형:&#39;, scale_method[0], &#39;, Polynomial Degree:&#39;, scale_method[1]) get_linear_reg_eval(&#39;Ridge&#39;, params = alphas, x_data_n=x_data_scaled, y_target_n= y_target, verbose=False, return_coeff = False) . 변환유형: None , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: MinMax , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: MinMax , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: Log , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 4.77 alpha 1 일때 평균 rmse: 4.68 alpha 10 일때 평균 rmse: 4.84 alpha 100 일때 평균 rmse: 6.24 . log 변환이 다른 변환에 비해 성능이 뛰어난 걸 볼 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/18/PythonMachine5_2.html",
            "relUrl": "/2021/09/18/PythonMachine5_2.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "파이썬 머신러닝 완벽가이드 5장-1",
            "content": ". &#44221;&#49324;&#54616;&#44053;&#48277; . import numpy as np import matplotlib.pyplot as plt %matplotlib inline np.random.seed(8) x = 2 * np.random.randn(100,1) y = 6 + 4 * x + np.random.randn(100,1) plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x7f6bcf5bb850&gt; . y = 4x + 6 근사 . np.random.randn =&gt; 표준정규분포에서 값 생성. 100,1 은 값 행렬 형식 선언 입니다. . def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y-y_pred)) / N return cost . 편차 제곱 평균을 계산해주는 함수. . np.square =&gt; 제곱 해주는 함수 . def get_weight_updates(w1, w0, x, y, learning_rate = 0.01): N = len(y) # w1, w0 동일한 행렬 크기를 갖는 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) #np.dot 행렬의 곱 y_pred = np.dot(x, w1.T) + w0 diff = y - y_pred w0_factors = np.ones((N, 1)) w1_update = -(2/N) * learning_rate * (np.dot(x.T, diff)) w0_update = -(2/N) * learning_rate * (np.dot(w0_factors.T, diff)) return w1_update, w0_update . 편미분한 w1, w0값을 이용해서 w0, w1값을 지속적으로 업데이트 해줍니다 . np.zeros_like(w1) =&gt; w1값과 같은 형태에 값은 0인 행렬 생성 . np.dot(,) =&gt; 행렬 연산 . def gradient_descent_steps(x,y, iters = 10000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, x, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . 위 두 함수를 통해 w1, w0 값을 지속적으로 업데이트 하여 최적에 값에 도달하게 합니다. . w1, w0 = gradient_descent_steps(x,y, 1000) print(&#39;w1 :&#39;, np.round(w1[0,0],4), &#39;w0 :&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱평균:&#39;, np.round(get_cost(y, y_pred),4)) . w1 : 3.9974 w0 : 5.9649 편차제곱평균: 1.1967 . plt.scatter(x,y) plt.plot(x,y_pred) . [&lt;matplotlib.lines.Line2D at 0x7f6bcf10f110&gt;] . 경사하강법을 이용해 회귀선이 잘 만들어졌습니다. . 다만 데이터에 개수가 100개보다 훨씬 많아지면 전체데이터로 계수를 업데이트 하지 못합니다. . 그 때문에 실전에서는 대부분 (미니배치)확률적 경사 하강법을 이용합니다. . 이 방식은 전체 데이터가 아닌 일부 데이터로 계수를 업데이트 하기 때문에 속도가 상대적으로 빠릅니다. . 이를 구현해보겠습니다. . def stochastic_gradient_descent_steps(x,y,batch_size = 10, iters = 1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index = 0 for ind in range(iters): np.random.seed(ind) stochastic_random_index = np.random.permutation(x.shape[0]) sample_x = x[stochastic_random_index[0:batch_size]] sample_y = y[stochastic_random_index[0:batch_size]] w1_update, w0_update = get_weight_updates(w1, w0, sample_x, sample_y) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . np.random.permutation(x.shape[0]) =&gt; 주어진 데이터를 셔플해서 출력함 . 앞 함수와 바뀐 부분은 x, y를 샘플링해서 넣는다는 점 입니다. . w1, w0 = stochastic_gradient_descent_steps(x,y,iters=1000) print(&#39;w1:&#39;, np.round(w1[0,0],3), &#39;w0:&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱 평균:&#39;, np.round(get_cost(y,y_pred),4)) . w1: 4.006 w0: 5.9135 편차제곱 평균: 1.1996 . 편차제곱 평균 값이 전체 x,y를 투입했을때와 큰 차이가 없습니다. . 그러므로 계산 속도가 훨씬 빠른 미니배치 경사하강법을 많이 사용합니다. . &#45800;&#49692; &#49440;&#54805; &#54924;&#44480;(&#48372;&#49828;&#53556; &#51452;&#53469; &#44032;&#44201;) . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;보스턴 데이터 세트 크기:&#39;, bostonDF.shape) bostonDF.head() . 보스턴 데이터 세트 크기: (506, 14) . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . 사이킷런에 내장되어있는 보스턴 주택 데이터를 불러왔습니다. . bostonDF.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 CRIM 506 non-null float64 1 ZN 506 non-null float64 2 INDUS 506 non-null float64 3 CHAS 506 non-null float64 4 NOX 506 non-null float64 5 RM 506 non-null float64 6 AGE 506 non-null float64 7 DIS 506 non-null float64 8 RAD 506 non-null float64 9 TAX 506 non-null float64 10 PTRATIO 506 non-null float64 11 B 506 non-null float64 12 LSTAT 506 non-null float64 13 PRICE 506 non-null float64 dtypes: float64(14) memory usage: 55.5 KB . 결측값은 없으며 모든 피처가 float 형 입니다. . fig, axs = plt.subplots(figsize=(16,8), ncols = 4, nrows = 2) lm_features = [&#39;RM&#39;,&#39;ZN&#39;, &#39;INDUS&#39;,&#39;NOX&#39;,&#39;AGE&#39;,&#39;PTRATIO&#39;,&#39;LSTAT&#39;,&#39;RAD&#39;] for i, feature in enumerate(lm_features): row = int(i/4) col = i%4 sns.regplot(x=feature, y=&#39;PRICE&#39;, data=bostonDF, ax=axs[row][col]) . sns.regplot(x,y) =&gt; x,y 산점도와 함께 회귀직선을 그려줌. . plt.subplots(ncols = , nrows= ) 여러개의 그림을 그릴 수 있게 해줌. . RM과 LSTAT 변수가 가장 PRICE 변수와 연관성이 있어보입니다. . from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) x_train, x_test, y_train, y_test = train_test_split(x_data, y_target, test_size = 0.3, random_state = 156) lr = LinearRegression() lr.fit(x_train, y_train) y_preds = lr.predict(x_test) mse = mean_squared_error(y_test, y_preds) rmse = np.sqrt(mse) print(&#39;mse :&#39;, np.round(mse,4), &#39;, rmse :&#39;, np.round(rmse, 4)) print(&#39;결정계수:&#39;, np.round(r2_score(y_test, y_preds), 4)) . mse : 17.2969 , rmse : 4.159 결정계수: 0.7572 . 모델을 어느정도 설명해 준 모습입니다. . print(&#39;절편 값:&#39;,lr.intercept_) print(&#39;회귀 계수값:&#39;, np.round(lr.coef_,1)) . 절편 값: 40.995595172164755 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] . coeff = pd.Series(data=np.round(lr.coef_, 1), index = x_data.columns) coeff.sort_values(ascending=False) . RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 B 0.0 TAX -0.0 AGE 0.0 INDUS 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 . 변수 이름과 추정 회귀 계수를 맵핑 시킨 모습입니다. . NOX 변수의 계수 값이 크게 작아보입니다. . from sklearn.model_selection import cross_val_score neg_mse_scores = cross_val_score(lr, x_data, y_target, scoring=&#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-12.46 -26.05 -33.07 -80.76 -33.31] rmse scores [3.53 5.1 5.75 8.99 5.77] 평균 rmse score: 5.83 . 5개의 폴드 세트를 이용한 교차검증 입니다. . scoring = &#39;neg_mean_squared_error&#39; 같은 경우 보통 모델 평가를 위한 값이 커야 좋은 값인데, mse 값은 작아야 좋습니다. . 그러므로 음수를 붙여서 보정해준다고 생각하면 좋습니다. . 다음에는 다항회귀, 릿지/라쏘 회귀 부분을 공부하겠습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/18/PythonMachine5_1.html",
            "relUrl": "/2021/09/18/PythonMachine5_1.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ksy1526.github.io/myblog//myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ksy1526.github.io/myblog//myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ksy1526.github.io/myblog//myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ksy1526.github.io/myblog//myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}