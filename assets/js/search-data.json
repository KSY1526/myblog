{
  
    
        "post0": {
            "title": "캐글 제품 분류 문제",
            "content": ". from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, 100) pd.set_option(&#39;display.max_rows&#39;, 100) from sklearn.preprocessing import LabelEncoder, OneHotEncoder from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV from sklearn.feature_selection import SelectFromModel from sklearn.metrics import accuracy_score, confusion_matrix, classification_report import xgboost as xg from collections import Counter !pip install kneed # kneed is not installed in kaggle. uncomment the above line. from kneed import KneeLocator import warnings warnings.filterwarnings(&quot;ignore&quot;) . Collecting kneed Downloading kneed-0.7.0-py2.py3-none-any.whl (9.4 kB) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from kneed) (1.4.1) Requirement already satisfied: numpy&gt;=1.14.2 in /usr/local/lib/python3.7/dist-packages (from kneed) (1.19.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from kneed) (3.2.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (3.0.6) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (0.11.0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;kneed) (1.15.0) Installing collected packages: kneed Successfully installed kneed-0.7.0 . path = &#39;/content/drive/MyDrive/otto_group/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sampleSubmission.csv&#39;) train.head() . id feat_1 feat_2 feat_3 feat_4 feat_5 feat_6 feat_7 feat_8 feat_9 feat_10 feat_11 feat_12 feat_13 feat_14 feat_15 feat_16 feat_17 feat_18 feat_19 feat_20 feat_21 feat_22 feat_23 feat_24 feat_25 feat_26 feat_27 feat_28 feat_29 feat_30 feat_31 feat_32 feat_33 feat_34 feat_35 feat_36 feat_37 feat_38 feat_39 feat_40 feat_41 feat_42 feat_43 feat_44 feat_45 feat_46 feat_47 feat_48 feat_49 feat_50 feat_51 feat_52 feat_53 feat_54 feat_55 feat_56 feat_57 feat_58 feat_59 feat_60 feat_61 feat_62 feat_63 feat_64 feat_65 feat_66 feat_67 feat_68 feat_69 feat_70 feat_71 feat_72 feat_73 feat_74 feat_75 feat_76 feat_77 feat_78 feat_79 feat_80 feat_81 feat_82 feat_83 feat_84 feat_85 feat_86 feat_87 feat_88 feat_89 feat_90 feat_91 feat_92 feat_93 target . 0 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 4 | 1 | 1 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | 0 | 11 | 0 | 1 | 1 | 0 | 1 | 0 | 7 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 1 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 2 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 3 4 | 1 | 0 | 0 | 1 | 6 | 1 | 5 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 2 | 2 | 0 | 0 | 0 | 58 | 0 | 10 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 2 | 0 | 1 | 2 | 1 | 3 | 0 | 0 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 5 | 0 | 0 | 4 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 2 | 0 | 22 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 4 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | Class_1 | . &#45936;&#51060;&#53552; &#53456;&#49353; . train.columns . Index([&#39;id&#39;, &#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, &#39;feat_6&#39;, &#39;feat_7&#39;, &#39;feat_8&#39;, &#39;feat_9&#39;, &#39;feat_10&#39;, &#39;feat_11&#39;, &#39;feat_12&#39;, &#39;feat_13&#39;, &#39;feat_14&#39;, &#39;feat_15&#39;, &#39;feat_16&#39;, &#39;feat_17&#39;, &#39;feat_18&#39;, &#39;feat_19&#39;, &#39;feat_20&#39;, &#39;feat_21&#39;, &#39;feat_22&#39;, &#39;feat_23&#39;, &#39;feat_24&#39;, &#39;feat_25&#39;, &#39;feat_26&#39;, &#39;feat_27&#39;, &#39;feat_28&#39;, &#39;feat_29&#39;, &#39;feat_30&#39;, &#39;feat_31&#39;, &#39;feat_32&#39;, &#39;feat_33&#39;, &#39;feat_34&#39;, &#39;feat_35&#39;, &#39;feat_36&#39;, &#39;feat_37&#39;, &#39;feat_38&#39;, &#39;feat_39&#39;, &#39;feat_40&#39;, &#39;feat_41&#39;, &#39;feat_42&#39;, &#39;feat_43&#39;, &#39;feat_44&#39;, &#39;feat_45&#39;, &#39;feat_46&#39;, &#39;feat_47&#39;, &#39;feat_48&#39;, &#39;feat_49&#39;, &#39;feat_50&#39;, &#39;feat_51&#39;, &#39;feat_52&#39;, &#39;feat_53&#39;, &#39;feat_54&#39;, &#39;feat_55&#39;, &#39;feat_56&#39;, &#39;feat_57&#39;, &#39;feat_58&#39;, &#39;feat_59&#39;, &#39;feat_60&#39;, &#39;feat_61&#39;, &#39;feat_62&#39;, &#39;feat_63&#39;, &#39;feat_64&#39;, &#39;feat_65&#39;, &#39;feat_66&#39;, &#39;feat_67&#39;, &#39;feat_68&#39;, &#39;feat_69&#39;, &#39;feat_70&#39;, &#39;feat_71&#39;, &#39;feat_72&#39;, &#39;feat_73&#39;, &#39;feat_74&#39;, &#39;feat_75&#39;, &#39;feat_76&#39;, &#39;feat_77&#39;, &#39;feat_78&#39;, &#39;feat_79&#39;, &#39;feat_80&#39;, &#39;feat_81&#39;, &#39;feat_82&#39;, &#39;feat_83&#39;, &#39;feat_84&#39;, &#39;feat_85&#39;, &#39;feat_86&#39;, &#39;feat_87&#39;, &#39;feat_88&#39;, &#39;feat_89&#39;, &#39;feat_90&#39;, &#39;feat_91&#39;, &#39;feat_92&#39;, &#39;feat_93&#39;, &#39;target&#39;], dtype=&#39;object&#39;) . 컬럼수는 93개 입니다. . train[&#39;target&#39;].unique() . array([&#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;, &#39;Class_6&#39;, &#39;Class_7&#39;, &#39;Class_8&#39;, &#39;Class_9&#39;], dtype=object) . Y 변수의 클레스 종류가 9개 입니다. . sample_submission.head() . id Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 0 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 5 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 저번이랑 비슷하게 제출 파일 형식은 각 클레스 별로 분류 될 확률을 기제하면 되겠네요. . sum((train.isnull()).sum()) . 0 . 결측값이 있는지 확인했습니다. info 함수로 확인하기에는 피처가 너무 커서 직관적으로 확인하기 힘듭니다. . from sklearn.preprocessing import LabelEncoder le=LabelEncoder() train[&#39;target&#39;]=le.fit_transform(train[&#39;target&#39;]) plt.figure(figsize=(12,5)) sns.countplot(train[&#39;target&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0cca07b90&gt; . 라벨 인코더를 통해 클레스 이름을 간단하게( Class_1 =&gt; 0) 바꿨습니다. . 클레스 개수가 각각 몇개있는지 파악했는데요. 균등하진 않아보입니다. . &#47784;&#45944; 1 . from sklearn.model_selection import train_test_split list_models=[] list_scores=[] y = train[&#39;target&#39;] x = train.drop([&#39;target&#39;, &#39;id&#39;],axis=1) x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2) . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score lr=LogisticRegression(max_iter=100000) lr.fit(x_train,y_train) pred_1=lr.predict(x_test) score_1=accuracy_score(y_test,pred_1) list_models.append(&#39;logistic regression&#39;) list_scores.append(score_1) . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_1,ax=axes[0]) sns.countplot(y_test,ax=axes[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0cbd85e10&gt; . 간단한 로지스틱 회귀로, 왼쪽이 예측값, 오른쪽이 실제값입니다. 실제 비율이 제일 높은 1번은 더 많이 예측하는 모습을 보입니다. . 비율이 높은 편인 5번, 7번, 8번은 실제 값과 예측 값이 비슷합니다. . 하지만 나머지 값들은 실제 값에 있는 비율 만큼 예측 값에서 비슷한 개수로 추정해주지 못했습니다. . 물론 이 현상만으로 비율을 일관적으로 예측할 수는 없습니다.(8번은 예측/실제 값 개수 비슷, 2번은 실제 값에 비해 예측값이 너무 적음) . 다만 불균형한 테스터 셋을 분류하는 문제에서 다음과 같은 문제가 있다는걸 인지해야겠습니다. . 개수가 많은 클레스를 예측하는 확률은 높아지고, 개수가 적은 클레스를 예측하는 확률은 낮아진다는 점 입니다. . from sklearn.ensemble import RandomForestClassifier rfc=RandomForestClassifier() rfc.fit(x_train,y_train) pred_2=rfc.predict(x_test) score_2=accuracy_score(y_test,pred_2) list_scores.append(score_2) list_models.append(&#39;random forest classifier&#39;) . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_2,ax=axes[0]) sns.countplot(y_test,ax=axes[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0c14ea1d0&gt; . 렌덤 포레스트 분류기법입니다. 앞서 말한것과 비슷한 일이 벌어집니다. . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_1,ax=axes[0]) axes[0].legend(title=&#39;predictions by logistic regression&#39;) sns.countplot(pred_2,ax=axes[1]) axes[1].legend(title=&#39;predictions by random forest&#39;) . No handles with labels found to put in legend. No handles with labels found to put in legend. . &lt;matplotlib.legend.Legend at 0x7fc0c1429ad0&gt; . 두 모델이 비슷한 현상을 보인다는 걸 다시한번 보여준 것 같습니다. . from sklearn.svm import SVC svm=SVC() svm.fit(x_train,y_train) pred_3=svm.predict(x_test) score_3=accuracy_score(y_test,pred_3) list_scores.append(score_3) list_models.append(&#39;support vector machines&#39;) . from xgboost import XGBClassifier xgb=XGBClassifier() xgb.fit(x_train,y_train) pred_4=xgb.predict(x_test) score_4=accuracy_score(y_test,pred_4) list_models.append(&#39;xgboost classifier&#39;) list_scores.append(score_4) . plt.figure(figsize=(12,5)) plt.bar(list_models,list_scores,width=0.3) plt.xlabel(&#39;classifictions models&#39;) plt.ylabel(&#39;accuracy scores&#39;) plt.show() . SVM, XGB 모델도 적용시켜보았습니다. . 랜덤 포레스트 분류 모델이 성능이 가장 괜찮아 보입니다. . &#47784;&#45944; 2 . !pip install &quot;autogluon.tabular[all]==0.1.1b20210312&quot; . Collecting autogluon.tabular[all]==0.1.1b20210312 Downloading autogluon.tabular-0.1.1b20210312-py3-none-any.whl (234 kB) |████████████████████████████████| 234 kB 4.2 MB/s Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.19.5) Collecting scikit-learn&lt;0.25,&gt;=0.22.0 Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB) |████████████████████████████████| 22.3 MB 1.6 MB/s Collecting autogluon.features==0.1.1b20210312 Downloading autogluon.features-0.1.1b20210312-py3-none-any.whl (48 kB) |████████████████████████████████| 48 kB 4.4 MB/s Requirement already satisfied: pandas&lt;2.0,&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.1.5) Collecting scipy==1.5.4 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) |████████████████████████████████| 25.9 MB 1.8 MB/s Requirement already satisfied: networkx&lt;3.0,&gt;=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (2.6.3) Collecting autogluon.core==0.1.1b20210312 Downloading autogluon.core-0.1.1b20210312-py3-none-any.whl (312 kB) |████████████████████████████████| 312 kB 50.1 MB/s Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (3.6.4) Requirement already satisfied: psutil&lt;=5.7.0,&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (5.4.8) Requirement already satisfied: torch&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.10.0+cu111) Requirement already satisfied: fastai&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.0.61) Collecting lightgbm&lt;4.0,&gt;=3.0 Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB) |████████████████████████████████| 2.0 MB 49.3 MB/s Collecting catboost&lt;0.25,&gt;=0.23.0 Downloading catboost-0.24.4-cp37-none-manylinux1_x86_64.whl (65.7 MB) |████████████████████████████████| 65.7 MB 46 kB/s Collecting xgboost&lt;1.4,&gt;=1.3.2 Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB) |████████████████████████████████| 157.5 MB 63 kB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.23.0) Collecting dill==0.3.3 Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB) |████████████████████████████████| 81 kB 9.6 MB/s Requirement already satisfied: autograd&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3) Collecting paramiko&gt;=2.4 Downloading paramiko-2.8.0-py2.py3-none-any.whl (206 kB) |████████████████████████████████| 206 kB 50.7 MB/s Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.29.24) Requirement already satisfied: tqdm&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.62.3) Requirement already satisfied: tornado&gt;=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (5.1.1) Collecting boto3 Downloading boto3-1.20.14-py3-none-any.whl (131 kB) |████████████████████████████████| 131 kB 49.8 MB/s Collecting ConfigSpace==0.4.18 Downloading ConfigSpace-0.4.18.tar.gz (950 kB) |████████████████████████████████| 950 kB 49.4 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.2.2) Collecting graphviz&lt;0.9.0,&gt;=0.8.1 Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB) Collecting distributed&gt;=2.6.0 Downloading distributed-2021.11.2-py3-none-any.whl (802 kB) |████████████████████████████████| 802 kB 50.6 MB/s Requirement already satisfied: dask&gt;=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.12.0) Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace==0.4.18-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.6) Requirement already satisfied: future&gt;=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd&gt;=1.3-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.16.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.15.0) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.4.1) Collecting dask&gt;=2.6.0 Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB) |████████████████████████████████| 1.0 MB 40.4 MB/s Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.4.0) Requirement already satisfied: toolz&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.2) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.11.3) Requirement already satisfied: click&gt;=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.1.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (57.4.0) Requirement already satisfied: msgpack&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.2) Requirement already satisfied: tblib&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.7.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.13) Requirement already satisfied: zict&gt;=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.0) Collecting cloudpickle&gt;=1.5.0 Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from dask&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (21.3) Collecting partd&gt;=0.3.10 Downloading partd-1.2.0-py3-none-any.whl (19 kB) Collecting fsspec&gt;=0.6.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 54.6 MB/s Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.1+cu111) Requirement already satisfied: spacy&gt;=2.0.18 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.2.4) Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.352.0) Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.7.3) Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.6.3) Requirement already satisfied: fastprogress&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.0) Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.1.2) Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.2) Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&lt;4.0,&gt;=3.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.37.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&lt;2.0,&gt;=1.0.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&lt;2.0,&gt;=1.0.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.8.2) Collecting cryptography&gt;=2.5 Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB) |████████████████████████████████| 3.6 MB 43.1 MB/s Collecting pynacl&gt;=1.0.1 Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB) |████████████████████████████████| 961 kB 45.2 MB/s Collecting bcrypt&gt;=3.1.3 Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB) |████████████████████████████████| 63 kB 2.3 MB/s Requirement already satisfied: cffi&gt;=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.15.0) Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.1-&gt;bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.21) Collecting locket Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&lt;0.25,&gt;=0.22.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&lt;0.25,&gt;=0.22.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.1.0) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.4.0) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.5) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.8.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.6) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.6) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.1.3) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.4.1) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.8.2) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.10.0.2) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.6.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.4) Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict&gt;=0.1.3-&gt;distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.1) Collecting jmespath&lt;1.0.0,&gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting botocore&lt;1.24.0,&gt;=1.23.14 Downloading botocore-1.23.14-py3-none-any.whl (8.2 MB) |████████████████████████████████| 8.2 MB 37.3 MB/s Collecting s3transfer&lt;0.6.0,&gt;=0.5.0 Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 7.5 MB/s Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 49.6 MB/s Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.3) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (21.2.0) Requirement already satisfied: more-itertools&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (8.11.0) Requirement already satisfied: atomicwrites&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.4.0) Requirement already satisfied: pluggy&lt;0.8,&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.7.1) Requirement already satisfied: py&gt;=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.11.0) Building wheels for collected packages: ConfigSpace Building wheel for ConfigSpace (PEP 517) ... done Created wheel for ConfigSpace: filename=ConfigSpace-0.4.18-cp37-cp37m-linux_x86_64.whl size=2880650 sha256=7b9c24d3da86378fe64cff390f09a143606ba3ac7a45f5b37fa2827e1aed4124 Stored in directory: /root/.cache/pip/wheels/36/f7/0f/36f368c419ea1a8024fc3d6c078c3111dfef43fa1d14cfebe0 Successfully built ConfigSpace Installing collected packages: urllib3, locket, jmespath, partd, fsspec, cloudpickle, botocore, scipy, s3transfer, pynacl, dask, cryptography, bcrypt, scikit-learn, paramiko, graphviz, distributed, dill, ConfigSpace, boto3, autogluon.core, autogluon.features, xgboost, lightgbm, catboost, autogluon.tabular Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 Attempting uninstall: cloudpickle Found existing installation: cloudpickle 1.3.0 Uninstalling cloudpickle-1.3.0: Successfully uninstalled cloudpickle-1.3.0 Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: dask Found existing installation: dask 2.12.0 Uninstalling dask-2.12.0: Successfully uninstalled dask-2.12.0 Attempting uninstall: scikit-learn Found existing installation: scikit-learn 1.0.1 Uninstalling scikit-learn-1.0.1: Successfully uninstalled scikit-learn-1.0.1 Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 Attempting uninstall: distributed Found existing installation: distributed 1.25.3 Uninstalling distributed-1.25.3: Successfully uninstalled distributed-1.25.3 Attempting uninstall: dill Found existing installation: dill 0.3.4 Uninstalling dill-0.3.4: Successfully uninstalled dill-0.3.4 Attempting uninstall: xgboost Found existing installation: xgboost 0.90 Uninstalling xgboost-0.90: Successfully uninstalled xgboost-0.90 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. multiprocess 0.70.12.2 requires dill&gt;=0.3.4, but you have dill 0.3.3 which is incompatible. gym 0.17.3 requires cloudpickle&lt;1.7.0,&gt;=1.2.0, but you have cloudpickle 2.0.0 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible. Successfully installed ConfigSpace-0.4.18 autogluon.core-0.1.1b20210312 autogluon.features-0.1.1b20210312 autogluon.tabular-0.1.1b20210312 bcrypt-3.2.0 boto3-1.20.14 botocore-1.23.14 catboost-0.24.4 cloudpickle-2.0.0 cryptography-36.0.0 dask-2021.11.2 dill-0.3.3 distributed-2021.11.2 fsspec-2021.11.1 graphviz-0.8.4 jmespath-0.10.0 lightgbm-3.3.1 locket-0.2.1 paramiko-2.8.0 partd-1.2.0 pynacl-1.4.0 s3transfer-0.5.0 scikit-learn-0.24.2 scipy-1.5.4 urllib3-1.25.11 xgboost-1.3.3 . from autogluon.tabular import TabularDataset, TabularPredictor from autogluon.tabular.models.knn.knn_rapids_model import KNNRapidsModel from autogluon.tabular.models.lr.lr_rapids_model import LinearRapidsModel path = &#39;/content/drive/MyDrive/otto_group/&#39; train = TabularDataset(path + &#39;train.csv&#39;) test = TabularDataset(path + &#39;test.csv&#39;) label = &#39;target&#39; . Loaded data from: /content/drive/MyDrive/otto_group/train.csv | Columns = 95 / 95 | Rows = 61878 -&gt; 61878 Loaded data from: /content/drive/MyDrive/otto_group/test.csv | Columns = 94 / 94 | Rows = 144368 -&gt; 144368 . !pip install cuml . Collecting cuml Downloading cuml-0.6.1.post1.tar.gz (1.1 kB) Building wheels for collected packages: cuml Building wheel for cuml (setup.py) ... error ERROR: Failed building wheel for cuml Running setup.py clean for cuml Failed to build cuml Installing collected packages: cuml Running setup.py install for cuml ... error ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c &#39;import io, os, sys, setuptools, tokenize; sys.argv[0] = &#39;&#34;&#39;&#34;&#39;/tmp/pip-install-d9q8bg1e/cuml_e5625faa4a144d1cb1dbda39971d1a35/setup.py&#39;&#34;&#39;&#34;&#39;; __file__=&#39;&#34;&#39;&#34;&#39;/tmp/pip-install-d9q8bg1e/cuml_e5625faa4a144d1cb1dbda39971d1a35/setup.py&#39;&#34;&#39;&#34;&#39;;f = getattr(tokenize, &#39;&#34;&#39;&#34;&#39;open&#39;&#34;&#39;&#34;&#39;, open)(__file__) if os.path.exists(__file__) else io.StringIO(&#39;&#34;&#39;&#34;&#39;from setuptools import setup; setup()&#39;&#34;&#39;&#34;&#39;);code = f.read().replace(&#39;&#34;&#39;&#34;&#39; r n&#39;&#34;&#39;&#34;&#39;, &#39;&#34;&#39;&#34;&#39; n&#39;&#34;&#39;&#34;&#39;);f.close();exec(compile(code, __file__, &#39;&#34;&#39;&#34;&#39;exec&#39;&#34;&#39;&#34;&#39;))&#39; install --record /tmp/pip-record-yfs4_fyl/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/cuml Check the logs for full command output. . predictor = TabularPredictor( label=label, eval_metric=&#39;log_loss&#39;, learner_kwargs={&#39;ignored_columns&#39;: [&#39;id&#39;]} ).fit( train, presets=&#39;best_quality&#39;, hyperparameters={ KNNRapidsModel: {}, LinearRapidsModel: {}, &#39;RF&#39;: {}, &#39;XGB&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;CAT&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;GBM&#39;: [{}, {&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, &#39;GBMLarge&#39;], &#39;NN&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;FASTAI&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, }, ) . No path specified. Models will be saved in: &#34;AutogluonModels/ag-20211127_122603/&#34; Presets specified: [&#39;best_quality&#39;] Beginning AutoGluon training ... AutoGluon will save models to &#34;AutogluonModels/ag-20211127_122603/&#34; AutoGluon Version: 0.1.1b20210312 Train Data Rows: 61878 Train Data Columns: 94 Preprocessing data ... AutoGluon infers your prediction problem is: &#39;multiclass&#39; (because dtype of label-column == object). 9 unique label values: [&#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;, &#39;Class_6&#39;, &#39;Class_7&#39;, &#39;Class_8&#39;, &#39;Class_9&#39;] If &#39;multiclass&#39; is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: [&#39;binary&#39;, &#39;multiclass&#39;, &#39;regression&#39;]) Train Data Class Count: 9 Using Feature Generators to preprocess the data ... Dropping user-specified ignored columns: [&#39;id&#39;] Fitting AutoMLPipelineFeatureGenerator... Available Memory: 12407.99 MB Train Data (Original) Memory Usage: 46.04 MB (0.4% of available memory) Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features. Stage 1 Generators: Fitting AsTypeFeatureGenerator... Stage 2 Generators: Fitting FillNaFeatureGenerator... Stage 3 Generators: Fitting IdentityFeatureGenerator... Stage 4 Generators: Fitting DropUniqueFeatureGenerator... Types of features in original data (raw dtype, special dtypes): (&#39;int&#39;, []) : 93 | [&#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, ...] Types of features in processed data (raw dtype, special dtypes): (&#39;int&#39;, []) : 93 | [&#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, ...] 0.5s = Fit runtime 93 features in original data used to generate 93 features in processed data. Train Data (Processed) Memory Usage: 46.04 MB (0.4% of available memory) Data preprocessing and feature engineering runtime = 0.7s ... AutoGluon will gauge predictive performance using evaluation metric: &#39;log_loss&#39; This metric expects predicted probabilities rather than predicted class labels, so you&#39;ll need to use predict_proba() instead of predict() To change this, specify the eval_metric argument of fit() Custom Model Type Detected: &lt;class &#39;autogluon.tabular.models.knn.knn_rapids_model.KNNRapidsModel&#39;&gt; Custom Model Type Detected: &lt;class &#39;autogluon.tabular.models.lr.lr_rapids_model.LinearRapidsModel&#39;&gt; . ModuleNotFoundError Traceback (most recent call last) /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/try_import.py in try_import_rapids_cuml() 162 try: --&gt; 163 import cuml 164 except ImportError: ModuleNotFoundError: No module named &#39;cuml&#39; During handling of the above exception, another exception occurred: ImportError Traceback (most recent call last) &lt;ipython-input-12-488de9012a7d&gt; in &lt;module&gt;() 14 &#39;GBM&#39;: [{}, {&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, &#39;GBMLarge&#39;], 15 &#39;NN&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &gt; 16 &#39;FASTAI&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, 17 }, 18 ) /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/decorators.py in _call(*args, **kwargs) 27 def _call(*args, **kwargs): 28 gargs, gkwargs = g(*other_args, *args, **kwargs) &gt; 29 return f(*gargs, **gkwargs) 30 return _call 31 return _unpack_inner /usr/local/lib/python3.7/dist-packages/autogluon/tabular/predictor/predictor.py in fit(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, **kwargs) 689 self._learner.fit(X=train_data, X_val=tuning_data, X_unlabeled=unlabeled_data, 690 holdout_frac=holdout_frac, num_bag_folds=num_bag_folds, num_bag_sets=num_bag_sets, num_stack_levels=num_stack_levels, --&gt; 691 hyperparameters=hyperparameters, core_kwargs=core_kwargs, time_limit=time_limit, verbosity=verbosity) 692 self._set_post_fit_vars() 693 /usr/local/lib/python3.7/dist-packages/autogluon/tabular/learner/abstract_learner.py in fit(self, X, X_val, **kwargs) 124 raise AssertionError(&#39;Learner is already fit.&#39;) 125 self._validate_fit_input(X=X, X_val=X_val, **kwargs) --&gt; 126 return self._fit(X=X, X_val=X_val, **kwargs) 127 128 def _fit(self, X: DataFrame, X_val: DataFrame = None, scheduler_options=None, hyperparameter_tune=False, /usr/local/lib/python3.7/dist-packages/autogluon/tabular/learner/default_learner.py in _fit(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, verbosity, **trainer_fit_kwargs) 93 94 self.save() &gt; 95 trainer.fit(X, y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, holdout_frac=holdout_frac, time_limit=time_limit_trainer, **trainer_fit_kwargs) 96 self.save_trainer(trainer=trainer) 97 time_end = time.time() /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/auto_trainer.py in fit(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, feature_prune, holdout_frac, num_stack_levels, core_kwargs, time_limit, **kwargs) 50 self._train_multi_and_ensemble(X, y, X_val, y_val, X_unlabeled=X_unlabeled, hyperparameters=hyperparameters, 51 feature_prune=feature_prune, &gt; 52 num_stack_levels=num_stack_levels, time_limit=time_limit, core_kwargs=core_kwargs) 53 54 def get_models_distillation(self, hyperparameters, **kwargs): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in _train_multi_and_ensemble(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, **kwargs) 1290 self._num_cols_train = len(list(X.columns)) 1291 model_names_fit = self.train_multi_levels(X, y, hyperparameters=hyperparameters, X_val=X_val, y_val=y_val, -&gt; 1292 X_unlabeled=X_unlabeled, level_start=1, level_end=num_stack_levels+1, time_limit=time_limit, **kwargs) 1293 if len(self.get_model_names()) == 0: 1294 raise ValueError(&#39;AutoGluon did not successfully train any models&#39;) /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in train_multi_levels(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, feature_prune, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack) 259 models=hyperparameters, level=level, base_model_names=base_model_names, 260 feature_prune=feature_prune, --&gt; 261 core_kwargs=core_kwargs_level, aux_kwargs=aux_kwargs_level, name_suffix=name_suffix, 262 ) 263 model_names_fit += base_model_names + aux_models /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in stack_new_level(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, feature_prune, core_kwargs, aux_kwargs, name_suffix) 285 aux_kwargs[&#39;name_suffix&#39;] = aux_kwargs.get(&#39;name_suffix&#39;, &#39;&#39;) + name_suffix 286 core_models = self.stack_new_level_core(X=X, y=y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, models=models, --&gt; 287 level=level, base_model_names=base_model_names, feature_prune=feature_prune, **core_kwargs) 288 289 if self.bagged_mode: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in stack_new_level_core(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, **kwargs) 342 )) 343 --&gt; 344 models, model_args_fit = get_models_func(hyperparameters=models, **get_models_kwargs) 345 if model_args_fit: 346 hyperparameter_tune_kwargs = { /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/auto_trainer.py in get_models(self, hyperparameters, **kwargs) 26 return get_preset_models(path=path, problem_type=problem_type, eval_metric=eval_metric, 27 num_classes=num_classes, hyperparameters=hyperparameters, invalid_model_names=invalid_model_names, &gt; 28 feature_metadata=feature_metadata, silent=silent, **kwargs) 29 30 def fit(self, X, y, hyperparameters, X_val=None, y_val=None, X_unlabeled=None, feature_prune=False, holdout_frac=0.1, num_stack_levels=0, core_kwargs: dict = None, time_limit=None, **kwargs): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/model_presets/presets.py in get_preset_models(path, problem_type, eval_metric, hyperparameters, feature_metadata, num_classes, level, ensemble_type, ensemble_kwargs, ag_args_fit, ag_args, ag_args_ensemble, name_suffix, default_priorities, invalid_model_names, excluded_model_types, hyperparameter_preprocess_func, hyperparameter_preprocess_kwargs, silent) 189 model = model_factory(model_cfg, path=path, problem_type=problem_type, eval_metric=eval_metric, 190 num_classes=num_classes, name_suffix=name_suffix, ensemble_type=ensemble_type, ensemble_kwargs=ensemble_kwargs, --&gt; 191 invalid_name_set=invalid_name_set, level=level, feature_metadata=feature_metadata) 192 invalid_name_set.add(model.name) 193 if &#39;hyperparameter_tune_kwargs&#39; in model_cfg[AG_ARGS]: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/model_presets/presets.py in model_factory(model, path, problem_type, eval_metric, num_classes, name_suffix, ensemble_type, ensemble_kwargs, invalid_name_set, level, feature_metadata) 296 model_params.pop(AG_ARGS, None) 297 model_params.pop(AG_ARGS_ENSEMBLE, None) --&gt; 298 model_init = model_type(path=path, name=name, problem_type=problem_type, eval_metric=eval_metric, num_classes=num_classes, hyperparameters=model_params, feature_metadata=feature_metadata) 299 300 if ensemble_kwargs is not None: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/knn/knn_model.py in __init__(self, **kwargs) 25 def __init__(self, **kwargs): 26 super().__init__(**kwargs) &gt; 27 self._model_type = self._get_model_type() 28 29 def _get_model_type(self): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/knn/knn_rapids_model.py in _get_model_type(self) 26 &#34;&#34;&#34; 27 def _get_model_type(self): &gt; 28 try_import_rapids_cuml() 29 from cuml.neighbors import KNeighborsClassifier, KNeighborsRegressor 30 if self.problem_type == REGRESSION: /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/try_import.py in try_import_rapids_cuml() 163 import cuml 164 except ImportError: --&gt; 165 raise ImportError(&#34;`import cuml` failed. n&#34; 166 &#34;Ensure that you have a GPU and CUDA installation, and then install RAPIDS. n&#34; 167 &#34;You will likely need to create a fresh conda environment based off of a RAPIDS install, and then install AutoGluon on it. n&#34; ImportError: `import cuml` failed. Ensure that you have a GPU and CUDA installation, and then install RAPIDS. You will likely need to create a fresh conda environment based off of a RAPIDS install, and then install AutoGluon on it. RAPIDS is highly experimental within AutoGluon, and we recommend to only use RAPIDS if you are an advanced user / developer. Please refer to RAPIDS install instructions for more information: https://rapids.ai/start.html#get-rapids NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &#34;Open Examples&#34; button below. . submission = test[[&#39;id&#39;]] test_pred_proba = predictor.predict_proba(test) submission = pd.concat([submission, test_pred_proba], axis=1) submission.to_csv(&#39;submission.csv&#39;, index=False) submission.head() . 오류가 지속적으로 나서 실행을 못했습니다. (autogluon 모델) . 자동으로 분석해주는 모델인것 같고 실제로 이 모델 점수 상위 1%를 기록했다고 합니다. . &#47784;&#45944; 3 . from patsy import dmatrices from sklearn.neural_network import MLPClassifier columns = train.columns[1:-1] X = train[columns] y = np.ravel(train[&#39;target&#39;]) model = MLPClassifier(solver=&#39;lbfgs&#39;, alpha=1e-5, hidden_layer_sizes = (30, 10), random_state = 0, verbose = True) model.fit(X, y) . MLPClassifier(alpha=1e-05, hidden_layer_sizes=(30, 10), random_state=0, solver=&#39;lbfgs&#39;, verbose=True) . pred = model.predict(X) print(model.score(X, y)) print(sum(pred == y) / len(y)) . 0.8057629529073338 0.8057629529073338 . Xtest = test[test.columns[1:]] test_prob = model.predict_proba(Xtest) solution = pd.DataFrame(test_prob, columns=[&#39;Class_1&#39;,&#39;Class_2&#39;,&#39;Class_3&#39;,&#39;Class_4&#39;,&#39;Class_5&#39;,&#39;Class_6&#39;,&#39;Class_7&#39;,&#39;Class_8&#39;,&#39;Class_9&#39;]) solution[&#39;id&#39;] = test[&#39;id&#39;] cols = solution.columns.tolist() cols = cols[-1:] + cols[:-1] solution = solution[cols] solution.to_csv(&#39;otto_prediction.csv&#39;, index = False) .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/11/27/kagglessu7.html",
            "relUrl": "/2021/11/27/kagglessu7.html",
            "date": " • Nov 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "신용카드 사용자 연체 예측 코드",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, 100) import warnings warnings.filterwarnings(&quot;ignore&quot;) from lightgbm import LGBMClassifier from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import OneHotEncoder import random train = pd.read_csv(&quot;/content/drive/MyDrive/carddata/train.csv&quot;) test = pd.read_csv(&#39;/content/drive/MyDrive/carddata/test.csv&#39;) sample_submission = pd.read_csv(&#39;/content/drive/MyDrive/carddata/sample_submission.csv&#39;) train.head() . index gender car reality child_num income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email occyp_type family_size begin_month credit . 0 0 | F | N | N | 0 | 202500.0 | Commercial associate | Higher education | Married | Municipal apartment | -13899 | -4709 | 1 | 0 | 0 | 0 | NaN | 2.0 | -6.0 | 1.0 | . 1 1 | F | N | Y | 1 | 247500.0 | Commercial associate | Secondary / secondary special | Civil marriage | House / apartment | -11380 | -1540 | 1 | 0 | 0 | 1 | Laborers | 3.0 | -5.0 | 1.0 | . 2 2 | M | Y | Y | 0 | 450000.0 | Working | Higher education | Married | House / apartment | -19087 | -4434 | 1 | 0 | 1 | 0 | Managers | 2.0 | -22.0 | 2.0 | . 3 3 | F | N | Y | 0 | 202500.0 | Commercial associate | Secondary / secondary special | Married | House / apartment | -15088 | -2092 | 1 | 0 | 1 | 0 | Sales staff | 2.0 | -37.0 | 0.0 | . 4 4 | F | Y | Y | 0 | 157500.0 | State servant | Higher education | Married | House / apartment | -15037 | -2105 | 1 | 0 | 0 | 0 | Managers | 2.0 | -26.0 | 2.0 | . &#44592;&#48376; &#48320;&#49688; &#49444;&#47749; . gender : 성별(F/M), car : 차량 소유 유무(Y/N), reality : 부동산 소유 유무(Y/N), child_num : 자녀 수 . income_total : 연간 소득, income_type : 소득 분류(5개로 분리), edu_type : 교육 수준(5개로 분리) . family_type : 결혼 여부(5개로 분리), house_type : 생활 방식(6개로 분리), DAYS_BIRTH : 출생일(수집일부터 음수로 계산) . DAYS_EMPLOYED : 업무 시작일(수집일부터 음수로 계산, 업무 안하는 사람은 365243 값 부여), FLAG_MOBIL : 핸드폰 소유 여부 . work_phone : 업무용 전화 소유 여부, phone : 가정용 전화 소유 여부, email : 이메일 소유 여부 . occyp_type : 직업 유형, family_size: 가족 규모, begin_month : 신용카드 발급 월(수집일로부터 음수 계산) . 반응변수 =&gt; credit : 사용자의 신용카드 대금 연체를 기준으로 한 신용도. 낮을수록 높은 신용임. . train.describe() . index child_num income_total DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email family_size begin_month credit . count 26457.000000 | 26457.000000 | 2.645700e+04 | 26457.000000 | 26457.000000 | 26457.0 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | . mean 13228.000000 | 0.428658 | 1.873065e+05 | -15958.053899 | 59068.750728 | 1.0 | 0.224742 | 0.294251 | 0.091280 | 2.196848 | -26.123294 | 1.519560 | . std 7637.622372 | 0.747326 | 1.018784e+05 | 4201.589022 | 137475.427503 | 0.0 | 0.417420 | 0.455714 | 0.288013 | 0.916717 | 16.559550 | 0.702283 | . min 0.000000 | 0.000000 | 2.700000e+04 | -25152.000000 | -15713.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | -60.000000 | 0.000000 | . 25% 6614.000000 | 0.000000 | 1.215000e+05 | -19431.000000 | -3153.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -39.000000 | 1.000000 | . 50% 13228.000000 | 0.000000 | 1.575000e+05 | -15547.000000 | -1539.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -24.000000 | 2.000000 | . 75% 19842.000000 | 1.000000 | 2.250000e+05 | -12446.000000 | -407.000000 | 1.0 | 0.000000 | 1.000000 | 0.000000 | 3.000000 | -12.000000 | 2.000000 | . max 26456.000000 | 19.000000 | 1.575000e+06 | -7705.000000 | 365243.000000 | 1.0 | 1.000000 | 1.000000 | 1.000000 | 20.000000 | 0.000000 | 2.000000 | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 26457 entries, 0 to 26456 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 index 26457 non-null int64 1 gender 26457 non-null object 2 car 26457 non-null object 3 reality 26457 non-null object 4 child_num 26457 non-null int64 5 income_total 26457 non-null float64 6 income_type 26457 non-null object 7 edu_type 26457 non-null object 8 family_type 26457 non-null object 9 house_type 26457 non-null object 10 DAYS_BIRTH 26457 non-null int64 11 DAYS_EMPLOYED 26457 non-null int64 12 FLAG_MOBIL 26457 non-null int64 13 work_phone 26457 non-null int64 14 phone 26457 non-null int64 15 email 26457 non-null int64 16 occyp_type 18286 non-null object 17 family_size 26457 non-null float64 18 begin_month 26457 non-null float64 19 credit 26457 non-null float64 dtypes: float64(4), int64(8), object(8) memory usage: 4.0+ MB . 유일하게 occyp_type(직업유형) 변수가 null 값이 존재합니다. . NAN으로 채워넣겠습니다. . train.fillna(&#39;NAN&#39;, inplace=True) test.fillna(&#39;NAN&#39;, inplace=True) . plt.subplots(figsize = (8,8)) plt.pie(train[&#39;credit&#39;].value_counts(), labels = train[&#39;credit&#39;].value_counts().index, autopct=&quot;%.2f%%&quot;, shadow = True, startangle = 90) plt.title(&#39;credit ratio&#39;, size=20) plt.show() . matplotlib 패키지 내 pie 차트를 이용해 반응변수의 비율을 확인했습니다. . 신용등급이 떨어지는 2번의 비율이 상당히 크군요. . &#48276;&#51452;&#54805; &#48320;&#49688;&#47484; &#49888;&#50857;&#46321;&#44553;&#48324;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . train_0 = train[train[&#39;credit&#39;]==0.0] train_1 = train[train[&#39;credit&#39;]==1.0] train_2 = train[train[&#39;credit&#39;]==2.0] def cat_plot(column): f, ax = plt.subplots(1, 3, figsize=(16, 6)) sns.countplot(x = column, data = train_0, ax = ax[0], order = train_0[column].value_counts().index) ax[0].tick_params(labelsize=12) ax[0].set_title(&#39;credit = 0&#39;) ax[0].set_ylabel(&#39;count&#39;) ax[0].tick_params(rotation=50) sns.countplot(x = column, data = train_1, ax = ax[1], order = train_1[column].value_counts().index) ax[1].tick_params(labelsize=12) ax[1].set_title(&#39;credit = 1&#39;) ax[1].set_ylabel(&#39;count&#39;) ax[1].tick_params(rotation=50) sns.countplot(x = column, data = train_2, ax = ax[2], order = train_2[column].value_counts().index) ax[2].tick_params(labelsize=12) ax[2].set_title(&#39;credit = 2&#39;) ax[2].set_ylabel(&#39;count&#39;) ax[2].tick_params(rotation=50) plt.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() cat_plot(&quot;gender&quot;) . train 데이터를 신용등급에 따라 분류한 뒤 설명변수와에 관계를 그래프로 보는 함수를 만들었습니다. . 성별에 대해서 살펴봤는데, 절대적으로 여성이 그냥 많은 것 같습니다. . 더불어 성별에 따른 신용등급 차이는 모두 비슷한 비율에 그래프인 것으로 보아 확인하기 힘듭니다. . cat_plot(&#39;car&#39;) . 우선 차량보유를 하지 않은 사람이 모든 비율에서 많습니다. . 다만 신용 등급과에 연관성은 그래프로 봤을땐 크게 없는 것 같네요. . cat_plot(&#39;reality&#39;) . 모든 신용 등급에서 부동산을 소유한 사람들이 많았습니다. . 딱히 신용 등급에 따른 차이가 존재하지 않는 것 같네요. . cat_plot(&#39;income_type&#39;) . 소득 종류 변수도 신용 등급 별로 차이가 두드러지진 않습니다. . 다만 학생은 신용등급 0에 없는 점이 눈에 띄네요. . cat_plot(&#39;edu_type&#39;) . 교육 수준 변수 또한 신용 등급별로 차이가 있어보이진 않네요. . cat_plot(&#39;family_type&#39;) . 가족 구성 변수에 따른 신용등급 변수도 차이가 없는 것 같아요. . 전반적으로 결혼한 사람이 많은 것이 눈에 띄네요. . cat_plot(&#39;house_type&#39;) . house_type 변수 또한 큰 의미가 없는 변수인 것 같습니다. 대부분 House / apartment 타입이기 때문에 의미가 더더욱 없습니다. . cat_plot(&#39;FLAG_MOBIL&#39;) . 여기에 나온 모든 사람은 스마트폰을 보유하고 있습니다. . cat_plot(&#39;work_phone&#39;) . 신용 등급 그룹 별 가정 전화 비율이 차이가 없습니다. 가정용 전화기 보유률이 떨어지는게 눈에 띄네요. . cat_plot(&#39;email&#39;) . 이메일 변수 또한 유의미하지 않아 보입니다. . f, ax = plt.subplots(1, 3, figsize=(16, 6)) sns.countplot(y = &#39;occyp_type&#39;, data = train_0, order = train_0[&#39;occyp_type&#39;].value_counts().index, ax=ax[0]) sns.countplot(y = &#39;occyp_type&#39;, data = train_1, order = train_1[&#39;occyp_type&#39;].value_counts().index, ax=ax[1]) sns.countplot(y = &#39;occyp_type&#39;, data = train_2, order = train_2[&#39;occyp_type&#39;].value_counts().index, ax=ax[2]) plt.subplots_adjust(wspace=0.5, hspace=0.3) plt.show() . 직업 유형 변수를 신용 등급별로 비교했습니다. . 전반적인 경향은 비슷하지만, 세세한 차이가 조금 있어보입니다. . &#50672;&#49549;&#54805; &#48320;&#49688;&#47484; &#49888;&#50857;&#46321;&#44553;&#48324;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . def num_plot(column): fig, axes = plt.subplots(1, 3, figsize=(16, 6)) sns.distplot(train_0[column], ax = axes[0]) axes[0].tick_params(labelsize=12) axes[0].set_title(&#39;credit = 0&#39;) axes[0].set_ylabel(&#39;count&#39;) sns.distplot(train_1[column], ax = axes[1]) axes[1].tick_params(labelsize=12) axes[1].set_title(&#39;credit = 1&#39;) axes[1].set_ylabel(&#39;count&#39;) sns.distplot(train_2[column], ax = axes[2]) axes[2].tick_params(labelsize=12) axes[2].set_title(&#39;credit = 2&#39;) axes[2].set_ylabel(&#39;count&#39;) plt.subplots_adjust(wspace=0.3, hspace=0.3) num_plot(&quot;child_num&quot;) . 자녀 수 변수입니다. 신용 등급별로 큰 차이는 없어보입니다. . 다만 신용등급 2에 자녀가 아주 많은 소수의 변수가 존재하는 걸 알 수 있습니다. . num_plot(&quot;family_size&quot;) . 가족 수 변수도 자식 수 변수와 마찬가지 결과를 보이는 것 같아요. . num_plot(&quot;income_total&quot;) . 신용등급에 따른 월간 소득 차이는 크게 없어 보입니다. (??) . sns.distplot(train_0[&#39;income_total&#39;],label=&#39;0.0&#39;, hist=False) sns.distplot(train_1[&#39;income_total&#39;],label=&#39;0.1&#39;, hist=False) sns.distplot(train_2[&#39;income_total&#39;],label=&#39;0.2&#39;, hist=False) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f8be19fa9d0&gt; . 정확히 확인하기 위해 그래프를 겹첬는데요. 조금 차이는 있으나 많이 비슷한 것을 볼 수 있습니다. . num_plot(&quot;DAYS_BIRTH&quot;) . 숫자의 절대값이 작을 수록 젊은 사람 변수 입니다. 그래프가 전반적으로 비슷해 보입니다. . train_0[&#39;Month&#39;] = abs(train_0[&#39;begin_month&#39;]) train_1[&#39;Month&#39;] = abs(train_1[&#39;begin_month&#39;]) train_2[&#39;Month&#39;] = abs(train_2[&#39;begin_month&#39;]) train_0 = train_0.astype({&#39;Month&#39;: &#39;int&#39;}) train_1 = train_1.astype({&#39;Month&#39;: &#39;int&#39;}) train_2 = train_2.astype({&#39;Month&#39;: &#39;int&#39;}) train_0[&#39;Month&#39;].head() num_plot(&quot;Month&quot;) . 카드 생성일 변수를 양수로 바꿔서 분석했습니다. . 전반적으로 흐름은 비슷해보이는데, 카드 발급 초기에서 약 70프로 정도는 신용등급 1을, 약 30프로는 0을 부여하는 것 같습니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . object_col = [] for col in train.columns: if train[col].dtype == &#39;object&#39;: object_col.append(col) enc = OneHotEncoder() enc.fit(train.loc[:,object_col]) train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), columns=enc.get_feature_names(object_col)) train.drop(object_col, axis=1, inplace=True) train = pd.concat([train, train_onehot_df], axis=1) test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), columns=enc.get_feature_names(object_col)) test.drop(object_col, axis=1, inplace=True) test = pd.concat([test, test_onehot_df], axis=1) . 범주형 변수는 모두 원-핫 인코딩을 해줍니다. . sample_submission . index 0 1 2 . 0 26457 | 0 | 0 | 0 | . 1 26458 | 0 | 0 | 0 | . 2 26459 | 0 | 0 | 0 | . 3 26460 | 0 | 0 | 0 | . 4 26461 | 0 | 0 | 0 | . ... ... | ... | ... | ... | . 9995 36452 | 0 | 0 | 0 | . 9996 36453 | 0 | 0 | 0 | . 9997 36454 | 0 | 0 | 0 | . 9998 36455 | 0 | 0 | 0 | . 9999 36456 | 0 | 0 | 0 | . 10000 rows × 4 columns . 이 대회는 0, 1, 2의 확률이 어떻게 되는지 예측하는 모델입니다. . skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) folds=[] for train_idx, valid_idx in skf.split(train, train[&#39;credit&#39;]): folds.append((train_idx, valid_idx)) random.seed(42) lgb_models={} for fold in range(5): print(f&#39;===================================={fold+1}============================================&#39;) train_idx, valid_idx = folds[fold] X_train, X_valid, y_train, y_valid = train.drop([&#39;credit&#39;],axis=1).iloc[train_idx].values, train.drop([&#39;credit&#39;],axis=1).iloc[valid_idx].values, train[&#39;credit&#39;][train_idx].values, train[&#39;credit&#39;][valid_idx].values lgb = LGBMClassifier(n_estimators=1000) lgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=30, verbose=100) lgb_models[fold]=lgb print(f&#39;================================================================================ n n&#39;) . ====================================1============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676692 valid_1&#39;s multi_logloss: 0.766702 [200] training&#39;s multi_logloss: 0.596634 valid_1&#39;s multi_logloss: 0.755074 [300] training&#39;s multi_logloss: 0.53456 valid_1&#39;s multi_logloss: 0.751863 [400] training&#39;s multi_logloss: 0.482683 valid_1&#39;s multi_logloss: 0.750901 Early stopping, best iteration is: [385] training&#39;s multi_logloss: 0.489523 valid_1&#39;s multi_logloss: 0.750597 ================================================================================ ====================================2============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.673988 valid_1&#39;s multi_logloss: 0.778812 [200] training&#39;s multi_logloss: 0.593911 valid_1&#39;s multi_logloss: 0.766056 [300] training&#39;s multi_logloss: 0.532019 valid_1&#39;s multi_logloss: 0.762532 Early stopping, best iteration is: [358] training&#39;s multi_logloss: 0.500235 valid_1&#39;s multi_logloss: 0.761024 ================================================================================ ====================================3============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676709 valid_1&#39;s multi_logloss: 0.771762 [200] training&#39;s multi_logloss: 0.593522 valid_1&#39;s multi_logloss: 0.758924 Early stopping, best iteration is: [236] training&#39;s multi_logloss: 0.57026 valid_1&#39;s multi_logloss: 0.758105 ================================================================================ ====================================4============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.675515 valid_1&#39;s multi_logloss: 0.7694 [200] training&#39;s multi_logloss: 0.597206 valid_1&#39;s multi_logloss: 0.758117 [300] training&#39;s multi_logloss: 0.533343 valid_1&#39;s multi_logloss: 0.753141 Early stopping, best iteration is: [308] training&#39;s multi_logloss: 0.528916 valid_1&#39;s multi_logloss: 0.752857 ================================================================================ ====================================5============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676696 valid_1&#39;s multi_logloss: 0.767947 [200] training&#39;s multi_logloss: 0.595696 valid_1&#39;s multi_logloss: 0.757343 [300] training&#39;s multi_logloss: 0.531936 valid_1&#39;s multi_logloss: 0.753206 Early stopping, best iteration is: [346] training&#39;s multi_logloss: 0.50629 valid_1&#39;s multi_logloss: 0.752064 ================================================================================ . sample_submission.iloc[:,1:]=0 for fold in range(5): sample_submission.iloc[:,1:] += lgb_models[fold].predict_proba(test)/5 sample_submission.to_csv(&#39;ssu6_submission.csv&#39;, index=False) sample_submission.head() . index 0 1 2 . 0 26457 | 0.018329 | 0.187203 | 0.794468 | . 1 26458 | 0.061934 | 0.121026 | 0.817041 | . 2 26459 | 0.027629 | 0.203945 | 0.768427 | . 3 26460 | 0.067723 | 0.199497 | 0.732780 | . 4 26461 | 0.079370 | 0.229451 | 0.691179 | .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/11/14/kagglessu6.html",
            "relUrl": "/2021/11/14/kagglessu6.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "따릉이 데이터 예측 코드",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) train = pd.read_csv(&quot;/content/drive/MyDrive/bicycle/train.csv&quot;) test = pd.read_csv(&#39;/content/drive/MyDrive/bicycle/test.csv&#39;) sample_submission = pd.read_csv(&#39;/content/drive/MyDrive/bicycle/sample_submission.csv&#39;) train.head() . date_time wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability number_of_rentals . 0 2018-04-01 | 207.500 | 4.000 | 0.000 | 3.050 | 75.000 | 12.600 | 21.000 | 30.000 | 22994 | . 1 2018-04-02 | 208.317 | 2.950 | 0.000 | 3.278 | 69.833 | 12.812 | 19.000 | 19.500 | 28139 | . 2 2018-04-03 | 213.516 | 2.911 | 0.000 | 2.690 | 74.879 | 10.312 | 15.316 | 19.113 | 26817 | . 3 2018-04-04 | 143.836 | 3.692 | 0.425 | 3.138 | 71.849 | 8.312 | 12.368 | 43.493 | 26034 | . 4 2018-04-05 | 95.905 | 4.000 | 0.723 | 3.186 | 73.784 | 5.875 | 10.421 | 63.378 | 2833 | . &#48320;&#49688; &#53456;&#49353; . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 273 entries, 0 to 272 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 date_time 273 non-null object 1 wind_direction 273 non-null float64 2 sky_condition 273 non-null float64 3 precipitation_form 273 non-null float64 4 wind_speed 273 non-null float64 5 humidity 273 non-null float64 6 low_temp 273 non-null float64 7 high_temp 273 non-null float64 8 Precipitation_Probability 273 non-null float64 9 number_of_rentals 273 non-null int64 dtypes: float64(8), int64(1), object(1) memory usage: 21.5+ KB . number_of_rentals : 따릉이 대여량(Y값), date_time : 날짜, wind_direction : 풍향 . sky_condition : 하늘 상태(1 : 맑음, 3 : 구름 많음, 4 : 흐림, 하루에 8번 측정한 값 평균) . precipitation_form : 강수 형태(0 : 맑음, 1 : 비, 마찬가지로 하루에 8번 측정한 값 평균) . wind_speed : 풍속, humidity : 습도, low_temp : 최저기온, high_temp : 최고기온, precipitation_Probability : 강수확률 . 결측값은 없습니다. . import matplotlib.pyplot as plt plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train[&#39;number_of_rentals&#39;].values)) plt.show() . 반응변수의 이상치은 관찰되지 않는 것으로 보입니다. . train[&#39;date_time&#39;] = pd.to_datetime(train[&#39;date_time&#39;]) test[&#39;date_time&#39;] = pd.to_datetime(test[&#39;date_time&#39;]) train[&#39;day&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).day test[&#39;day&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).day train[&#39;month&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).month test[&#39;month&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).month train[&#39;year&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).year test[&#39;year&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).year train[&#39;weekday&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).weekday test[&#39;weekday&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).weekday . date_time이 날짜 변수이기 때문에 데이터 형식을 datetime으로 바꾸어줍니다. . 그 후 datetime 데이터 형식으로 얻을 수 있는 이점, 날/달/연/주말 변수를 추출합니다. . train[&#39;wind_direction&#39;].hist() train[&#39;wind_direction&#39;].max() . 321.622 . wind_direction은 바람 방향 변수입니다. 아마 방향을 360도로 생각해서 만든 변수인 것 같습니다. . 다만 바람 방향과 따릉이 대여량은 상관 없을 것 같습니다. . 물론, 서울 자전거 도로가 한강 기준으로 많이 구성되어 있어 도로가 동-서 기준으로 많이 있긴 합니다. . 하지만 바람 방향이 오늘은 이쪽이니 자전거를 타자라는 생각을 하진 않을 것 같습니다. 바람 세기가 더 중요하죠. . 또 바람 방향 변수는 하루에도 계속 바뀌기 때문에 평균적인 방향인 것 같은데, 만약 바람이 주로 0에서 20, 340에서 360 각도로 불었을때 평균치는 약 180입니다. . (방향이 동쪽에서 위 아래로만 움직인다면 10에서 350으로 쉽게 바뀔 수 있습니다.) . 이 수치가 과연 유의미할지 개인적으로 의문이 들어서, 이 변수는 빼는 것이 좋아보입니다. . train[&#39;precipitation_form&#39;].corr(train[&#39;Precipitation_Probability&#39;]) . 0.9106089542607185 . train[&#39;precipitation_form&#39;].corr(train[&#39;sky_condition&#39;]) . 0.6738137525457335 . 비가 오는 상황을 예측하는 두 변수 precipitation_form와 Precipitation_Probability간 상관관계는 당연히 높습니다. . 다만 Precipitation_Probability는 강우 확률 예측 변수 입니다. . 때문에 일일 강우 단기예측 기록인 precipitation_form 변수가 하루 비가 오는 날을 더 잘 표현할 것으로 생각됩니다. . 비슷한 부분을 설명하는 두 변수이기 때문에 precipitation_form 변수만 사용하겠습니다. . precipitation_form 변수는 하늘 상태를 나타내는 sky_condition 변수와도 상관관계가 높지만 극단적이진 않습니다. . 날씨가 흐린것 자체가 따릉이 대여량에 부정적인 영향을 준다고 생각하기 때문에 sky_condition 변수는 사용하겠습니다. . import matplotlib.pyplot as plt plt.figure(figsize=(20, 10)) plt.bar(train[&#39;date_time&#39;][train[&#39;year&#39;] == 2018], train[&#39;number_of_rentals&#39;][train[&#39;year&#39;] == 2018], width=0.6, color=&#39;grey&#39;) . &lt;BarContainer object of 91 artists&gt; . train[&#39;day&#39;][train[&#39;month&#39;] == 5] += 30 train[&#39;day&#39;][train[&#39;month&#39;] == 6] += 61 test[&#39;day&#39;][test[&#39;month&#39;] == 5] += 30 test[&#39;day&#39;][test[&#39;month&#39;] == 6] += 61 . 따릉이 대여량을 2018년 기준으로 날짜순으로 확인했습니다. . 4~6월 데이터인 만큼, 날이 점점 따뜻해지는 영향으로 변동이 심하긴 하지만 증가하는 추세가 보이는 것 같습니다. . (중간중간 값이 급격히 작아지는 것은 아마 비가 오는날인거 같습니다.) . 그래서 날짜 변수를 쓰는것 보다, 누적된 날짜가 몇일인지를 기록하는 변수를 쓰는게 좋을 것 같습니다. . (4월 15일 =&gt; 15일, 5월 2일 =&gt; 30일 + 2일 = 32일, 6월 10일 =&gt; 30일 + 31일 + 10일 = 71일) . 이렇게 되면 달 변수 또한 쓰지 않는게 좋을 것 같습니다. 만든 변수가 달 변수가 설명할 부분까지 설명하기 때문이죠. . import seaborn as sns def barplots(variable): plot = train.groupby(variable)[&#39;number_of_rentals&#39;].mean() sns.barplot(plot.index,plot.values) barplots(&#39;year&#39;) . 연도별 따릉이 이용자수를 나타내는 그래프 입니다. . 시간이 지날수록 따릉이 이용자수가 늘어나는 것을 확인할 수 있습니다. 그러므로 연도 변수는 매우 중요한 변수임을 알 수 있겠죠. . barplots(&#39;weekday&#39;) . 요일별 따릉이 이용자수를 나타내는 그래프 입니다. weekday 변수는 0은 월요일, 6은 일요일을 나타내는 요일 변수입니다. . 직관적으로 확인했을때 일요일에 따릉이 이용자수가 유의미하게 적은 것이 눈에 띕니다. . train_label = train[&#39;number_of_rentals&#39;] train.drop([&#39;date_time&#39;,&#39;wind_direction&#39;, &#39;Precipitation_Probability&#39;, &#39;month&#39;, &#39;number_of_rentals&#39;], axis = 1, inplace= True) test.drop([&#39;date_time&#39;,&#39;wind_direction&#39;, &#39;Precipitation_Probability&#39;, &#39;month&#39;], axis = 1, inplace= True) . 앞서 설명한 변수들을 제거합니다. . &#47784;&#45944; &#51201;&#54633; . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;number_of_rentals&#39;] = rf.predict(test) sample_submission.to_csv(&#39;bicycle_final_4.csv&#39;,encoding=&#39;UTF-8&#39;,index=False) . from xgboost import XGBRegressor xgb = XGBRegressor() xgb.fit(train,train_label) sample_submission[&#39;number_of_rentals&#39;] = xgb.predict(test) sample_submission.to_csv(&#39;bicycle_final_7.csv&#39;,encoding=&#39;UTF-8&#39;,index=False) . [11:30:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . 간단한 랜덤 포레스트 모델을 사용했습니다. 다른 모델을 사용하거나 하이퍼 파라미터를 조정하면 점수가 더 오를수도 있겠죠? .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/11/04/kagglessu5_plus.html",
            "relUrl": "/2021/11/04/kagglessu5_plus.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "판매량 예측 데이터 분석",
            "content": ". &#52880;&#44544;&#44284; &#50672;&#46041;&#54616;&#44592; . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;23e68db36970b65937516103c630ba75&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c competitive-data-science-predict-future-sales . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sample_submission.csv.zip to /content 0% 0.00/468k [00:00&lt;?, ?B/s] 100% 468k/468k [00:00&lt;00:00, 69.0MB/s] Downloading sales_train.csv.zip to /content 38% 5.00M/13.3M [00:00&lt;00:01, 5.79MB/s] 100% 13.3M/13.3M [00:00&lt;00:00, 14.4MB/s] Downloading item_categories.csv to /content 0% 0.00/3.49k [00:00&lt;?, ?B/s] 100% 3.49k/3.49k [00:00&lt;00:00, 2.51MB/s] Downloading shops.csv to /content 0% 0.00/2.91k [00:00&lt;?, ?B/s] 100% 2.91k/2.91k [00:00&lt;00:00, 10.6MB/s] Downloading test.csv.zip to /content 0% 0.00/1.02M [00:00&lt;?, ?B/s] 100% 1.02M/1.02M [00:00&lt;00:00, 156MB/s] Downloading items.csv.zip to /content 0% 0.00/368k [00:00&lt;?, ?B/s] 100% 368k/368k [00:00&lt;00:00, 117MB/s] . !unzip items.csv.zip !unzip sales_train.csv.zip !unzip sample_submission.csv.zip !unzip test.csv.zip . Archive: items.csv.zip inflating: items.csv Archive: sales_train.csv.zip inflating: sales_train.csv Archive: sample_submission.csv.zip inflating: sample_submission.csv Archive: test.csv.zip inflating: test.csv . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from matplotlib import pylab as plt import matplotlib.dates as mdates plt.rcParams[&#39;figure.figsize&#39;] = (15.0, 8.0) import seaborn as sns . train = pd.read_csv(&#39;./sales_train.csv&#39;) print (&#39;number of shops: &#39;, train[&#39;shop_id&#39;].max()) print (&#39;number of items: &#39;, train[&#39;item_id&#39;].max()) num_month = train[&#39;date_block_num&#39;].max() print (&#39;number of month: &#39;, num_month) print (&#39;size of train: &#39;, train.shape) train.head() . number of shops: 59 number of items: 22169 number of month: 33 size of train: (2935849, 6) . date date_block_num shop_id item_id item_price item_cnt_day . 0 02.01.2013 | 0 | 59 | 22154 | 999.00 | 1.0 | . 1 03.01.2013 | 0 | 25 | 2552 | 899.00 | 1.0 | . 2 05.01.2013 | 0 | 25 | 2552 | 899.00 | -1.0 | . 3 06.01.2013 | 0 | 25 | 2554 | 1709.05 | 1.0 | . 4 15.01.2013 | 0 | 25 | 2555 | 1099.00 | 1.0 | . 변수 설명 . date : 날짜 변수, date_block_num : 달 변수(2013년 1월 =&gt; 0, 2015년 10월 =&gt; 33) . shop_id, item_id : 상점/제품의 고유번호 변수 . item_price : 제품의 가격 변수, item_cnt_dat : 그 날 제품이 팔린 개수 . (여기서 item_cnt_dat 변수가 음수인 것은 물건이 반품된 것을 의미하는 것 같습니다.) . test = pd.read_csv(&#39;./test.csv&#39;) test.head() . ID shop_id item_id . 0 0 | 5 | 5037 | . 1 1 | 5 | 5320 | . 2 2 | 5 | 5233 | . 3 3 | 5 | 5232 | . 4 4 | 5 | 5268 | . sub = pd.read_csv(&#39;./sample_submission.csv&#39;) sub.head() . ID item_cnt_month . 0 0 | 0.5 | . 1 1 | 0.5 | . 2 2 | 0.5 | . 3 3 | 0.5 | . 4 4 | 0.5 | . 2015년 11월 데이터를 예측하는 캐글 대회입니다. . date_block_num 변수는 34가 되겠죠. . items = pd.read_csv(&#39;./items.csv&#39;) print (&#39;number of categories: &#39;, items[&#39;item_category_id&#39;].max()) # the maximun number of category id items.head() . number of categories: 83 . item_name item_id item_category_id . 0 ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.) D | 0 | 40 | . 1 !ABBYY FineReader 12 Professional Edition Full... | 1 | 76 | . 2 ***В ЛУЧАХ СЛАВЫ (UNV) D | 2 | 40 | . 3 ***ГОЛУБАЯ ВОЛНА (Univ) D | 3 | 40 | . 4 ***КОРОБКА (СТЕКЛО) D | 4 | 40 | . train_clean = train.drop(labels = [&#39;date&#39;, &#39;item_price&#39;], axis = 1) train_clean.head() . date_block_num shop_id item_id item_cnt_day . 0 0 | 59 | 22154 | 1.0 | . 1 0 | 25 | 2552 | 1.0 | . 2 0 | 25 | 2552 | -1.0 | . 3 0 | 25 | 2554 | 1.0 | . 4 0 | 25 | 2555 | 1.0 | . 날짜는 대체하는 date_block_num 변수가 있기 때문에 빼줍니다. . 또 제품 가격 변수 또한 빼줍니다. . train_clean = train_clean.groupby([&quot;item_id&quot;,&quot;shop_id&quot;,&quot;date_block_num&quot;]).sum().reset_index() train_clean = train_clean.rename(index=str, columns = {&quot;item_cnt_day&quot;:&quot;item_cnt_month&quot;}) train_clean = train_clean[[&quot;item_id&quot;,&quot;shop_id&quot;,&quot;date_block_num&quot;,&quot;item_cnt_month&quot;]] train_clean . item_id shop_id date_block_num item_cnt_month . 0 0 | 54 | 20 | 1.0 | . 1 1 | 55 | 15 | 2.0 | . 2 1 | 55 | 18 | 1.0 | . 3 1 | 55 | 19 | 1.0 | . 4 1 | 55 | 20 | 1.0 | . ... ... | ... | ... | ... | . 1609119 22168 | 12 | 8 | 1.0 | . 1609120 22168 | 16 | 1 | 1.0 | . 1609121 22168 | 42 | 1 | 1.0 | . 1609122 22168 | 43 | 2 | 1.0 | . 1609123 22169 | 25 | 14 | 1.0 | . 1609124 rows × 4 columns . 같은 달별로(= date_block_num 변수가 같은 값으로) 묶어줍니다. . 테스트 데이터에서 예측하고자 하는 값의 범위가 달 단위이기 때문입니다. . 변수 이름 또한 그에 맞게 item_cnt_month로 바꿨습니다. . &#49884;&#44228;&#50676; &#45936;&#51060;&#53552; &#50672;&#49845;&#54616;&#44592; . check = train_clean[[&quot;shop_id&quot;,&quot;item_id&quot;,&quot;date_block_num&quot;,&quot;item_cnt_month&quot;]] check = check.loc[check[&#39;shop_id&#39;] == 5] check = check.loc[check[&#39;item_id&#39;] == 5037] check . shop_id item_id date_block_num item_cnt_month . 400439 5 | 5037 | 20 | 1.0 | . 400440 5 | 5037 | 22 | 1.0 | . 400441 5 | 5037 | 23 | 2.0 | . 400442 5 | 5037 | 24 | 2.0 | . 400443 5 | 5037 | 28 | 1.0 | . 400444 5 | 5037 | 29 | 1.0 | . 400445 5 | 5037 | 30 | 1.0 | . 400446 5 | 5037 | 31 | 3.0 | . 400447 5 | 5037 | 32 | 1.0 | . 특정 shop_id와 item_id 값을 가지는 값만 모았습니다. . 시계열 분석을 처음하기 때문에 1차로 소량의 데이터를 다루었습니다. . 이렇게 데이터 분석을 공부하면 보다 직관적으로 LSTM 모델을 학습할 수 있을 것 같습니다. . plt.figure(figsize=(10,4)) plt.title(&#39;Check - Sales of Item 5037 at Shop 5&#39;) plt.xlabel(&#39;Month&#39;) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) plt.plot(check[&quot;date_block_num&quot;],check[&quot;item_cnt_month&quot;]); . 단순히 Y값에 대해 그림을 그려보았습니다. . month_list=[i for i in range(num_month+1)] # num_month = train[&#39;date_block_num&#39;].max(), 최고값 shop = [] for i in range(num_month+1): shop.append(5) item = [] for i in range(num_month+1): item.append(5037) months_full = pd.DataFrame({&#39;shop_id&#39;:shop, &#39;item_id&#39;:item,&#39;date_block_num&#39;:month_list}) months_full.head(10) . shop_id item_id date_block_num . 0 5 | 5037 | 0 | . 1 5 | 5037 | 1 | . 2 5 | 5037 | 2 | . 3 5 | 5037 | 3 | . 4 5 | 5037 | 4 | . 5 5 | 5037 | 5 | . 6 5 | 5037 | 6 | . 7 5 | 5037 | 7 | . 8 5 | 5037 | 8 | . 9 5 | 5037 | 9 | . 빈 데이터를 없애기 위해 처음부터 데이터프레임을 세팅하는 모습입니다. . shop = [] for i in range(num_month+1): shop.append(5) . 다만 이 코드 보다는 [5]*(num_month+1) 식으로 리스트를 구성하는게 더 깔끔한 것 같습니다. . sales_33month = pd.merge(check, months_full, how=&#39;right&#39;, on=[&#39;shop_id&#39;,&#39;item_id&#39;,&#39;date_block_num&#39;]) sales_33month = sales_33month.sort_values(by=[&#39;date_block_num&#39;]) sales_33month.fillna(0.00,inplace=True) plt.figure(figsize=(10,4)) plt.title(&#39;Check - Sales of Item 5037 at Shop 5 for whole period&#39;) plt.xlabel(&#39;Month&#39;) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) plt.plot(sales_33month[&quot;date_block_num&quot;],sales_33month[&quot;item_cnt_month&quot;]); . 물품 구매가 없는 데이터까지 0 값을 넣어서 그림을 그렸습니다. . for i in range(1,6): sales_33month[&quot;T_&quot; + str(i)] = sales_33month.item_cnt_month.shift(i) sales_33month.fillna(0.0, inplace=True) df = sales_33month[[&#39;shop_id&#39;,&#39;item_id&#39;,&#39;date_block_num&#39;,&#39;T_1&#39;,&#39;T_2&#39;,&#39;T_3&#39;,&#39;T_4&#39;,&#39;T_5&#39;, &#39;item_cnt_month&#39;]].reset_index() df = df.drop(labels = [&#39;index&#39;], axis = 1) df . shop_id item_id date_block_num T_1 T_2 T_3 T_4 T_5 item_cnt_month . 0 5 | 5037 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 5 | 5037 | 1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 5 | 5037 | 2 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 5 | 5037 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 5 | 5037 | 4 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 5 | 5037 | 5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 6 5 | 5037 | 6 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 7 5 | 5037 | 7 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 8 5 | 5037 | 8 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 9 5 | 5037 | 9 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 10 5 | 5037 | 10 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 11 5 | 5037 | 11 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 12 5 | 5037 | 12 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 13 5 | 5037 | 13 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 14 5 | 5037 | 14 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 15 5 | 5037 | 15 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 16 5 | 5037 | 16 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 17 5 | 5037 | 17 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 18 5 | 5037 | 18 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 19 5 | 5037 | 19 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 20 5 | 5037 | 20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 21 5 | 5037 | 21 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 22 5 | 5037 | 22 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 23 5 | 5037 | 23 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 2.0 | . 24 5 | 5037 | 24 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | 2.0 | . 25 5 | 5037 | 25 | 2.0 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 26 5 | 5037 | 26 | 0.0 | 2.0 | 2.0 | 1.0 | 0.0 | 0.0 | . 27 5 | 5037 | 27 | 0.0 | 0.0 | 2.0 | 2.0 | 1.0 | 0.0 | . 28 5 | 5037 | 28 | 0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 1.0 | . 29 5 | 5037 | 29 | 1.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | . 30 5 | 5037 | 30 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 31 5 | 5037 | 31 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 3.0 | . 32 5 | 5037 | 32 | 3.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | . 33 5 | 5037 | 33 | 1.0 | 3.0 | 1.0 | 1.0 | 1.0 | 0.0 | . 시계열 분석을 기초부터 뜯어본 것 같습니다. . T1 ~ T5에 의미는 최근 5달간 이전 Y값의 기록입니다. 예를 들면 T1은 한달 전 Y값을 나타냅니다. . 시간의 흐름에 따라 예측값이 영향을 받기 때문에 이러한 방식이 지금 이 데이터에서 적절합니다. . LSTM &#47784;&#45944; &#49324;&#50857; . train_df = df[:-3] val_df = df[-3:] x_train,y_train = train_df.drop([&quot;item_cnt_month&quot;],axis=1),train_df.item_cnt_month x_val,y_val = val_df.drop([&quot;item_cnt_month&quot;],axis=1),val_df.item_cnt_month . 맨 마지막 3개 데이터를 test 데이터로 사용합니다. . from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM model_lstm = Sequential() model_lstm.add(LSTM(15, input_shape=(1,8))) model_lstm.add(Dense(1)) model_lstm.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . from sklearn.preprocessing import StandardScaler,MinMaxScaler scaler = StandardScaler() scaler = MinMaxScaler(feature_range=(-1, 1)) x_train_scaled = scaler.fit_transform(x_train) x_valid_scaled = scaler.fit_transform(x_val) . x_train_reshaped = x_train_scaled.reshape((x_train_scaled.shape[0], 1, x_train_scaled.shape[1])) x_val_resaped = x_valid_scaled.reshape((x_valid_scaled.shape[0], 1, x_valid_scaled.shape[1])) history = model_lstm.fit(x_train_reshaped, y_train, validation_data=(x_val_resaped, y_val),epochs=70, batch_size=12, verbose=2, shuffle=False) y_pre = model_lstm.predict(x_val_resaped) . Epoch 1/70 3/3 - 2s - loss: 0.4119 - accuracy: 0.7742 - val_loss: 3.6385 - val_accuracy: 0.3333 Epoch 2/70 3/3 - 0s - loss: 0.3959 - accuracy: 0.7742 - val_loss: 3.5825 - val_accuracy: 0.3333 Epoch 3/70 3/3 - 0s - loss: 0.3818 - accuracy: 0.7742 - val_loss: 3.5290 - val_accuracy: 0.3333 Epoch 4/70 3/3 - 0s - loss: 0.3689 - accuracy: 0.7742 - val_loss: 3.4781 - val_accuracy: 0.3333 Epoch 5/70 3/3 - 0s - loss: 0.3571 - accuracy: 0.7742 - val_loss: 3.4296 - val_accuracy: 0.3333 Epoch 6/70 3/3 - 0s - loss: 0.3464 - accuracy: 0.7742 - val_loss: 3.3839 - val_accuracy: 0.3333 Epoch 7/70 3/3 - 0s - loss: 0.3368 - accuracy: 0.7742 - val_loss: 3.3409 - val_accuracy: 0.3333 Epoch 8/70 3/3 - 0s - loss: 0.3281 - accuracy: 0.7742 - val_loss: 3.3008 - val_accuracy: 0.3333 Epoch 9/70 3/3 - 0s - loss: 0.3203 - accuracy: 0.7742 - val_loss: 3.2637 - val_accuracy: 0.3333 Epoch 10/70 3/3 - 0s - loss: 0.3132 - accuracy: 0.7742 - val_loss: 3.2296 - val_accuracy: 0.3333 Epoch 11/70 3/3 - 0s - loss: 0.3069 - accuracy: 0.7742 - val_loss: 3.1984 - val_accuracy: 0.3333 Epoch 12/70 3/3 - 0s - loss: 0.3012 - accuracy: 0.7742 - val_loss: 3.1702 - val_accuracy: 0.3333 Epoch 13/70 3/3 - 0s - loss: 0.2960 - accuracy: 0.7742 - val_loss: 3.1451 - val_accuracy: 0.3333 Epoch 14/70 3/3 - 0s - loss: 0.2913 - accuracy: 0.7742 - val_loss: 3.1228 - val_accuracy: 0.3333 Epoch 15/70 3/3 - 0s - loss: 0.2869 - accuracy: 0.7742 - val_loss: 3.1035 - val_accuracy: 0.3333 Epoch 16/70 3/3 - 0s - loss: 0.2829 - accuracy: 0.7742 - val_loss: 3.0871 - val_accuracy: 0.3333 Epoch 17/70 3/3 - 0s - loss: 0.2791 - accuracy: 0.7742 - val_loss: 3.0733 - val_accuracy: 0.3333 Epoch 18/70 3/3 - 0s - loss: 0.2755 - accuracy: 0.7742 - val_loss: 3.0623 - val_accuracy: 0.3333 Epoch 19/70 3/3 - 0s - loss: 0.2720 - accuracy: 0.7742 - val_loss: 3.0537 - val_accuracy: 0.3333 Epoch 20/70 3/3 - 0s - loss: 0.2687 - accuracy: 0.7742 - val_loss: 3.0476 - val_accuracy: 0.3333 Epoch 21/70 3/3 - 0s - loss: 0.2654 - accuracy: 0.7742 - val_loss: 3.0437 - val_accuracy: 0.3333 Epoch 22/70 3/3 - 0s - loss: 0.2622 - accuracy: 0.7742 - val_loss: 3.0419 - val_accuracy: 0.3333 Epoch 23/70 3/3 - 0s - loss: 0.2590 - accuracy: 0.7742 - val_loss: 3.0421 - val_accuracy: 0.3333 Epoch 24/70 3/3 - 0s - loss: 0.2558 - accuracy: 0.8065 - val_loss: 3.0440 - val_accuracy: 0.3333 Epoch 25/70 3/3 - 0s - loss: 0.2527 - accuracy: 0.8065 - val_loss: 3.0477 - val_accuracy: 0.3333 Epoch 26/70 3/3 - 0s - loss: 0.2495 - accuracy: 0.8387 - val_loss: 3.0528 - val_accuracy: 0.3333 Epoch 27/70 3/3 - 0s - loss: 0.2463 - accuracy: 0.8387 - val_loss: 3.0592 - val_accuracy: 0.3333 Epoch 28/70 3/3 - 0s - loss: 0.2432 - accuracy: 0.8387 - val_loss: 3.0669 - val_accuracy: 0.3333 Epoch 29/70 3/3 - 0s - loss: 0.2401 - accuracy: 0.8387 - val_loss: 3.0756 - val_accuracy: 0.3333 Epoch 30/70 3/3 - 0s - loss: 0.2370 - accuracy: 0.8387 - val_loss: 3.0853 - val_accuracy: 0.3333 Epoch 31/70 3/3 - 0s - loss: 0.2339 - accuracy: 0.8387 - val_loss: 3.0958 - val_accuracy: 0.3333 Epoch 32/70 3/3 - 0s - loss: 0.2308 - accuracy: 0.8387 - val_loss: 3.1070 - val_accuracy: 0.3333 Epoch 33/70 3/3 - 0s - loss: 0.2278 - accuracy: 0.8065 - val_loss: 3.1187 - val_accuracy: 0.6667 Epoch 34/70 3/3 - 0s - loss: 0.2248 - accuracy: 0.8065 - val_loss: 3.1310 - val_accuracy: 0.6667 Epoch 35/70 3/3 - 0s - loss: 0.2219 - accuracy: 0.8065 - val_loss: 3.1436 - val_accuracy: 0.6667 Epoch 36/70 3/3 - 0s - loss: 0.2190 - accuracy: 0.7742 - val_loss: 3.1565 - val_accuracy: 0.6667 Epoch 37/70 3/3 - 0s - loss: 0.2162 - accuracy: 0.7742 - val_loss: 3.1696 - val_accuracy: 0.3333 Epoch 38/70 3/3 - 0s - loss: 0.2134 - accuracy: 0.7742 - val_loss: 3.1829 - val_accuracy: 0.3333 Epoch 39/70 3/3 - 0s - loss: 0.2107 - accuracy: 0.8065 - val_loss: 3.1963 - val_accuracy: 0.3333 Epoch 40/70 3/3 - 0s - loss: 0.2081 - accuracy: 0.8065 - val_loss: 3.2096 - val_accuracy: 0.3333 Epoch 41/70 3/3 - 0s - loss: 0.2056 - accuracy: 0.8065 - val_loss: 3.2229 - val_accuracy: 0.3333 Epoch 42/70 3/3 - 0s - loss: 0.2031 - accuracy: 0.8065 - val_loss: 3.2361 - val_accuracy: 0.3333 Epoch 43/70 3/3 - 0s - loss: 0.2008 - accuracy: 0.8065 - val_loss: 3.2492 - val_accuracy: 0.3333 Epoch 44/70 3/3 - 0s - loss: 0.1985 - accuracy: 0.8065 - val_loss: 3.2621 - val_accuracy: 0.3333 Epoch 45/70 3/3 - 0s - loss: 0.1963 - accuracy: 0.8065 - val_loss: 3.2748 - val_accuracy: 0.3333 Epoch 46/70 3/3 - 0s - loss: 0.1941 - accuracy: 0.8065 - val_loss: 3.2872 - val_accuracy: 0.3333 Epoch 47/70 3/3 - 0s - loss: 0.1921 - accuracy: 0.8065 - val_loss: 3.2994 - val_accuracy: 0.3333 Epoch 48/70 3/3 - 0s - loss: 0.1901 - accuracy: 0.8065 - val_loss: 3.3113 - val_accuracy: 0.3333 Epoch 49/70 3/3 - 0s - loss: 0.1882 - accuracy: 0.8065 - val_loss: 3.3229 - val_accuracy: 0.3333 Epoch 50/70 3/3 - 0s - loss: 0.1864 - accuracy: 0.8065 - val_loss: 3.3342 - val_accuracy: 0.3333 Epoch 51/70 3/3 - 0s - loss: 0.1847 - accuracy: 0.8065 - val_loss: 3.3451 - val_accuracy: 0.3333 Epoch 52/70 3/3 - 0s - loss: 0.1830 - accuracy: 0.8065 - val_loss: 3.3558 - val_accuracy: 0.3333 Epoch 53/70 3/3 - 0s - loss: 0.1814 - accuracy: 0.8065 - val_loss: 3.3661 - val_accuracy: 0.3333 Epoch 54/70 3/3 - 0s - loss: 0.1799 - accuracy: 0.8065 - val_loss: 3.3760 - val_accuracy: 0.3333 Epoch 55/70 3/3 - 0s - loss: 0.1785 - accuracy: 0.8065 - val_loss: 3.3855 - val_accuracy: 0.3333 Epoch 56/70 3/3 - 0s - loss: 0.1771 - accuracy: 0.8065 - val_loss: 3.3947 - val_accuracy: 0.3333 Epoch 57/70 3/3 - 0s - loss: 0.1757 - accuracy: 0.8065 - val_loss: 3.4036 - val_accuracy: 0.3333 Epoch 58/70 3/3 - 0s - loss: 0.1745 - accuracy: 0.8065 - val_loss: 3.4120 - val_accuracy: 0.3333 Epoch 59/70 3/3 - 0s - loss: 0.1732 - accuracy: 0.8065 - val_loss: 3.4201 - val_accuracy: 0.3333 Epoch 60/70 3/3 - 0s - loss: 0.1720 - accuracy: 0.8065 - val_loss: 3.4278 - val_accuracy: 0.3333 Epoch 61/70 3/3 - 0s - loss: 0.1709 - accuracy: 0.8065 - val_loss: 3.4351 - val_accuracy: 0.3333 Epoch 62/70 3/3 - 0s - loss: 0.1698 - accuracy: 0.8065 - val_loss: 3.4420 - val_accuracy: 0.3333 Epoch 63/70 3/3 - 0s - loss: 0.1687 - accuracy: 0.8065 - val_loss: 3.4485 - val_accuracy: 0.3333 Epoch 64/70 3/3 - 0s - loss: 0.1677 - accuracy: 0.8065 - val_loss: 3.4547 - val_accuracy: 0.3333 Epoch 65/70 3/3 - 0s - loss: 0.1667 - accuracy: 0.8065 - val_loss: 3.4605 - val_accuracy: 0.3333 Epoch 66/70 3/3 - 0s - loss: 0.1658 - accuracy: 0.8065 - val_loss: 3.4659 - val_accuracy: 0.3333 Epoch 67/70 3/3 - 0s - loss: 0.1648 - accuracy: 0.8065 - val_loss: 3.4710 - val_accuracy: 0.3333 Epoch 68/70 3/3 - 0s - loss: 0.1639 - accuracy: 0.8065 - val_loss: 3.4758 - val_accuracy: 0.3333 Epoch 69/70 3/3 - 0s - loss: 0.1631 - accuracy: 0.8065 - val_loss: 3.4802 - val_accuracy: 0.3333 Epoch 70/70 3/3 - 0s - loss: 0.1622 - accuracy: 0.8065 - val_loss: 3.4844 - val_accuracy: 0.3333 . fig, ax = plt.subplots() ax.plot(x_val[&#39;date_block_num&#39;], y_val, label=&#39;Actual&#39;) ax.plot(x_val[&#39;date_block_num&#39;], y_pre, label=&#39;Predicted&#39;) plt.title(&#39;LSTM Prediction vs Actual Sales for last 3 months&#39;) plt.xlabel(&#39;Month&#39;) plt.xticks(x_val[&#39;date_block_num&#39;]) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) ax.legend() plt.show() . LSTM 모델을 적용시킨 모습입니다. . 잘 맞췄다면 잘 맞췄다고도 말 할수 있고 아쉽다면 아쉽다고 할 수 있는 결과인 것 같습니다. . &#45936;&#51060;&#53552; &#53456;&#49353; . sales_data = pd.read_csv(&#39;./sales_train.csv&#39;) item_cat = pd.read_csv(&#39;./item_categories.csv&#39;) items = pd.read_csv(&#39;./items.csv&#39;) shops = pd.read_csv(&#39;./shops.csv&#39;) sample_submission = pd.read_csv(&#39;./sample_submission.csv&#39;) test_data = pd.read_csv(&#39;./test.csv&#39;) . def basic_eda(df): print(&quot;-TOP 5 RECORDS--&quot;) print(df.head(5)) print(&quot;-INFO--&quot;) print(df.info()) print(&quot;-Describe-&quot;) print(df.describe()) print(&quot;-Columns--&quot;) print(df.columns) print(&quot;-Data Types--&quot;) print(df.dtypes) print(&quot;-Missing Values-&quot;) print(df.isnull().sum()) print(&quot;-NULL values-&quot;) print(df.isna().sum()) print(&quot;--Shape Of Data-&quot;) print(df.shape) print(&quot;=============================Sales Data=============================&quot;) basic_eda(sales_data) print(&quot;=============================Test data=============================&quot;) basic_eda(test_data) print(&quot;=============================Item Categories=============================&quot;) basic_eda(item_cat) print(&quot;=============================Items=============================&quot;) basic_eda(items) print(&quot;=============================Shops=============================&quot;) basic_eda(shops) print(&quot;=============================Sample Submission=============================&quot;) basic_eda(sample_submission) . =============================Sales Data============================= -TOP 5 RECORDS-- date date_block_num shop_id item_id item_price item_cnt_day 0 02.01.2013 0 59 22154 999.00 1.0 1 03.01.2013 0 25 2552 899.00 1.0 2 05.01.2013 0 25 2552 899.00 -1.0 3 06.01.2013 0 25 2554 1709.05 1.0 4 15.01.2013 0 25 2555 1099.00 1.0 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2935849 entries, 0 to 2935848 Data columns (total 6 columns): # Column Dtype -- 0 date object 1 date_block_num int64 2 shop_id int64 3 item_id int64 4 item_price float64 5 item_cnt_day float64 dtypes: float64(2), int64(3), object(1) memory usage: 134.4+ MB None -Describe- date_block_num shop_id item_id item_price item_cnt_day count 2.935849e+06 2.935849e+06 2.935849e+06 2.935849e+06 2.935849e+06 mean 1.456991e+01 3.300173e+01 1.019723e+04 8.908532e+02 1.242641e+00 std 9.422988e+00 1.622697e+01 6.324297e+03 1.729800e+03 2.618834e+00 min 0.000000e+00 0.000000e+00 0.000000e+00 -1.000000e+00 -2.200000e+01 25% 7.000000e+00 2.200000e+01 4.476000e+03 2.490000e+02 1.000000e+00 50% 1.400000e+01 3.100000e+01 9.343000e+03 3.990000e+02 1.000000e+00 75% 2.300000e+01 4.700000e+01 1.568400e+04 9.990000e+02 1.000000e+00 max 3.300000e+01 5.900000e+01 2.216900e+04 3.079800e+05 2.169000e+03 -Columns-- Index([&#39;date&#39;, &#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;, &#39;item_price&#39;, &#39;item_cnt_day&#39;], dtype=&#39;object&#39;) -Data Types-- date object date_block_num int64 shop_id int64 item_id int64 item_price float64 item_cnt_day float64 dtype: object -Missing Values- date 0 date_block_num 0 shop_id 0 item_id 0 item_price 0 item_cnt_day 0 dtype: int64 -NULL values- date 0 date_block_num 0 shop_id 0 item_id 0 item_price 0 item_cnt_day 0 dtype: int64 --Shape Of Data- (2935849, 6) =============================Test data============================= -TOP 5 RECORDS-- ID shop_id item_id 0 0 5 5037 1 1 5 5320 2 2 5 5233 3 3 5 5232 4 4 5 5268 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214200 entries, 0 to 214199 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 ID 214200 non-null int64 1 shop_id 214200 non-null int64 2 item_id 214200 non-null int64 dtypes: int64(3) memory usage: 4.9 MB None -Describe- ID shop_id item_id count 214200.000000 214200.000000 214200.000000 mean 107099.500000 31.642857 11019.398627 std 61834.358168 17.561933 6252.644590 min 0.000000 2.000000 30.000000 25% 53549.750000 16.000000 5381.500000 50% 107099.500000 34.500000 11203.000000 75% 160649.250000 47.000000 16071.500000 max 214199.000000 59.000000 22167.000000 -Columns-- Index([&#39;ID&#39;, &#39;shop_id&#39;, &#39;item_id&#39;], dtype=&#39;object&#39;) -Data Types-- ID int64 shop_id int64 item_id int64 dtype: object -Missing Values- ID 0 shop_id 0 item_id 0 dtype: int64 -NULL values- ID 0 shop_id 0 item_id 0 dtype: int64 --Shape Of Data- (214200, 3) =============================Item Categories============================= -TOP 5 RECORDS-- item_category_name item_category_id 0 PC - Гарнитуры/Наушники 0 1 Аксессуары - PS2 1 2 Аксессуары - PS3 2 3 Аксессуары - PS4 3 4 Аксессуары - PSP 4 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 84 entries, 0 to 83 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 item_category_name 84 non-null object 1 item_category_id 84 non-null int64 dtypes: int64(1), object(1) memory usage: 1.4+ KB None -Describe- item_category_id count 84.000000 mean 41.500000 std 24.392622 min 0.000000 25% 20.750000 50% 41.500000 75% 62.250000 max 83.000000 -Columns-- Index([&#39;item_category_name&#39;, &#39;item_category_id&#39;], dtype=&#39;object&#39;) -Data Types-- item_category_name object item_category_id int64 dtype: object -Missing Values- item_category_name 0 item_category_id 0 dtype: int64 -NULL values- item_category_name 0 item_category_id 0 dtype: int64 --Shape Of Data- (84, 2) =============================Items============================= -TOP 5 RECORDS-- item_name item_id item_category_id 0 ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.) D 0 40 1 !ABBYY FineReader 12 Professional Edition Full... 1 76 2 ***В ЛУЧАХ СЛАВЫ (UNV) D 2 40 3 ***ГОЛУБАЯ ВОЛНА (Univ) D 3 40 4 ***КОРОБКА (СТЕКЛО) D 4 40 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22170 entries, 0 to 22169 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 item_name 22170 non-null object 1 item_id 22170 non-null int64 2 item_category_id 22170 non-null int64 dtypes: int64(2), object(1) memory usage: 519.7+ KB None -Describe- item_id item_category_id count 22170.00000 22170.000000 mean 11084.50000 46.290753 std 6400.07207 15.941486 min 0.00000 0.000000 25% 5542.25000 37.000000 50% 11084.50000 40.000000 75% 16626.75000 58.000000 max 22169.00000 83.000000 -Columns-- Index([&#39;item_name&#39;, &#39;item_id&#39;, &#39;item_category_id&#39;], dtype=&#39;object&#39;) -Data Types-- item_name object item_id int64 item_category_id int64 dtype: object -Missing Values- item_name 0 item_id 0 item_category_id 0 dtype: int64 -NULL values- item_name 0 item_id 0 item_category_id 0 dtype: int64 --Shape Of Data- (22170, 3) =============================Shops============================= -TOP 5 RECORDS-- shop_name shop_id 0 !Якутск Орджоникидзе, 56 фран 0 1 !Якутск ТЦ &#34;Центральный&#34; фран 1 2 Адыгея ТЦ &#34;Мега&#34; 2 3 Балашиха ТРК &#34;Октябрь-Киномир&#34; 3 4 Волжский ТЦ &#34;Волга Молл&#34; 4 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 60 entries, 0 to 59 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 shop_name 60 non-null object 1 shop_id 60 non-null int64 dtypes: int64(1), object(1) memory usage: 1.1+ KB None -Describe- shop_id count 60.000000 mean 29.500000 std 17.464249 min 0.000000 25% 14.750000 50% 29.500000 75% 44.250000 max 59.000000 -Columns-- Index([&#39;shop_name&#39;, &#39;shop_id&#39;], dtype=&#39;object&#39;) -Data Types-- shop_name object shop_id int64 dtype: object -Missing Values- shop_name 0 shop_id 0 dtype: int64 -NULL values- shop_name 0 shop_id 0 dtype: int64 --Shape Of Data- (60, 2) =============================Sample Submission============================= -TOP 5 RECORDS-- ID item_cnt_month 0 0 0.5 1 1 0.5 2 2 0.5 3 3 0.5 4 4 0.5 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214200 entries, 0 to 214199 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 ID 214200 non-null int64 1 item_cnt_month 214200 non-null float64 dtypes: float64(1), int64(1) memory usage: 3.3 MB None -Describe- ID item_cnt_month count 214200.000000 214200.0 mean 107099.500000 0.5 std 61834.358168 0.0 min 0.000000 0.5 25% 53549.750000 0.5 50% 107099.500000 0.5 75% 160649.250000 0.5 max 214199.000000 0.5 -Columns-- Index([&#39;ID&#39;, &#39;item_cnt_month&#39;], dtype=&#39;object&#39;) -Data Types-- ID int64 item_cnt_month float64 dtype: object -Missing Values- ID 0 item_cnt_month 0 dtype: int64 -NULL values- ID 0 item_cnt_month 0 dtype: int64 --Shape Of Data- (214200, 2) . 앞 코드와 다른 사람 코드입니다. . 여기서 train 데이터 프레임을 이 사람은 sales_data 이름으로 했네요. . 사실 데이터 탐색하는 함수를 잘 만들어 놓은것 같아서 향후 다른 데이터 분석시 복사를 위해 가져왔습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . sales_data[&#39;date&#39;] = pd.to_datetime(sales_data[&#39;date&#39;],format = &#39;%d.%m.%Y&#39;) dataset = sales_data.pivot_table(index = [&#39;shop_id&#39;,&#39;item_id&#39;], values = [&#39;item_cnt_day&#39;],columns = [&#39;date_block_num&#39;],fill_value = 0,aggfunc=&#39;sum&#39;) dataset.reset_index(inplace = True) dataset.head() . shop_id item_id item_cnt_day . date_block_num 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 . 0 0 | 30 | 0 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 31 | 0 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 32 | 6 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 33 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 35 | 1 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 판다스 내 피벗 테이블을 사용하는 모습입니다. group_by 함수를 확장한 것으로 생각할 수 있습니다. . 피벗 테이블은 우선 index로 데이터를 구분 짓습니다. 여기서 shop_id, item_id가 모두 같은 값을 가진 행끼리 그룹을 짓습니다. . 다음으로 columns로 한번 더 데이터를 구분 짓습니다. 같은 상점, 같은 제품을 달별로 나누었습니다. . values는 실제 적용되는 값을 의미합니다. 여기서는 item_cnt_day 변수를 사용했습니다. . 상점, 제품, 달이 같은 데이터 별로 구분했을때 여러개의 item_cnt_day 값을 더해주는 함수(aggfunc=&#39;sum&#39;)를 사용합니다. . 빈 값도 충분히 존재할 가능성이 있는데, 그 경우 거래 기록이 존재하지 않았다는 의미이므로 0값을 채웁니다.(fill_value = 0) . dataset = pd.merge(test_data,dataset,on = [&#39;item_id&#39;,&#39;shop_id&#39;],how = &#39;left&#39;) dataset.fillna(0,inplace = True) dataset.head() . /usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right) warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) . ID shop_id item_id (item_cnt_day, 0) (item_cnt_day, 1) (item_cnt_day, 2) (item_cnt_day, 3) (item_cnt_day, 4) (item_cnt_day, 5) (item_cnt_day, 6) (item_cnt_day, 7) (item_cnt_day, 8) (item_cnt_day, 9) (item_cnt_day, 10) (item_cnt_day, 11) (item_cnt_day, 12) (item_cnt_day, 13) (item_cnt_day, 14) (item_cnt_day, 15) (item_cnt_day, 16) (item_cnt_day, 17) (item_cnt_day, 18) (item_cnt_day, 19) (item_cnt_day, 20) (item_cnt_day, 21) (item_cnt_day, 22) (item_cnt_day, 23) (item_cnt_day, 24) (item_cnt_day, 25) (item_cnt_day, 26) (item_cnt_day, 27) (item_cnt_day, 28) (item_cnt_day, 29) (item_cnt_day, 30) (item_cnt_day, 31) (item_cnt_day, 32) (item_cnt_day, 33) . 0 0 | 5 | 5037 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 2.0 | 2.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 1.0 | 3.0 | 1.0 | 0.0 | . 1 1 | 5 | 5320 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 2 | 5 | 5233 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 2.0 | 0.0 | 1.0 | 3.0 | 1.0 | . 3 3 | 5 | 5232 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 4 4 | 5 | 5268 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 피벗 테이블을 사용해 같은 상점, 제품을 달 별로 거래기록이 몇건 있었는가를 나타내는 데이터 프레임입니다. . 이를 활용해 test 데이터 프레임과 병합한다면 테스트 데이터에 있는 상점, 제품의 이전 달별 거래기록을 전부 알 수 있습니다. . 이때 만약 병합이 안된 데이터가 있다면(이전 거래기록이 없는 데이터이겠죠?) 0으로 값을 넣어줍니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . dataset.drop([&#39;shop_id&#39;,&#39;item_id&#39;,&#39;ID&#39;],inplace = True, axis = 1) dataset.head() X_train = np.expand_dims(dataset.values[:,:-1],axis = 2) y_train = dataset.values[:,-1:] X_test = np.expand_dims(dataset.values[:,1:],axis = 2) print(X_train.shape,y_train.shape,X_test.shape) . (214200, 33, 1) (214200, 1) (214200, 33, 1) . 데이터를 모델링 하기 위해 상점, 제품 데이터를 지우고, train과 test 데이터 셋을 만들었습니다. . X_train : 0번째 달부터 32번째 달까지 거래 기록 데이터 . y_train : 33번째 달 거래 기록 데이터 . X_test : 1번째 달부터 33번째 달까지 거래 기록 데이터(train과 test간 데이터 형식을 맞추기 위해) . 우리가 예측해야할 y_test는 34번째 달 거래 기록 데이터, 즉 2015년 10월 거래 기록 데이터 입니다. . from keras.models import Sequential from keras.layers import LSTM,Dense,Dropout my_model = Sequential() my_model.add(LSTM(units = 64,input_shape = (33,1))) my_model.add(Dropout(0.4)) my_model.add(Dense(1)) my_model.compile(loss = &#39;mse&#39;,optimizer = &#39;adam&#39;, metrics = [&#39;mean_squared_error&#39;]) my_model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_1 (LSTM) (None, 64) 16896 _________________________________________________________________ dropout (Dropout) (None, 64) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ . my_model.fit(X_train,y_train,batch_size = 4096,epochs = 10) . Epoch 1/10 53/53 [==============================] - 27s 471ms/step - loss: 30.6011 - mean_squared_error: 30.6011 Epoch 2/10 53/53 [==============================] - 25s 466ms/step - loss: 30.2430 - mean_squared_error: 30.2430 Epoch 3/10 53/53 [==============================] - 24s 462ms/step - loss: 30.0014 - mean_squared_error: 30.0014 Epoch 4/10 53/53 [==============================] - 25s 481ms/step - loss: 29.8476 - mean_squared_error: 29.8476 Epoch 5/10 53/53 [==============================] - 26s 482ms/step - loss: 29.7404 - mean_squared_error: 29.7404 Epoch 6/10 53/53 [==============================] - 26s 487ms/step - loss: 29.7396 - mean_squared_error: 29.7396 Epoch 7/10 53/53 [==============================] - 25s 480ms/step - loss: 29.7369 - mean_squared_error: 29.7369 Epoch 8/10 53/53 [==============================] - 25s 473ms/step - loss: 29.6503 - mean_squared_error: 29.6503 Epoch 9/10 53/53 [==============================] - 25s 472ms/step - loss: 29.6353 - mean_squared_error: 29.6353 Epoch 10/10 53/53 [==============================] - 25s 468ms/step - loss: 29.5096 - mean_squared_error: 29.5096 . &lt;keras.callbacks.History at 0x7f2b3e51ff90&gt; . 모델을 LSTM(시계열 분석) 방법을 사용해서 분석합니다. 사실 LSTM 모델을 처음 사용했는데요. . 이번주에 다소 시간이 부족해 LSTM 모델의 사용방법이나 원리 등은 아직 파악하지 못했네요. (다른 사람 발표를 경청하겠습니다.) . submission_pfs = my_model.predict(X_test) submission_pfs = submission_pfs.clip(0,20) submission = pd.DataFrame({&#39;ID&#39;:test_data[&#39;ID&#39;],&#39;item_cnt_month&#39;:submission_pfs.ravel()}) submission.to_csv(&#39;./submission.csv&#39;,index = False) submission . ID item_cnt_month . 0 0 | 0.396485 | . 1 1 | 0.103207 | . 2 2 | 0.743674 | . 3 3 | 0.135947 | . 4 4 | 0.103207 | . ... ... | ... | . 214195 214195 | 0.331131 | . 214196 214196 | 0.103207 | . 214197 214197 | 0.097571 | . 214198 214198 | 0.103207 | . 214199 214199 | 0.069235 | . 214200 rows × 2 columns . 데이터를 모델에 적용시켜 예측값을 찾은 뒤, 제출 형식에 맞게 데이터 프레임 형식을 조정했습니다. . 이때 clip 함수는 이상치 조정 함수입니다. . clip(최솟값, 최댓값) 구조로 범위를 벗어나면 범위 내로 값을 조정시켜줍니다. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; data = {&#39;col_0&#39;: [9, -3, 0, -1, 5], &#39;col_1&#39;: [-2, -7, 6, 8, -5]} df = pd.DataFrame(data) df . col_0 col_1 . 0 9 | -2 | . 1 -3 | -7 | . 2 0 | 6 | . 3 -1 | 8 | . 4 5 | -5 | . df.clip(-4, 6) . col_0 col_1 . 0 6 | -2 | . 1 -3 | -4 | . 2 0 | 6 | . 3 -1 | 6 | . 4 5 | -4 | . 예시를 보면 보다 직관적으로 이해가 가능할 것 같습니다. . 이 함수는 범용성이 넓으니 다른 데이터 분석에 자주 쓰일 수 있어 따로 정리했네요. . !kaggle competitions submit -c competitive-data-science-predict-future-sales -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 3.55M/3.55M [00:04&lt;00:00, 769kB/s] Successfully submitted to Predict Future Sales . 캐글에 파일을 자동 제출하는 코드입니다. . 스코어는 약 1.02로 만 2천명 중 6천등 정도를 기록합니다. . &#45712;&#45184;&#51216; . 우선 공부하기 좋은 데이터를 찾아 줘서 고맙습니다. . 시계열 자료가 현실에서 상당히 많아 꼭 공부해보고 싶은 분야였는데, 이번 기회에 분석하게 되서 너무 좋습니다. . 개인적으로 공부하고 싶은 분야가 이미지 분류같은 것 보다는 자연어 처리, 시계열 분석 등 현실 세계를 설명할 수 있는 것 입니다. . 이번엔 시간이 다소 부족해서 자주쓰는 시계열 모델인 LSTM 모델의 탐구가 부족했습니다. . 다른 사람 발표 경청하고, 시간이 있을때 LSTM 모델을 열심히 공부해보고 싶네요. . 감사합니다. . &lt;/div&gt;",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/10/28/kagglessu4.html",
            "relUrl": "/2021/10/28/kagglessu4.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "시뮬레이션 과목 복습",
            "content": ". &#53076;&#47017;&#50640;&#49436; R &#49324;&#50857;&#48277; by &#54805;&#46973; . https://colab.research.google.com/notebook#create=true&amp;language=r . 뒷부분에 language=r 만 붙여주면 정상적으로 코랩 R버전이 실행됩니다. . for (i in 1:10){ print(i) } . [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 . 6.6&#51208; &#48372;&#54744;&#44552; &#52397;&#44396; &#47928;&#51228; . 기존 보험 가입자 n0(사용값 1명), 기존 자본금 a0 (사용값 25000), 기간 365. 보함 가입자는 기간 1당 보험금 C(사용값 11000)을 각각 지불합니다. . 이때 자본금이 음이 되지 않을 확률을 모의실험으로 구하는 문제 입니다. . 일어날 사건은 보험금청구, 신규고객 가입, 기존계약해지 인데요. . 보험금 청구는 도착률 알파(사용값 10)인 포아송 과정, 이때 청구 금액은 지수분포 (사용 람다값 1/1000)을 따릅니다. . 포아송 과정이란 사건 발생 시간 분포가 평균 1/알파인 지수분포 입니다. . 이 모의실험을 300번 실시해봅니다. . (신규고객 가입, 기존계약 해지는 무시합니다.) . n.sim &lt;- 300 # 모의실험 실행 횟수 n0 &lt;- 1; a0 &lt;- 25000; T &lt;- 365; c &lt;- 11000 # 가입자, 자본금, 기간, 단위기간당 보험금 초기값 부여 alpha &lt;- 10; nu &lt;- 0; mu &lt;- 0 # 알파값, 신규계약과 기존계약 해지는 무시합니다. generate.Y &lt;- function() rexp(1, rate = 1/1000) # 청구금액 만드는 함수를 생성합니다. I &lt;- numeric(length = n.sim) # 자본금이 음이되는지 여부를 실험마다 기록하는 변수 입니다. for (i in 1:n.sim){ # 실험 n.sim(300)번 실행 t &lt;- 0; a &lt;- a0; n &lt;- n0 # 시점, 자본금, 고객 수 초기값 부여 total.rate &lt;- nu + n * mu + n * alpha # 사건 발생 람다값 부여. 여기서 유효한 값은 n * alpha(보험금 청구) 입니다. tE &lt;- rexp(1, rate = total.rate) # 첫 사건 발생 시간 repeat{ if (tE &gt; T) { # 주어진 기간을 초과했을 경우 I[i] &lt;- 1 # 중간에 중단되지 않고 주어진 기간(365)를 무사히 초과했기 때문에 이번 실험은 성공임을 기록해줍니다. break # 반복분 끝내기. 다음 모의 실험이 실행되겠죠. } if (tE &lt;= T){ # 주어진 기간 내. a &lt;- a + n * c * (tE - t) # 보험금 수금. 여기서 tE는 새 사건 발생 시간, t는 과거 사건 발생시간. t &lt;- tE # 시점을 새 사건 발생시간에 맞춰줍니다. J &lt;- sample(1:3, 1, prob = c(nu, n*mu, n*alpha)) # 이번 사건은 어떤사건인지 정해줍니다. 하지만 여기선 무조건 J는 3이됩니다. if (J == 1) n &lt;- n + 1 # 신규고객 가입 if (J == 2) n &lt;- n - 1 # 기존고객 해지 if (J == 3){ Y &lt;- generate.Y(); # 보험금 청구 금액 찾기 if (Y &gt; a){ # 현재 자본금보다 보험 청구 금액이 많으면 = 자본금이 음수가 됨. I[i] &lt;- 0 # 이번 실험은 실패임을 기록 break # 반복문 끝내기 } else a &lt;- a - Y # 자본금이 음수가 되는 일이 벌어지지 않으면 자본금에서 돈을 쓰면 되겠죠. } tE &lt;- t + rexp(1,rate=total.rate) # 다음 사건이 일어날 시점을 탐색합니다. } } } mean(I) cat(&#39;자본금이 남아있을 확률 95%신뢰구간 [&#39;,mean(I)- 1.96*sd(I) /sqrt(n.sim),&#39;,&#39;,mean(I) + 1.96*sd(I)/sqrt(n.sim), &#39;] n&#39;) . 0.916666666666667 자본금이 남아있을 확률 95%신뢰구간 [ 0.8853385 , 0.9479949 ] . 자본금이 음수가 되지 않을 확률이 90%정도 됩니다. 신뢰구간도 구할 수 있군요. . 강의는 여기까지 가르쳤는데요. 시험문제는 이를 응용하는 문제가 나올 수 있습니다. . 제일 쉬운 예시로 고객의 가입과 탈퇴가 포함된 함수를 만드는 문제가 나올수도 있겠습니다. . 신규고객 가입을 람다가 1인 포아송 과정으로, 기존 고객 탈퇴를 람다가 0.1인 포아송 과정으로 하겠습니다. . 그리고 기존 고객의 초기 수를 10으로 하겠습니다. . n.sim &lt;- 100 n0 &lt;- 10; a0 &lt;- 25000; T &lt;- 365; c &lt;- 11000 alpha &lt;- 10; nu &lt;- 1; mu &lt;- 0.1 # 이부분만 바꿔주면 됨 generate.Y &lt;- function() rexp(1, rate = 1/1000) I &lt;- numeric(length = n.sim) for (i in 1:n.sim){ t &lt;- 0; a &lt;- a0; n &lt;- n0 total.rate &lt;- nu + n * mu + n * alpha tE &lt;- rexp(1, rate = total.rate) repeat{ if (tE &gt; T) { I[i] &lt;- 1 break } if (tE &lt;= T){ a &lt;- a + n * c * (tE - t) t &lt;- tE J &lt;- sample(1:3, 1, prob = c(nu, n*mu, n*alpha)) if (J == 1) n &lt;- n + 1 if (J == 2){ n &lt;- n - 1 if (n == 0){ #보험금이 0이된 경우. I[i] &lt;- 0 break } } if (J == 3){ Y &lt;- generate.Y(); if (Y &gt; a){ I[i] &lt;- 0 break } else a &lt;- a - Y } tE &lt;- t + rexp(1,rate=total.rate) } } } mean(I) cat(&#39;자본금이 남아있을 확률 95%신뢰구간 [&#39;,mean(I)- 1.96*sd(I) /sqrt(n.sim),&#39;,&#39;,mean(I) + 1.96*sd(I)/sqrt(n.sim), &#39;] n&#39;) . 0.36 자본금이 남아있을 확률 95%신뢰구간 [ 0.265446 , 0.454554 ] . 이미 세 사건이 일어날걸 가정하고 함수를 다 만들어나서 단순히 nu와 mu값만 넣어주면 됩니다. . 다만 보험 가입자가 0명이 될 경우도 있는데 그 경우 또한 실패로 하겠습니다. . 확실히 가입자가 늘어나고 탈퇴를 할 수 있는 등 불확실성이 커지니 자본금이 음이 될 확률이 줄어들었습니다. . 6.8&#51208; &#51452;&#49885; &#50741;&#49496; &#54665;&#49324; &#51204;&#47029; . t시점에 주식 가격 S(t) = S(0) * exp(x1 + x2 .. + xt) 이라고 가정합니다. 이때 xi는 iid인 정규분포 변수입니다. . 알파 &lt; 평균 + 분산 / 2 (정규분포 평균, 분산) 일때 좋은 전략이 다음과 같이 알려져있습니다. . P(m) = S(N-m) 이라고 할때(만기를 m 앞둔 시점에서의 주가) 다음 조건이 만족하면 옵션을 행사합니다. . P(m) &gt; K(옵션권한가격 = 초기가격) 쉽게 얘기해 주식 가격이 초기가격보다 높아야합니다. 당연하죠. | P(m) &gt; K + f(i) 을 i = 1,2, .. , m 구간에서 모두 만족해야합니다. f(i)는 식이 복잡해 생략합니다. | 또 다른 전략은 끝나는 시점까지 기다렸다가 최종 시점 주식 가격이 K보다 클때만 사는 전략입니다. . 두 전략중 어떤 전략이 좋을지 모의실험 1000회를 통해 알아봅시다. . n.sim &lt;- 1000 # 실험횟수 N &lt;- 20; K &lt;- 100; S.zero &lt;- 100; mu &lt;- -0.05 # N : 기간, K, S.zero : 초기 금액(사실 어느값을 써도 비슷함) sg &lt;- 0.3; alp &lt;- mu + 0.5*sg^2 # mu = -0.05, 시그마 = 0.3 E &lt;- numeric(length = n.sim) E2 &lt;- numeric(length = n.sim) for (i in 1:n.sim){ S &lt;- S.zero * exp(cumsum(rnorm(N,mu,sg))) # 주식 가격을 구해놓음. E2[i] &lt;- max(S[N] - K, 0) # 최종시점 주식 가격이 K보다 크면 그만큼 이득, 아니면 이득 0. P &lt;- numeric(length=N+1) # P[0]은 R에서 쓸수 없음. 그래서 길이 자체를 N+1로 해줌. P[N+1] &lt;- S.zero #초기값 부여 m &lt;- N - 1 # m 초기값 부여. 미리 값 1을 뺀 모양새.(위에서 P[N+1] 초기값을 부여했기 때문에) flag &lt;- FALSE repeat{ m.plus &lt;- m + 1 # P에서는 m값을 1을 올려서 해줌. P[m.plus] &lt;- S[N-m] # 값 넣어줌. if(P[m.plus] &gt; K) flag &lt;- TRUE # 1번조건 만족 표현 if(flag &amp; m &gt; 0){ # 조건1만족 + m이 0아닐때(m이 0일때는 1번조건만 따짐.) b &lt;- ((1:m)*mu - log(K/P[m.plus])) / (sg*sqrt(1:m)) op &lt;- P[m.plus] * exp((1:m)*alp)* pnorm(sg*sqrt(1:m) + b) - K * pnorm(b) # 복잡한 식 f(i), 벡터 형태로 되어있음. flag &lt;- all(P[m.plus] &gt; K + op) # all은 모든 조건이 true일때만 true를 보내줌. } if(flag) break else m &lt;- m - 1 # 조건을 모두 만족하면 즉시 옵션 행사. if(m &lt; 0) break # 시점이 모두 끝났으면 종료. } if (flag) E[i] &lt;- P[m.plus] - K else E[i] &lt;- 0 # 조건 만족시 이득본 만큼 기록. } cat(&#39;전략1 95%신뢰구간 [&#39;,mean(E) - 1.96*sd(E)/sqrt(n.sim), &#39;,&#39;,mean(E) + 1.96*sd(E)/sqrt(n.sim), &#39;] n&#39;) cat(&#39;전략2 95%신뢰구간 [&#39;,mean(E2) - 1.96*sd(E2)/sqrt(n.sim), &#39;,&#39;,mean(E2) + 1.96*sd(E2)/sqrt(n.sim), &#39;] n&#39;) . 전략1 95%신뢰구간 [ 35.36105 , 45.49005 ] 전략2 95%신뢰구간 [ 28.57401 , 44.68403 ] . 여러번 실행을 해보면 전략1과 전략2의 이득 평균이 비슷합니다. . 다만 전략2의 신뢰구간이 큰 것은 그만큼 전략2가 불안정한 전략임을 알 수 있습니다. . &#48512;&#53944;&#49828;&#53944;&#47129; &#51060;&#47200; . 뽑힌 표본들을 새로운 분포로 가정하여 반복추출(복원추출)을 통해 모수를 추정하는 방법 입니다. . 표본들이 실제 분포와 비슷할 수록 모수 추정이 더 정확해집니다. . 이 방법의 장점은 중심극한정리 등 분포가정을 하지 않아도 된다는 점입니다. . 다음은 부트스트렙 예시로, 표본들이 있을때 Var(s^2)의 추정량을 구하는 문제입니다. . x &lt;- c(5,4,9,6,21,17,11,20,7,10,21,15,13,16,8) n &lt;- 15 B &lt;- 400 # 400번 실시 f.var &lt;- function(x) var(sample(x, n, rep = T)) b.var &lt;- replicate(B, f.var(x)) var(b.var) # estimate of var(s^2) hist(b.var) . 57.6508361857024",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/10/21/%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98_%EA%B3%BC%EB%AA%A9_%EB%B3%B5%EC%8A%B5.html",
            "relUrl": "/2021/10/21/%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98_%EA%B3%BC%EB%AA%A9_%EB%B3%B5%EC%8A%B5.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "택시 데이터 분석",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle (3).json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;23e68db36970b65937516103c630ba75&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c nyc-taxi-trip-duration . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) train.zip: Skipping, found more recently modified local copy (use --force to force download) test.zip: Skipping, found more recently modified local copy (use --force to force download) sample_submission.zip: Skipping, found more recently modified local copy (use --force to force download) . !unzip train.zip !unzip test.zip !unzip sample_submission.zip . Archive: train.zip replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: train.csv Archive: test.zip replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: test.csv Archive: sample_submission.zip replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: sample_submission.csv . 압축되어 있는 데이터라서 압축 풀어줍니다. . %matplotlib inline import pandas as pd from datetime import datetime import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge,BayesianRidge from sklearn.cluster import MiniBatchKMeans from sklearn.metrics import mean_squared_error from math import radians, cos, sin, asin, sqrt import seaborn as sns import matplotlib import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [16, 10] . train = pd.read_csv(&#39;./train.csv&#39;) test = pd.read_csv(&#39;./test.csv&#39;) . &#45936;&#51060;&#53552; &#53456;&#49353; . train.head() . id vendor_id pickup_datetime dropoff_datetime passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude store_and_fwd_flag trip_duration . 0 id2875421 | 2 | 2016-03-14 17:24:55 | 2016-03-14 17:32:30 | 1 | -73.982155 | 40.767937 | -73.964630 | 40.765602 | N | 455 | . 1 id2377394 | 1 | 2016-06-12 00:43:35 | 2016-06-12 00:54:38 | 1 | -73.980415 | 40.738564 | -73.999481 | 40.731152 | N | 663 | . 2 id3858529 | 2 | 2016-01-19 11:35:24 | 2016-01-19 12:10:48 | 1 | -73.979027 | 40.763939 | -74.005333 | 40.710087 | N | 2124 | . 3 id3504673 | 2 | 2016-04-06 19:32:31 | 2016-04-06 19:39:40 | 1 | -74.010040 | 40.719971 | -74.012268 | 40.706718 | N | 429 | . 4 id2181028 | 2 | 2016-03-26 13:30:55 | 2016-03-26 13:38:10 | 1 | -73.973053 | 40.793209 | -73.972923 | 40.782520 | N | 435 | . train.describe() . vendor_id passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude trip_duration . count 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | . mean 1.534950e+00 | 1.664530e+00 | -7.397349e+01 | 4.075092e+01 | -7.397342e+01 | 4.075180e+01 | 9.594923e+02 | . std 4.987772e-01 | 1.314242e+00 | 7.090186e-02 | 3.288119e-02 | 7.064327e-02 | 3.589056e-02 | 5.237432e+03 | . min 1.000000e+00 | 0.000000e+00 | -1.219333e+02 | 3.435970e+01 | -1.219333e+02 | 3.218114e+01 | 1.000000e+00 | . 25% 1.000000e+00 | 1.000000e+00 | -7.399187e+01 | 4.073735e+01 | -7.399133e+01 | 4.073588e+01 | 3.970000e+02 | . 50% 2.000000e+00 | 1.000000e+00 | -7.398174e+01 | 4.075410e+01 | -7.397975e+01 | 4.075452e+01 | 6.620000e+02 | . 75% 2.000000e+00 | 2.000000e+00 | -7.396733e+01 | 4.076836e+01 | -7.396301e+01 | 4.076981e+01 | 1.075000e+03 | . max 2.000000e+00 | 9.000000e+00 | -6.133553e+01 | 5.188108e+01 | -6.133553e+01 | 4.392103e+01 | 3.526282e+06 | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1458644 entries, 0 to 1458643 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 id 1458644 non-null object 1 vendor_id 1458644 non-null int64 2 pickup_datetime 1458644 non-null object 3 dropoff_datetime 1458644 non-null object 4 passenger_count 1458644 non-null int64 5 pickup_longitude 1458644 non-null float64 6 pickup_latitude 1458644 non-null float64 7 dropoff_longitude 1458644 non-null float64 8 dropoff_latitude 1458644 non-null float64 9 store_and_fwd_flag 1458644 non-null object 10 trip_duration 1458644 non-null int64 dtypes: float64(4), int64(3), object(4) memory usage: 122.4+ MB . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 625134 entries, 0 to 625133 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 id 625134 non-null object 1 vendor_id 625134 non-null int64 2 pickup_datetime 625134 non-null object 3 passenger_count 625134 non-null int64 4 pickup_longitude 625134 non-null float64 5 pickup_latitude 625134 non-null float64 6 dropoff_longitude 625134 non-null float64 7 dropoff_latitude 625134 non-null float64 8 store_and_fwd_flag 625134 non-null object dtypes: float64(4), int64(2), object(3) memory usage: 42.9+ MB . dropoff_datetime 변수가 test에는 없습니다. 도착 시간을 맞추는 예제이기 때문에 그렇습니다. . &#48152;&#51025;&#48320;&#49688; &#44288;&#52272; . plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train.trip_duration.values)) plt.xlabel(&#39;index&#39;, fontsize=12) plt.ylabel(&#39;trip duration&#39;, fontsize=12) plt.show() . 반응변수의 이상치가 많아보입니다. 제거하겠습니다. . m = np.mean(train[&#39;trip_duration&#39;]) s = np.std(train[&#39;trip_duration&#39;]) train = train[train[&#39;trip_duration&#39;] &lt;= m + 2*s] train = train[train[&#39;trip_duration&#39;] &gt;= m - 2*s] plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train.trip_duration.values)) plt.xlabel(&#39;index&#39;, fontsize=12) plt.ylabel(&#39;trip duration&#39;, fontsize=12) plt.show() . 이상치는 대부분 제거된 것 같습니다. 다만 일부 데이터가 큰 값을 갖는거 같아요. . plt.hist(train[&#39;trip_duration&#39;].values, bins=100) plt.xlabel(&#39;trip_duration&#39;) plt.ylabel(&#39;number of train records&#39;) plt.show() . 히스토그램으로 확인하니 그렇습니다. 우측 꼬리가 긴 모양으로 로그변환이 필요해보입니다. . train[&#39;log_trip_duration&#39;] = np.log(train[&#39;trip_duration&#39;].values + 1) plt.hist(train[&#39;log_trip_duration&#39;].values, bins=100) plt.xlabel(&#39;log(trip_duration)&#39;) plt.ylabel(&#39;number of train records&#39;) plt.show() sns.distplot(train[&quot;log_trip_duration&quot;], bins =100) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f99495dd450&gt; . 확실히 그래프 모양이 괜찮아졌습니다. distplot 함수를 통해 그리기도 하였네요 . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . train = train[train[&#39;pickup_longitude&#39;] &lt;= -73.75] train = train[train[&#39;pickup_longitude&#39;] &gt;= -74.03] train = train[train[&#39;pickup_latitude&#39;] &lt;= 40.85] train = train[train[&#39;pickup_latitude&#39;] &gt;= 40.63] train = train[train[&#39;dropoff_longitude&#39;] &lt;= -73.75] train = train[train[&#39;dropoff_longitude&#39;] &gt;= -74.03] train = train[train[&#39;dropoff_latitude&#39;] &lt;= 40.85] train = train[train[&#39;dropoff_latitude&#39;] &gt;= 40.63] . 뉴욕의 위도는 (-74.03, -73.75) 경도는 (40.63, 40.85) 사이 입니다. . 이 값을 벗어나는 위도/경도 데이터를 제거하겠습니다. . train[&#39;pickup_datetime&#39;] = pd.to_datetime(train.pickup_datetime) test[&#39;pickup_datetime&#39;] = pd.to_datetime(test.pickup_datetime) train.loc[:, &#39;pickup_date&#39;] = train[&#39;pickup_datetime&#39;].dt.date test.loc[:, &#39;pickup_date&#39;] = test[&#39;pickup_datetime&#39;].dt.date train[&#39;dropoff_datetime&#39;] = pd.to_datetime(train.dropoff_datetime) #Not in Test . to_datetime 함수로 datetime 변수로 바궈주었습니다. . plt.plot(train.groupby(&#39;pickup_date&#39;).count()[[&#39;id&#39;]], &#39;o-&#39;, label=&#39;train&#39;) plt.plot(test.groupby(&#39;pickup_date&#39;).count()[[&#39;id&#39;]], &#39;o-&#39;, label=&#39;test&#39;) plt.title(&#39;Trips over Time.&#39;) plt.legend(loc=0) plt.ylabel(&#39;Trips&#39;) plt.show() . 트레인과 테스트 데이터를 같이 그리니 유사한 측면을 발견하기가 쉬운것 같아요. . 1월 하순경 이동횟수가 급격하게 감소한것이 관찰됩니다. 또 5월 하순경 감소세가 또 관찰됩니다. . 계절적으로 추운것도 있겠지만 작성자는 다른 요인이 있지 않을까 생각하네요. . import warnings warnings.filterwarnings(&quot;ignore&quot;) plot_vendor = train.groupby(&#39;vendor_id&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=800) plt.ylim(ymax=840) sns.barplot(plot_vendor.index,plot_vendor.values) plt.title(&#39;Time per Vendor&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) . No handles with labels found to put in legend. . Text(0, 0.5, &#39;Time in Seconds&#39;) . 범위를 800~840으로 두어서 그렇지 두 vendor 간 큰 차이를 보이진 않습니다. . snwflag = train.groupby(&#39;store_and_fwd_flag&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=0) plt.ylim(ymax=1100) plt.title(&#39;Time per store_and_fwd_flag&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) sns.barplot(snwflag.index,snwflag.values) . No handles with labels found to put in legend. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f993c2c18d0&gt; . 공급업체에 보내기 전 기록이 잘 저장되었는지 나타내는 변수로 꽤 많이 차이가 납니다. . 작성자는 일부 직원이 이동시간을 정확히 기록하지 못해 발생하는 왜곡이라고 말합니다. . pc = train.groupby(&#39;passenger_count&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=0) plt.ylim(ymax=1100) plt.title(&#39;Time per store_and_fwd_flag&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) sns.barplot(pc.index,pc.values) . No handles with labels found to put in legend. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f993c2b0550&gt; . 승객 수는 뚜렷한 여행을 주지 못합니다. . 승객을 아무도 태우지 않았는데 4분정도 이동한 것은 직원의 실수로 보입니다. . train.groupby(&#39;passenger_count&#39;).size() . passenger_count 0 52 1 1018715 2 206864 3 58989 4 27957 5 76912 6 47639 dtype: int64 . &#50948;&#52824; &#45936;&#51060;&#53552; . city_long_border = (-74.03, -73.75) city_lat_border = (40.63, 40.85) fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True) ax[0].scatter(train[&#39;pickup_longitude&#39;].values[:100000], train[&#39;pickup_latitude&#39;].values[:100000], color=&#39;blue&#39;, s=1, label=&#39;train&#39;, alpha=0.1) ax[1].scatter(test[&#39;pickup_longitude&#39;].values[:100000], test[&#39;pickup_latitude&#39;].values[:100000], color=&#39;green&#39;, s=1, label=&#39;test&#39;, alpha=0.1) fig.suptitle(&#39;Train and test area complete overlap.&#39;) ax[0].legend(loc=0) ax[0].set_ylabel(&#39;latitude&#39;) ax[0].set_xlabel(&#39;longitude&#39;) ax[1].set_xlabel(&#39;longitude&#39;) ax[1].legend(loc=0) plt.ylim(city_lat_border) plt.xlim(city_long_border) plt.show() . 자세한 코드 관찰은 위치 데이터 분석을 할때 다시 확인하겠습니다. . train, test 간 위치 데이터가 매우 유사함을 알 수 있습니다. . def haversine_array(lat1, lng1, lat2, lng2): lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) AVG_EARTH_RADIUS = 6371 # in km lat = lat2 - lat1 lng = lng2 - lng1 d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) return h def dummy_manhattan_distance(lat1, lng1, lat2, lng2): a = haversine_array(lat1, lng1, lat1, lng2) b = haversine_array(lat1, lng1, lat2, lng1) return a + b def bearing_array(lat1, lng1, lat2, lng2): AVG_EARTH_RADIUS = 6371 # in km lng_delta_rad = np.radians(lng2 - lng1) lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) y = np.sin(lng_delta_rad) * np.cos(lat2) x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) return np.degrees(np.arctan2(y, x)) . train.loc[:, &#39;distance_haversine&#39;] = haversine_array(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;distance_haversine&#39;] = haversine_array(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) train.loc[:, &#39;distance_dummy_manhattan&#39;] = dummy_manhattan_distance(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;distance_dummy_manhattan&#39;] = dummy_manhattan_distance(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) train.loc[:, &#39;direction&#39;] = bearing_array(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;direction&#39;] = bearing_array(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) . 위도/경도를 활용하여 다양한 관측값을 나타내는 함수입니다. . 이해하기에 조금 벅차서 일단 다양한 변수를 추가해줄수 있구나 하고 넘어갔네요. . coords = np.vstack((train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]].values, train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]].values)) sample_ind = np.random.permutation(len(coords))[:500000] kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind]) train.loc[:, &#39;pickup_cluster&#39;] = kmeans.predict(train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]]) train.loc[:, &#39;dropoff_cluster&#39;] = kmeans.predict(train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]]) test.loc[:, &#39;pickup_cluster&#39;] = kmeans.predict(test[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]]) test.loc[:, &#39;dropoff_cluster&#39;] = kmeans.predict(test[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]]) . np.vstack는 데이터를 묶어주는 함수입니다. . 위도, 경도 데이터를 클러스트로 묶어주었습니다. . fig, ax = plt.subplots(ncols=1, nrows=1) ax.scatter(train.pickup_longitude.values[:500000], train.pickup_latitude.values[:500000], s=10, lw=0, c=train.pickup_cluster[:500000].values, cmap=&#39;autumn&#39;, alpha=0.2) ax.set_xlim(city_long_border) ax.set_ylim(city_lat_border) ax.set_xlabel(&#39;Longitude&#39;) ax.set_ylabel(&#39;Latitude&#39;) plt.show() . 군집화가 잘된 것을 시각적으로 확인하였습니다. . &#45216;&#51676; &#45936;&#51060;&#53552; . train[&#39;Month&#39;] = train[&#39;pickup_datetime&#39;].dt.month test[&#39;Month&#39;] = test[&#39;pickup_datetime&#39;].dt.month train[&#39;DayofMonth&#39;] = train[&#39;pickup_datetime&#39;].dt.day test[&#39;DayofMonth&#39;] = test[&#39;pickup_datetime&#39;].dt.day train[&#39;Hour&#39;] = train[&#39;pickup_datetime&#39;].dt.hour test[&#39;Hour&#39;] = test[&#39;pickup_datetime&#39;].dt.hour train[&#39;dayofweek&#39;] = train[&#39;pickup_datetime&#39;].dt.dayofweek test[&#39;dayofweek&#39;] = test[&#39;pickup_datetime&#39;].dt.dayofweek . 픽업된 시간으로 다양한 파생 날짜/시간 데이터를 생성한 모습입니다. datetime 변수이기에 가능한 모습입니다. . 여기서 dayofweek 변수는 요일변수로 0을 일요일로 생각하여 6을 토요일까지 쓰는 변수입니다. . train.loc[:, &#39;avg_speed_h&#39;] = 1000 * train[&#39;distance_haversine&#39;] / train[&#39;trip_duration&#39;] train.loc[:, &#39;avg_speed_m&#39;] = 1000 * train[&#39;distance_dummy_manhattan&#39;] / train[&#39;trip_duration&#39;] fig, ax = plt.subplots(ncols=3, sharey=True) ax[0].plot(train.groupby(&#39;Hour&#39;).mean()[&#39;avg_speed_h&#39;], &#39;bo-&#39;, lw=2, alpha=0.7) ax[1].plot(train.groupby(&#39;dayofweek&#39;).mean()[&#39;avg_speed_h&#39;], &#39;go-&#39;, lw=2, alpha=0.7) ax[2].plot(train.groupby(&#39;Month&#39;).mean()[&#39;avg_speed_h&#39;], &#39;ro-&#39;, lw=2, alpha=0.7) ax[0].set_xlabel(&#39;Hour of Day&#39;) ax[1].set_xlabel(&#39;Day of Week&#39;) ax[2].set_xlabel(&#39;Month of Year&#39;) ax[0].set_ylabel(&#39;Average Speed&#39;) fig.suptitle(&#39;Average Traffic Speed by Date-part&#39;) plt.show() . 정확히 이해하진 못했지만 distance_haversine가 위치 변수를 보고 만든 거리 변수입니다. . 그렇기 때문에 거리 / 시간 = 평균속도 변수를 만들었습니다. 이 평균속도를 시각/요일/달 별로 얼마나 다른지 시각화했습니다. . 물론 분모인 시간이 반응변수 이기 때문에 분석에 사용할수는 없습니다. . 보통 오전 5시~9시, 오후 5시(17시) ~ 7시(19시) 사이가 가장 도로가 혼잡해 속도가 떨어집니다. . 예상과 어느정도 일치하면서도 출/퇴근 이외 근무시간도 속도가 출/퇴근 시간과 비슷하게 떨어집니다. . 또 금토일의 평균속도가 상대적으로 빠르며 달별로는 겨울의 평균속도가 빠릅니다. . &#50896;&#54635;&#51064;&#53076;&#46377; . vendor_train = pd.get_dummies(train[&#39;vendor_id&#39;], prefix=&#39;vi&#39;, prefix_sep=&#39;_&#39;) vendor_test = pd.get_dummies(test[&#39;vendor_id&#39;], prefix=&#39;vi&#39;, prefix_sep=&#39;_&#39;) passenger_count_train = pd.get_dummies(train[&#39;passenger_count&#39;], prefix=&#39;pc&#39;, prefix_sep=&#39;_&#39;) passenger_count_test = pd.get_dummies(test[&#39;passenger_count&#39;], prefix=&#39;pc&#39;, prefix_sep=&#39;_&#39;) store_and_fwd_flag_train = pd.get_dummies(train[&#39;store_and_fwd_flag&#39;], prefix=&#39;sf&#39;, prefix_sep=&#39;_&#39;) store_and_fwd_flag_test = pd.get_dummies(test[&#39;store_and_fwd_flag&#39;], prefix=&#39;sf&#39;, prefix_sep=&#39;_&#39;) cluster_pickup_train = pd.get_dummies(train[&#39;pickup_cluster&#39;], prefix=&#39;p&#39;, prefix_sep=&#39;_&#39;) cluster_pickup_test = pd.get_dummies(test[&#39;pickup_cluster&#39;], prefix=&#39;p&#39;, prefix_sep=&#39;_&#39;) cluster_dropoff_train = pd.get_dummies(train[&#39;dropoff_cluster&#39;], prefix=&#39;d&#39;, prefix_sep=&#39;_&#39;) cluster_dropoff_test = pd.get_dummies(test[&#39;dropoff_cluster&#39;], prefix=&#39;d&#39;, prefix_sep=&#39;_&#39;) month_train = pd.get_dummies(train[&#39;Month&#39;], prefix=&#39;m&#39;, prefix_sep=&#39;_&#39;) month_test = pd.get_dummies(test[&#39;Month&#39;], prefix=&#39;m&#39;, prefix_sep=&#39;_&#39;) dom_train = pd.get_dummies(train[&#39;DayofMonth&#39;], prefix=&#39;dom&#39;, prefix_sep=&#39;_&#39;) dom_test = pd.get_dummies(test[&#39;DayofMonth&#39;], prefix=&#39;dom&#39;, prefix_sep=&#39;_&#39;) hour_train = pd.get_dummies(train[&#39;Hour&#39;], prefix=&#39;h&#39;, prefix_sep=&#39;_&#39;) hour_test = pd.get_dummies(test[&#39;Hour&#39;], prefix=&#39;h&#39;, prefix_sep=&#39;_&#39;) dow_train = pd.get_dummies(train[&#39;dayofweek&#39;], prefix=&#39;dow&#39;, prefix_sep=&#39;_&#39;) dow_test = pd.get_dummies(test[&#39;dayofweek&#39;], prefix=&#39;dow&#39;, prefix_sep=&#39;_&#39;) . 범주형 변수들을 전부 원핫인코딩을 했습니다. . prefix 와 prefix_sep 으로 원핫인코딩 변수 이름도 설정할수 있네요. . passenger_count_test = passenger_count_test.drop(&#39;pc_9&#39;, axis = 1) . 다만 9명이 탑승한 2건은 표본이 너무 적어 과적합될수도 있고 직관적으로도 말이 안되서 열을 삭제합니다. . train = train.drop([&#39;id&#39;,&#39;vendor_id&#39;,&#39;passenger_count&#39;,&#39;store_and_fwd_flag&#39;,&#39;Month&#39;,&#39;DayofMonth&#39;,&#39;Hour&#39;,&#39;dayofweek&#39;,&#39;pickup_datetime&#39;, &#39;pickup_date&#39;,&#39;pickup_longitude&#39;,&#39;pickup_latitude&#39;,&#39;dropoff_longitude&#39;,&#39;dropoff_latitude&#39;],axis = 1) Test_id = test[&#39;id&#39;] test = test.drop([&#39;id&#39;,&#39;vendor_id&#39;,&#39;passenger_count&#39;,&#39;store_and_fwd_flag&#39;,&#39;Month&#39;,&#39;DayofMonth&#39;,&#39;Hour&#39;,&#39;dayofweek&#39;, &#39;pickup_datetime&#39;, &#39;pickup_date&#39;, &#39;pickup_longitude&#39;,&#39;pickup_latitude&#39;,&#39;dropoff_longitude&#39;,&#39;dropoff_latitude&#39;], axis = 1) train = train.drop([&#39;dropoff_datetime&#39;,&#39;avg_speed_h&#39;,&#39;avg_speed_m&#39;,&#39;trip_duration&#39;], axis = 1) . 원핫인코딩 된 변수들, 시각화를 위해 만들었던 변수들, 변환한 변수들, id 등 필요없는 변수를 제거합니다. . Train_Master = pd.concat([train, vendor_train, passenger_count_train, store_and_fwd_flag_train, cluster_pickup_train, cluster_dropoff_train, month_train, dom_train, hour_test, dow_train ], axis=1) Test_master = pd.concat([test, vendor_test, passenger_count_test, store_and_fwd_flag_test, cluster_pickup_test, cluster_dropoff_test, month_test, dom_test, hour_test, dow_test], axis=1) Train_Master.shape,Test_master.shape . ((1446345, 285), (625134, 284)) . 원핫인코딩했던 변수들을 합쳐줍니다. . &#47784;&#45944; &#51201;&#54633; . X_train = Train_Master.drop([&#39;log_trip_duration&#39;], axis=1) Y_train = Train_Master[&quot;log_trip_duration&quot;] Y_train = Y_train.reset_index().drop(&#39;index&#39;,axis = 1) . 이 코드 이후로 모델적합을 해야하는데 코랩에서 계속 램이 부족하다고 하네요. . 데이터도 크고, 열 개수도 원핫인코딩으로 늘려서 그런거 같습니다. . XGB였다가 LGB로 바꾸고, 노말모델로 하고 어떻게 해도 계속 램이 부족해서 실행이 안되네요. . 코드를 리뷰하는 목적이고 요즘 시간이 넉넉하지 못해서 여기까지 하겠습니다. . from lightgbm import LGBMRegressor model = LGBMRegressor() model.fit(X_train, Y_train) . pred = model.predict(Test_master) pred = np.exp(pred) submission = pd.concat([Test_id, pd.DataFrame(pred)], axis=1) submission.columns = [&#39;id&#39;,&#39;trip_duration&#39;] submission[&#39;trip_duration&#39;] = submission.apply(lambda x : 1 if (x[&#39;trip_duration&#39;] &lt;= 0) else x[&#39;trip_duration&#39;], axis = 1) submission.to_csv(&quot;./submission.csv&quot;, index=False) . !kaggle competitions submit -c nyc-taxi-trip-duration -f submission.csv -m &quot;Message&quot; . &#45712;&#45184;&#51216; . 우선 스스로 고른 데이터인데 위도, 경도를 이용한 데이터여서 조금 어려웠습니다. . 주에 하나씩 코드 리뷰를 하는데 열심히 하면 위치 데이터를 이해하는데 큰 도움이 되겠지만 당장 필요한 기술이 아니라 넘어갔네요. . 한것만 보면 크게 고생한거 같진 않지만, 너무 복잡한 코드들이 많아서 어느정도 했다가 어려워서 처음부터 다시 세번정도 한거 같습니다. . 그래도 남의 코드를 보면서 참 많은걸 배우네요. 시간이 생각보다 많이 들긴 했는데, 그 만큼 배워가는게 있는거 같아요. . 여기에 못담고 지운 코드들 중에도 배운 코드가 많아요. 예를 들어 판다스 옵션을 건드는 코드? . 데이콘 대회에도 도움이 될 거 같습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/10/07/kagglestudy3.html",
            "relUrl": "/2021/10/07/kagglestudy3.html",
            "date": " • Oct 7, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "주성분 분석(PCA)",
            "content": ". &#48531;&#44867; &#45936;&#51060;&#53552; . from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;] irisDF = pd.DataFrame(iris.data, columns = columns) irisDF[&#39;target&#39;] = iris.target irisDF.head(3) . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . markers = [&#39;^&#39;,&#39;s&#39;,&#39;o&#39;] for i, marker in enumerate(markers): x_axis_data = irisDF[irisDF[&#39;target&#39;] == i][&#39;sepal_length&#39;] y_axis_data = irisDF[irisDF[&#39;target&#39;] == i][&#39;sepal_width&#39;] plt.scatter(x_axis_data, y_axis_data, marker = marker, label = iris.target_names[i]) plt.legend() plt.xlabel(&#39;sepal length&#39;) plt.ylabel(&#39;sepal width&#39;) plt.show() . 길이를 x축 너비를 y축으로, 도형으로 붓꽃 데이터를 구분했습니다. . 파란색 데이터는 y축값 3이상, x축값 6이하인 곳에 일정하게 분포돼 있습니다. . 노란색과 초록색 데이터는 이 두 특성으로 구분하기 힘듭니다. . from sklearn.preprocessing import StandardScaler iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:,:-1]) . 타겟 값을 제외한 모든 특성을 표준 정규 분포를 따르게 변환했습니다. . PCA방법은 특성의 스케일에 영향을 받기 때문에 동일한 스케일로 변환하는 것이 필수입니다. . from sklearn.decomposition import PCA pca = PCA(n_components = 2) pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) print(iris_pca.shape) . (150, 2) . 4차원 데이터를 2차원 PCA 데이터로 변환하였습니다. . pca_columns = [&#39;pca_component_1&#39;, &#39;pca_component_2&#39;] irisDF_pca = pd.DataFrame(iris_pca, columns = pca_columns) irisDF_pca[&#39;target&#39;] = iris.target irisDF_pca.head(3) . pca_component_1 pca_component_2 target . 0 -2.264703 | 0.480027 | 0 | . 1 -2.080961 | -0.674134 | 0 | . 2 -2.364229 | -0.341908 | 0 | . 만들어진 PCA 특성 값으로 데이터 프레임을 만들었습니다. . markers = [&#39;^&#39;,&#39;s&#39;,&#39;o&#39;] for i, marker in enumerate(markers): x_axis_data = irisDF_pca[irisDF[&#39;target&#39;] == i][&#39;pca_component_1&#39;] y_axis_data = irisDF_pca[irisDF[&#39;target&#39;] == i][&#39;pca_component_2&#39;] plt.scatter(x_axis_data, y_axis_data, marker = marker, label = iris.target_names[i]) plt.legend() plt.xlabel(&#39;pca_component_1&#39;) plt.ylabel(&#39;pca_component_2&#39;) plt.show() . 두 개의 pca 특성 값으로 노란색과 초록색 데이터 까지 분류가 가능해집니다. . 사실 두 개의 pca 특성 값에 네 개의 특성값이 섞여있다고 볼 수 있는데요. . 삼차원 이상에 데이터는 시각화 하기 힘들기 때문에 이렇게 시각화 할 수 있는것이 pca분석의 장점이라고 할 수 있습니다. . print(pca.explained_variance_ratio_) . [0.72962445 0.22850762] . explained_varianceratio 값은 변환 된 특성이 얼마나 변동을 설명하는 가를 보여쥽니다. . 두 개의 pca 특성이 약 95% 정도에 변동을 설명하고 있습니다. . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np rcf = RandomForestClassifier(random_state = 156) scores = cross_val_score(rcf, iris.data, iris.target, scoring = &#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도 :&#39;, scores) print(&#39;평균 정확도 :&#39;, np.mean(scores)) . 개별 정확도 : [0.98 0.94 0.96] 평균 정확도 : 0.96 . 기존 4차원 데이터를 랜덤포레스트 기법을 이용해서 검정했습니다. . 평균 정확도는 약 96%가 나옵니다. . pca_x = irisDF_pca[[&#39;pca_component_1&#39;,&#39;pca_component_2&#39;]] scores_pca = cross_val_score(rcf, pca_x, iris.target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도 :&#39;, scores_pca) print(&#39;평균 정확도 :&#39;, np.mean(scores_pca)) . 개별 정확도 : [0.88 0.88 0.88] 평균 정확도 : 0.88 . PCA기법으로 변환한 데이터를 통해 분석한 결과, 평균 정확도는 약 88%가 나옵니다. . 성능이 다소 감소했다고도 볼 수 있습니다. . 하지만 특성 수가 절반이 된 걸 생각해보면 원본 데이터의 특성을 상당부분 잘 유지하고 있다고도 볼 수 있습니다. . &#49888;&#50857;&#52852;&#46300; &#44256;&#44061; &#45936;&#51060;&#53552; &#49464;&#53944; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd df = pd.read_excel(&#39;/content/drive/MyDrive/credit_card.xls&#39;, header = 1, sheet_name=&#39;Data&#39;).iloc[0:,1:] print(df.shape) df.head(3) . (30000, 24) . LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | 3913 | 3102 | 689 | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | 2682 | 1725 | 2682 | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 29239 | 14027 | 13559 | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 24개의 특성과 3만개의 데이터가 있습니다. . df.rename(columns={&#39;PAY_0&#39;:&#39;PAY_1&#39;, &#39;default payment next month&#39;:&#39;default&#39;}, inplace=True) y_target = df[&#39;default&#39;] x_features = df.drop(&#39;default&#39;, axis = 1) . pay_0 다음 pay_2 칼럼이 있어서 pay_1로 이름 변경했습니다. . default.. 칼럼도 길어서 짧게 바꿨습니다. . import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline corr = x_features.corr() plt.figure(figsize = (14,14)) sns.heatmap(corr, annot=True, fmt = &#39;.1g&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f76d75da490&gt; . 상관계수 행렬을 관찰해본 결과 PAY 변수끼리, 또 BILL 변수 끼리 상관계수가 매우 높은 것을 알 수 있습니다. . 다중공선성 등 상당부분 문제가 있기 때문에 PCA 방법으로 조정해보겠습니다. . from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler cols_bill = [&#39;BILL_AMT&#39;+str(i) for i in range(1,7)] print(&#39;대상 속성명:&#39;, cols_bill) scaler = StandardScaler() df_cols_scaled = scaler.fit_transform(x_features[cols_bill]) pca = PCA(n_components = 2) pca.fit(df_cols_scaled) print(&#39;변동성:&#39;, pca.explained_variance_ratio_) . 대상 속성명: [&#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;] 변동성: [0.90555253 0.0509867 ] . 단 두 개의 pca 특성으로 변동성을 95프로이상 설명할 수 있습니다. . import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score rcf = RandomForestClassifier(n_estimators = 300, random_state = 156) scores = cross_val_score(rcf, x_features, y_target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도:&#39;, scores) print(&#39;평균 정확도:&#39;, np.mean(scores)) . 개별 정확도: [0.8083 0.8196 0.8232] 평균 정확도: 0.8170333333333333 . 원본 데이터를 그대로 적용했을 때 정확도 입니다. . scaler = StandardScaler() df_scaled = scaler.fit_transform(x_features) pca = PCA(n_components = 6) df_pca = pca.fit_transform(df_scaled) scores_pca = cross_val_score(rcf, df_pca, y_target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도:&#39;, scores_pca) print(&#39;평균 정확도:&#39;, np.mean(scores_pca)) . 개별 정확도: [0.7924 0.7969 0.8012] 평균 정확도: 0.7968333333333334 . 전체 23개의 속성중 6개 속성만 이용했음에도 정확도가 원본 데이터 대비 크게 떨어지지 않습니다. . 이 기법은 최근 컴퓨터 비전 분야에 많이 쓰입니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/10/06/PythonMachine6_1.html",
            "relUrl": "/2021/10/06/PythonMachine6_1.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "파이썬 머신러닝 완벽 가이드 5장-실습(자전거 대여 수요 예측)",
            "content": ". &#52880;&#44544;&#50640;&#49436; &#45936;&#51060;&#53552; &#51649;&#51217; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;0c820de52cea65ec11954012ef8b00d2&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ # Permission Warning이 발생하지 않도록 해줍니다. !chmod 600 ~/.kaggle/kaggle.json . ! kaggle competitions download -c bike-sharing-demand . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sampleSubmission.csv to /content 0% 0.00/140k [00:00&lt;?, ?B/s] 100% 140k/140k [00:00&lt;00:00, 50.8MB/s] Downloading test.csv to /content 0% 0.00/316k [00:00&lt;?, ?B/s] 100% 316k/316k [00:00&lt;00:00, 44.9MB/s] Downloading train.csv to /content 0% 0.00/633k [00:00&lt;?, ?B/s] 100% 633k/633k [00:00&lt;00:00, 40.4MB/s] . &#45936;&#51060;&#53552; &#46168;&#47084;&#48372;&#44592; &#48143; &#44032;&#44277; . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;, category = RuntimeWarning) bike_df = pd.read_csv(&#39;./train.csv&#39;) print(bike_df.shape) bike_df.head() . (10886, 12) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . 변수는 11개, 10886개 데이터가 있습니다. datetime변수는 가공이 필요합니다. . bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB . 결측값은 없습니다. . bike_df[&#39;datetime&#39;] = bike_df.datetime.apply(pd.to_datetime) bike_df[&#39;year&#39;] = bike_df.datetime.apply(lambda x : x.year) bike_df[&#39;month&#39;] = bike_df.datetime.apply(lambda x : x.month) bike_df[&#39;day&#39;] = bike_df.datetime.apply(lambda x : x.day) bike_df[&#39;hour&#39;] = bike_df.datetime.apply(lambda x : x.hour) bike_df.head(3) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | 2011 | 1 | 1 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | 2011 | 1 | 1 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | 2011 | 1 | 1 | 2 | . pd.to_datetime 함수를 통해 데이터 타임을 datetime으로 바꿨습니다. . datetime 데이터 타입은 year, month 등등으로 구분할 수 있습니다. . 이를 활용하여 년, 달, 날, 시간 변수로 각각 생성하였습니다. . drop_columns = [&#39;datetime&#39;, &#39;casual&#39;, &#39;registered&#39;] bike_df.drop(drop_columns, axis = 1, inplace = True) . datetime 변수는 분해를 했기 때문에 원본 변수가 필요 없어졌습니다. . casual + registered = count 변수 이므로 두 변수 모두 제외하겠습니다. . from sklearn.metrics import mean_squared_error, mean_absolute_error def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle def rmse(y, pred): return np.sqrt(mean_squared_error(y, pred)) def evaluate_regr(y, pred): rmsle_val = rmsle(y, pred) rmse_val = rmse(y, pred) mae_val = mean_absolute_error(y, pred) print(&#39;rmsle :&#39;, np.round(rmsle_val, 4), &#39;rmse :&#39;, np.round(rmse_val, 4), &#39;mse :&#39;, np.round(mae_val, 4)) . 이번 분석의 성능 평가 방법은 rmsle 이기 때문에 이를 구현했습니다. . &#52395;&#48264;&#51704; &#48516;&#49437; . from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso y_target = bike_df[&#39;count&#39;] x_features = bike_df.drop([&#39;count&#39;], axis = 1, inplace = False) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target, test_size = 0.3, random_state = 0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) evaluate_regr(y_test, pred) . rmsle : 1.1647 rmse : 140.8996 mse : 105.9244 . 실제 타겟 값이 대여 횟수임으로 지금 rmse 값은 매우 크다고 볼 수 있습니다. . def get_top_error_data(y_test, pred, n_tops = 5): result_df = pd.DataFrame(y_test.values, columns=[&#39;real_count&#39;]) result_df[&#39;predicted_count&#39;] = np.round(pred) result_df[&#39;diff&#39;] = np.abs(result_df[&#39;real_count&#39;] - result_df[&#39;predicted_count&#39;]) print(result_df.sort_values(&#39;diff&#39;, ascending= False)[:n_tops]) get_top_error_data(y_test, pred) . real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 . 실제값과 예측값이 가장 차이가 큰 5개 데이터를 출력했습니다. . 상당히 차이가 많이 나는걸 볼 수 있는데요. . 타겟값의 분포가 치우쳐 있는지 확인을 해볼 필요가 있겠습니다. . y_target.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c8fd090&gt; . 오른쪽 꼬리가 매우 두터운 형태임을 알 수 있습니다. . 이런 형태일 때 가장 자주 쓰이는 로그변환을 적용해보겠습니다. . y_log_transform = np.log1p(y_target) y_log_transform.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c792690&gt; . 정규분포와는 다소 차이가 있지만 변환 전보다 왜곡 정도가 많이 개선됐습니다. . y_target_log = np.log1p(y_target) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target_log, test_size= 0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) y_test_exp = np.expm1(y_test) pred_exp = np.expm1(pred) evaluate_regr(y_test_exp, pred_exp) . rmsle : 1.0168 rmse : 162.5943 mse : 109.2862 . mse 값은 전보다 개선 되었지만 rmse 값은 더 증가하였습니다. . 무슨 이유일까요? . &#46160;&#48264;&#51704; &#48516;&#49437; . coef = pd.Series(lr_reg.coef_, index=x_features.columns) coef_sort = coef.sort_values(ascending = False) sns.barplot(x=coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c1f0990&gt; . 다른 값에 비해 year값이 높습니다. . year값은 년도인데 년도가 이렇게 큰 영향을 미치는 것을 일반적인 사실로 받아들이기 힘듭니다. . 이유를 추정해보자면 연도 변수의 값이 큰 점을 들 수 있습니다.(2011,2012) . 비슷한 이유로 범주형 변수로 변환할 필요가 있는 변수들을 원핫인코딩방식으로 변환하겠습니다. . x_features_ohe = pd.get_dummies(x_features, columns = [&#39;year&#39;, &#39;month&#39;,&#39;day&#39;,&#39;hour&#39;,&#39;holiday&#39;, &#39;workingday&#39;, &#39;season&#39;, &#39;weather&#39;]) x_features_ohe.shape . (10886, 73) . 원핫 인코딩 결과 열 개수가 73개로 크게 늘어났습니다. . x_train, x_test, y_train, y_test = train_test_split(x_features_ohe, y_target_log, test_size= 0.3, random_state=0) def get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = False): model.fit(x_train, y_train) pred = model.predict(x_test) if is_expm1: y_test = np.expm1(y_test) pred = np.expm1(pred) print(model.__class__.__name__) evaluate_regr(y_test, pred) lr_reg = LinearRegression() ridge_reg = Ridge(alpha = 10) lasso_reg = Lasso(alpha = 0.01) for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = True) . LinearRegression rmsle : 0.5896 rmse : 97.6878 mse : 63.3821 Ridge rmsle : 0.5901 rmse : 98.5286 mse : 63.8934 Lasso rmsle : 0.6348 rmse : 113.2188 mse : 72.8027 . 원핫 인코딩을 적용한 후 결과가 눈에 띄게 좋아졌습니다. . coef = pd.Series(lr_reg.coef_, index=x_features_ohe.columns) coef_sort = coef.sort_values(ascending = False)[:20] sns.barplot(x = coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c772bd0&gt; . 회귀계수가 높은 피처 20개를 출력해보았습니다. . &#49464;&#48264;&#51704; &#48516;&#49437; . from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor rf_reg = RandomForestRegressor(n_estimators = 500) gbm_reg = GradientBoostingRegressor(n_estimators = 500) xgb_reg = XGBRegressor(n_estimaters = 500) lgbm_reg = LGBMRegressor(n_estimaters = 500) for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: get_model_predict(model, x_train.values, x_test.values, y_train.values, y_test.values, is_expm1=True) . RandomForestRegressor rmsle : 0.3549 rmse : 50.2976 mse : 31.1562 GradientBoostingRegressor rmsle : 0.3299 rmse : 53.3352 mse : 32.7448 [16:01:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor rmsle : 0.4828 rmse : 95.6137 mse : 59.2047 LGBMRegressor rmsle : 0.3315 rmse : 51.3807 mse : 31.8325 . 부스팅 모델을 사용하면 더 좋은 성능을 보일 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/10/02/PythonMachine5_4.html",
            "relUrl": "/2021/10/02/PythonMachine5_4.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "머신러닝 가이드) 5-3 다양한 회귀",
            "content": ". &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression cancer = load_breast_cancer() . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split scaler = StandardScaler() data_scaled = scaler.fit_transform(cancer.data) x_train, x_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size = 0.3, random_state = 0) . 평균이 0, 분산이 1인 정규분포 형태로 형 변환을 했습니다. . 로지스틱 회귀기법은 선형 회귀 방식에 응용으로 데이터의 정규분포도에 영향을 많이 받습니다. . from sklearn.metrics import accuracy_score, roc_auc_score import numpy as np lr_clf = LogisticRegression() lr_clf.fit(x_train, y_train) lr_preds = lr_clf.predict(x_test) print(&#39;정확도 :&#39;, np.round(accuracy_score(y_test, lr_preds), 4)) print(&#39;roc 커브 :&#39;, np.round(roc_auc_score(y_test, lr_preds), 4)) . 정확도 : 0.9766 roc 커브 : 0.9716 . from sklearn.model_selection import GridSearchCV params = {&#39;penalty&#39; : [&#39;l2&#39;, &#39;l1&#39;], &#39;C&#39; : [0.01, 0.1, 1, 5, 10]} grid_clf = GridSearchCV(lr_clf, param_grid = params, scoring = &#39;accuracy&#39;, cv = 3) grid_clf.fit(data_scaled, cancer.target) print(&#39;최적 파라미터 : &#39;, grid_clf.best_params_, &#39;최적 평균 정확도&#39;, grid_clf.best_score_) . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터 : {&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;} 최적 평균 정확도 0.975392184164114 . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터는 l2 규제로(릿지 회귀) c가(알파의 역수) 1일때 입니다. . &#53944;&#47532; &#44592;&#48152; &#54924;&#44480; &#47784;&#45944; . from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor import pandas as pd import numpy as np boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) rf = RandomForestRegressor(random_state = 0, n_estimators = 1000) neg_mse_scores = cross_val_score(rf, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse score : &#39;, np.round(neg_mse_scores, 4)) print(&#39;rmse score : &#39;, np.round(rmse_scores, 4)) print(&#39;평균 rmse score : &#39;, np.round(avg_rmse, 4)) . mse score : [ -7.933 -13.0584 -20.5278 -46.3057 -18.8008] rmse score : [2.8166 3.6136 4.5308 6.8048 4.336 ] 평균 rmse score : 4.4204 . 랜덤 포레스트 회귀 입니다. 평균 rmse 값은 4.42로 꽤 좋은 수치 입니다. . def get_model_cv_prediction(model, x_data, y_target): neg_mse_scores = cross_val_score(model, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(model.__class__.__name__) print(&#39;평균 rmse : &#39;, np.round(avg_rmse, 4)) . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor dt_reg = DecisionTreeRegressor(random_state = 0, max_depth = 4) rf_reg = RandomForestRegressor(random_state = 0, n_estimators = 1000) gb_reg = GradientBoostingRegressor(random_state = 0, n_estimators = 1000) xgb_reg = XGBRegressor(random_state = 0, n_estimators = 1000) lgb_reg = LGBMRegressor(random_state = 0, n_estimators = 1000) models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] for model in models: get_model_cv_prediction(model, x_data, y_target) . DecisionTreeRegressor 평균 rmse : 6.2377 RandomForestRegressor 평균 rmse : 4.4204 GradientBoostingRegressor 평균 rmse : 4.2692 [13:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor 평균 rmse : 4.0889 LGBMRegressor 평균 rmse : 4.6464 . 여러 모델을 테스트 해보았습니다. . xgb부스팅 모델의 성능이 가장 우수하게 나왔습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/30/PythonMachine5_3.html",
            "relUrl": "/2021/09/30/PythonMachine5_3.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "머신러닝 가이드) 5-2 선형회귀응용",
            "content": ". &#45796;&#54637; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures import numpy as np x = np.arange(4).reshape(2,2) # 행 부터 숫자 채워짐 print(&#39;일차 단항식 계수 피처: n&#39;, x) poly = PolynomialFeatures(degree = 2) poly.fit(x) poly_ftr = poly.transform(x) print(&#39;변환된 2차 다항식 계수 피처: n&#39;, poly_ftr) . 일차 단항식 계수 피처: [[0 1] [2 3]] 변환된 2차 다항식 계수 피처: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] . 2차 다항계수는 [1, x1, x2, x1^2, x1x2, x2^2] 로 구성되어 있습니다. . def polynomial_func(x): y = 1 + 2 * x[:,0] + 3 * x[:,0] **2 + 4 * x[:,1] **3 return y y = polynomial_func(x) . from sklearn.linear_model import LinearRegression poly_ftr = PolynomialFeatures(degree = 3).fit_transform(x) print(&#39;3차 다항식 계수 feature: n&#39;, poly_ftr) model = LinearRegression() model.fit(poly_ftr, y) print(&#39;회귀 계수 n&#39;, np.round(model.coef_,2)) print(&#39;회귀 shape&#39;, model.coef_.shape) . 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] 회귀 shape (10,) . poly함수로 다항식 계수를 생성한 뒤 단순 선형 회귀 함수에 대입해줍니다. . 원하는 값인 [1,2,0,3,0,0,0,0,0,4] 와 다소 차이가 있긴 합니다. . &#47551;&#51648; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline import numpy as np model = Pipeline([(&#39;poly&#39;, PolynomialFeatures(degree=3)), (&#39;linear&#39;, LinearRegression())]) model = model.fit(x,y) print(&#39;회귀 계수 n&#39;, np.round(model.named_steps[&#39;linear&#39;].coef_,2)) . 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] . 파이프 라인 함수로 다항식으로에 변환과 선형 회귀를 한번에 한 모습입니다. . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score from sklearn.datasets import load_boston import pandas as pd boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) ridge = Ridge(alpha = 10) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-11.42 -24.29 -28.14 -74.6 -28.52] rmse scores [3.38 4.93 5.31 8.64 5.34] 평균 rmse score: 5.52 . 단순 선형회귀 모델 rmse 평균값이 5.84로 릿지 회귀가 더 좋은 퍼포먼스를 보입니다. . alphas = [0,0.1,1,10,100] for alpha in alphas: ridge = Ridge(alpha=alpha) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 5.8287 alpha 값 0.1 일때 평균 rmse : 5.7885 alpha 값 1 일때 평균 rmse : 5.6526 alpha 값 10 일때 평균 rmse : 5.5182 alpha 값 100 일때 평균 rmse : 5.3296 . alpha 값이 100일때가 가장 값이 좋습니다. . import matplotlib.pyplot as plt import seaborn as sns fig, axs = plt.subplots(figsize= (18,6), nrows = 1, ncols = 5) coeff_df = pd.DataFrame() for pos, alpha in enumerate(alphas): ridge = Ridge(alpha = alpha) ridge.fit(x_data, y_target) coeff = pd.Series(data=ridge.coef_, index = x_data.columns) colname = &#39;alpha:&#39;+str(alpha) coeff_df[colname] = coeff coeff = coeff.sort_values(ascending = False) axs[pos].set_title(colname) axs[pos].set_xlim(-3, 6) sns.barplot(x=coeff.values, y = coeff.index, ax = axs[pos]) plt.show() . 알파 값이 커지면(=규제가 세지면) 회귀계수 값이 전반적으로 작아집니다. . 다만 릿지 회귀에 경우 회귀 계수를 0으로 만들지는 않습니다. . &#46972;&#50136; &#54924;&#44480; . from sklearn.linear_model import Lasso, ElasticNet def get_linear_reg_eval(model_name, params = None, x_data_n = None, y_target_n = None, verbose= True, return_coeff = True): coeff_df = pd.DataFrame() if verbose : print(model_name) for param in params: if model_name ==&#39;Ridge&#39; : model = Ridge(alpha = param) elif model_name ==&#39;Lasso&#39; : model = Lasso(alpha = param) elif model_name ==&#39;ElasticNet&#39; : model = ElasticNet(alpha = param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, x_data_n, y_target_n, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores)) print(&#39;alpha &#39;, param, &#39;일때 평균 rmse:&#39;, np.round(avg_rmse,2)) model.fit(x_data_n, y_target_n) if return_coeff: coeff = pd.Series(data=model.coef_, index = x_data_n.columns) colname = &#39;alpha:&#39;+str(param) coeff_df[colname] = coeff return coeff_df . lasso_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;Lasso&#39;,params=lasso_alphas, x_data_n = x_data, y_target_n= y_target) . Lasso alpha 0.07 일때 평균 rmse: 5.61 alpha 0.1 일때 평균 rmse: 5.62 alpha 0.5 일때 평균 rmse: 5.67 alpha 1 일때 평균 rmse: 5.78 alpha 3 일때 평균 rmse: 6.19 . 알파 값이 0.07일때 최고 성능을 보여줍니다. . 앞서 한 릿지보다는 성능이 떨어지지만, 단순 선형 회귀 모델보다 값이 크므로 쓰임새가 있습니다. . sort_column = &#39;alpha:&#39;+str(lasso_alphas[0]) coeff_lasso_df.sort_values(by = sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.789725 | 3.703202 | 2.498212 | 0.949811 | 0.000000 | . CHAS 1.434343 | 0.955190 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.270936 | 0.274707 | 0.277451 | 0.264206 | 0.061864 | . ZN 0.049059 | 0.049211 | 0.049544 | 0.049165 | 0.037231 | . B 0.010248 | 0.010249 | 0.009469 | 0.008247 | 0.006510 | . NOX -0.000000 | -0.000000 | -0.000000 | -0.000000 | 0.000000 | . AGE -0.011706 | -0.010037 | 0.003604 | 0.020910 | 0.042495 | . TAX -0.014290 | -0.014570 | -0.015442 | -0.015212 | -0.008602 | . INDUS -0.042120 | -0.036619 | -0.005253 | -0.000000 | -0.000000 | . CRIM -0.098193 | -0.097894 | -0.083289 | -0.063437 | -0.000000 | . LSTAT -0.560431 | -0.568769 | -0.656290 | -0.761115 | -0.807679 | . PTRATIO -0.765107 | -0.770654 | -0.758752 | -0.722966 | -0.265072 | . DIS -1.176583 | -1.160538 | -0.936605 | -0.668790 | -0.000000 | . 계수가 0인것이 보입니다. 알파값이 커질수록 회귀 계수가 0인 것이 늘어납니다. . &#50648;&#46972;&#49828;&#54001; &#54924;&#44480; . 다음은 엘라스틱 회귀 입니다. 쉽게 라쏘회귀 + 릿지 회귀로 볼 수 있습니다. . 라쏘 회귀에 경우 서로 상관관계가 높은 피처가 있으면 중요 피처를 제외하고 모두 회귀계수를 0으로 만듭니다. . 이를 다소 완화해주기 위한 목적으로 만들어졌습니다. 다만 수행시간이 다소 깁니다. . 여기서 알파는 알파1 + 알파 2 이며, l1_ratio는 말 그대로 l1규제(라쏘) 비율입니다. . elastic_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;ElasticNet&#39;, params=elastic_alphas, x_data_n= x_data, y_target_n= y_target) . ElasticNet alpha 0.07 일때 평균 rmse: 5.54 alpha 0.1 일때 평균 rmse: 5.53 alpha 0.5 일때 평균 rmse: 5.47 alpha 1 일때 평균 rmse: 5.6 alpha 3 일때 평균 rmse: 6.07 . 알파값이 0.5일때 가장 좋은 예측 성능을 보여줍니다. . sort_column = &#39;alpha:&#39;+str(elastic_alphas[0]) coeff_lasso_df.sort_values(by= sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.574162 | 3.414154 | 1.918419 | 0.938789 | 0.000000 | . CHAS 1.330724 | 0.979706 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.278880 | 0.283443 | 0.300761 | 0.289299 | 0.146846 | . ZN 0.050107 | 0.050617 | 0.052878 | 0.052136 | 0.038268 | . B 0.010122 | 0.010067 | 0.009114 | 0.008320 | 0.007020 | . AGE -0.010116 | -0.008276 | 0.007760 | 0.020348 | 0.043446 | . TAX -0.014522 | -0.014814 | -0.016046 | -0.016218 | -0.011417 | . INDUS -0.044855 | -0.042719 | -0.023252 | -0.000000 | -0.000000 | . CRIM -0.099468 | -0.099213 | -0.089070 | -0.073577 | -0.019058 | . NOX -0.175072 | -0.000000 | -0.000000 | -0.000000 | -0.000000 | . LSTAT -0.574822 | -0.587702 | -0.693861 | -0.760457 | -0.800368 | . PTRATIO -0.779498 | -0.784725 | -0.790969 | -0.738672 | -0.423065 | . DIS -1.189438 | -1.173647 | -0.975902 | -0.725174 | -0.031208 | . 라쏘모델에 비해 회귀계수를 0으로 만드는 개수가 다소 줄었습니다. . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#51012; &#50948;&#54620; &#45936;&#51060;&#53552; &#48320;&#54872; . 선형 회귀에서 중요한 것 중 하나가 데이터 분포도의 정규화 입니다. . 특히 타깃값의 분포가 정규분포가 아닌 왜곡(skew)된 분포는 예측 성능에 부정적입니다. . 따라서 선형 회귀 모델을 적용하기 전 먼저 데이터 스케일링/정규화 작업을 수행해주어야 합니다. . from sklearn.preprocessing import StandardScaler, MinMaxScaler def get_scaled_data(method=&#39;None&#39;, p_degree = None, input_data = None): if method == &#39;Standard&#39;: scaled_data = StandardScaler().fit_transform(input_data) elif method == &#39;MinMax&#39;: scaled_data = MinMaxScaler().fit_transform(input_data) if method == &#39;Log&#39;: scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data . alphas = [0.1, 1, 10, 100] scale_methods=[(None, None), (&#39;Standard&#39;, None), (&#39;Standard&#39;,2), (&#39;MinMax&#39;,None), (&#39;MinMax&#39;, 2), (&#39;Log&#39;, None)] for scale_method in scale_methods: x_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=x_data) print(&#39; n 변환유형:&#39;, scale_method[0], &#39;, Polynomial Degree:&#39;, scale_method[1]) get_linear_reg_eval(&#39;Ridge&#39;, params = alphas, x_data_n=x_data_scaled, y_target_n= y_target, verbose=False, return_coeff = False) . 변환유형: None , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: MinMax , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: MinMax , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: Log , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 4.77 alpha 1 일때 평균 rmse: 4.68 alpha 10 일때 평균 rmse: 4.84 alpha 100 일때 평균 rmse: 6.24 . log 변환이 다른 변환에 비해 성능이 뛰어난 걸 볼 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/25/PythonMachine5_2.html",
            "relUrl": "/2021/09/25/PythonMachine5_2.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "SSUDA) 심장병 데이터 분석",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd data = pd.read_csv(&quot;/content/drive/MyDrive/heart.csv&quot;) . Verson 1. &#49900;&#54540;&#54620; &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; &#47784;&#54805; . &#45936;&#51060;&#53552; &#51060;&#54644; . df = data.copy() df.head() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . 디폴트 값은 5입니다. . df.columns . Index([&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalachh&#39;, &#39;exng&#39;, &#39;oldpeak&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;, &#39;output&#39;], dtype=&#39;object&#39;) . df.columns.values.tolist() . [&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalachh&#39;, &#39;exng&#39;, &#39;oldpeak&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;, &#39;output&#39;] . 컬럼은 이런 방식으로 확인할 수 있습니다. . 밑에 DataFrame.columns.values.tolist() 함수는 컬럼 추출 중 가장 런타임이 빠르다고 합니다. . print(&#39;Shape is&#39;,df.shape) . Shape is (303, 14) . 303개 데이터, 14개 특성값이 있습니다. . df.isnull().sum() . age 0 sex 0 cp 0 trtbps 0 chol 0 fbs 0 restecg 0 thalachh 0 exng 0 oldpeak 0 slp 0 caa 0 thall 0 output 0 dtype: int64 . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trtbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalachh 303 non-null int64 8 exng 303 non-null int64 9 oldpeak 303 non-null float64 10 slp 303 non-null int64 11 caa 303 non-null int64 12 thall 303 non-null int64 13 output 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . 글쓴이는 윗방식으로 null값 유무를 체크했습니다. . 그러나 df.info() 방식이 여러가지 정보를 같이 줘 더 효율적입니다. . df.describe() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | . mean 54.366337 | 0.683168 | 0.966997 | 131.623762 | 246.264026 | 0.148515 | 0.528053 | 149.646865 | 0.326733 | 1.039604 | 1.399340 | 0.729373 | 2.313531 | 0.544554 | . std 9.082101 | 0.466011 | 1.032052 | 17.538143 | 51.830751 | 0.356198 | 0.525860 | 22.905161 | 0.469794 | 1.161075 | 0.616226 | 1.022606 | 0.612277 | 0.498835 | . min 29.000000 | 0.000000 | 0.000000 | 94.000000 | 126.000000 | 0.000000 | 0.000000 | 71.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 47.500000 | 0.000000 | 0.000000 | 120.000000 | 211.000000 | 0.000000 | 0.000000 | 133.500000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 55.000000 | 1.000000 | 1.000000 | 130.000000 | 240.000000 | 0.000000 | 1.000000 | 153.000000 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 61.000000 | 1.000000 | 2.000000 | 140.000000 | 274.500000 | 0.000000 | 1.000000 | 166.000000 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 77.000000 | 1.000000 | 3.000000 | 200.000000 | 564.000000 | 1.000000 | 2.000000 | 202.000000 | 1.000000 | 6.200000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . 데이터를 보면 어느정도 스케일링이 필요하다는 것을 알 수 있습니다. . &#53945;&#49457; &#49828;&#52992;&#51068;&#47553; . df[&#39;age&#39;] = df[&#39;age&#39;]/max(df[&#39;age&#39;]) df[&#39;cp&#39;] = df[&#39;cp&#39;]/max(df[&#39;cp&#39;]) df[&#39;trtbps&#39;] = df[&#39;trtbps&#39;]/max(df[&#39;trtbps&#39;]) df[&#39;chol&#39;] = df[&#39;chol&#39;]/max(df[&#39;chol&#39;]) df[&#39;thalachh&#39;] = df[&#39;thalachh&#39;]/max(df[&#39;thalachh&#39;]) . df.describe() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | . mean 0.706056 | 0.683168 | 0.322332 | 0.658119 | 0.436638 | 0.148515 | 0.528053 | 0.740826 | 0.326733 | 1.039604 | 1.399340 | 0.729373 | 2.313531 | 0.544554 | . std 0.117949 | 0.466011 | 0.344017 | 0.087691 | 0.091898 | 0.356198 | 0.525860 | 0.113392 | 0.469794 | 1.161075 | 0.616226 | 1.022606 | 0.612277 | 0.498835 | . min 0.376623 | 0.000000 | 0.000000 | 0.470000 | 0.223404 | 0.000000 | 0.000000 | 0.351485 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 0.616883 | 0.000000 | 0.000000 | 0.600000 | 0.374113 | 0.000000 | 0.000000 | 0.660891 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 0.714286 | 1.000000 | 0.333333 | 0.650000 | 0.425532 | 0.000000 | 1.000000 | 0.757426 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 0.792208 | 1.000000 | 0.666667 | 0.700000 | 0.486702 | 0.000000 | 1.000000 | 0.821782 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 2.000000 | 1.000000 | 1.000000 | 6.200000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . 이전과 달리 특성 스케일이 확실히 비슷해졌습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . from sklearn.model_selection import train_test_split #splitting data into training data and testing data X_train, X_test, y_train, y_test = train_test_split( df.drop([&#39;output&#39;], axis=1), df.output, test_size= 0.2, # 20% test data &amp; 80% train data random_state=0, stratify=df.output ) . stratify 속성 =&gt; y값의 공평한 분배를 위해 사용하는 속성입니다. . from sklearn.linear_model import LogisticRegression clf = LogisticRegression() clf.fit(X_train, y_train) from sklearn.metrics import accuracy_score Y_pred = clf.predict(X_test) acc=accuracy_score(y_test, Y_pred) print(&#39;Accuracy is&#39;,round(acc,2)*100,&#39;%&#39;) . Accuracy is 89.0 % . 로지스틱 회귀 모형을 별다른 튜닝 없이 사용했습니다. . 정확도 측면에서만 보면 캐글에 있는 다른 코드와 별반 다르지 않습니다. . Verson 2. &#49900;&#54540;&#54620; &#46373;&#47084;&#45789; &#47784;&#54805; . &#45936;&#51060;&#53552; &#51060;&#54644;2 . df = data.copy() df.output.value_counts() . 1 165 0 138 Name: output, dtype: int64 . 이전 모델에서 생략(?)된 부분인거 같은데 1과 0 값의 비율이 조금 차이가 있습니다. . df.corr().abs()[&#39;output&#39;].sort_values(ascending = False) . output 1.000000 exng 0.436757 cp 0.433798 oldpeak 0.430696 thalachh 0.421741 caa 0.391724 slp 0.345877 thall 0.344029 sex 0.280937 age 0.225439 trtbps 0.144931 restecg 0.137230 chol 0.085239 fbs 0.028046 Name: output, dtype: float64 . Y값과의 상관계수가 어느정도 되는지 확인해보았습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553;2 . from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score X = df.drop(&#39;output&#39;, axis = 1) y = df[&#39;output&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) X_train.shape . (242, 13) . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 여기서는 StandardScaler를 사용해 스케일링을 했습니다. . 평균 0, 분산 1로 조정합니다. 이 스케일링은 이상치가 있을때 잘 작용하지 않을 수 있습니다. . from tensorflow import keras model = keras.Sequential( [ keras.layers.Dense( 256, activation=&quot;relu&quot;, input_shape=[13] ), keras.layers.Dense(515, activation=&quot;relu&quot;), keras.layers.Dropout(0.3), keras.layers.Dense(50, activation=&quot;relu&quot;), keras.layers.Dropout(0.3), keras.layers.Dense(1, activation=&quot;sigmoid&quot;), ] ) model.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_20 (Dense) (None, 256) 3584 _________________________________________________________________ dense_21 (Dense) (None, 515) 132355 _________________________________________________________________ dropout_10 (Dropout) (None, 515) 0 _________________________________________________________________ dense_22 (Dense) (None, 50) 25800 _________________________________________________________________ dropout_11 (Dropout) (None, 50) 0 _________________________________________________________________ dense_23 (Dense) (None, 1) 51 ================================================================= Total params: 161,790 Trainable params: 161,790 Non-trainable params: 0 _________________________________________________________________ . 활성화 함수로 제일 많이 사용하는 relu와 sigmoid함수를 사용했습니다. . relu함수 : 입력이 양수일 경우 그대로 반환, 음수일경우 0으로 만듭니다. . sigmoid함수 : 1 / (1 + e^z) 함수. 값을 0에서 1 사이로 변환합니다. . 첫번째 구간에 아웃풋 값을 256개 주었는데, 변수값이 13개임으로 모수가 14개입니다. . 그래서 256*14 = 3584개 파라미터가 나오게 된 것입니다. . 중간에 있는 드롭아웃은 일정 비율만큼 뉴런을 랜덤하게 꺼서 과대적합을 막는 역할을 합니다. . model.compile(optimizer = &#39;Adam&#39;, loss = &#39;binary_crossentropy&#39;, metrics = [&#39;binary_accuracy&#39;]) early_stopping = keras.callbacks.EarlyStopping( patience = 20, min_delta = 0.001, restore_best_weights =True ) history = model.fit( X_train, y_train, validation_data=(X_test, y_test), batch_size=15, epochs=50, callbacks = [early_stopping], verbose=1, ) . Epoch 1/50 17/17 [==============================] - 1s 15ms/step - loss: 0.5500 - binary_accuracy: 0.7190 - val_loss: 0.3814 - val_binary_accuracy: 0.8852 Epoch 2/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3823 - binary_accuracy: 0.8347 - val_loss: 0.3797 - val_binary_accuracy: 0.8852 Epoch 3/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3354 - binary_accuracy: 0.8719 - val_loss: 0.4391 - val_binary_accuracy: 0.8197 Epoch 4/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3017 - binary_accuracy: 0.8802 - val_loss: 0.4147 - val_binary_accuracy: 0.8689 Epoch 5/50 17/17 [==============================] - 0s 6ms/step - loss: 0.2589 - binary_accuracy: 0.9091 - val_loss: 0.4388 - val_binary_accuracy: 0.8689 Epoch 6/50 17/17 [==============================] - 0s 6ms/step - loss: 0.2579 - binary_accuracy: 0.9256 - val_loss: 0.4795 - val_binary_accuracy: 0.8525 Epoch 7/50 17/17 [==============================] - 0s 7ms/step - loss: 0.2019 - binary_accuracy: 0.9256 - val_loss: 0.4895 - val_binary_accuracy: 0.8689 Epoch 8/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1889 - binary_accuracy: 0.9298 - val_loss: 0.5359 - val_binary_accuracy: 0.8361 Epoch 9/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1887 - binary_accuracy: 0.9215 - val_loss: 0.5324 - val_binary_accuracy: 0.8525 Epoch 10/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1578 - binary_accuracy: 0.9545 - val_loss: 0.5441 - val_binary_accuracy: 0.8689 Epoch 11/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1686 - binary_accuracy: 0.9215 - val_loss: 0.6338 - val_binary_accuracy: 0.8689 Epoch 12/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1448 - binary_accuracy: 0.9504 - val_loss: 0.6872 - val_binary_accuracy: 0.8197 Epoch 13/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1065 - binary_accuracy: 0.9628 - val_loss: 0.7682 - val_binary_accuracy: 0.8197 Epoch 14/50 17/17 [==============================] - 0s 7ms/step - loss: 0.0879 - binary_accuracy: 0.9835 - val_loss: 0.8583 - val_binary_accuracy: 0.8197 Epoch 15/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0877 - binary_accuracy: 0.9711 - val_loss: 0.9300 - val_binary_accuracy: 0.8361 Epoch 16/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0688 - binary_accuracy: 0.9835 - val_loss: 0.9281 - val_binary_accuracy: 0.8361 Epoch 17/50 17/17 [==============================] - 0s 8ms/step - loss: 0.0615 - binary_accuracy: 0.9835 - val_loss: 0.9688 - val_binary_accuracy: 0.8361 Epoch 18/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0496 - binary_accuracy: 0.9835 - val_loss: 1.0818 - val_binary_accuracy: 0.8197 Epoch 19/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0915 - binary_accuracy: 0.9628 - val_loss: 1.3326 - val_binary_accuracy: 0.8525 Epoch 20/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0953 - binary_accuracy: 0.9669 - val_loss: 1.1602 - val_binary_accuracy: 0.8525 Epoch 21/50 17/17 [==============================] - 0s 7ms/step - loss: 0.0366 - binary_accuracy: 0.9959 - val_loss: 1.1617 - val_binary_accuracy: 0.8525 Epoch 22/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0407 - binary_accuracy: 0.9876 - val_loss: 1.2300 - val_binary_accuracy: 0.8361 . model.evaluate(X_test, y_test) . 2/2 [==============================] - 0s 7ms/step - loss: 0.3797 - binary_accuracy: 0.8852 . [0.3796648383140564, 0.8852459192276001] . predictions =(model.predict(X_test)&gt;0.5).astype(&quot;int32&quot;) from sklearn.metrics import classification_report, confusion_matrix, accuracy_score accuracy_score(y_test, predictions) . 0.8852459016393442 . 아까 결과와 비슷한 수치를 보입니다. . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.00 0.00 0.00 29 1 0.52 1.00 0.69 32 accuracy 0.52 61 macro avg 0.26 0.50 0.34 61 weighted avg 0.28 0.52 0.36 61 . classification_report 함수가 상당히 유용한 걸 알 수있습니다. . 한번에 정밀도, 재현율, f1-score 값 까지 보여줍니다. . &#45712;&#45184;&#51216; . 분류에 기본적인 로지스틱 회귀모형과 단순한 딥러닝 코드를 따라해봤습니다. . 특히 딥러닝 부분에 경우 정말 기본적인 것밖에 몰라 코드 해석에 시간이 많이 걸렸네요. . 여러가지로 코드를 만져가며 느낀점은 이번 데이터에 경우 스케일링이 많이 중요한 것 같습니다. . 스케일링 종류에 따라서 정확도 값이 크게 변하는 것을 관찰했습니다. . 특히 트리기반 부스팅 모델이 아니라 더 그런 것 같습니다. . 너무 복잡한 모델을 급하게 이해하기 보다, 이해할 수 있는 모델을 관찰하며 데이터 분석은 어떤 과정으로 하는가를 살펴봤습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/24/kagglessu2.html",
            "relUrl": "/2021/09/24/kagglessu2.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "머신러닝 가이드) 5-1 단순선형회귀",
            "content": ". &#44221;&#49324;&#54616;&#44053;&#48277; . import numpy as np import matplotlib.pyplot as plt %matplotlib inline np.random.seed(8) x = 2 * np.random.randn(100,1) y = 6 + 4 * x + np.random.randn(100,1) plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x7f6bcf5bb850&gt; . y = 4x + 6 근사 . np.random.randn =&gt; 표준정규분포에서 값 생성. 100,1 은 값 행렬 형식 선언 입니다. . def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y-y_pred)) / N return cost . 편차 제곱 평균을 계산해주는 함수. . np.square =&gt; 제곱 해주는 함수 . def get_weight_updates(w1, w0, x, y, learning_rate = 0.01): N = len(y) # w1, w0 동일한 행렬 크기를 갖는 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) #np.dot 행렬의 곱 y_pred = np.dot(x, w1.T) + w0 diff = y - y_pred w0_factors = np.ones((N, 1)) w1_update = -(2/N) * learning_rate * (np.dot(x.T, diff)) w0_update = -(2/N) * learning_rate * (np.dot(w0_factors.T, diff)) return w1_update, w0_update . 편미분한 w1, w0값을 이용해서 w0, w1값을 지속적으로 업데이트 해줍니다 . np.zeros_like(w1) =&gt; w1값과 같은 형태에 값은 0인 행렬 생성 . np.dot(,) =&gt; 행렬 연산 . def gradient_descent_steps(x,y, iters = 10000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, x, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . 위 두 함수를 통해 w1, w0 값을 지속적으로 업데이트 하여 최적에 값에 도달하게 합니다. . w1, w0 = gradient_descent_steps(x,y, 1000) print(&#39;w1 :&#39;, np.round(w1[0,0],4), &#39;w0 :&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱평균:&#39;, np.round(get_cost(y, y_pred),4)) . w1 : 3.9974 w0 : 5.9649 편차제곱평균: 1.1967 . plt.scatter(x,y) plt.plot(x,y_pred) . [&lt;matplotlib.lines.Line2D at 0x7f6bcf10f110&gt;] . 경사하강법을 이용해 회귀선이 잘 만들어졌습니다. . 다만 데이터에 개수가 100개보다 훨씬 많아지면 전체데이터로 계수를 업데이트 하지 못합니다. . 그 때문에 실전에서는 대부분 (미니배치)확률적 경사 하강법을 이용합니다. . 이 방식은 전체 데이터가 아닌 일부 데이터로 계수를 업데이트 하기 때문에 속도가 상대적으로 빠릅니다. . 이를 구현해보겠습니다. . def stochastic_gradient_descent_steps(x,y,batch_size = 10, iters = 1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index = 0 for ind in range(iters): np.random.seed(ind) stochastic_random_index = np.random.permutation(x.shape[0]) sample_x = x[stochastic_random_index[0:batch_size]] sample_y = y[stochastic_random_index[0:batch_size]] w1_update, w0_update = get_weight_updates(w1, w0, sample_x, sample_y) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . np.random.permutation(x.shape[0]) =&gt; 주어진 데이터를 셔플해서 출력함 . 앞 함수와 바뀐 부분은 x, y를 샘플링해서 넣는다는 점 입니다. . w1, w0 = stochastic_gradient_descent_steps(x,y,iters=1000) print(&#39;w1:&#39;, np.round(w1[0,0],3), &#39;w0:&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱 평균:&#39;, np.round(get_cost(y,y_pred),4)) . w1: 4.006 w0: 5.9135 편차제곱 평균: 1.1996 . 편차제곱 평균 값이 전체 x,y를 투입했을때와 큰 차이가 없습니다. . 그러므로 계산 속도가 훨씬 빠른 미니배치 경사하강법을 많이 사용합니다. . &#45800;&#49692; &#49440;&#54805; &#54924;&#44480;(&#48372;&#49828;&#53556; &#51452;&#53469; &#44032;&#44201;) . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;보스턴 데이터 세트 크기:&#39;, bostonDF.shape) bostonDF.head() . 보스턴 데이터 세트 크기: (506, 14) . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . 사이킷런에 내장되어있는 보스턴 주택 데이터를 불러왔습니다. . bostonDF.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 CRIM 506 non-null float64 1 ZN 506 non-null float64 2 INDUS 506 non-null float64 3 CHAS 506 non-null float64 4 NOX 506 non-null float64 5 RM 506 non-null float64 6 AGE 506 non-null float64 7 DIS 506 non-null float64 8 RAD 506 non-null float64 9 TAX 506 non-null float64 10 PTRATIO 506 non-null float64 11 B 506 non-null float64 12 LSTAT 506 non-null float64 13 PRICE 506 non-null float64 dtypes: float64(14) memory usage: 55.5 KB . 결측값은 없으며 모든 피처가 float 형 입니다. . fig, axs = plt.subplots(figsize=(16,8), ncols = 4, nrows = 2) lm_features = [&#39;RM&#39;,&#39;ZN&#39;, &#39;INDUS&#39;,&#39;NOX&#39;,&#39;AGE&#39;,&#39;PTRATIO&#39;,&#39;LSTAT&#39;,&#39;RAD&#39;] for i, feature in enumerate(lm_features): row = int(i/4) col = i%4 sns.regplot(x=feature, y=&#39;PRICE&#39;, data=bostonDF, ax=axs[row][col]) . sns.regplot(x,y) =&gt; x,y 산점도와 함께 회귀직선을 그려줌. . plt.subplots(ncols = , nrows= ) 여러개의 그림을 그릴 수 있게 해줌. . RM과 LSTAT 변수가 가장 PRICE 변수와 연관성이 있어보입니다. . from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) x_train, x_test, y_train, y_test = train_test_split(x_data, y_target, test_size = 0.3, random_state = 156) lr = LinearRegression() lr.fit(x_train, y_train) y_preds = lr.predict(x_test) mse = mean_squared_error(y_test, y_preds) rmse = np.sqrt(mse) print(&#39;mse :&#39;, np.round(mse,4), &#39;, rmse :&#39;, np.round(rmse, 4)) print(&#39;결정계수:&#39;, np.round(r2_score(y_test, y_preds), 4)) . mse : 17.2969 , rmse : 4.159 결정계수: 0.7572 . 모델을 어느정도 설명해 준 모습입니다. . print(&#39;절편 값:&#39;,lr.intercept_) print(&#39;회귀 계수값:&#39;, np.round(lr.coef_,1)) . 절편 값: 40.995595172164755 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] . coeff = pd.Series(data=np.round(lr.coef_, 1), index = x_data.columns) coeff.sort_values(ascending=False) . RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 B 0.0 TAX -0.0 AGE 0.0 INDUS 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 . 변수 이름과 추정 회귀 계수를 맵핑 시킨 모습입니다. . NOX 변수의 계수 값이 크게 작아보입니다. . from sklearn.model_selection import cross_val_score neg_mse_scores = cross_val_score(lr, x_data, y_target, scoring=&#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-12.46 -26.05 -33.07 -80.76 -33.31] rmse scores [3.53 5.1 5.75 8.99 5.77] 평균 rmse score: 5.83 . 5개의 폴드 세트를 이용한 교차검증 입니다. . scoring = &#39;neg_mean_squared_error&#39; 같은 경우 보통 모델 평가를 위한 값이 커야 좋은 값인데, mse 값은 작아야 좋습니다. . 그러므로 음수를 붙여서 보정해준다고 생각하면 좋습니다. . 다음에는 다항회귀, 릿지/라쏘 회귀 부분을 공부하겠습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/20/PythonMachine5_1.html",
            "relUrl": "/2021/09/20/PythonMachine5_1.html",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "SSUDA) 주택 가격 예측",
            "content": ". https://www.kaggle.com/c/house-prices-advanced-regression-techniques . 캐글에 있는 주택 가격 예측 데이터 분석입니다. . 부스팅 모델들이 튜닝하는데 시간이 걸리기 때문에 좀 더 간단한 선형 회귀 모델을 사용하겠습니다. . 분류 관련 공부를 조금 해본 경험으로, 회귀에 기본인 선형 회귀모델을 이번 데이터를 이용해 공부해보겠습니다. . 이번 분석에 핵심 포인트는 숫자 변수 대부분이 치우쳐 있으므로 숫자 변수를 log_transform하는 것입니다. . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; &#48143; &#46168;&#47084;&#48372;&#44592; . import pandas as pd import numpy as np import seaborn as sns import matplotlib import matplotlib.pyplot as plt from scipy.stats import skew from scipy.stats.stats import pearsonr %config InlineBackend.figure_format = &#39;retina&#39; #set &#39;png&#39; here when working on notebook %matplotlib inline . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . train = pd.read_csv(&quot;/content/drive/MyDrive/house/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/house/test.csv&quot;) . train.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating ... CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | ... | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | ... | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | ... | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | ... | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | ... | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 81 columns): # Column Non-Null Count Dtype -- -- 0 Id 1460 non-null int64 1 MSSubClass 1460 non-null int64 2 MSZoning 1460 non-null object 3 LotFrontage 1201 non-null float64 4 LotArea 1460 non-null int64 5 Street 1460 non-null object 6 Alley 91 non-null object 7 LotShape 1460 non-null object 8 LandContour 1460 non-null object 9 Utilities 1460 non-null object 10 LotConfig 1460 non-null object 11 LandSlope 1460 non-null object 12 Neighborhood 1460 non-null object 13 Condition1 1460 non-null object 14 Condition2 1460 non-null object 15 BldgType 1460 non-null object 16 HouseStyle 1460 non-null object 17 OverallQual 1460 non-null int64 18 OverallCond 1460 non-null int64 19 YearBuilt 1460 non-null int64 20 YearRemodAdd 1460 non-null int64 21 RoofStyle 1460 non-null object 22 RoofMatl 1460 non-null object 23 Exterior1st 1460 non-null object 24 Exterior2nd 1460 non-null object 25 MasVnrType 1452 non-null object 26 MasVnrArea 1452 non-null float64 27 ExterQual 1460 non-null object 28 ExterCond 1460 non-null object 29 Foundation 1460 non-null object 30 BsmtQual 1423 non-null object 31 BsmtCond 1423 non-null object 32 BsmtExposure 1422 non-null object 33 BsmtFinType1 1423 non-null object 34 BsmtFinSF1 1460 non-null int64 35 BsmtFinType2 1422 non-null object 36 BsmtFinSF2 1460 non-null int64 37 BsmtUnfSF 1460 non-null int64 38 TotalBsmtSF 1460 non-null int64 39 Heating 1460 non-null object 40 HeatingQC 1460 non-null object 41 CentralAir 1460 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1460 non-null int64 44 2ndFlrSF 1460 non-null int64 45 LowQualFinSF 1460 non-null int64 46 GrLivArea 1460 non-null int64 47 BsmtFullBath 1460 non-null int64 48 BsmtHalfBath 1460 non-null int64 49 FullBath 1460 non-null int64 50 HalfBath 1460 non-null int64 51 BedroomAbvGr 1460 non-null int64 52 KitchenAbvGr 1460 non-null int64 53 KitchenQual 1460 non-null object 54 TotRmsAbvGrd 1460 non-null int64 55 Functional 1460 non-null object 56 Fireplaces 1460 non-null int64 57 FireplaceQu 770 non-null object 58 GarageType 1379 non-null object 59 GarageYrBlt 1379 non-null float64 60 GarageFinish 1379 non-null object 61 GarageCars 1460 non-null int64 62 GarageArea 1460 non-null int64 63 GarageQual 1379 non-null object 64 GarageCond 1379 non-null object 65 PavedDrive 1460 non-null object 66 WoodDeckSF 1460 non-null int64 67 OpenPorchSF 1460 non-null int64 68 EnclosedPorch 1460 non-null int64 69 3SsnPorch 1460 non-null int64 70 ScreenPorch 1460 non-null int64 71 PoolArea 1460 non-null int64 72 PoolQC 7 non-null object 73 Fence 281 non-null object 74 MiscFeature 54 non-null object 75 MiscVal 1460 non-null int64 76 MoSold 1460 non-null int64 77 YrSold 1460 non-null int64 78 SaleType 1460 non-null object 79 SaleCondition 1460 non-null object 80 SalePrice 1460 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 924.0+ KB . all_data = pd.concat((train.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;], test.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;])) . id(고유번호)와 설명변수를 뺀 나머지 변수들을 전처리를 위해 all_data 변수로 합쳐주었습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 이 코드의 데이터 전처리는 화려하지 않습니다. 기본에 충실합니다. . 다음 3가지로 요약할 수 있습니다. . 로그(기능 + 1)를 사용하여 오른쪽으로 꼬리가 긴 그래프를 변환합니다. 그러면 어느정도 정규화됩니다. | 범주형 형상에 대한 더미 변수 생성 | 숫자 결측값(NaN)을 각 열의 평균으로 바꾸기 | 설명변수를 로그변환 해보기 . matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 6.0) prices = pd.DataFrame({&quot;price&quot;:train[&quot;SalePrice&quot;], &quot;log(price + 1)&quot;:np.log1p(train[&quot;SalePrice&quot;])}) prices.hist() . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1089890&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1052b10&gt;]], dtype=object) . 로그변환 전 우측 꼬리가 두터운 느낌이였는데 잘 정규화 된 모습입니다. . all_data.dtypes . MSSubClass int64 MSZoning object LotFrontage float64 LotArea int64 Street object ... MiscVal int64 MoSold int64 YrSold int64 SaleType object SaleCondition object Length: 79, dtype: object . train[&quot;SalePrice&quot;] = np.log1p(train[&quot;SalePrice&quot;]) numeric_feats = all_data.dtypes[all_data.dtypes != &quot;object&quot;].index . all_data.dtypes =&gt; 데이터 타입 나열. 여기서 인덱스는 변수이름이기 때문에 이런 방식으로 쉽게 추출. . skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) skewed_feats = skewed_feats[skewed_feats &gt; 0.75] skewed_feats = skewed_feats.index all_data[skewed_feats] = np.log1p(all_data[skewed_feats]) . shew = 왜도 값을 나타네는 함수. 왜도란 그래프가 비 대칭적인 모양인 것 . shew값이 큰 양수값이면 오른쪽으로 긴 꼬리를 가지는 분포를 가집니다. . 그러므로 shew값을 기준으로 로그변환을 할 변수를 찾을 수 있습니다. . 참고로 apply 함수는 파이썬 데이터 프레임에 적용하는 함수인데, 원하는 함수를 적용하고 싶을때 사용합니다. . 이때 apply 기본인자는 axis = 0이므로 열을 기준으로 함수를 적용합니다. . all_data = pd.get_dummies(all_data) all_data.head(5) . MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold MSZoning_C (all) MSZoning_FV MSZoning_RH MSZoning_RL ... GarageFinish_Unf GarageQual_Ex GarageQual_Fa GarageQual_Gd GarageQual_Po GarageQual_TA GarageCond_Ex GarageCond_Fa GarageCond_Gd GarageCond_Po GarageCond_TA PavedDrive_N PavedDrive_P PavedDrive_Y PoolQC_Ex PoolQC_Fa PoolQC_Gd Fence_GdPrv Fence_GdWo Fence_MnPrv Fence_MnWw MiscFeature_Gar2 MiscFeature_Othr MiscFeature_Shed MiscFeature_TenC SaleType_COD SaleType_CWD SaleType_Con SaleType_ConLD SaleType_ConLI SaleType_ConLw SaleType_New SaleType_Oth SaleType_WD SaleCondition_Abnorml SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Family SaleCondition_Normal SaleCondition_Partial . 0 4.110874 | 4.189655 | 9.042040 | 7 | 5 | 2003 | 2003 | 5.283204 | 6.561031 | 0.0 | 5.017280 | 6.753438 | 6.753438 | 6.751101 | 0.0 | 7.444833 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 8 | 0 | 2003.0 | 2.0 | 548.0 | 0.000000 | 4.127134 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 1 3.044522 | 4.394449 | 9.169623 | 6 | 8 | 1976 | 1976 | 0.000000 | 6.886532 | 0.0 | 5.652489 | 7.141245 | 7.141245 | 0.000000 | 0.0 | 7.141245 | 0.0 | 0.693147 | 2 | 0 | 3 | 0.693147 | 6 | 1 | 1976.0 | 2.0 | 460.0 | 5.700444 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 5 | 2007 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 4.110874 | 4.234107 | 9.328212 | 7 | 5 | 2001 | 2002 | 5.093750 | 6.188264 | 0.0 | 6.075346 | 6.825460 | 6.825460 | 6.765039 | 0.0 | 7.488294 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 6 | 1 | 2001.0 | 2.0 | 608.0 | 0.000000 | 3.761200 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 9 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 3 4.262680 | 4.110874 | 9.164401 | 7 | 5 | 1915 | 1970 | 0.000000 | 5.379897 | 0.0 | 6.293419 | 6.629363 | 6.869014 | 6.629363 | 0.0 | 7.448916 | 1.0 | 0.000000 | 1 | 0 | 3 | 0.693147 | 7 | 1 | 1998.0 | 3.0 | 642.0 | 0.000000 | 3.583519 | 5.609472 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2006 | 0 | 0 | 0 | 1 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 4 4.110874 | 4.442651 | 9.565284 | 8 | 5 | 2000 | 2000 | 5.860786 | 6.486161 | 0.0 | 6.196444 | 7.044033 | 7.044033 | 6.960348 | 0.0 | 7.695758 | 1.0 | 0.000000 | 2 | 1 | 4 | 0.693147 | 9 | 1 | 2000.0 | 3.0 | 836.0 | 5.262690 | 4.442651 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 12 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 288 columns . get_dummies 함수로 모든 object형 값이 원핫인코딩 됐습니다. . 저번에 프로젝트 할 때 변수를 하나하나 입력했던 것이 생각나는데 더 편한 방식을 알게 되었습니다. . all_data = all_data.fillna(all_data.mean()) . 결측값이 있을때 각 열의 평균값으로 대체하는 일반적인 방식입니다. . 윗 코드와 마찬가지로 저번 프로젝트에서 열마다 함수를 돌려 사용했는데 더 편한 방식을 알게 됐습니다. . X_train = all_data[:train.shape[0]] X_test = all_data[train.shape[0]:] y = train.SalePrice . 저번 프로젝트에서 트레인, 테스트 데이터에 각각 전처리를 적용했습니다. . 하지만 이 방법처럼 all_data로 묶고 한번에 전처리 하는 방식이 깔끔한 것 같습니다. . &#47551;&#51648; &#47784;&#45944; . 선형 회귀 모델 적합을 하겠습니다. . 이때 라쏘, 릿지 방법을 모두 사용해서 최적의 rmse 값을 찾겠습니다. . from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso from sklearn.model_selection import cross_val_score def rmse_cv(model): rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)) return(rmse) . cross_val_score 함수는 교차 검증 후 정확도를 리스트로 보여줍니다. . 여기서 cv = 5 이기 때문에 5-fold로 교차검증 하게 됩니다. . model_ridge = Ridge() . 릿지 모델의 주요 파라미터는 알파입니다. . 알파값이 높아지면 규제가 심해지고 과적합을 방지해줍니다. . 다만 너무 많이 높아지면 과소적합이 되기 때문에 적절한 값을 찾아야합니다. . alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas] . 다양한 알파값을 릿지 함수에 적용시켰습니다. . 여기서 [값 for alpha in alphas] 는 for루프를 리스트 내에서 돌리는 것 입니다. . cv_ridge = pd.Series(cv_ridge, index = alphas) cv_ridge.plot(title = &quot;Validation - Just Do It&quot;) plt.xlabel(&quot;alpha&quot;) plt.ylabel(&quot;rmse&quot;) . Text(0, 0.5, &#39;rmse&#39;) . 시리즈에 plot를 하면 그래프가 생깁니다. . 이때 x축은 인덱스, y축은 본 값이 들어갑니다. . 알파값이 10일때 rmse값이 최소로, 알파는 10을 쓰는 것이 좋겠습니다. . 보통 규제하는 변수와 예측도를 측정하는 값간에 그래프는 U자형태가 잘 나옵니다. . 그 이유는 규제가 약할때와 쌜 때 각각 과소적합, 과적합이 일어나 예측도를 측정하는 값이 커지기 때문입니다. . cv_ridge.min() . 0.1273373466867076 . 최적의 rmse값은 0.1273입니다. . &#46972;&#50136; &#47784;&#45944; . 이번엔 라쏘 모델입니다. . 라쏘 모델은 릿지 모델과 다르게 영향력이 작은 변수의 계수를 0으로 만듭니다. . 변수 선택 과정까지 한번에 할 수 있다는 것이 장점입니다. . model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y) . LassoCV 함수로 여러가지 알파값을 동시에 검정할 수 있습니다. . model_lasso.alpha_ . 0.0005 . rmse_cv(model_lasso).mean() . 0.12256735885048142 . 라쏘 모델이 rmse 값이 훨씬 낮아서 좋습니다. . 라쏘 모델을 사용하겠습니다. . coef = pd.Series(model_lasso.coef_, index = X_train.columns) . 회귀 모델.coef_ =&gt; 계수를 컬럼순으로 보여줍니다. . print(&quot;Lasso picked &quot; + str(sum(coef != 0)) + &quot; variables and eliminated the other &quot; + str(sum(coef == 0)) + &quot; variables&quot;) . Lasso picked 110 variables and eliminated the other 178 variables . 110개 변수는 선택되었고 178개 변수는 계수가 0, 즉 선택하지 않은 변수들입니다. . coef . MSSubClass -0.007480 LotFrontage 0.000000 LotArea 0.071826 OverallQual 0.053160 OverallCond 0.043027 ... SaleCondition_AdjLand 0.000000 SaleCondition_Alloca -0.000000 SaleCondition_Family -0.007925 SaleCondition_Normal 0.019666 SaleCondition_Partial 0.000000 Length: 288, dtype: float64 . imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)]) matplotlib.rcParams[&#39;figure.figsize&#39;] = (8.0, 10.0) imp_coef.plot(kind = &quot;barh&quot;) plt.title(&quot;Coefficients in the Lasso Model&quot;) . Text(0.5, 1.0, &#39;Coefficients in the Lasso Model&#39;) . sort_values() 함수는 범주형 변수의 히스토그램을 아는데 유용한 함수입니다. . 여기선 정렬기능으로 사용했는데, 정렬기능으로도 충분히 우수한 것을 보여줬습니다. . 정렬된 값 상위 10개, 하위 10개를 시각화했는데, 이 변수들이 핵심 변수입니다. . 왜냐하면 계수의 절대값이 큰 값이기 때문입니다. . 양의 값으로 가장 큰 GrLivArea변수는 면적으로 주택가격에 당연히 큰 영향을 끼칩니다. . matplotlib.rcParams[&#39;figure.figsize&#39;] = (6.0, 6.0) preds = pd.DataFrame({&quot;preds&quot;:model_lasso.predict(X_train), &quot;true&quot;:y}) preds[&quot;residuals&quot;] = preds[&quot;true&quot;] - preds[&quot;preds&quot;] preds.plot(x = &quot;preds&quot;, y = &quot;residuals&quot;,kind = &quot;scatter&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3bd2529490&gt; . 잔차 그림도 큰 이상이 없습니다. . model_lasso = Lasso(alpha = 0.0005).fit(X_train, y) pred = model_lasso.predict(X_test) pred2 = np.exp(pred) - 1 X_test[&#39;SalePrice&#39;] = pred2 X_test[&#39;Id&#39;] = test[&#39;Id&#39;] final = X_test[[&#39;Id&#39;,&#39;SalePrice&#39;]] final.to_csv(&#39;/content/drive/MyDrive/houselasso2.csv&#39;,encoding=&#39;UTF-8&#39;, index=False) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . 알파값 0.0005인 라쏘 모델로 모델을 적합시키고 그 모델로 예측 파일을 만들었습니다. .",
            "url": "https://ksy1526.github.io/myblog//myblog/2021/09/15/kagglessu1.html",
            "relUrl": "/2021/09/15/kagglessu1.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ksy1526.github.io/myblog//myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ksy1526.github.io/myblog//myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ksy1526.github.io/myblog//myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ksy1526.github.io/myblog//myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}