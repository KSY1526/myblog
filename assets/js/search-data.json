{
  
    
        "post0": {
            "title": "[처음 시작하는 딥러닝] 3. 밑바닥부터 만들어보는 CNN",
            "content": ". &#44592;&#48376; &#49464;&#54021; . import numpy as np from numpy import ndarray . def assert_same_shape(output, output_grad): assert output.shape == output_grad.shape, &#39;&#39;&#39; 두 ndarray의 모양이 같아야 하는데, 첫 번째 ndarray의 모양은 {0}이고 두 번째 ndarray의 모양은 {1}이다. &#39;&#39;&#39;.format(tuple(output_grad.shape), tuple(output.shape)) return None def assert_dim(t, dim): assert len(t.shape) == dim, &#39;&#39;&#39; 이 텐서는 {0}차원이어야 하는데, {1}차원이다. &#39;&#39;&#39;.format(dim, len(t.shape)) return None . 필요한 차원을 잘 입력했는지 확인하는 함수를 선언합니다. . 1&#52264;&#50896; &#54633;&#49457; &#44273; . input_1d = np.array([1,2,3,4,5]) param_1d = np.array([1,1,1]) . def _pad_1d(inp, num): # 원래 데이터와 패딩 길이 입력 z = np.array([0]) z = np.repeat(z, num) return np.concatenate([z, inp, z]) _pad_1d(input_1d, 1) . array([0, 1, 2, 3, 4, 5, 0]) . 간단한 함수로 1차원 데이터를 패딩한 모습입니다. . 입력 데이터와 합성곱 연산을 한 출력 데이터의 크기를 같게하기 위해선 벗어나는 범위에 대해서 0 값을 채워주는 패딩을 하게 됩니다. . 패딩 크기는 필터 크기를 2로 나눈 값에 정수부분이 입력과 출력을 같게하는 패딩의 크기가 됩니다. . def conv_1d(inp, param): # 입력 값과 필터 값 입력 # 1차원 입력인지 확인합니다 assert_dim(inp, 1) assert_dim(param, 1) # 입력 값에 패딩을 덧붙입니다. param_len = param.shape[0] param_mid = param_len // 2 input_pad = _pad_1d(inp, param_mid) # 초기값 부여 out = np.zeros(inp.shape) # 1차원 합성곱 연산 수행 for o in range(out.shape[0]): for p in range(param_len): out[o] += param[p] * input_pad[o+p] # 출력 모양이 입력과 동일한지 확인 assert_same_shape(inp, out) return out conv_1d(input_1d, param_1d) . array([ 3., 6., 9., 12., 9.]) . 가중치가 [1,1,1] 인 간단한 합성곱 연산을 진행했습니다. . def conv_1d_sum(inp, param): out = conv_1d(inp, param) return np.sum(out) input_1d = np.array([1,2,3,4,5]) input_1d_2 = np.array([1,2,3,4,6]) input_1d_3 = np.array([1,2,3,5,5]) param_1d = np.array([1,1,1]) param_1d_2 = np.array([2,1,1]) print(conv_1d_sum(input_1d, param_1d)) print(conv_1d_sum(input_1d_2, param_1d)) print(conv_1d_sum(input_1d_3, param_1d)) print(conv_1d_sum(input_1d, param_1d_2)) . 39.0 41.0 42.0 49.0 . 입력값과 필터값이 달라짐에 따라 출력값의 합이 어떻게 바뀌는지 비교했습니다. . 끝 쪽 입력값이 1 증가할때는 출력값의 합이 2증가, 가운데 쪽 입력값(패딩 영향 안받는)이 1 증가할때는 출력값의 합이 3증가합니다. . 또 필터값이 1 증가할때 출력값의 합이 10 증가합니다. . def _param_grad_1d(inp, param, output_grad = None): # 입력값 패딩 추가 param_len = param.shape[0] param_mid = param_len // 2 input_pad = _pad_1d(inp, param_mid) if output_grad is None: # 출력값의 기울기를 입력하지 않으면 1로 초기화. # 왜냐하면 출력값의 합의 기울기이기 때문에 기울기를 유지하는 1을 쓰면됨. output_grad = np.ones_like(inp) else: assert_same_shape(inp, output_grad) # 모든 기울기의 초기값을 0으로 줍니다. param_grad = np.zeros_like(param) input_grad = np.zeros_like(inp) for o in range(inp.shape[0]): # 0~4 for p in range(param.shape[0]): # 0~2 # 필터값의 기울기는 실제 영향을 받는 입력값의 합으로 됨 param_grad[p] += input_pad[o+p] * output_grad[o] assert_same_shape(param_grad, param) return param_grad _param_grad_1d(input_1d, param_1d) . array([10, 15, 14]) . 1차원 합성 곱의 역방향 함수중 먼저 필터(파라미터) 기울기를 구하는 함수 입니다. . 조금 어려운데 결과값을 간단히 해석하면 파라미터가 1 증가했을때 출력값의 합이 각각 10, 15, 14 증가한다는 것 입니다. . def _input_grad_1d(inp, param, output_grad = None): # 입력값 패딩 추가 param_len = param.shape[0] param_mid = param_len // 2 input_pad = _pad_1d(inp, param_mid) if output_grad is None: # 출력값의 기울기를 입력하지 않으면 1로 초기화. # 왜냐하면 출력값의 합의 기울기이기 때문에 기울기를 유지하는 1을 쓰면됨. output_grad = np.ones_like(inp) else: assert_same_shape(inp, output_grad) # 원할한 연산을 위해 범위 내 값은 1을, 범위를 벗어나는 것들은 0으로함. # 패딩도 같은 효과를 냄. output_pad = _pad_1d(output_grad, param_mid) # 모든 기울기의 초기값을 0으로 줍니다. param_grad = np.zeros_like(param) input_grad = np.zeros_like(inp) for o in range(inp.shape[0]): # 0~4 for f in range(param.shape[0]): # 0~2 # 입력값의 기울기는 실제 영향을 받는 필터값의 합으로 됨 input_grad[o] += output_pad[o + param_len - f - 1] * param[f] assert_same_shape(param_grad, param) return input_grad _input_grad_1d(input_1d, param_1d) . array([2, 3, 3, 3, 2]) . 입력값에 따른 출력값의 변동이 얼마나 되는지를 나타내는 함수 입니다. . 첫번째와 마지막은 입력값이 1 증가할때 크기가 1 작은 2만큼 증가하고 나머지 값들은 3만큼 증가합니다. . 패딩한 것에 영향받는 값을 제외하고 필터의 개수(3)만큼 영향력이 있다고 생각하면 됩니다. . &#48176;&#52824; &#51077;&#47141; &#51201;&#50857;&#54616;&#44592; . input_1d_batch = np.array([[0,1,2,3,4,5,6], [1,2,3,4,5,6,7]]) def _pad_1d_batch(inp, num): outs = [_pad_1d(obs, num) for obs in inp] return np.stack(outs) _pad_1d_batch(input_1d_batch, 1) . array([[0, 0, 1, 2, 3, 4, 5, 6, 0], [0, 1, 2, 3, 4, 5, 6, 7, 0]]) . 입력값이 2개 이상인 배치에도 적용하기 위해 기존 구현한 함수를 확장하겠습니다. . 패딩의 경우 기존함수를 반복문을 이용해서 여러번 호출하면 됩니다. . def conv_1d_batch(inp, param): outs = [conv_1d(obs, param) for obs in inp] return np.stack(outs) conv_1d_batch(input_1d_batch, param_1d) . array([[ 1., 3., 6., 9., 12., 15., 11.], [ 3., 6., 9., 12., 15., 18., 13.]]) . 순방향 계산에 경우에도 같은 방식으로 확장했습니다. . def input_grad_1d_batch(inp, param): out = conv_1d_batch(inp, param) out_grad = np.ones_like(out) # 출력기울기 값의 형태가 배치이므로 이에 맞게 조정 batch_size = out_grad.shape[0] # 배치 크기가 나옴 grads = [_input_grad_1d(inp[i], param, out_grad[i]) for i in range(batch_size)] return np.stack(grads) input_grad_1d_batch(input_1d_batch, param_1d) . array([[2, 3, 3, 3, 3, 3, 2], [2, 3, 3, 3, 3, 3, 2]]) . 입력값에 따른 출력값이 얼마나 되는지 구하는 함수를 배치로 확장했습니다. . 기울기는 입력값에 영향이 있지 않기 때문에 어느 입력값이 입력되던 그대로 출력됩니다. . def param_grad_1d_batch(inp, param): output_grad = np.ones_like(inp) # 단순 합의 기울기이기 때문에 모든 값을 1로 둡니다. inp_pad = _pad_1d_batch(inp, 1) out_pad = _pad_1d_batch(inp, 1) param_grad = np.zeros_like(param) for i in range(inp.shape[0]): # 배치 크기만큼 for o in range(inp.shape[1]): # 인풋 길이만큼 for p in range(param.shape[0]): # 필터 길이만큼 # 전부 합해줍니다. param_grad[p] += inp_pad[i][o+p] * output_grad[i][o] return param_grad param_grad_1d_batch(input_1d_batch, param_1d) . array([36, 49, 48]) . 필터값에 따른 출력값이 얼마나 변하는지를 구하는 함수를 배치로 확장했습니다. . 이때 필터에 대한 기울기는 배치 단위인데 필터는 모든 관찰과 합성곱 연선이 이뤄지므로 모든 값을 다 더해야합니다. . 즉 모든 요소의 합이 필터 값이 바뀜에 따라서 얼마나 바뀌는지를 구하는 것 입니다. . 2&#52264;&#50896; &#54633;&#49457;&#44273; . imgs_2d_batch = np.random.randn(3, 28, 28) param_2d = np.random.randn(3,3) def _pad_2d_obs(inp, num): # 가로 단위로 앞 뒷 값 각각 패딩 inp_pad = _pad_1d_batch(inp, num) # 가로로 윗 2줄, 아래 2줄 패딩 other = np.zeros((num, inp.shape[0] + num * 2)) return np.concatenate([other, inp_pad, other]) def _pad_2d(inp, num): # 첫번째 차원은 배치 크기에 해당함. outs = [_pad_2d_obs(obs, num) for obs in inp] return np.stack(outs) _pad_2d(imgs_2d_batch, 1).shape . (3, 30, 30) . 2차원 단위에 입력값을 가지고 패딩을 진행했습니다. . _pad_2d_obs 함수는 패딩을 실질적으로 진행하는 함수이고, _pad_2d 함수는 배치 단위로 확장하는 함수 입니다. . def _compute_output_obs_2d(obs, param): param_mid = param.shape[0] // 2 obs_pad = _pad_2d_obs(obs, param_mid) out = np.zeros_like(obs) # 2차원 필터를 거처 출력값을 만듭니다. for o_w in range(out.shape[0]): # 출력값 가로길이 for o_h in range(out.shape[1]): # 출력값 세로길이 for p_w in range(param.shape[0]): # 필터 가로길이 for p_h in range(param.shape[1]): # 필터 세로길이 out[o_w][o_h] += param[p_w][p_h] * obs_pad[o_w+p_w][o_h+p_h] return out def _compute_output_2d(img_batch, param): assert_dim(img_batch, 3) outs = [_compute_output_obs_2d(obs, param) for obs in img_batch] return np.stack(outs) _compute_output_2d(imgs_2d_batch, param_2d).shape . (3, 28, 28) . 2차원 단위에 순방향 계산입니다. 패딩을 먼저 시킨 뒤 2차원 필터를 통과시켜 모든 값을 합친 값을 출력해줍니다. . def _compute_grads_obs_2d(input_obs, output_grad_obs, param): # 입력을 나타내는 2차원값, 출력 기울기를 나타내는 2차원값(여기선 모두 1을 사용), 2차원 필터 param_size = param.shape[0] # 2차원 필터의 가로 세로가 같다고 가정합니다. # 출력 기울기에 패딩을 먼저 덧붙입니다. 원본값은 1로, 나머지 값은 0으로하여 원본값만 유지하게 합니다. output_obs_pad = _pad_2d_obs(output_grad_obs, param_size // 2) input_grad = np.zeros_like(input_obs) # 초기 기울기는 0으로 합니다. for i_w in range(input_obs.shape[0]): # 입력값 가로길이 for i_h in range(input_obs.shape[1]): # 입력값 세로길이 for p_w in range(param_size): # 필터 가로길이 for p_h in range(param_size): # 필터 세로길이 input_grad[i_w][i_h] += output_obs_pad[i_w + param_size - p_w -1][i_h + param_size - p_h -1] * param[p_w][p_h] return input_grad def _compute_grads_2d(inp, output_grad, param): grads = [_compute_grads_obs_2d(inp[i], output_grad[i], param) for i in range(output_grad.shape[0])] return np.stack(grads) img_grads = _compute_grads_2d(imgs_2d_batch, np.ones_like(imgs_2d_batch), param_2d) img_grads.shape . (3, 28, 28) . 역방향 계산을 2차원으로 구현했습니다. 그 중 입력 기울기를 계산하는 절차인데요. . 출력 기울기에 패딩을 덧붙이고 해당하는 가중치와의 합 연산을 하면 입력 기울기를 계산할 수 있습니다. . def _param_grad_2d(inp, output_grad, param): # 입력을 나타내는 3차원값, 출력 기울기를 나타내는 3차원값(여기선 모두 1을 사용), 2차원 필터 param_size = param.shape[0] # 2차원 필터의 가로 세로가 같다고 가정합니다. inp_pad = _pad_2d(inp, param_size // 2) # 입력 값을 패딩합니다. param_grad = np.zeros_like(param) # 초기 가중치 기울기를 0으로 합니다. img_shape = output_grad.shape[1:] # 첫 값은 배치크기이므로 빼고 실행하기 위해. for i in range(inp.shape[0]): # 배치 크기 for o_w in range(img_shape[0]): # 입력값 가로길이 for o_h in range(img_shape[1]): # 입력값 세로길이 for p_w in range(param_size): # 필터 가로길이 for p_h in range(param_size): # 필터 세로길이 param_grad[p_w][p_h] += inp_pad[i][o_w+p_w][o_h+p_h] * output_grad[i][o_w][o_h] return param_grad param_grad = _param_grad_2d(imgs_2d_batch, np.ones_like(imgs_2d_batch), param_2d) param_grad . array([[108.03493534, 116.92134058, 112.40044695], [112.79682302, 122.16317892, 122.1838074 ], [106.24163816, 116.07072373, 116.86259153]]) . 역방향 계산 중 필터 기울기를 구하는 부분을 2차원으로 구현했습니다. . 여기서는 입력값에 패딩을 덧붙이고 합 연산을 했는데, 1차원하고 크게 다를게 없습니다. . 배치 연산까지 한번에 진행하는 함수를 구현했는데 모든 배치의 입력값을 순회합니다. . &#52292;&#45328; &#52628;&#44032;&#54616;&#44592; . def _compute_output_obs(obs, param): assert_dim(obs, 3) assert_dim(param, 4) param_size = param.shape[2] param_mid = param_size // 2 obs_pad = _pad_2d_channel(obs, param_mid) in_channels = param.shape[0] out_channels = param.shape[1] img_size = obs.shape[1] out = np.zeros((out_channels,) + obs.shape[1:]) for c_in in range(in_channels): for c_out in range(out_channels): for o_w in range(img_size): for o_h in range(img_size): for p_w in range(param_size): for p_h in range(param_size): out[c_out][o_w][o_h] += param[c_in][c_out][p_w][p_h] * obs_pad[c_in][o_w+p_w][o_h+p_h] return out def _output(inp, param): outs = [_compute_output_obs(obs, param) for obs in inp] return np.stack(outs) . 합성곱층은 2차원으로 서로 엮인 뉴런 외에도 특징 맵과 같은 수의 채널을 갖습니다. . 이런식으로 입력된 데이터를 다루기 위해 채널이 있는 순방향 연산을 구현했습니다. 역방향 연산은 생략합니다. . &#44592;&#53440; &#44032;&#48316;&#50868; &#51060;&#47200; . 이미지 데이터에서는 서로 가까운 픽셀 간에 유의미한 의미가 있는 조합이 나올 가능성이 높습니다. . 즉 픽셀간 얼마나 공간적으로 가까운지를 나타내기 위해 합성곱 연산을 수행합니다. . 풀링이란 각 특징 맵을 다운샘플링하여 데이터의 크기를 줄이는 방법입니다. 예시로 각 영역 픽셀 값의 최대값을 사용할 수 있습니다. . 계산양 감소라는 이점이 있으나, 정보 손실또한 크기 때문에 이 방법을 쓰는데 다양한 의견이 있습니다. . 스트라이드란 필터가 움직이는 간격이 지금까지는 1이였는데 이 값을 의미하는 용어로 커질수록 다운샘플링 효과가 커집니다. . 최근 제안되는 고급 합성곱 신경망 구조에서는 풀링 대신 스트라이드를 2이상으로 설정해 다운샘플링 효과를 얻습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/deep%20learning/matrix/math/class/2022/01/23/FirstDeep3.html",
            "relUrl": "/book/jupyter/deep%20learning/matrix/math/class/2022/01/23/FirstDeep3.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[SSUDA] 텐서플로로 데이터 적재하기1",
            "content": ". &#45936;&#51060;&#53552; API . import sys import sklearn import tensorflow as tf from tensorflow import keras import numpy as np import pandas as pd import os import matplotlib.pyplot as plt . X = tf.range(10) dataset = tf.data.Dataset.from_tensor_slices(X) # 주어진 데이터 소스를 여러 텐서로 자릅니다. dataset # tf.data.Dataset.range(10)과 동등함. . &lt;TensorSliceDataset shapes: (), types: tf.int32&gt; . for item in dataset: print(item) . tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32) tf.Tensor(4, shape=(), dtype=int32) tf.Tensor(5, shape=(), dtype=int32) tf.Tensor(6, shape=(), dtype=int32) tf.Tensor(7, shape=(), dtype=int32) tf.Tensor(8, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32) . from_tensor_slices 함수는 텐서를 받아 10개의 아이템(0~9)으로 쪼갭니다. . 이 때 shapes가 () 인것은 크기가 1인 텐서 10개로 쪼개졌다는 것을 의미합니다. . dataset = dataset.repeat(3).batch(7) for item in dataset: print(item) . tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32) tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32) tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32) tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32) tf.Tensor([8 9], shape=(2,), dtype=int32) . 원래 dataset에 repeat 함수를 적용하면 원본 데이터 아이템을 세차레 반복하는 새로운 데이터셋을 반환합니다. . 그 뒤 batch 함수를 적용하면 아이템을 7개씩 그룹으로 묶습니다. 이 때 마지막 데이터셋은 2개가 됩니다. . dataset = dataset.map(lambda x: x*2) for item in dataset: print(item) . tf.Tensor([ 0 2 4 6 8 10 12], shape=(7,), dtype=int32) tf.Tensor([14 16 18 0 2 4 6], shape=(7,), dtype=int32) tf.Tensor([ 8 10 12 14 16 18 0], shape=(7,), dtype=int32) tf.Tensor([ 2 4 6 8 10 12 14], shape=(7,), dtype=int32) tf.Tensor([16 18], shape=(2,), dtype=int32) . map 함수로 데이터 셋 내 아이템을 변환할 수 있습니다. . tf.random.set_seed(42) dataset = tf.data.Dataset.range(10).repeat(3) dataset = dataset.shuffle(buffer_size=5, seed = 42).batch(7) for item in dataset: print(item) . tf.Tensor([0 1 6 5 7 3 9], shape=(7,), dtype=int64) tf.Tensor([8 2 1 0 4 6 4], shape=(7,), dtype=int64) tf.Tensor([7 2 5 9 2 1 3], shape=(7,), dtype=int64) tf.Tensor([4 3 8 7 9 5 0], shape=(7,), dtype=int64) tf.Tensor([8 6], shape=(2,), dtype=int64) . buffer_size 만큼 버퍼를 만든 뒤 데이터를 앞 부분부터 순서대로 버퍼 최대 크기만큼 뽑아 버퍼를 채웁니다. 그 후 버퍼 중 한 개 데이터를 뽑습니다. . 뽑은 데이터는 첫번째 결과값으로 지정하고 다시 데이터 셋에서 다음 순서 데이터를 뽑아 버퍼를 채웁니다. . 그 후 버퍼 중 렌덤하게 한 개 데이터를 뽑아 두번째 결과값으로 지정합니다. 이 과정을 반복하면 데이터가 셔플하게 됩니다. . 하지만 여전히 앞 부분 데이터는 앞 부분에 있을 확률이 커 셔플이 됬다고 하기 어렵습니다. 버퍼 크기가 현저히 작을 경우 그런 경향이 더 심합니다. . 이를 해결하기 위해 원본 데이터를 에포크마다 한번씩 섞어줘야 합니다. . &#50668;&#47084; CSV &#54028;&#51068;&#50640;&#49436; &#54620; &#51460;&#50473; &#48264;&#44040;&#50500; &#51069;&#44592; . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split( housing.data, housing.target.reshape(-1, 1), random_state=42) X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, random_state=42) scaler = StandardScaler() scaler.fit(X_train) X_mean = scaler.mean_ X_std = scaler.scale_ . 주택 데이터셋을 다운받습니다. 훈련/검증/테스트 로 나누고 스케일을 조정합니다. . def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10): housing_dir = os.path.join(&quot;datasets&quot;, &quot;housing&quot;) os.makedirs(housing_dir, exist_ok=True) path_format = os.path.join(housing_dir, &quot;my_{}_{:02d}.csv&quot;) filepaths = [] m = len(data) for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)): part_csv = path_format.format(name_prefix, file_idx) filepaths.append(part_csv) with open(part_csv, &quot;wt&quot;, encoding=&quot;utf-8&quot;) as f: if header is not None: f.write(header) f.write(&quot; n&quot;) for row_idx in row_indices: f.write(&quot;,&quot;.join([repr(col) for col in data[row_idx]])) f.write(&quot; n&quot;) return filepaths train_data = np.c_[X_train, y_train] valid_data = np.c_[X_valid, y_valid] test_data = np.c_[X_test, y_test] header_cols = housing.feature_names + [&quot;MedianHouseValue&quot;] header = &quot;,&quot;.join(header_cols) train_filepaths = save_to_multiple_csv_files(train_data, &quot;train&quot;, header, n_parts=20) valid_filepaths = save_to_multiple_csv_files(valid_data, &quot;valid&quot;, header, n_parts=10) test_filepaths = save_to_multiple_csv_files(test_data, &quot;test&quot;, header, n_parts=10) . 데이터를 여러개의 CSV파일로 쪼갰습니다. (이 부분은 핸즈온머신러닝 깃허브 코드를 복사했습니다.) . train_filepaths . [&#39;datasets/housing/my_train_00.csv&#39;, &#39;datasets/housing/my_train_01.csv&#39;, &#39;datasets/housing/my_train_02.csv&#39;, &#39;datasets/housing/my_train_03.csv&#39;, &#39;datasets/housing/my_train_04.csv&#39;, &#39;datasets/housing/my_train_05.csv&#39;, &#39;datasets/housing/my_train_06.csv&#39;, &#39;datasets/housing/my_train_07.csv&#39;, &#39;datasets/housing/my_train_08.csv&#39;, &#39;datasets/housing/my_train_09.csv&#39;, &#39;datasets/housing/my_train_10.csv&#39;, &#39;datasets/housing/my_train_11.csv&#39;, &#39;datasets/housing/my_train_12.csv&#39;, &#39;datasets/housing/my_train_13.csv&#39;, &#39;datasets/housing/my_train_14.csv&#39;, &#39;datasets/housing/my_train_15.csv&#39;, &#39;datasets/housing/my_train_16.csv&#39;, &#39;datasets/housing/my_train_17.csv&#39;, &#39;datasets/housing/my_train_18.csv&#39;, &#39;datasets/housing/my_train_19.csv&#39;] . 분할된 CSV파일 저장 경로 리스트 입니다. . filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed = 42) for filepath in filepath_dataset: print(filepath) . tf.Tensor(b&#39;datasets/housing/my_train_15.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_08.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_03.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_01.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_10.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_05.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_19.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_16.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_02.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_09.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_00.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_07.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_12.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_04.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_17.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_11.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_14.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_18.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_06.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_13.csv&#39;, shape=(), dtype=string) . list_files 함수는 파일 경로를 섞은 데이터셋을 반환합니다. . n_readers = 5 dataset = filepath_dataset.interleave( lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # 파일의 첫번째 줄은 열이름이므로 skip함. cycle_length = n_readers ) for line in dataset.take(5): print(line.numpy()) . b&#39;4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504&#39; b&#39;8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159&#39; b&#39;3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598&#39; b&#39;3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526&#39; b&#39;3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625&#39; . interleave 함수는 데이터셋(파일경로) 내 다섯 개에서 데이터를 읽는 데이터셋을 만듭니다. . 이 때 함수 내 lambda 함수를 이용해 새로운 데이터셋을 만들 것입니다. . 인터러브 데이터셋을 반복 구문에 적용하면 다섯 개의 TextLineDataset을 순회하고, 모든 데이터셋의 아이템이 소진될때까지 한 줄씩 읽습니다. . 출력된 값은 바이트 스트링입니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; &#54616;&#44592; . [0.] * 8 + [tf.constant([], dtype = tf.float32)] . [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, &lt;tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)&gt;] . n_inputs = X_train.shape[-1] def preprocess(line): defs = [0.] * n_inputs + [tf.constant([], dtype = tf.float32)] # 디폴트값 텐서 형태로 만들기 fields = tf.io.decode_csv(line, record_defaults=defs) # 스칼라 텐서의 리스트를 반환 x = tf.stack(fields[:-1]) # 마지막 전까지 y = tf.stack(fields[-1:]) return (x-X_mean) / X_std, y preprocess(b&#39;4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504&#39;) . (&lt;tf.Tensor: shape=(8,), dtype=float32, numpy= array([ 0.39593136, 0.74167496, -0.16415128, -0.40340805, -0.6199179 , -0.18355484, -1.4084505 , 1.2565969 ], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.504], dtype=float32)&gt;) . 전처리 하는 함수를 만들었습니다. . &#45936;&#51060;&#53552; &#51201;&#51116;, &#51204;&#52376;&#47532; &#54632;&#49688; &#47564;&#46308;&#44592; . def csv_reader_dataset(filepaths, repeat = 1, n_readers = 5, n_read_threads = None, shuffle_buffer_size = 10000, n_parse_threads = 5, batch_size = 32): dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat) # 파일 경로를 섞은 데이터셋 반환 dataset = dataset.interleave( # 한번에 n_readers 파일 수만큼 한 줄씩 번갈아 읽음. lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # 첫줄은 열이름이므로 제외. cycle_length = n_readers, num_parallel_calls = n_read_threads # 여러파일에서 병렬로 읽고싶을때 사용. ) dataset = dataset.shuffle(shuffle_buffer_size) # 데이터를 셔플합니다. 버퍼는 shuffle_buffer_size만큼. dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads) # preprocess 함수를 데이터 내 적용. return dataset.batch(batch_size).prefetch(1) # batch_size만큼 배치 함수를 이용해 데이터를 묶습니다. # 프리페치는 속도를 향상시킵니다. . 데이터를 적재하고 전처리 하는 부분까지 한번에 하는 함수를 구현했습니다. . tf.random.set_seed(42) train_set = csv_reader_dataset(train_filepaths, batch_size = 3) for X_batch, y_batch in train_set.take(2): print(&#39;X = &#39;, X_batch) print(&#39;y =&#39;, y_batch) . X = tf.Tensor( [[ 0.5804519 -0.20762321 0.05616303 -0.15191229 0.01343246 0.00604472 1.2525111 -1.3671792 ] [ 5.818099 1.8491895 1.1784915 0.28173092 -1.2496178 -0.3571987 0.7231292 -1.0023477 ] [-0.9253566 0.5834586 -0.7807257 -0.28213993 -0.36530012 0.27389365 -0.76194876 0.72684526]], shape=(3, 8), dtype=float32) y = tf.Tensor( [[1.752] [1.313] [1.535]], shape=(3, 1), dtype=float32) X = tf.Tensor( [[-0.8324941 0.6625668 -0.20741376 -0.18699841 -0.14536144 0.09635526 0.9807942 -0.67250353] [-0.62183803 0.5834586 -0.19862501 -0.3500319 -1.1437552 -0.3363751 1.107282 -0.8674123 ] [ 0.8683102 0.02970133 0.3427381 -0.29872298 0.7124906 0.28026953 -0.72915536 0.86178064]], shape=(3, 8), dtype=float32) y = tf.Tensor( [[0.919] [1.028] [2.182]], shape=(3, 1), dtype=float32) . 직접 구현한 함수를 통해 여러개로 나누어진 csv의 경로를 랜덤하게 배치 사이즈만큼 적재하고 전처리한 모습입니다. . train_set = csv_reader_dataset(train_filepaths, repeat = None) valid_set = csv_reader_dataset(valid_filepaths) test_set = csv_reader_dataset(test_filepaths) model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=X_train.shape[1:]), keras.layers.Dense(1), ]) model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(learning_rate=1e-3)) . csv 경로를 이용해 데이터셋을 적재하고 전처리 한 뒤 케라스로 간단한 딥러닝 모델을 구축합니다. . batch_size = 32 model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set) # steps_per_epoch 은 한 에포크당 몇번 진행하는지. 입력값이 train_set이기 때문에 확실히 모름. . Epoch 1/10 362/362 [==============================] - 1s 3ms/step - loss: 2.3908 - val_loss: 1.7373 Epoch 2/10 362/362 [==============================] - 1s 3ms/step - loss: 0.8812 - val_loss: 0.7719 Epoch 3/10 362/362 [==============================] - 1s 2ms/step - loss: 0.7535 - val_loss: 0.7118 Epoch 4/10 362/362 [==============================] - 1s 2ms/step - loss: 0.7236 - val_loss: 1.0734 Epoch 5/10 362/362 [==============================] - 1s 2ms/step - loss: 0.6724 - val_loss: 0.6422 Epoch 6/10 362/362 [==============================] - 1s 3ms/step - loss: 0.6682 - val_loss: 0.6402 Epoch 7/10 362/362 [==============================] - 1s 3ms/step - loss: 0.6197 - val_loss: 0.8125 Epoch 8/10 362/362 [==============================] - 1s 3ms/step - loss: 0.6181 - val_loss: 0.5822 Epoch 9/10 362/362 [==============================] - 1s 3ms/step - loss: 0.5769 - val_loss: 1.0522 Epoch 10/10 362/362 [==============================] - 1s 3ms/step - loss: 0.5596 - val_loss: 0.6446 . &lt;keras.callbacks.History at 0x7fa4918802d0&gt; . 잘 진행되는 모습입니다. . &#45712;&#45184;&#51216; . 데이터 API를 이용해 텐서플로에서 데이터 적재하는 방법을 간단히 알아봤습니다. . 실제로 대용량 데이터를 저장하고 효율적으로 읽기 위해선 TFRecord를 활용한다고 하는데 이후에 추가로 공부해보겠습니다. . 딥러닝에 대해 지금까지 가볍게 공부하고 있는데 솔직히 막연하다는 생각이 듭니다. 실전 경험이 없어서일까요. . 이와 별개로 개인적으로 스스로에게 휴식을 주고자 합니다. 몸상태가 좋지 못하고, 학기종강 이후 쉼을 못준 것 같네요. . 아마 당분간은 주 1회 SSUDA 스터디 코드만 올라갈 것 같습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/14/handssu4.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/14/handssu4.html",
            "date": " • Jan 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[SSUDA] 텐서플로 사용하기",
            "content": ". &#45336;&#54028;&#51060;&#52376;&#47100; &#53584;&#49436;&#54540;&#47196; &#49324;&#50857;&#54616;&#44592; . import numpy as np import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.4&quot; # 버전이 2.4보다 커야합니다. np.random.seed(42) tf.random.set_seed(42) . 넘파이와 텐서플로 패키지를 설치합니다. . tf.constant([[1., 2., 3.], [4., 5., 6.]]) . &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy= array([[1., 2., 3.], [4., 5., 6.]], dtype=float32)&gt; . constant 함수로 텐서를 생성했습니다. 넘파이 행렬과 유사합니다. . 유의할 점은 텐서는 변경이 불가능한 객체입니다. . tf.constant(42) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt; . 스칼라 값도 입력이 가능합니다. . t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) t.shape . TensorShape([2, 3]) . t.dtype . tf.float32 . 넘파이 행렬과 비슷하게 크기(shape)와 데이터 타입(dtype)을 가집니다. . t[:, 1:] . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[2., 3.], [5., 6.]], dtype=float32)&gt; . t[:, 1, tf.newaxis] . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[2.], [5.]], dtype=float32)&gt; . tf.square(t) . &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy= array([[ 1., 4., 9.], [16., 25., 36.]], dtype=float32)&gt; . t @ tf.transpose(t) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[14., 32.], [32., 77.]], dtype=float32)&gt; . 텐서 데이터 타입은 넘파이와 마찬가지로 여러가지 연산이 가능합니다. . a = np.array([2., 4., 5.]) tf.constant(a) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])&gt; . t.numpy() . array([[1., 2., 3.], [4., 5., 6.]], dtype=float32) . 텐서롤 넘파이로, 넘파이를 텐서로 쉽게 변형이 가능합니다. . 다만 넘파이는 64비트 기반 텐서는 32비트 기반이기 때문에 넘파이 배열로 텐서를 만들때는 dtype = tf.float32로 해야합니다. . v = tf.Variable([[1.,2.,3.], [4., 5., 6.]]) v.assign(2 * v) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[ 2., 4., 6.], [ 8., 10., 12.]], dtype=float32)&gt; . v[0, 1].assign(42) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[ 2., 42., 6.], [ 8., 10., 12.]], dtype=float32)&gt; . tf.Tensor은 앞서 말한데로 변경이 불가능한 객체입니다. 이 객체만으로는 지속적으로 업데이트되는 신경망 가중치등을 사용할 수 없습니다. . tf.Variable은 이 경우에 필요합니다. assign 함수를 이용해 변수 값을 바꿀 수 있습니다. . &#51452;&#53469; &#45936;&#51060;&#53552;&#47196; &#49324;&#50857;&#51088; &#51221;&#51032; &#49552;&#49892; &#54632;&#49688; &#51201;&#50857;&#54644;&#48372;&#44592; . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split( housing.data, housing.target.reshape(-1, 1), random_state=42) X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, random_state=42) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_valid_scaled = scaler.transform(X_valid) X_test_scaled = scaler.transform(X_test) . 주택 가격 데이터를 입력해 트레인, 벨리드, 테스트 데이터로 분할하고 스케일링 해줍니다. . input_shape = X_train.shape[1:] model = keras.models.Sequential([ keras.layers.Dense(30, activation = &#39;selu&#39;, kernel_initializer= &#39;lecun_normal&#39;, # LeCun 정규분포에서 값을 추출해 가중치 초기값을 설정합니다. input_shape = input_shape), keras.layers.Dense(1) ]) . 간단한 딥러닝 모델을 구축합니다. 이때 활성화 함수로 자기정규화를 일으키는 selu을 사용합니다. . def huber_fn(y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) &lt; 1 # 작은 오차라면 squared_loss = tf.square(error) / 2 # 오차 제곱의 절반값 linear_loss = tf.abs(error) - 0.5 # 큰 오차라면 오차의 절대값 - 0.5 # where 함수는 조건에 따른 벡터 출력에 최적화 되어있습니다. # 참일때는 2번 입력값을 거짓일때는 3번입력값을 출력한 것을 묶어 벡터로 출력합니다. return tf.where(is_small_error, squared_loss, linear_loss) . 손실함수를 직접 구현합니다. 평균 제곱 오차는 큰 오차에 너무 과한 벌칙이 있고 절대값 오차는 이상치에 너무 관대합니다. . 그러므로 앞서 말한 단점을 보안한 후버 손실 함수를 직접 구현하였습니다. . model.compile(loss = huber_fn, optimizer = &#39;nadam&#39;, metrics = &#39;mae&#39;) model.fit(X_train_scaled, y_train, epochs = 2, validation_data=(X_valid_scaled, y_valid)) . Epoch 1/2 363/363 [==============================] - 2s 2ms/step - loss: 0.6235 - mae: 0.9953 - val_loss: 0.2862 - val_mae: 0.5866 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2197 - mae: 0.5177 - val_loss: 0.2382 - val_mae: 0.5281 . &lt;keras.callbacks.History at 0x7f4ed1f30110&gt; . 앞서 정의한 손실함수를 사용해 간단한 모델을 적합시켰습니다. . model.save(&#39;my_model_with_first&#39;) model = keras.models.load_model(&#39;my_model_with_first&#39;, custom_objects = {&#39;huber_fn&#39; : huber_fn}) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . INFO:tensorflow:Assets written to: my_model_with_first/assets Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2054 - mae: 0.4981 - val_loss: 0.2253 - val_mae: 0.5090 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.1993 - mae: 0.4891 - val_loss: 0.2154 - val_mae: 0.5019 . &lt;keras.callbacks.History at 0x7f4ed197b690&gt; . 모델을 저장하고 다시 불러왔습니다. 앞선 모델과 꽤 유사하게 정상적으로 잘 작동합니다. . &#49324;&#50857;&#51088; &#51221;&#51032; &#49552;&#49892; &#54632;&#49688;&#47484; &#53364;&#47000;&#49828;&#47196; &#51201;&#50857; &#54644;&#48372;&#44592; . class HuberLoss(keras.losses.Loss): # 케라스 loss 클래스 상속 def __init__(self, threshold = 1.0, **kwargs): self.threshold = threshold super().__init__(**kwargs) def call(self, y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) &lt; self.threshold squared_loss = tf.square(error) / 2 linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2/ 2 return tf.where(is_small_error, squared_loss, linear_loss) def get_config(self): # 이 함수를 구현하여 threshold 값을 모델 로드시에도 유지할 수 있습니다. base_config = super().get_config() return {**base_config, &#39;threshold&#39; : self.threshold} . 손실함수를 함수로 구현한다면 함수 내 매개변수가 있을 때 그 값은 모델 로드시 날아갑니다. . 이를 방지하려면 클래스로 손실함수를 구현해야합니다. 케라스 loss 클래스를 상속하고 get_config 함수를 구현하면 됩니다. . model.compile(loss = HuberLoss(2.), optimizer=&#39;nadam&#39;, metrics = &#39;mae&#39;) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2217 - mae: 0.4879 - val_loss: 0.2560 - val_mae: 0.4909 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2176 - mae: 0.4832 - val_loss: 0.2389 - val_mae: 0.4881 . &lt;keras.callbacks.History at 0x7f4ed1986990&gt; . model.save(&#39;my_model_with_second&#39;) model = keras.models.load_model(&#39;my_model_with_second&#39;, custom_objects = {&#39;HuberLoss&#39; : HuberLoss}) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . INFO:tensorflow:Assets written to: my_model_with_second/assets Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2145 - mae: 0.4790 - val_loss: 0.2345 - val_mae: 0.4753 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.2117 - mae: 0.4753 - val_loss: 0.2183 - val_mae: 0.4731 . &lt;keras.callbacks.History at 0x7f4ece428dd0&gt; . 모델을 처음 만들었을때와 저장 후 로드한 모델을 새로 사용했을 때 비슷한 결과를 보이는 것을 알 수 있습니다. . 이는 매개변수가 잘 저장됬다는 것을 보여줍니다. (threshold = 2) . &#54876;&#49457;&#54868; &#54632;&#49688;, &#52488;&#44592;&#54868;, &#44508;&#51228;, &#51228;&#54620;&#51012; &#52964;&#49828;&#53552;&#47560;&#51060;&#51669;&#54616;&#44592; . keras.backend.clear_session() # 환경을 초기화 해줍니다. np.random.seed(42) tf.random.set_seed(42) def my_softplus(z): return tf.math.log(tf.exp(z) + 1.0) def my_glorot_initializer(shape, dtype = tf.float32): stddev = tf.sqrt(2. / (shape[0] + shape[1])) return tf.random.normal(shape, stddev = stddev, dtype = dtype) def my_l1_regularizer(weights): return tf.reduce_sum(tf.abs(0.01 * weights)) def my_positive_weights(weights): return tf.where(weights &lt; 0., tf.zeros_like(weights), weights) . 활성화 함수, 가중치 초기값 부여방식, 규제, 제한을 이미 텐서플로 내 존재하지만 직접 구현해보았습니다. . model = keras.models.Sequential([ keras.layers.Dense(30, activation = &#39;selu&#39;, kernel_initializer=&#39;lecun_normal&#39;, input_shape = input_shape), keras.layers.Dense(1, activation= my_softplus, kernel_regularizer = my_l1_regularizer, kernel_constraint = my_positive_weights, kernel_initializer = my_glorot_initializer), ]) model.compile(loss = &#39;mse&#39;, optimizer = &#39;nadam&#39;, metrics = [&#39;mae&#39;]) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 1.5542 - mae: 0.8962 - val_loss: 1.4154 - val_mae: 0.5607 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5943 - mae: 0.5256 - val_loss: 1.4399 - val_mae: 0.5137 . &lt;keras.callbacks.History at 0x7f4ed1607c10&gt; . 이를 적용시킨 모델을 만들었습니다. . model.save(&#39;my_model_with_third&#39;) model = keras.models.load_model(&#39;my_model_with_third&#39;, custom_objects={ &quot;my_l1_regularizer&quot;: my_l1_regularizer, &quot;my_positive_weights&quot;: my_positive_weights, &quot;my_glorot_initializer&quot;: my_glorot_initializer, &quot;my_softplus&quot;: my_softplus, }) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) . INFO:tensorflow:Assets written to: my_model_with_third/assets Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5253 - mae: 0.4974 - val_loss: 1.4448 - val_mae: 0.4931 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5007 - mae: 0.4874 - val_loss: 1.1597 - val_mae: 0.4913 . &lt;keras.callbacks.History at 0x7f4ed01fda50&gt; . 잘 작동합니다. . &#49324;&#50857;&#51088; &#51221;&#51032; &#52789; . class MyDense(keras.layers.Layer): def __init__(self, units, activation = None, **kwargs): super().__init__(**kwargs) self.units = units self.activation = keras.activations.get(activation) def build(self, batch_input_shape): # 층이 처음 사용될 때 호출되는 함수 self.kernel = self.add_weight( name = &#39;kernel&#39;, shape = [batch_input_shape[-1], self.units], initializer = &#39;glorot_normal&#39; ) self.bias = self.add_weight( name = &#39;bias&#39;, shape = [self.units], initializer = &#39;zeros&#39; ) super().build(batch_input_shape) def call(self, X): return self.activation(X @ self.kernel + self.bias) def compute_output_shape(self, batch_input_shape): return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units]) def get_config(self): # 환경설정 저장 base_config = super().get_config() return {**base_config, &#39;units&#39;: self.units, &#39;activation&#39; : keras.activations.serialize(self.activation)} . 층을 직접 구현해보았습니다. 이 사용자 정의 층은 보통의 층과 동일하게 사용할 수 있습니다. . model = keras.models.Sequential([ MyDense(30, activation = &#39;relu&#39;, input_shape = input_shape), MyDense(1) ]) model.compile(loss = &#39;mse&#39;, optimizer = &#39;nadam&#39;) model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid)) model.evaluate(X_test_scaled, y_test) . Epoch 1/2 363/363 [==============================] - 1s 2ms/step - loss: 1.2326 - val_loss: 1.3653 Epoch 2/2 363/363 [==============================] - 1s 2ms/step - loss: 0.5817 - val_loss: 0.7383 162/162 [==============================] - 0s 1ms/step - loss: 0.4993 . 0.4993399679660797 . 사용자 정의 층을 사용해 모델을 만들어봤어요. . &#54632;&#49688;&#54805; API . X_train_A, X_train_B = X_train_scaled[:, :5], X_train_scaled[:, 2:] # 0~4까지 첫번째 입력, 2~7까지 두번째 입력 X_valid_A, X_valid_B = X_valid_scaled[:, :5], X_valid_scaled[:, 2:] X_test_A, X_test_B = X_test_scaled[:, :5], X_test_scaled[:, 2:] X_new_A, X_new_B = X_test_A[:3], X_test_B[:3] # 입력층이 2개인 모델 구축 input_A = keras.layers.Input(shape = [5], name = &#39;wide_input&#39;) input_B = keras.layers.Input(shape = [6], name = &#39;depp_input&#39;) hidden1 = keras.layers.Dense(30, activation = &#39;relu&#39;)(input_B) hidden2 = keras.layers.Dense(30, activation = &#39;relu&#39;)(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) output = keras.layers.Dense(1, name = &#39;output&#39;)(concat) model = keras.Model(inputs = [input_A, input_B], outputs = [output]) . 입력층이 2개인 모델을 만들기 위해 데이터를 먼저 분할합니다. 그 다음 입력층을 두 개 받습니다. . 입력층_B에서 은닉층 2개를 통과한 뒤 입력층_A와 층 연결을 하고 아웃풋을 출력하는 모델을 만들었습니다. . model.compile(loss = &#39;mse&#39;, optimizer = keras.optimizers.SGD(lr = 1e-3)) history = model.fit((X_train_A, X_train_B), y_train, epochs = 20, validation_data = ((X_valid_A, X_valid_B), y_valid)) mse_test = model.evaluate((X_test_A, X_test_B), y_test) y_pred = model.predict((X_new_A, X_new_B)) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(SGD, self).__init__(name, **kwargs) . Epoch 1/20 363/363 [==============================] - 1s 2ms/step - loss: 2.1094 - val_loss: 1.0289 Epoch 2/20 363/363 [==============================] - 1s 2ms/step - loss: 0.7849 - val_loss: 0.6881 Epoch 3/20 363/363 [==============================] - 1s 2ms/step - loss: 0.6517 - val_loss: 0.6039 Epoch 4/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5965 - val_loss: 0.5446 Epoch 5/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5595 - val_loss: 0.5129 Epoch 6/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5311 - val_loss: 0.4873 Epoch 7/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5084 - val_loss: 0.4673 Epoch 8/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4898 - val_loss: 0.4499 Epoch 9/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4748 - val_loss: 0.4373 Epoch 10/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4641 - val_loss: 0.4269 Epoch 11/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4561 - val_loss: 0.4202 Epoch 12/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4496 - val_loss: 0.4141 Epoch 13/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4443 - val_loss: 0.4102 Epoch 14/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4398 - val_loss: 0.4063 Epoch 15/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4362 - val_loss: 0.4027 Epoch 16/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4332 - val_loss: 0.4012 Epoch 17/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4306 - val_loss: 0.3977 Epoch 18/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4282 - val_loss: 0.3951 Epoch 19/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4256 - val_loss: 0.3950 Epoch 20/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4240 - val_loss: 0.3968 162/162 [==============================] - 0s 1ms/step - loss: 0.4183 . 모델을 적합시키고 출력시켰습니다. 성능은 비슷한 것 같아요. . 비슷한 매커니즘으로 출력을 여러개 하는 것 또한 가능합니다. 이 때 손실함수는 각각 필요하며, 어느 출력물에 가중치를 늘리는것도 가능합니다. . &#49436;&#48652;&#53364;&#47000;&#49905; API&#47196; &#46041;&#51201; &#47784;&#45944; &#47564;&#46308;&#44592; . class WideAndDeepModel(keras.models.Model): def __init__(self, units = 30, activation = &#39;relu&#39;, **kwargs): super().__init__(**kwargs) self.hidden1 = keras.layers.Dense(units, activation = activation) self.hidden2 = keras.layers.Dense(units, activation = activation) self.main_output = keras.layers.Dense(1) self.aux_output = keras.layers.Dense(1) def call(self, inputs): input_A, input_B = inputs hidden1 = self.hidden1(input_B) hidden2 = self.hidden2(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) main_output = self.main_output(concat) aux_output = self.aux_output(hidden2) return main_output, aux_output model = WideAndDeepModel(30, activation = &#39;relu&#39;) . 앞서 만든 함수형 API는 정적이기 때문에 반복문/조건문 등 여러 동적인 구조를 필요로 하는 경우 사용이 힘듭니다. . 이를 극복하기 위해 서브클레싱 API 모델을 만들었습니다. 이전과 동일하나 출력층이 2개인 구조입니다. . 생성자에 층 구성을 하고 call 함수로 정방향 계산을 만들었습니다. 이때 input 클래스의 객체는 만들 필요가 없습니다. . model.compile(loss = &#39;mse&#39;, loss_weights= [0.9, 0.1], optimizer = keras.optimizers.SGD(learning_rate=1e-3)) history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10, validation_data = ((X_valid_A, X_valid_B), (y_valid, y_valid))) total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test)) y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B)) . Epoch 1/10 363/363 [==============================] - 2s 3ms/step - loss: 2.2525 - output_1_loss: 2.0189 - output_2_loss: 4.3545 - val_loss: 1.6827 - val_output_1_loss: 1.4017 - val_output_2_loss: 4.2116 Epoch 2/10 363/363 [==============================] - 1s 2ms/step - loss: 1.0489 - output_1_loss: 0.8583 - output_2_loss: 2.7644 - val_loss: 1.0083 - val_output_1_loss: 0.8151 - val_output_2_loss: 2.7478 Epoch 3/10 363/363 [==============================] - 1s 2ms/step - loss: 0.8499 - output_1_loss: 0.7183 - output_2_loss: 2.0346 - val_loss: 0.8146 - val_output_1_loss: 0.6664 - val_output_2_loss: 2.1485 Epoch 4/10 363/363 [==============================] - 1s 2ms/step - loss: 0.7706 - output_1_loss: 0.6679 - output_2_loss: 1.6955 - val_loss: 0.7338 - val_output_1_loss: 0.6259 - val_output_2_loss: 1.7048 Epoch 5/10 363/363 [==============================] - 1s 2ms/step - loss: 0.7236 - output_1_loss: 0.6343 - output_2_loss: 1.5267 - val_loss: 0.6865 - val_output_1_loss: 0.5929 - val_output_2_loss: 1.5288 Epoch 6/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6894 - output_1_loss: 0.6060 - output_2_loss: 1.4392 - val_loss: 0.6652 - val_output_1_loss: 0.5730 - val_output_2_loss: 1.4948 Epoch 7/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6610 - output_1_loss: 0.5809 - output_2_loss: 1.3827 - val_loss: 0.6579 - val_output_1_loss: 0.5670 - val_output_2_loss: 1.4753 Epoch 8/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6364 - output_1_loss: 0.5584 - output_2_loss: 1.3385 - val_loss: 0.6042 - val_output_1_loss: 0.5226 - val_output_2_loss: 1.3384 Epoch 9/10 363/363 [==============================] - 1s 2ms/step - loss: 0.6117 - output_1_loss: 0.5351 - output_2_loss: 1.3005 - val_loss: 0.5785 - val_output_1_loss: 0.4940 - val_output_2_loss: 1.3390 Epoch 10/10 363/363 [==============================] - 1s 2ms/step - loss: 0.5894 - output_1_loss: 0.5144 - output_2_loss: 1.2646 - val_loss: 0.5881 - val_output_1_loss: 0.5014 - val_output_2_loss: 1.3680 162/162 [==============================] - 0s 1ms/step - loss: 0.5706 - output_1_loss: 0.4966 - output_2_loss: 1.2357 . 서브클래싱 API로 만든 모델입니다. 잘 실행됩니다. . &#49324;&#50857;&#51088; &#51221;&#51032; &#47784;&#45944; . . 다음과 같은 모델을 직접 정의하려고 합니다. . class ResidualBlock(keras.layers.Layer): def __init__(self, n_layers, n_neurons, **kwargs): super().__init__(**kwargs) # 히든층을 만들어줍니다. n_layers 개수만큼 층을 쌓습니다. self.hidden = [keras.layers.Dense(n_neurons, activation = &#39;elu&#39;, kernel_initializer = &#39;he_normal&#39;) for _ in range(n_layers)] def call(self, inputs): # 활성화 함수를 쓸때 쓰는 함수 # 인풋을 받으면 계속 히든 층에 레이어에 투입시킵니다. Z = inputs for layer in self.hidden: Z = layer(Z) return inputs + Z . 레이어의 수를 입력받아 그만큼 히든층을 만드는 작업을 해줍니다. 이때 잔차 블록은 출력에 입력을 더합니다. . class ResidualRegressor(keras.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self.hidden1 = keras.layers.Dense(30, activation = &#39;elu&#39;, kernel_initializer = &#39;he_normal&#39;) self.block1 = ResidualBlock(2, 30) self.block2 = ResidualBlock(2, 30) self.out = keras.layers.Dense(output_dim) def call(self, inputs): Z = self.hidden1(inputs) for _ in range(1 + 3): Z = self.block1(Z) Z = self.block2(Z) return self.out(Z) . 생성자에서 층을 만들고 call 메서드에서 이를 사용합니다. 이 모델은 다른 일반 모델처럼 사용도 가능합니다. . model = ResidualRegressor(1) model.compile(loss = &#39;mse&#39;, optimizer = &#39;nadam&#39;) history = model.fit(X_train_scaled, y_train, epochs = 5) score = model.evaluate(X_test_scaled, y_test) . Epoch 1/5 363/363 [==============================] - 2s 2ms/step - loss: 7.8337 Epoch 2/5 363/363 [==============================] - 1s 2ms/step - loss: 1.1220 Epoch 3/5 363/363 [==============================] - 1s 2ms/step - loss: 1.3495 Epoch 4/5 363/363 [==============================] - 1s 2ms/step - loss: 0.9622 Epoch 5/5 363/363 [==============================] - 1s 2ms/step - loss: 0.5799 162/162 [==============================] - 0s 1ms/step - loss: 0.5701 . 방금 정의한 모델을 적용시킨 결과입니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/12/handssu3.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/12/handssu3.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[SSUDA] 심층 신경망 훈련하기",
            "content": ". import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . &#44592;&#52488; &#54876;&#49457;&#54868; &#54632;&#49688; . x = np.arange(-10, 10, 0.1) y = 1 / (1 + np.exp(-x)) plt.plot(x,y) plt.axhline(0.5,linewidth=0.5) plt.show() . &lt;matplotlib.lines.Line2D at 0x7fe0c8707a50&gt; . 시그모이드 함수 형태입니다. 입력값에 따라 값을 0~1로 변환해 주기 때문에 출력층에 사용하기 좋은 함수입니다. . 단조 증가함수이며 비선형함수라는 점은 활성화 함수로 사용하는데 유리합니다. . 하지만 최대 기울기는 0.25로 기울기가 상대적으로 평탄해 역방향 계산시 신경망 학습이 늦어지는 단점이 있습니다. . x = np.arange(-10, 10, 0.1) y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 하이퍼탄젠트 함수입니다. 시그모이드 함수와 전반적으로 비슷한 형태이고, 출력값이 -1~1로 차이가 있습니다. . 단조 증가함수이며 비선형함수라는 점이 시그모이드 함수와 비슷한 특징을 가집니다. . 그리고 기울기의 최댓값이 1이고 값의 평균이 0으로, 시그모이드 함수보다 큰 장점이 있습니다. . 하지만 입력값의 절대값이 2를 넘어가는 순간 출력값이 수렴하는 모습을 볼 수 있습니다. (시그모이드 또한 조금 늦게 수렴하지만 비슷합니다.) . 이 경우 기울기가 0에 가까워지게 되고 그레이언트 소실 문제가 생깁니다. . 쉽게 말해서 기울기가 O에 가깝기 때문에 학습이 진행이 안되는 현상이 발생합니다. 이를 방지하기 위해 RELU함수를 사용합니다. . x = np.arange(-10, 10, 0.1) y = np.where(x &gt; 0, x, 0) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 렐루 함수는 입력값이 0보다 크면 그대로, 0보다 작으면 0을 출력하는 함수입니다. . 입력값에 따라 on/off 해준다는 관점으로 볼 수 있고, 기울기도 1로 잘 유지가 됩니다. . Gradient Vanishing 문제(layer가 늘어날때 값이 사라지는 현상)가 해결되기 때문에, 가장 기본적인 활성화 함수로 사용합니다. . 하지만 0을 기준으로 명확하게 성질이 갈린다는 것은 단점입니다. 이를 극복하기 위해 leaky RELU 함수를 사용합니다. . &#47120;&#47336; &#54632;&#49688; &#48320;&#54805; . x = np.arange(-10, 5, 0.1) y = np.where(x &gt; 0, x, 0.01 * x) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 리키 렐루 함수는 0이상의 입력값일때 렐루함수와 같으며 0이하의 입력값일때 입력을 미세하게 유지하는 함수입니다. . 0이하의 입력값에 0.01을 곱하는 대신 다른 방식의 값을 준 함수들도 계속 소계하겠습니다. . 다만 연산비용이 상대적으로 크기 때문에 실제로 일반 렐루함수 또한 많이 사용합니다. . a = 1 # 조정가능 x = np.arange(-10, 10, 0.1) y = np.where(x &gt; 0, x, a*(np.exp(x) - 1)) plt.plot(x,y) plt.axhline(0,linewidth=0.5) plt.show() . 활성화 함수로 많이 사용하는 EUL 함수 입니다. 0 이하 입력값에서 비선형함수를 쓴 것이 특징입니다. . 0 이하의 입력값에서 죽은 뉴런을 만들지 않는 것이 장점이고, 비선형함수가 섞여있기에 성능도 좋습니다. . 하지만 지수함수의 계산이기 때문에 속도가 확실히 느려진다는 단점이 있습니다. . 다음으로 SELU 함수입니다. ELU함수의 변종으로 자기 정규화가 일어나는것이 특징입니다. . 자기 정규화란 입력특성이 표준화되어 있을때 각 층의 출력이 평균 0과 표준편차 1를 유지하는 경향을 말합니다. . 이는 매우 좋은 성질로, 그레이디언트 소실, 폭주 문제를 막아주는 효과가 있습니다. . 자기 정규화를 일으키는 조건을 만족시킨다면 SELU가 성능이 우수하고, 조건을 만족하기 힘들때 ELU를 많이 사용합니다. . &#49324;&#51204; &#54617;&#49845; . 큰 규모의 DNN을 처음부터 새로 훈련하는 것 보다 비슷한 유형의 신경망을 찾은 이후에 그 신경망의 상위 은닉층을 재사용 하는 방법도 있습니다. . 이렇게 할 경우 훈련 속도는 당연히 크게 높아지고, 훈련 데이터도 적게 써도 좋은 성능을 낼 수 있습니다. . &#50741;&#54000;&#47560;&#51060;&#51200; . . 손실함수를 작게 하기 위해 최적의 파라미터를 찾는 방법들입니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : SGD . Stochastic Gradient Descent(확률적 경사 하강법) 가장 기본이 되는 옵티마이저 입니다. . 값을 하나씩 대입하여 가장 최적의 파라미터를 찾는 방법입니다. 이때 최적의 파라미터는 손실함수를 작게 만드는 값이죠. . 일반적으로 일부 데이터(미니 배치)를 이용해서 기울기를 구한 뒤 최적의 파라미터를 찾습니다. . 하지만 최적의 값을 찾아가기 위한 방향설정이 뒤죽박죽하고, 최적의 러닝레이트(학습률)을 찾기 힘들기 때문에 대체제가 등장합니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : Momentum . 파라미터 업데이트시 지역 최솟값에 빠질수가 있습니다. SGD는 이런 문제를 겪을 가능성이 높죠. . 이를 해결하기 위해 이전 기울기도 파라미터 업데이트 하는 계산에 포함시킵니다. 물론 오래될수록 작은 비율로 포함합니다. . 이런 방식의 옵티마이저를 Momentum 라고 합니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : Adagrad . SGD는 모든 파라미터에 같은 러닝레이트(학습률)을 적용합니다. 하지만 수렴지역으로 빨리 수렴하기 위해선 다른방법이 필요한데요. . 파라미터마다 많이 업데이트 된 파라미터가 있을 것이고 적게 업데이트된 파라미터도 있을 것입니다. . 왜냐하면 특정 입력값이 0이 많이 나올경우 기울기가 0이기 때문에 파라미터 업데이트가 되지 않습니다. . 그러면 그 입력값은 업데이트가 많이 안됬기 때문에 다른 입력값보다 상대적으로 최적값과 차이가 큽니다. 다른 입력값보다 학습률이 커야합니다. . 이런 이유로 각 파라미터의 업데이트 빈도 수에 따라 러닝레이트(학습률)을 다르게 줍니다. 빈도가 높을수록 학습률을 낮게 설정합니다. . 이런 방식의 옵티마이저를 Adagrad 라고 합니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : RMSprop . RMSprop은 Adagrad 에서 학습률이 너무 작아지는 문제를 해결하기 위해 나온 옵티마이저 입니다. . 기존 Adagrad에서는 항들을 그냥 더하지만 RMSprop에서는 지수평균으로 더해집니다. . 즉 시간이 지난 배치일수록 영향력이 줄어드는 것을 알 수 있습니다. . &#50741;&#54000;&#47560;&#51060;&#51200; : Adam . Adam은 Momentum과 RMSprop의 장점을 결합한 옵티마이저로 현재 가장 많이 사용되는 옵티마이저 중 하나입니다. . 방향을 중심으로 한 Momentum 과 보폭을 중심으로 한 RMSprop이 합쳐저 보폭도, 방향도 적절하게 조절한 옵티마이저 입니다. . 잘 모르겠다 싶으면 Adam을 사용하면 됩니다. . &#44508;&#51228; . 우선 l1과 l2 규제가 있습니다. l1규제는 파라미터 절대값을, l2규제는 파라미터 제곱값을 손실함수에 포함한 모형입니다. . l1은 라쏘, l2는 릿지 회귀와 유사합니다. 이는 딥러닝 모델이 과적합되는것을 막아줍니다. . 또 드롭아웃 규제가 있습니다. 이는 간단한데, 매 훈련 스텝에서 각 뉴런은 드롭아웃될 확률을 가집니다. . 이 방식은 이웃한 뉴런에 맞추어 적응하기 보다 자기 자신이 유용한 방식으로 학습합니다. 즉 몇 개의 입력 뉴런에만 지나치게 의존하지 않죠. . &#45712;&#45184;&#51216; . 오늘은 딥러닝을 하기 위해 알아야하는, 튜닝 가능한 것들에 대해 살펴봤습니다. . 이름만 알고 지나갔었던 것이 많았는데 조금 지루하긴 하지만 스스로 확실히 다지고 갔기 때문에 유익했던 것 같아요. . 실력은 아직 많이 부족하지만 빨리 딥러닝 관련해서 실습도 하고 싶습니다. . &#52280;&#44256; . 활성화 함수부분 . https://wooono.tistory.com/209 . https://hwk0702.github.io/ml/dl/deep%20learning/2020/07/09/activation_function/ . 옵티마이저 부분 . https://dbstndi6316.tistory.com/297 . https://seamless.tistory.com/38 . https://hiddenbeginner.github.io/deeplearning/2019/09/22/optimization_algorithms_in_deep_learning.html#Adagrad .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/05/handssu2.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/math/2022/01/05/handssu2.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[SSUDA] 케라스로 다층퍼셉트론 구현하기",
            "content": ". &#45936;&#51060;&#53552; &#45796;&#50868;&#47196;&#46300; . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . from tensorflow import keras fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data() . 케라스 내 대표적인 예제 데이터인 fashion_mnist 데이터를 사용했습니다. . X_train_full.shape . (60000, 28, 28) . X_test.shape . (10000, 28, 28) . 데이터는 총 7만개이고, 28행 28열 값 입니다. . X_train_full.dtype . dtype(&#39;uint8&#39;) . 데이터 내 0부터 255까지 픽셀값이 들어가고, 70000 28 28개로 데이터 양이 꽤 많습니다. . 그러므로 메모리를 절약하기 위해 uint8 자료형을 사용합니다. . plt.imshow(X_train_full[0], cmap = &#39;binary&#39;) plt.axis(&#39;off&#39;) plt.show() . 첫번째 샘플은 부츠인것 같군요. . Sequential Model &#44396;&#52629; . model = keras.models.Sequential() # Flatten은 입력데이터의 shape을 일렬로 변경하는 클래스입니다.(reshape(-1,1)과 유사) model.add(keras.layers.Flatten(input_shape=[28,28])) # Dence는 이전 뉴런과 완전 연결된 밀집뉴런층을 의미합니다. 뉴런 수와 활성화 함수를 정의합니다. model.add(keras.layers.Dense(300, activation = &#39;relu&#39;)) model.add(keras.layers.Dense(100, activation = &#39;relu&#39;)) model.add(keras.layers.Dense(10, activation = &#39;softmax&#39;)) . 딥러닝 모델을 케라스를 통해서 구축합니다. . 분류기 모델이기 때문에 마지막 활성화 함수를 소프트멕스로 해서 0~1 사이로 값을 출력하게 합니다. . model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_1 (Flatten) (None, 784) 0 dense_3 (Dense) (None, 300) 235500 dense_4 (Dense) (None, 100) 30100 dense_5 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________ . 모델의 정보를 summary 함수를 통해서 출력했습니다. . model.layers[1].get_weights() . [array([[-0.02904556, 0.01724293, 0.00127908, ..., -0.03015002, 0.02537476, 0.02381387], [-0.01983445, 0.04740578, -0.07353676, ..., -0.03010103, -0.05431781, 0.02538292], [-0.04257569, 0.00067744, -0.01834078, ..., -0.06939676, -0.04211871, 0.05452839], ..., [ 0.05041559, 0.01164866, -0.01733586, ..., -0.06081748, 0.03213695, 0.01312949], [ 0.0624944 , -0.04938972, -0.03898798, ..., -0.01868138, -0.01819434, 0.04299803], [ 0.06238632, -0.05809005, 0.03214301, ..., 0.06427555, 0.02793135, -0.03146121]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] . 각 층의 있는 파라미터 값을 get_weights 함수를 이용해서 출력할 수 있습니다. . &#47784;&#45944; &#54617;&#49845;&#54616;&#44592; . model.compile(loss = &#39;sparse_categorical_crossentropy&#39;, # 다중 분류 손실 함수. Y가 원핫인코딩이 아닐때 optimizer = &#39;sgd&#39;, # 확률적 경사 하강법 사용. metrics = [&#39;accuracy&#39;]) # 학습을 진행할 때 마다 출력할 평가방식. . 모델을 컴파일합니다. 이때 손실함수, 옵티마이저, 테스트 셋으로 측청하는 것을 입력해줍니다. . X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0 y_valid, y_train = y_train_full[:5000], y_train_full[5000:] X_test = X_test/255.0 . 이후 출처를 밝히겠지만 인터넷 내 블로그 코드를 보고 따라했는데요. 이 부분이 중간에 생략되어있어서 많은 혼란이 있었습니다. . 왜냐하면 딥러닝에서 가장 중요한 것은 X 데이터의 스케일링입니다. 한 개의 데이터당 28 * 28 = 786 개의 값이 입력됩니다. . 이 786개의 값이 스케일이 물론 같지만, 0~255 값을 가지게 되면 밑에 코드의 학습이 전혀 되지 않습니다.(정확도가 계속 0.1에 머무름) . 반드시! 255로 나누어줘서 X 값의 범위를 0~1로 만들어야합니다. 긴 시간 시행착오를 겪었지만 피와 살이 되는 경험이였고 잊지 않을 것 같아요. . # X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=34) history = model.fit(X_train, y_train, epochs = 30, # 에포크는 학습반복수 validation_data = (X_valid, y_valid)) . Epoch 1/30 1719/1719 [==============================] - 14s 8ms/step - loss: 0.7189 - accuracy: 0.7660 - val_loss: 0.5232 - val_accuracy: 0.8210 Epoch 2/30 1719/1719 [==============================] - 12s 7ms/step - loss: 0.4830 - accuracy: 0.8316 - val_loss: 0.4391 - val_accuracy: 0.8532 Epoch 3/30 1719/1719 [==============================] - 12s 7ms/step - loss: 0.4382 - accuracy: 0.8471 - val_loss: 0.4311 - val_accuracy: 0.8524 Epoch 4/30 1719/1719 [==============================] - 11s 6ms/step - loss: 0.4107 - accuracy: 0.8562 - val_loss: 0.4112 - val_accuracy: 0.8538 Epoch 5/30 1719/1719 [==============================] - 11s 6ms/step - loss: 0.3917 - accuracy: 0.8620 - val_loss: 0.3939 - val_accuracy: 0.8576 Epoch 6/30 1719/1719 [==============================] - 11s 7ms/step - loss: 0.3749 - accuracy: 0.8676 - val_loss: 0.3780 - val_accuracy: 0.8642 Epoch 7/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3615 - accuracy: 0.8722 - val_loss: 0.3638 - val_accuracy: 0.8702 Epoch 8/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3502 - accuracy: 0.8766 - val_loss: 0.3508 - val_accuracy: 0.8750 Epoch 9/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3404 - accuracy: 0.8789 - val_loss: 0.3416 - val_accuracy: 0.8782 Epoch 10/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3309 - accuracy: 0.8826 - val_loss: 0.3549 - val_accuracy: 0.8754 Epoch 11/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3223 - accuracy: 0.8858 - val_loss: 0.3391 - val_accuracy: 0.8788 Epoch 12/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.3145 - accuracy: 0.8877 - val_loss: 0.3287 - val_accuracy: 0.8810 Epoch 13/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.3073 - accuracy: 0.8912 - val_loss: 0.3309 - val_accuracy: 0.8818 Epoch 14/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.2999 - accuracy: 0.8929 - val_loss: 0.3256 - val_accuracy: 0.8840 Epoch 15/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2933 - accuracy: 0.8942 - val_loss: 0.3213 - val_accuracy: 0.8864 Epoch 16/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2877 - accuracy: 0.8968 - val_loss: 0.3121 - val_accuracy: 0.8884 Epoch 17/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2820 - accuracy: 0.8977 - val_loss: 0.3247 - val_accuracy: 0.8810 Epoch 18/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.2765 - accuracy: 0.9009 - val_loss: 0.3157 - val_accuracy: 0.8874 Epoch 19/30 1719/1719 [==============================] - 11s 7ms/step - loss: 0.2710 - accuracy: 0.9027 - val_loss: 0.3118 - val_accuracy: 0.8868 Epoch 20/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2660 - accuracy: 0.9040 - val_loss: 0.3133 - val_accuracy: 0.8876 Epoch 21/30 1719/1719 [==============================] - 12s 7ms/step - loss: 0.2614 - accuracy: 0.9063 - val_loss: 0.3317 - val_accuracy: 0.8800 Epoch 22/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2565 - accuracy: 0.9075 - val_loss: 0.3060 - val_accuracy: 0.8890 Epoch 23/30 1719/1719 [==============================] - 9s 5ms/step - loss: 0.2524 - accuracy: 0.9091 - val_loss: 0.3061 - val_accuracy: 0.8886 Epoch 24/30 1719/1719 [==============================] - 9s 5ms/step - loss: 0.2482 - accuracy: 0.9097 - val_loss: 0.2999 - val_accuracy: 0.8904 Epoch 25/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2441 - accuracy: 0.9123 - val_loss: 0.2960 - val_accuracy: 0.8912 Epoch 26/30 1719/1719 [==============================] - 7s 4ms/step - loss: 0.2397 - accuracy: 0.9134 - val_loss: 0.2991 - val_accuracy: 0.8922 Epoch 27/30 1719/1719 [==============================] - 6s 4ms/step - loss: 0.2358 - accuracy: 0.9148 - val_loss: 0.3040 - val_accuracy: 0.8914 Epoch 28/30 1719/1719 [==============================] - 8s 5ms/step - loss: 0.2317 - accuracy: 0.9157 - val_loss: 0.2910 - val_accuracy: 0.8932 Epoch 29/30 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2281 - accuracy: 0.9179 - val_loss: 0.3280 - val_accuracy: 0.8824 Epoch 30/30 1719/1719 [==============================] - 8s 4ms/step - loss: 0.2244 - accuracy: 0.9203 - val_loss: 0.2989 - val_accuracy: 0.8900 . 테스트와 트레인 셋으로 나눈뒤 모델을 학습합니다. 에포크가 지날때 마다 반드시 개선되지는 않습니다. . &#54617;&#49845; &#49884;&#44033;&#54868;&#54616;&#44592; . import pandas as pd pd.DataFrame(history.history).plot(figsize = (8,5)) plt.grid(True) plt.gca().set_ylim(0, 1) plt.show() . 학습데이터는 에포크가 지날때마다 계속 개선이 되는것으로 보이나 val 데이터는 개선이 되었다가 감소하는걸 반복합니다. . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552;&#47196; &#54217;&#44032;&#50752; &#50696;&#52769;&#54616;&#44592; . model.evaluate(X_test, y_test) . 313/313 [==============================] - 1s 2ms/step - loss: 0.3289 - accuracy: 0.8798 . [0.32892856001853943, 0.879800021648407] . 테스트 데이터로 모델을 평가했습니다. 과적합 되지 않고 비슷한 성능을 보입니다. . X_new = X_test[:3] y_proba = model.predict(X_new) y_proba.round(2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99], [0. , 0. , 0.99, 0. , 0.01, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32) . predict 함수를 이용하면 범주마다의 확률을 예측해줍니다. . y_proba.argmax(axis=-1) . array([9, 2, 1]) . 이전에는 predict_classes로 범주를 뽑아낼 수 있었으나 케라스 버전 업그레이드 이후 이 함수가 사라졌습니다. . argmax 함수를 통해 가장 큰 확률을 가진 범주를 추출했습니다. . &#54924;&#44480; &#45936;&#51060;&#53552; &#45796;&#50868;&#47196;&#46300; . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state = 42) X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_valid = scaler.transform(X_valid) X_test = scaler.transform(X_test) . california_housing 데이터를 이용해 주택가격을 예측하는 모델을 케라스를 이용해 딥러닝으로 구현해보겠습니다. . 우선 딥러닝 모델 적용시 가장 중요한 스케일링을 진행합니다. 이때 데이터 리키지가 일어나는것 항상 조심해야겠죠. . &#54924;&#44480; &#47784;&#45944; &#54617;&#49845;&#54616;&#44592; . model = keras.models.Sequential([ keras.layers.Dense(30, activation = &#39;relu&#39;, input_shape = X_train.shape[1:]), keras.layers.Dense(1) ]) model.compile(loss = &#39;mean_squared_error&#39;, optimizer = keras.optimizers.SGD(lr = 1e-3)) history = model.fit(X_train, y_train, epochs = 20, validation_data =(X_valid, y_valid)) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(SGD, self).__init__(name, **kwargs) . Epoch 1/20 363/363 [==============================] - 2s 4ms/step - loss: 2.0042 - val_loss: 1.5977 Epoch 2/20 363/363 [==============================] - 1s 3ms/step - loss: 0.7294 - val_loss: 0.6821 Epoch 3/20 363/363 [==============================] - 1s 3ms/step - loss: 0.6309 - val_loss: 0.5893 Epoch 4/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5921 - val_loss: 0.5399 Epoch 5/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5632 - val_loss: 0.5151 Epoch 6/20 363/363 [==============================] - 1s 3ms/step - loss: 0.5404 - val_loss: 0.4985 Epoch 7/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5214 - val_loss: 0.4943 Epoch 8/20 363/363 [==============================] - 1s 2ms/step - loss: 0.5060 - val_loss: 0.4857 Epoch 9/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4932 - val_loss: 0.4814 Epoch 10/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4827 - val_loss: 0.4671 Epoch 11/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4734 - val_loss: 0.4758 Epoch 12/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4657 - val_loss: 0.4704 Epoch 13/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4588 - val_loss: 0.4573 Epoch 14/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4529 - val_loss: 0.4710 Epoch 15/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4478 - val_loss: 0.4527 Epoch 16/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4430 - val_loss: 0.4503 Epoch 17/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4386 - val_loss: 0.4426 Epoch 18/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4349 - val_loss: 0.4592 Epoch 19/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4313 - val_loss: 0.4514 Epoch 20/20 363/363 [==============================] - 1s 2ms/step - loss: 0.4281 - val_loss: 0.4557 . 우선 회귀모델이니 마지막 층에 소프트맥스 같은 활성화함수는 필요하지 않습니다. 또 손실함수는 MSE를 사용하였네요. . &#54924;&#44480; &#47784;&#45944; &#54217;&#44032;&#50752; &#50696;&#52769; . mse_test = model.evaluate(X_test, y_test) . 162/162 [==============================] - 0s 1ms/step - loss: 0.4182 . 테스트 데이터로 모델을 평가했는데, 로스값이 더 좋은 모습입니다. . X_new = X_test[:3] model.predict(X_new) . array([[0.54969954], [1.6468697 ], [3.592553 ]], dtype=float32) . 모델을 이용해 새 데이터를 예측했습니다. . plt.plot(pd.DataFrame(history.history)) plt.grid(True) plt.gca().set_ylim(0,1) plt.show() . 에포크가 진행될수록 트레인 데이터의 mse는 감소합니다. 하지만 valid 데이터는 일정수준이상이 되면 진동합니다. . &#49888;&#44221;&#47581; &#54616;&#51060;&#54140;&#54028;&#46972;&#48120;&#53552; &#53916;&#45789;&#54616;&#44592; . 머신러닝의 다른 모델들과 다르게 신경망은 매우 많은 하이퍼 파라미터가 있습니다. . 예시로 배치 사이즈, 옵티마이저, 학습률, 은닉층의 수, 활성화함수, loss, 드롭아웃 등 많습니다. . 우선 배치 사이즈의 경우 크면 메모리 문제가 많이 생깁니다. 또 훈련 초기 종종 불안정하기 훈련되기도 합니다. . 만약 너무 작으면 학습시간이 오래걸리고 노이즈도 커지는데요. gpu를 사용하기 때문에 2의 제곱수를 많이 사용합니다.(주로 32) . 이 부분은 의견이 많이 갈리기도 하는 부분이에요. . 다음은 옵티마이저 입니다. 간단하게 사진으로 대체했는데 사실 정확히 이해하기 힘듭니다. 따로 시간내서 공부해야합니다. . 일반적으로 Adam이 성능이 좋다고 합니다. . . 학습률은 디폴트가 0.01으로 크면 발산하고 작으면 지역 최저점에 머물 확률이 높아요. 옵티마이저 내부옵션으로도 사용됩니다. . 활성화함수는 은닉층에서 주로 Relu를 주로 사용합니다. 모델에 따라 tanh등을 쓰기도 합니다. . 은닉층은 많아지면 학습시간 문제, 과적합 문제가 많이 생깁니다. . &#45712;&#45184;&#51216; . 이번엔 텐서플로우 내 케라스라는 패키지를 이용해 딥러닝을 간단히 맛보았습니다. . 케라스는 고수준 패키지로 쉽게 사용할 수 있으나 세밀한 작업은 하지 못하는데요. . 간단한 예제 데이터를 통해 딥러닝이라는 것을 해봤습니다. 오늘 느낀것은 공부할 것이 참 많겠다 생각이 들었어요. . 계속 발전하는 분야라서 인터넷 글마다 정보도 다르고 한데, 저도 발전되는 그 곳에 함께하고 싶네요. . 더 열의를 다지는 시간이였던것 같아요. . &#52280;&#44256; . 케라스 예제: https://beoks.tistory.com/56?category=854491 . 하이퍼파라미터 부분 : https://velog.io/@qksekf/%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9DETF-Keras-Tuner#6-activation-fuction%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98 .",
            "url": "https://ksy1526.github.io/myblog/ssuda/book/jupyter/deep%20learning/hands/karas/classifier/regression/2022/01/02/handssu1.html",
            "relUrl": "/ssuda/book/jupyter/deep%20learning/hands/karas/classifier/regression/2022/01/02/handssu1.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[처음 시작하는 딥러닝] 2. 밑바닥부터 만들어보는 딥러닝",
            "content": ". &#49888;&#44221;&#47581;&#51032; &#44396;&#49457;&#50836;&#49548;: &#50672;&#49328; . def assert_same_shape(array, array_grad): assert array.shape == array_grad.shape return None . class Operation(object): def __init__(self): pass def forward(self, input_): # input_은 ndarray self.input_ = input_ self.output = self._output() return self.output def backward(self, output_grad): # output_grad은 ndarray assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) assert_same_shape(self.input_, self.input_grad) return self.input_grad # 서브 클래스에서 오버라이딩 하기 위해 있는 부분 # 이 클래스에서는 추상 메소드만 선언함 def _output(self): raise NotImplementedError() def _input_grad(self, output_grad): raise NotImplementedError() . 상속을 해주기 위한 간단한 연산 클래스를 만들었습니다. . forward 부분은 input_을 받고 아웃풋을 내는 함수를 호출합니다. . backward 부분은 output_grad를 받고 _input_grad 함수를 호출합니다. . 여기서 assert_same_shape은 입력값이 정상인지 확인하는 역할을 합니다. . class ParamOperation(Operation): def __init__(self, param): super().__init__() # 상속 받은 클레스의 생성자를 실행해줘야함 self.param = param # 파라미터도 입력받음. def backward(self, output_grad): assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) self.param_grad = self._param_grad(output_grad) assert_same_shape(self.input_, self.input_grad) assert_same_shape(self.param, self.param_grad) return self.input_grad def _param_grad(self, output_grad): raise NotImplementedError() . 파라미터가 있는 연산를 정의하기 위해 제일 기본적인 연산 클래스를 상속받아 만들었습니다. . backward 함수는 _input_grad, _param_grad(출력물에 대한 입력값/파라미터 기울기) 함수를 호출해 값을 구합니다. . 이전 backward와 달라진점은 _param_grad 부분이 추가했다는 것입니다. . 이 클래스를 상속하는 클래스는 _output, _input_grad, _pram_grad 함수를 정의해야합니다. . import numpy as np # 신경망의 가중치 행렬 곱 연산, 순방향/역방향 모두 제공. class WeightMultiply(ParamOperation): def __init__(self, W): super().__init__(W) # Operation 클래스 내 forward 함수에서 내부 호출 당하는 함수, 순방향 출력 def _output(self): return np.dot(self.input_, self.param) # forward 함수 내 self.input_은 정해줌. # Operation 클래스 내 backward 함수에서 내부 호출 당하는 함수, 역방향 출력 # 입력의 대한 기울기, 파라미터에 대한 기울기를 두 함수로 만듬. def _input_grad(self, output_grad): # 입력에 대한 기울기 출력. return np.dot(output_grad, np.transpose(self.param, (1,0))) def _param_grad(self, output_grad): return np.dot(np.transpose(self.input_, (1,0)), output_grad) . ParamOperation를 상속해 신경망의 가중치 행렬 곱 연산을 하는 WeightMultiply를 만들었습니다. . class BiasAdd(ParamOperation): def __init__(self, B): assert B.shape[0] == 1 # 상속받은 클래스(ParamOperation)의 생성자(param) 값에 B를 넣어줌. super().__init__(B) # forward 호출하는 내부함수. def _output(self): return self.input_ + self.param # backward에서 호출하는 내부함수(output_grad 이미 입력받음), 입력에 대한 기울기 def _input_grad(self, output_grad): return np.ones_like(self.input_) * output_grad # 파라미터에 대한 기울기 def _param_grad(self, output_grad): param_grad = np.ones_like(self.param) * output_grad return np.sum(param_grad, axis = 0).reshape(1, param_grad.shape[1]) . 편향을 더하는 연산도 마찬가지로 ParamOperation를 상속하여 만들었습니다. . class Sigmoid(Operation): def __init__(self): super().__init__() def _output(self): return 1.0/(1.0 + np.exp(-1.0 * self.input_)) # 입력에 대한 기울기 계산 def _input_grad(self, output_grad): sigmoid_backward = self.output * (1.0 - self.output) input_grad = sigmoid_backward * output_grad return input_grad . 시그모이드 연산은 파라미터가 없기 때문에 Operation 클래스를 상속했습니다. . class Linear(Operation): def __init__(self): super().__init__() def _output(self): return self.input_ def _input_grad(self, output_grad): return output_grad . 입력을 받은 대로 출력해주는 Linear 클래스 입니다. . &#49888;&#44221;&#47581;&#51032; &#44396;&#49457;&#50836;&#49548;: &#52789; . class Layer(object): # 뉴런의 개수는 층의 너비에 해당 def __init__(self, neurons): # neurons : 층의 너비 self.neurons = neurons self.first = True self.params = [] self.param_grads = [] self.operations = [] # 층을 구현하는 메서드 def _setup_layer(self, num_in): raise NotImplementedError() # 입력값을 연산에 순서대로 통과시켜 순방향 계산을 함. def forward(self, input_): if self.first: # 처음 층을 만드는 것이면, _setup_layer 함수 실행. self._setup_layer(input_) self.first = False self.input_ = input_ for operation in self.operations: # 여러개의 operations 들의 합 input_ = operation.forward(input_) self.output = input_ return self.output # output_grad를 각 연산에 역순으로 통과시켜 역방향 계산을 함 def backward(self, output_grad): assert_same_shape(self.output, output_grad) for operation in reversed(self.operations): output_grad = operation.backward(output_grad) input_grad = output_grad self._param_grads() return input_grad # 각 operation 객체에서 _param_grad 값을 꺼냄 def _param_grads(self): self.param_grads = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): # 서브클래스에 속하는가? self.param_grads.append(operation.param_grad) # 각 operationn 객체에서 _params 값을 꺼냄 def _params(self): self.params = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): self.params.append(operation.param) . 층을 정의합니다. 이때 Operation 객체의 리스트를 operations 속성에 담고 있습니다. . class Dence(Layer): def __init__(self, neurons, activation): super().__init__(neurons) self.activation = activation # 밀집층의 연산 정의 def _setup_layer(self, input_): if self.seed: np.random.seed(self.seed) self.params = [] # 가중치 self.params.append(np.random.randn(input_.shape[1], self.neurons)) #편향 self.params.append(np.random.randn(1, self.neurons)) self.operations = [WeightMultiply(self.params[0]), # 신경망의 가중치 행렬곱 연산 BiasAdd(self.params[1]), self.activation] return None . layer 클래스에 _setup_layer 함수를 추가로 구현하기 위해 Dence(밀집층) 클래스를 생성했습니다. . 랜덤 시드를 받아서 파라미터 초기값에 랜덤값을 넣어줍니다. . 여기서 operatings을 정의하는데 층을 하나 만들기 위해서 여러 연산 클래스를 사용합니다. . &#49888;&#44221;&#47581;&#51032; &#44396;&#49457;&#50836;&#49548;: &#49552;&#49892;&#54632;&#49688; . class Loss(object): def __init__(self): pass # 실제 손실값을 계산하는 함수 def forward(self, prediction, target): assert_same_shape(prediction, target) self.prediction = prediction self.target = target loss_value = self._output() return loss_value # 손실함수의 입력값에 대해 손실의 기울기를 계산. def backward(self): self.input_grad = self._input_grad() assert_same_shape(self.prediction, self.input_grad) return self.input_grad def _output(slef): raise NotImplementedError() def _input_grad(slef): raise NotImplementedError() . 손실함수를 구성하는 Loss 클래스 입니다. 타겟값과 예측값을 가지고 _output 함수를 돌려 loss를 구합니다. . backward 함수는 입력 값에 따른 손실의 기울기를 계산해줍니다. . class MeanSquaredError(Loss): def __init__(self): super().__init__() # 평균 제곱오차 손실함수 def _output(self): loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0] return loss # 예측값에 대한 평균제곱오차 손실의 기울기를 계산 def _input_grad(self): return 2.0 * (self.prediction - self.target) / self.prediction.shape[0] . Loss 클래스를 상속받아 MeanSquaredError 클래스를 만들었는데요. _output, _input_grad 두 함수의 수식을 구체적으로 구현했습니다. . &#46373;&#47084;&#45789;&#51032; &#44396;&#49457;&#50836;&#49548;: &#45684;&#47088; . class NeuralNetwork(object): def __init__(self, layers, loss, seed = 1): self.layers = layers # 신경망의 층 정의, layers 클래스를 받음.(리스트로 받을수도) self.loss = loss # loss 클래스를 받음. self.seed = seed if seed: for layer in self.layers: setattr(layer, &#39;seed&#39;, self.seed) # layer.seed = self.seed와 동일 # 데이터를 각 층에 순서대로 통과시킴 def forward(self, x_batch): # x_batch는 ndarray. x_out = x_batch for layer in self.layers: x_out = layer.forward(x_out) return x_out # 데이터를 각 층에 역순으로 통과시킴 def backward(self, loss_grad): grad = loss_grad for layer in reversed(self.layers): grad = layer.backward(grad) return None def train_batch(self, x_batch, y_batch): # 순방향 계산 수행. predictions = self.forward(x_batch) # 손실값 계산 loss = self.loss.forward(predictions, y_batch) # 역방향 계산 수행 self.backward(self.loss.backward()) return loss # 신경망의 파라미터 값을 받음 def params(self): for layer in self.layers: yield from layer.params # 리스트에 있는 요소를 한개씩 밖으로 전달. # 신경망의 각 파라미터에 대한 손실값의 기울기를 받음. def param_grads(self): for layer in self.layers: yield from layer.param_grads . neural_network = NeuralNetwork( layers = [Dence(neurons = 13, activation = Sigmoid()), Dence(neurons = 1, activation = Linear())], loss = MeanSquaredError(), #learning_rata = 0.01 ) . 딥러닝 구현을 위해 NeuralNetwork 클래스를 구현했습니다. 앞서 구현한 층, 손실함수를 이용했습니다. . 우선 layers로 층 클래스를 리스트로 받습니다. 층 클래스는 또 뉴런 개수와 활성화함수인 연산 클래스를 입력해야합니다. . 또 loss에는 손실함수 클래스를 넣어주면 됩니다. . forward 부분에서는 입력된 x값을 여러 층에 차레대로 넣습니다. 층에서는 또 차레대로 연산을 해서 prediction을 출력합니다. . 그 후 loss 클래스를 이용해서 손실값을 계산합니다. 다음으로 손실의 기울기를 backward 함수에 넣습니다. . backward 함수에서는 손실의 기울기를 여러 층에 앞선 차레와 반대 순서로 넣습니다. . 이렇게 나온 backward 함수의 최종 값은 input의 기울기 입니다. 이 값을 통해 loss를 줄여가는게 학습에 방향이겠죠. . &#46373;&#47084;&#45789;&#51032; &#44396;&#49457;&#50836;&#49548;: &#50741;&#54000;&#47560;&#51060;&#51200; . class Optimizer(object): def __init__(self, lr = 0.01): self.lr = lr def step(self): pass . 옵티마이저의 간단한 추상클래스입니다. lr은 학습률을 의미합니다. . class SGD(Optimizer): def __init__(self, lr = 0.01): super().__init__(lr) def step(self): for (param, param_grad) in zip(self.net.params(), self.net.param_grads()): # 뉴런에 있는 파라미터들을 꺼내오는 함수를 씀. param -= self.lr * param_grad # 이게 과연 층이나 연산 클래스 내 param까지 영향을 끼칠까?? . 옵티마이저에서 step 부분을 SGD(확률적 경사 하강법)을 이용해서 구성한 모습입니다. . 구체적으로 학습률 * (loss값에 영향을 주는 param_grad값)으로 param 값을 업데이트 해나가는 방식입니다. . &#46373;&#47084;&#45789;&#51032; &#44396;&#49457;&#50836;&#49548;: Trainer . def permute_data(X, y): perm = np.random.permutation(X.shape[0]) # 크기만큼 데이터를 셔플해줌 return X[perm], y[perm] class Trainer(object): def __init__(self, net, optim): # net은 NeuralNetwork, optim은 Optimizer self.net = net self.optim = optim setattr(self.optim, &#39;net&#39;, self.net) def generate_batches(self, X, y, size = 32): # 배치 사이즈로 데이터를 쪼개는 함수. assert X.shape[0] == y.shape[0] N = X.shape[0] for ii in range(0, N, size): X_batch, y_batch = X[ii:ii+size], y[ii:ii+size] # 배치만큼 잘라서 yield X_batch, y_batch # 지속적으로 내보냄 def fit(self, X_train, y_train, X_test, y_test, epochs = 100, eval_every = 10, batch_size = 32, seed = 1, restart = True): # eval_every 주기로 테스트 데이터를 사용해 예측성능 추정 np.random.seed(seed) if restart: for layer in self.net.layers: # 뉴런 내 모든 층은 layer.first = True # 층을 초기화하라. for e in range(epochs): X_train, y_train = permute_data(X_train, y_train) # 데이터 셔플 # 데이터가 배치 크기만큼 쪼개짐. batch_generator = self.generate_batches(X_train, y_train, batch_size) for ii, (X_batch, y_batch) in enumerate(batch_generator): # enumerate는 인덱스를 함께 출력해줌. self.net.train_batch(X_batch, y_batch) # 학습. self.optim.step() # 학습 후 나온 파라미터를 업데이트 해줌. if (e+1) % eval_every ==0: test_preds = self.net.forward(X_test) loss = self.net.loss.forward(test_preds, y_test) print(f&#39;{e+1}에폭에서 검증 데이터에 대한 손실값: {loss:.3f}&#39;) . 뉴런과 옵티마이저 클래스를 사용하는 트레이너 클래스입니다. . fit 함수로 데이터를 입력받아 데이터를 배치 단위로 쪼갠뒤 배치 데이터를 적용시켜 loss와 파라미터 기울기를 구합니다. . 그 후 옵티마이저 내 step 함수를 사용해 파라미터를 파라미터 기울기를 사용해서 업데이트 해줍니다. . &#50696;&#51228; &#51088;&#47308; &#50629;&#47196;&#46300; . from sklearn.datasets import load_boston boston = load_boston() data = boston.data target = boston.target features = boston.feature_names . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2. The Boston housing prices dataset has an ethical problem. You can refer to the documentation of this function for further details. The scikit-learn maintainers therefore strongly discourage the use of this dataset unless the purpose of the code is to study and educate about ethical issues in data science and machine learning. In this special case, you can fetch the dataset from the original source:: import pandas as pd import numpy as np data_url = &#34;http://lib.stat.cmu.edu/datasets/boston&#34; raw_df = pd.read_csv(data_url, sep=&#34; s+&#34;, skiprows=22, header=None) data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]) target = raw_df.values[1::2, 2] Alternative datasets include the California housing dataset (i.e. :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing dataset. You can load the datasets as follows:: from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() for the California housing dataset and:: from sklearn.datasets import fetch_openml housing = fetch_openml(name=&#34;house_prices&#34;, as_frame=True) for the Ames housing dataset. warnings.warn(msg, category=FutureWarning) . from sklearn.preprocessing import StandardScaler s = StandardScaler() data = s.fit_transform(data) . def to_2d_np(a,type = &#39;col&#39;): &#39;&#39;&#39; 1차원 텐서를 2차원으로 변환 &#39;&#39;&#39; assert a.ndim == 1, &quot;입력된 텐서는 1차원이어야 함&quot; if type == &quot;col&quot;: return a.reshape(-1, 1) elif type == &quot;row&quot;: return a.reshape(1, -1) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718) # 목푯값을 2차원 배열로 변환 y_train, y_test = to_2d_np(y_train), to_2d_np(y_test) . &#50696;&#49884; . optimizer = SGD(lr = 0.01) trainer = Trainer(neural_network, optimizer) trainer.fit(X_train, y_train, X_test, y_test, epochs = 50) . 10에폭에서 검증 데이터에 대한 손실값: 32.121 20에폭에서 검증 데이터에 대한 손실값: 26.972 30에폭에서 검증 데이터에 대한 손실값: 20.426 40에폭에서 검증 데이터에 대한 손실값: 18.131 50에폭에서 검증 데이터에 대한 손실값: 16.930 . &#45712;&#45184;&#51216; . 딥러닝 관련해서 저번에 신경망을 간단하게 구현을 했었습니다. . 오늘은 은닉층이 더 복잡해지기 때문에 일반화에 용이한 클래스로 딥러닝을 구현했습니다. . 자바로 객체지향프로그래밍을 조금 안 상태에서 학습을 해도 파이썬 문법하고 다른 측면이 있어서 학습이 다소 힘들긴 했습니다. . 처음 연산/층 클래스를 구현할 때는 이게 무슨 코드인지 이해가 안되고 재미도 없었는데 뉴런 부분을 구현할 때 전반적으로 책에서 정리를 해줘서 그 때 전반적인 감을 잡았던 것 같아요. . 코드도 일부 누락되어 있어 깃허브 찾아보면서 매꾸는 등 어려운 과정이 참 많았지만 하길 잘 한것 같습니다. . 딥러닝이란 무엇인가 정말 피부로 체감을 할 수 있었습니다. 그만큼 하나하나 천천히 이해하는데 시간이 오래걸리긴 했지만요. . 맨날 나오는 신경망, 뉴런, 옵티마이저, 트레이너 등등 단어의 의미를 이전보다 훨씬 직관적으로 이해를 잘 할 수 있었던 시간인것 같습니다. . 과정이 다소 복잡하기 때문에 주기적으로 복습을 하며 더 딥러닝과 가까워질 생각입니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/deep%20learning/matrix/math/class/2021/12/29/FirstDeep2.html",
            "relUrl": "/book/jupyter/deep%20learning/matrix/math/class/2021/12/29/FirstDeep2.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "[시계열분석] 2. ARIMA",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . install.packages(&quot;TTR&quot;) install.packages(&quot;forecast&quot;) install.packages(&quot;tseries&quot;) library(TTR) library(forecast) library(tseries) data &lt;- scan(&quot;http://robjhyndman.com/tsdldata/data/nybirths.dat&quot;) birth &lt;- ts(data, frequency = 12, start = c(1946, 1)) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) also installing the dependencies ‘xts’, ‘zoo’ Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) also installing the dependencies ‘quadprog’, ‘quantmod’, ‘fracdiff’, ‘lmtest’, ‘timeDate’, ‘tseries’, ‘urca’, ‘RcppArmadillo’ Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) Registered S3 method overwritten by &#39;quantmod&#39;: method from as.zoo.data.frame zoo . 필요한 패키지들을 설치하고, &#39;뉴욕 월별 출생자수&#39; 데이터를 입력받습니다. . ts 함수를 쓰면 보다 편리하게 원 자료를 시계열 자료로 변환이 가능합니다. . data . &lt;ol class=list-inline&gt;26.663 | 23.598 | 26.931 | 24.74 | 25.806 | 24.364 | 24.477 | 23.901 | 23.175 | 23.227 | 21.672 | 21.87 | 21.439 | 21.089 | 23.709 | 21.669 | 21.752 | 20.761 | 23.479 | 23.824 | 23.105 | 23.11 | 21.759 | 22.073 | 21.937 | 20.035 | 23.59 | 21.672 | 22.222 | 22.123 | 23.95 | 23.504 | 22.238 | 23.142 | 21.059 | 21.573 | 21.548 | 20 | 22.424 | 20.615 | 21.761 | 22.874 | 24.104 | 23.748 | 23.262 | 22.907 | 21.519 | 22.025 | 22.604 | 20.894 | 24.677 | 23.673 | 25.32 | 23.583 | 24.671 | 24.454 | 24.122 | 24.252 | 22.084 | 22.991 | 23.287 | 23.049 | 25.076 | 24.037 | 24.43 | 24.667 | 26.451 | 25.618 | 25.014 | 25.11 | 22.964 | 23.981 | 23.798 | 22.27 | 24.775 | 22.646 | 23.988 | 24.737 | 26.276 | 25.816 | 25.21 | 25.199 | 23.162 | 24.707 | 24.364 | 22.644 | 25.565 | 24.062 | 25.431 | 24.635 | 27.009 | 26.606 | 26.268 | 26.462 | 25.246 | 25.18 | 24.657 | 23.304 | 26.982 | 26.199 | 27.21 | 26.122 | 26.706 | 26.878 | 26.152 | 26.379 | 24.712 | 25.688 | 24.99 | 24.239 | 26.721 | 23.475 | 24.767 | 26.219 | 28.361 | 28.599 | 27.914 | 27.784 | 25.693 | 26.881 | 26.217 | 24.218 | 27.914 | 26.975 | 28.527 | 27.139 | 28.982 | 28.169 | 28.056 | 29.136 | 26.291 | 26.987 | 26.589 | 24.848 | 27.543 | 26.896 | 28.878 | 27.39 | 28.065 | 28.141 | 29.048 | 28.484 | 26.634 | 27.735 | 27.132 | 24.924 | 28.963 | 26.589 | 27.931 | 28.009 | 29.229 | 28.759 | 28.405 | 27.945 | 25.912 | 26.619 | 26.076 | 25.286 | 27.66 | 25.951 | 26.398 | 25.565 | 28.865 | 30 | 29.261 | 29.012 | 26.992 | 27.897 | &lt;/ol&gt; birth . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 194626.663 | 23.598 | 26.931 | 24.740 | 25.806 | 24.364 | 24.477 | 23.901 | 23.175 | 23.227 | 21.672 | 21.870 | . 194721.439 | 21.089 | 23.709 | 21.669 | 21.752 | 20.761 | 23.479 | 23.824 | 23.105 | 23.110 | 21.759 | 22.073 | . 194821.937 | 20.035 | 23.590 | 21.672 | 22.222 | 22.123 | 23.950 | 23.504 | 22.238 | 23.142 | 21.059 | 21.573 | . 194921.548 | 20.000 | 22.424 | 20.615 | 21.761 | 22.874 | 24.104 | 23.748 | 23.262 | 22.907 | 21.519 | 22.025 | . 195022.604 | 20.894 | 24.677 | 23.673 | 25.320 | 23.583 | 24.671 | 24.454 | 24.122 | 24.252 | 22.084 | 22.991 | . 195123.287 | 23.049 | 25.076 | 24.037 | 24.430 | 24.667 | 26.451 | 25.618 | 25.014 | 25.110 | 22.964 | 23.981 | . 195223.798 | 22.270 | 24.775 | 22.646 | 23.988 | 24.737 | 26.276 | 25.816 | 25.210 | 25.199 | 23.162 | 24.707 | . 195324.364 | 22.644 | 25.565 | 24.062 | 25.431 | 24.635 | 27.009 | 26.606 | 26.268 | 26.462 | 25.246 | 25.180 | . 195424.657 | 23.304 | 26.982 | 26.199 | 27.210 | 26.122 | 26.706 | 26.878 | 26.152 | 26.379 | 24.712 | 25.688 | . 195524.990 | 24.239 | 26.721 | 23.475 | 24.767 | 26.219 | 28.361 | 28.599 | 27.914 | 27.784 | 25.693 | 26.881 | . 195626.217 | 24.218 | 27.914 | 26.975 | 28.527 | 27.139 | 28.982 | 28.169 | 28.056 | 29.136 | 26.291 | 26.987 | . 195726.589 | 24.848 | 27.543 | 26.896 | 28.878 | 27.390 | 28.065 | 28.141 | 29.048 | 28.484 | 26.634 | 27.735 | . 195827.132 | 24.924 | 28.963 | 26.589 | 27.931 | 28.009 | 29.229 | 28.759 | 28.405 | 27.945 | 25.912 | 26.619 | . 195926.076 | 25.286 | 27.660 | 25.951 | 26.398 | 25.565 | 28.865 | 30.000 | 29.261 | 29.012 | 26.992 | 27.897 | . class(birth) . &#39;ts&#39; &#45936;&#51060;&#53552; &#44288;&#52272; . plot(birth) . 전반적인 추세도 조금 있는 것 같고, 계절성이 있는 것 같아요. . 평균과 분산이 시간에 따라 달라지는 것으로 관찰되는데 정상성을 만족하진 않는 자료인것 같습니다. . autoplot(decompose(birth)) . 시계열 자료를 원 데이터, 트렌드, 계절성, 그외 오차로 나눠주는 autoplot(decompose()) 함수 입니다. . 자료를 시각적으로 보기 매우 좋은 것 같아요. . acf(birth, lag.max=20) . acf 자기상관계수 입니다. 빠르게 감소하는 지점이 있다면 MA를 쓰는게 좋으나 그렇지 않아 보입니다. . pacf(birth, lag.max=20) . pacf 편자기상관계수 입니다. 빠르게 감소하는 지점이 있다면 AR을 쓰는 것이 적절합니다. . AR &#47784;&#45944; &#51201;&#54633; . fit &lt;- ar(birth, method = &#39;mle&#39;) fit . Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): “possible convergence problem: optim gave code = 1” Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): “possible convergence problem: optim gave code = 1” . Call: ar(x = birth, method = &#34;mle&#34;) Coefficients: 1 2 3 4 5 6 7 8 0.4043 0.2923 -0.0888 -0.0623 0.0544 -0.0443 -0.0171 -0.0640 9 10 11 12 0.1755 0.1134 -0.2982 0.5094 Order selected 12 sigma^2 estimated as 0.8879 . 자동으로 P값을 정해서 계산해줍니다. . est.1 &lt;- arima(birth, order = c(11,0,0), fixed = c(0,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA)) est.1 . Warning message in arima(birth, order = c(11, 0, 0), fixed = c(0, NA, NA, NA, NA, : “some AR parameters were fixed: setting transform.pars = FALSE” . Call: arima(x = birth, order = c(11, 0, 0), fixed = c(0, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)) Coefficients: ar1 ar2 ar3 ar4 ar5 ar6 ar7 ar8 ar9 0 0.5759 0.1475 -0.1288 0.0248 -0.0355 -0.0245 -0.1414 0.1476 s.e. 0 0.0773 0.0818 0.0880 0.0871 0.0894 0.0907 0.0897 0.0897 ar10 ar11 intercept 0.3941 -0.0112 25.6374 s.e. 0.0816 0.0800 1.2054 sigma^2 estimated as 1.319: log likelihood = -263.59, aic = 551.18 . order = (p,d,q) 이고 fixed는 0인경우 그 항의 계수를 사용하지 않고 NA인 경우 계수를 사용하겠다는 의미입니다. . acf(est$residuals) . ACF 값이 안정회된 모습입니다. . Box.test(est$residuals) . Box-Pierce test data: est$residuals X-squared = 0.30321, df = 1, p-value = 0.5819 . 통계적 검증도 할 수 있습니다. . plot(birth, type = &#39;l&#39;) lines(fitted(est), col = 2, lty = 1) . 예측한 값을 그래프로 표현했습니다. . fitted(est, h = 3) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1946 NA | NA | NA | 26.39963 | 24.84025 | 26.07144 | 25.12577 | 26.22498 | 24.46344 | 25.62999 | 24.50757 | 24.29708 | . 194725.62441 | 22.54370 | 25.18824 | 22.03637 | 24.05069 | 22.91059 | 23.23670 | 22.26126 | 21.60718 | 22.83138 | 22.08378 | 22.79384 | . 194822.55647 | 21.83367 | 23.23437 | 21.66766 | 21.93387 | 21.82316 | 23.70223 | 23.53885 | 23.31222 | 23.58903 | 22.09234 | 22.28697 | . 194922.71987 | 20.63233 | 23.14931 | 21.52470 | 22.57879 | 22.01042 | 23.20781 | 22.89247 | 22.59651 | 23.66815 | 21.93154 | 22.83969 | . 195022.21951 | 20.81767 | 22.75981 | 21.74831 | 22.83386 | 24.12212 | 25.20356 | 25.09516 | 23.69563 | 23.70509 | 22.50564 | 23.09822 | . 195123.88766 | 22.16144 | 24.98893 | 23.51125 | 25.64303 | 23.99667 | 25.15761 | 24.15729 | 24.38610 | 25.20321 | 23.47784 | 24.36874 | . 195224.38436 | 23.63832 | 24.98735 | 24.15418 | 24.32212 | 24.60992 | 25.34410 | 24.73596 | 24.71976 | 25.34774 | 23.78109 | 24.75321 | . 195324.32585 | 22.72202 | 24.97050 | 23.41424 | 24.66142 | 25.16567 | 26.33526 | 25.78175 | 24.92267 | 25.88477 | 24.23444 | 25.65019 | . 195425.37727 | 24.27083 | 25.75235 | 24.18496 | 25.68157 | 25.46286 | 27.66508 | 27.08732 | 26.73748 | 26.32181 | 25.21637 | 25.31379 | . 195525.37057 | 24.11308 | 26.99524 | 25.63835 | 26.85388 | 25.90547 | 25.44878 | 25.33983 | 25.80368 | 27.18613 | 26.03324 | 27.22084 | . 195626.26596 | 24.77874 | 26.70676 | 24.55815 | 25.67215 | 27.00288 | 29.10629 | 29.16034 | 27.72744 | 28.09431 | 25.79614 | 27.04030 | . 195727.25297 | 25.49749 | 27.86099 | 26.58632 | 27.96767 | 26.60047 | 28.58001 | 28.08253 | 27.79993 | 28.32824 | 26.17898 | 27.54414 | . 195826.78286 | 25.84530 | 28.03310 | 26.76667 | 27.88224 | 27.63906 | 28.01756 | 27.48962 | 28.56473 | 28.70882 | 26.94130 | 27.60250 | . 195927.18522 | 25.08607 | 27.93733 | 25.79046 | 27.72664 | 27.22245 | 28.27216 | 27.24954 | 26.55731 | 27.49123 | 26.86171 | 27.76910 | . fitted 함수를 가지고 모델로 만든 예측값을 사용했습니다. h를 3으로 제한했기 때문에 앞 3개 값은 나올 수가 없습니다. . MA &#47784;&#45944; &#51201;&#54633; . ma.est = arima(birth, order = c(0,0,9)) ma.est . Call: arima(x = birth, order = c(0, 0, 9)) Coefficients: ma1 ma2 ma3 ma4 ma5 ma6 ma7 ma8 ma9 0.6346 0.9636 0.5796 0.9329 0.6558 0.7272 0.5426 0.2053 0.4116 s.e. 0.0728 0.1019 0.1162 0.1137 0.1242 0.1304 0.1380 0.1186 0.0779 intercept 25.0739 s.e. 0.5602 sigma^2 estimated as 1.244: log likelihood = -261.17, aic = 544.33 . order = c(p, d, q) 에서 q값이 MA 개수를 결정합니다. . fitted(ma.est, h = 1) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 194625.95251 | 25.67143 | 25.40561 | 24.82399 | 26.19920 | 24.85453 | 24.84318 | 24.56051 | 23.66604 | 23.85843 | 22.47204 | 22.74461 | . 194721.84185 | 21.66282 | 21.80943 | 23.05167 | 23.45515 | 21.67424 | 22.22553 | 22.93592 | 24.34254 | 23.68077 | 22.79472 | 23.63520 | . 194821.75215 | 22.04347 | 21.34192 | 22.10192 | 23.88129 | 21.04898 | 23.39525 | 24.50027 | 23.74538 | 23.87659 | 22.05090 | 22.97825 | . 194921.21271 | 20.77855 | 21.63952 | 21.70288 | 22.21829 | 21.45152 | 24.03647 | 24.66171 | 23.90069 | 24.40842 | 22.92290 | 22.43632 | . 195021.67009 | 22.10285 | 22.36761 | 22.99342 | 25.27430 | 24.98947 | 25.48045 | 24.39333 | 25.02516 | 24.68061 | 22.67505 | 23.35984 | . 195122.19523 | 23.37591 | 23.54196 | 24.50552 | 25.98286 | 23.95424 | 25.24065 | 26.22603 | 25.54268 | 25.34561 | 24.44959 | 24.05728 | . 195223.30611 | 23.58397 | 23.01512 | 24.02431 | 23.98693 | 22.79686 | 26.13166 | 25.64112 | 26.16362 | 26.38574 | 24.02239 | 24.91265 | . 195323.25952 | 23.93089 | 23.87380 | 23.50638 | 25.78781 | 24.67518 | 25.76932 | 26.51528 | 27.00995 | 26.64015 | 25.28893 | 25.92493 | . 195424.85972 | 24.11851 | 23.56204 | 25.59806 | 27.34463 | 25.78523 | 27.48760 | 26.78627 | 26.53038 | 26.71510 | 24.66289 | 25.81467 | . 195524.39575 | 24.44927 | 25.02809 | 25.43336 | 25.62500 | 23.56793 | 26.44265 | 27.81548 | 28.05461 | 28.43752 | 27.09566 | 27.21027 | . 195625.36979 | 25.56654 | 24.94684 | 25.30917 | 27.68991 | 27.06423 | 28.63151 | 28.20405 | 28.76610 | 28.30218 | 26.83588 | 27.38308 | . 195725.26392 | 26.02184 | 25.01887 | 25.93747 | 28.28903 | 26.93810 | 29.01126 | 27.38416 | 27.39822 | 29.44724 | 26.91105 | 26.89012 | . 195826.88089 | 26.44548 | 25.57053 | 26.45435 | 28.26293 | 26.67500 | 28.00386 | 28.40625 | 29.06361 | 28.44631 | 26.25983 | 27.32193 | . 195925.22626 | 24.88989 | 25.60540 | 26.21734 | 26.92255 | 25.54339 | 26.56431 | 27.82080 | 29.60430 | 29.32033 | 28.02514 | 27.98380 | . fitted(ma.est, h = 10) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1946 NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | 25.07392 | 25.07392 | . 194725.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 194825.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 194925.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195025.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195125.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195225.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195325.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195425.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195525.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195625.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195725.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195825.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . 195925.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | 25.07392 | . h가 일정 수준 이상 커지면 MA 모델은 평균값으로 수렴합니다. . ARIMA &#47784;&#45944; &#51088;&#46041; &#51201;&#54633; . est &lt;- auto.arima(birth, stepwise = FALSE, max.p = 12, max.q = 10) est . Series: birth ARIMA(2,1,1)(1,1,1)[12] Coefficients: ar1 ar2 ma1 sar1 sma1 0.4349 -0.241 -0.4999 -0.2474 -0.8465 s.e. 0.1846 0.085 0.1854 0.0986 0.1004 sigma^2 estimated as 0.406: log likelihood=-157.78 AIC=327.56 AICc=328.12 BIC=345.82 . auto.arima 함수를 가지고 자동으로 arima 작업을 했습니다. . 이론적으로 p, q 값을 5 이하로 하는 것이 적당한데, 그에 맞는 결과입니다. . d값은 추세를 의미합니다. . fitted(est) . A Time Series: 14 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 194626.64761 | 23.59349 | 26.92523 | 24.73763 | 25.80315 | 24.36295 | 24.47600 | 23.90066 | 23.17539 | 23.22730 | 21.67376 | 21.88578 | . 194721.54343 | 19.36388 | 24.12830 | 21.13535 | 22.29703 | 20.42994 | 21.87548 | 23.07928 | 22.54888 | 22.71591 | 21.52740 | 22.16409 | . 194821.69823 | 19.93556 | 23.14138 | 21.42456 | 22.19807 | 21.02772 | 23.25265 | 23.36304 | 22.38182 | 22.31920 | 21.64650 | 21.36331 | . 194921.40590 | 19.95606 | 22.92007 | 20.36740 | 21.30675 | 20.88777 | 24.16941 | 23.37374 | 22.76478 | 23.29871 | 21.34524 | 22.00278 | . 195021.89446 | 20.86780 | 23.86333 | 22.58268 | 24.01855 | 24.13627 | 24.79698 | 24.52997 | 23.73809 | 24.46630 | 22.48823 | 22.58123 | . 195123.01638 | 21.62586 | 25.75329 | 22.76266 | 24.69739 | 23.99375 | 26.15594 | 26.02250 | 24.71870 | 25.22931 | 23.51161 | 23.49823 | . 195224.15874 | 22.04979 | 25.46094 | 23.01360 | 23.79163 | 23.55099 | 26.06118 | 25.66630 | 24.91792 | 25.29679 | 23.45490 | 23.78538 | . 195324.85942 | 22.82602 | 25.62198 | 24.11012 | 24.91516 | 24.91610 | 26.02685 | 26.60103 | 25.70685 | 26.28274 | 24.53334 | 25.73220 | . 195425.09205 | 23.41113 | 26.34662 | 25.32678 | 26.84850 | 26.69116 | 27.42612 | 26.53563 | 26.46959 | 26.25946 | 24.49761 | 25.56646 | . 195525.61437 | 23.52570 | 27.14373 | 24.89106 | 24.60156 | 24.94032 | 27.99554 | 27.51837 | 27.65685 | 27.69714 | 25.91558 | 26.36729 | . 195626.86700 | 24.66566 | 27.40157 | 26.69697 | 27.84578 | 27.73466 | 28.38714 | 28.71826 | 27.52005 | 28.28738 | 27.28773 | 26.76079 | . 195726.97638 | 25.41965 | 27.82162 | 25.86653 | 27.95865 | 28.52111 | 28.65298 | 28.17282 | 27.79869 | 29.06388 | 26.44817 | 27.42565 | . 195827.46425 | 25.68166 | 28.10134 | 27.39812 | 27.54637 | 27.78314 | 29.67634 | 28.82805 | 28.17901 | 28.78699 | 25.98107 | 26.86454 | . 195926.45649 | 24.74951 | 28.23975 | 26.15617 | 27.31077 | 26.00112 | 27.37088 | 28.74329 | 29.21407 | 28.90049 | 26.94265 | 27.87884 | . plot(birth, type = &#39;l&#39;) lines(fitted(est), col = 2, lty = 1) . 이전 모델보다 더 잘 적합된 모습입니다. . &#45712;&#45184;&#51216; . 간단한 ARIMA 모형에 대해서 공부했습니다. . AR와 MA가 무엇인지 부분적으로는 이해했는데 완벽히, 직관적으로, 누구에게 설명할 정도로 이해한 것 같진 않아서 조금 걱정입니다. . 요즘 딥러닝으로 시계열 자료를 많이 예측합니다만 소규모 데이터 셋은 과적합 방지를 위해, 또 설명력을 가지기 위해 ARIMA 모델도 사용합니다. . 시계열분석에 기초인 ARIMA 모델을 배워서 그동안 시계열 데이터를 보면 막막했던게 조금 사라진 것 같아요. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/timeseries/r/arima/2021/12/28/TimeSeries2.html",
            "relUrl": "/book/jupyter/timeseries/r/arima/2021/12/28/TimeSeries2.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "[Do it 자연어] 6. 질문에 답하는 웹 서비스 만들기",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.4 MB/s Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 4.8 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 16.6 MB/s Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 44.7 MB/s Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 44.4 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 45.4 MB/s Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 53.9 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 53.6 MB/s Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 25.7 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 550 kB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 51.6 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 44.2 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 6.2 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 51.0 MB/s Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 50.6 MB/s Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 51.1 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=a5637edc2a6d78b74eca383b627dad92eef464234aac303f8617646dbc1f4c65 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount= True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.qa import QATrainArguments args = QATrainArguments( pretrained_model_name= &#39;beomi/kcbert-base&#39;, # 프리트레인 모델 downstream_corpus_name=&#39;korquad-v1&#39;, # 입력 데이터 downstream_model_dir= &#39;/gdrive/My Drive/nlpbook/checkpoing-qa&#39;, # 중간저장위치 max_seq_length=128, # 입력문장 최대 길이(질문+지문) max_query_length=32, # 질문 최대 길이 doc_stride = 64, # 지문에서 몇개 토큰을 슬라이딩 해갈지? 결정 batch_size = 32 if torch.cuda.is_available() else 4, learning_rate=5e-5, epochs = 3, tpu_cores=0 if torch.cuda.is_available() else 8, seed = 7, ) . kcbert-base 모델로 프리트레인을 하였습니다. . LG CNS가 공개한 korqued 1.0 데이터를 활용해 모델을 구축하겠습니다. . 여기서 doc_stride은 만약 입력문장이 128보다 클 경우 지문 앞 부분을 입력값(64)만큼 제거하고 잘린 뒷부분을 가져와 붙입니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters QATrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_corpus_name=&#39;korquad-v1&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoing-qa&#39;, max_seq_length=128, doc_stride=64, max_query_length=32, threads=4, cpu_workers=2, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, fp16=False, tpu_cores=0, tqdm_enabled=True) INFO:ratsnlp:Training/evaluation parameters QATrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_corpus_name=&#39;korquad-v1&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoing-qa&#39;, max_seq_length=128, doc_stride=64, max_query_length=32, threads=4, cpu_workers=2, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, fp16=False, tpu_cores=0, tqdm_enabled=True) . set seed: 7 . 랜덤시드와 로거를 설정했습니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . nlpbook.download_downstream_dataset(args) . Downloading: 38.5MB [00:00, 71.7MB/s] Downloading: 3.88MB [00:00, 60.6MB/s] . downstream_corpus_name=&#39;korquad-v1&#39; 로 선언된 데이터를 내려받습니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . pretrained_model_name= &#39;beomi/kcbert-base&#39; 로 프리트레인을 해서 토크나이저를 생성합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.qa import KorQuADV1Corpus, QADataset corpus = KorQuADV1Corpus() # JSON 포멧의 KorQuAD 데이터를 읽어옴. train_dataset = QADataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39; ) . INFO:ratsnlp:Creating features from train dataset file at /root/Korpora/korquad-v1 INFO:ratsnlp:Creating features from train dataset file at /root/Korpora/korquad-v1 100%|██████████| 1420/1420 [00:00&lt;00:00, 4333.43it/s] convert squad examples to features: 100%|██████████| 57688/57688 [07:46&lt;00:00, 123.67it/s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:answer: 교 ##향 ##곡 INFO:ratsnlp:answer: 교 ##향 ##곡 INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=45, end_positions=47) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=45, end_positions=47) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP] INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=108, end_positions=110) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=108, end_positions=110) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP] INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:answer: 1 ##악 ##장을 INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=44, end_positions=46) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=44, end_positions=46) INFO:ratsnlp:*** Example *** INFO:ratsnlp:*** Example *** INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:question &amp; context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:answer: [CLS] INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/korquad-v1/cached_train_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 33.918 s] INFO:ratsnlp:Saving features into cached file /root/Korpora/korquad-v1/cached_train_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 33.918 s] . KorQuADV1Corpus 클레스로 KorQuAD 데이터를 QAExample 형식으로 읽어옵니다. . 여기서 QAExample은 질문, 지문, 정답, 정답의 시작 인덱스로 구성됩니다. . QADataset에서는 QAExample 형식으로 입력된 값들을 QAFeatures 형태로 가공해서 모델이 학습할 수 있도록 변환해줍니다. . 여기서 QAFeatures은 input_ids(입력 시퀀스), attention_mask(패딩 여부), token_type_ids(세그먼트 정보, 질문은 0/지문은 1), start(or end)_positions(정답 문장 위치) 총 5개로 구성됩니다. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . 데이터 로더를 구축했습니다. 일정 배치 크기만큼 뽑아서 모델을 학습시킬 수 있어요. . &#54217;&#44032; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = QADataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;val&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), collate_fn = nlpbook.data_collator, drop_last=False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Loading features from cached file /root/Korpora/korquad-v1/cached_val_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 4.527 s] INFO:ratsnlp:Loading features from cached file /root/Korpora/korquad-v1/cached_val_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 4.527 s] . 테스트용 데이터와 로더를 만들었습니다. . &#47784;&#45944; &#54617;&#49845; . from transformers import BertConfig, BertForQuestionAnswering pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, ) model = BertForQuestionAnswering.from_pretrained( args.pretrained_model_name, config = pretrained_model_config, ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForQuestionAnswering: [&#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;] - This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;qa_outputs.bias&#39;, &#39;qa_outputs.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . 윗 코드는 BERT 모델을 프리트레인 했고, 아랫 코드는 윗 코드로 만든 모델에 질의응답용 태스크 모듈을 덧붙힌 모델을 만들었습니다. . 지금 만든 모델은 빈 껍데기(초기 모델) 입니다. . from ratsnlp.nlpbook.qa import QATask task = QATask(model, args) trainer = nlpbook.get_trainer(args) . GPU available: True, used: True TPU available: False, using: 0 TPU cores . 옵티마이저와 러닝레이트를 정한 태스크를 만들어줍니다. . 그 후 트레이너를 정의하는데 모델에 세부 설정을 해줍니다.(GPU, 체크포인트 등) . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader, ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params 0 | model | BertForQuestionAnswering | 108 M 108 M Trainable params 0 Non-trainable params 108 M Total params 433.318 Total estimated model params size (MB) . &#51656;&#47928;&#50640; &#45813;&#54616;&#45716; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . from ratsnlp.nlpbook.ner import QADeployArguments args = QADeployArguments( pretrained_model_name= &#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, max_seq_length=128, max_query_length = 32, ) . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 인퍼런스 설정, 토크나이저 초기화를 합니다. . import torch from transformers import BertConfig, BertForQuestionAnswering fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, ) model = BertForQuestionAnswering(pretrained_model_config) . 체크포인트 로드, BERT 설정 로드, BERT 모델 초기화를 했습니다. . model.load_state_dict({k.replace(&#39;model.&#39;,&#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].ithes()}) model.eval() . 체크포인트를 주입하고 모델을 평가모드로 바꿉니다. . def inference_fn(question, context): if question and context: # 질문을 토큰화하고 인덱싱. max_query_length보다 길면 자르기 truncated_query = tokenizer.encode( question, add_special_tokens = False, truncation = True, max_length = args.max_query_length, ) # 앞서 처리한 질문을 지문과 함께 토큰화하고 인덱싱. max_seq_length보다 길면 자르기 inputs = tokenizer.encode_plus( text = truncated_query, text_pair = context, truncation = &#39;only_second&#39;, padding = &#39;max_length&#39;, max_length = args.max_seq_length, return_token_type_ids = True, ) with torch.no_grad(): outputs = model(**{k: torch.tensor([v]) for k, v in inputs.items()}) start_pred = outputs.start_logits.argmax(dim = -1).item() end_pred = outputs.end_logits.argmax(dim = -1).item() # 정답 시작부터 끝까지 토큰들을 이어붙여서 정답 만들기 pred_text = tokenizer.decode(inputs[&#39;input_ids&#39;][start_pred:end_pred+1]) else: pred_text = &#39;&#39; return{ &#39;question&#39;:question, &#39;context&#39;:context, &#39;answer&#39;:pred_text, } . 실제 값이 들어왔을때 처리해주는 함수입니다. . from ratsnlp.nlpbook.ner import get_web_service_app app = get_web_service_app(inference_fn) app.run() . 웹 서비스를 구현합니다. . (이쪽 개념을 잘 몰라서 실제 구현에는 실패했습니다.) .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/answer/2021/12/28/Do_natural_language6.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/answer/2021/12/28/Do_natural_language6.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "[DACON] 펭귄 몸무게 예측 경진대회",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import StandardScaler import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/Penguin/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 0 0 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 50.0 | 15.3 | 220 | MALE | 8.30515 | -25.19017 | 5550 | . 1 1 | Chinstrap penguin (Pygoscelis antarctica) | Dream | No | 49.5 | 19.0 | 200 | MALE | 9.63074 | -24.34684 | 3800 | . 2 2 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 45.1 | 14.4 | 210 | FEMALE | 8.51951 | -27.01854 | 4400 | . 3 3 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 44.5 | 14.7 | 214 | FEMALE | 8.20106 | -26.16524 | 4850 | . 4 4 | Gentoo penguin (Pygoscelis papua) | Biscoe | No | 49.6 | 16.0 | 225 | MALE | 8.38324 | -26.84272 | 5700 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 혹시 이 코드를 그대로 사용하신다면 path만 파일이 저장되어있는 주소로 바꾸시면 됩니다. . print(train.shape) print(test.shape) . (114, 11) (228, 10) . 우선 id포함 변수 개수가 10개이며 데이터 개수가 114, 228개로 다른 대회에 비해 적은 것을 알 수 있어요. . 복잡한 모델을 사용해 과적합이 되는 것을 많이 조심해야겠습니다. . 또 하나 특이한 점은 테스트 데이터가 약 2배 더 많네요. . &#44592;&#48376;&#48320;&#49688; &#49444;&#47749; . train[&#39;Species&#39;].value_counts() . Gentoo penguin (Pygoscelis papua) 48 Adelie Penguin (Pygoscelis adeliae) 41 Chinstrap penguin (Pygoscelis antarctica) 25 Name: Species, dtype: int64 . train[&#39;Island&#39;].value_counts() . Biscoe 57 Dream 44 Torgersen 13 Name: Island, dtype: int64 . train[&#39;Sex&#39;].value_counts() . MALE 56 FEMALE 55 Name: Sex, dtype: int64 . train.drop([&#39;id&#39;], axis = 1, inplace = True) test.drop([&#39;id&#39;], axis = 1, inplace = True) discrete_names = [&#39;Species&#39;, &#39;Island&#39;, &#39;Clutch Completion&#39;, &#39;Sex&#39;] continuous_names = [&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;, &#39;Flipper Length (mm)&#39;, &#39;Delta 15 N (o/oo)&#39;, &#39;Delta 13 C (o/oo)&#39;] . id : 데이터를 구분하기 위한 고유값, Species : 펭귄의 종(총 3개 종이 있음), Island : 샘플이 수집된 근처 섬(총 3개 섬이 있음) . Clutch Completion : 팽귄 둥지 알 개수가 2개인 경우 Yes, Culmen Length/Culmen Depth : 펭귄 부리의 가로/세로 길이 . Flipper Length : 펭귄 날개 길이, Sex : 펭귄 성별, Delta 15 N : 토양과 관련된 동위원소 비율, Delta 13 C : 먹이에 관련된 동위원소 비율 . id는 당연히 의미가 없으니 제외하고 Species/Island/Sex은 이산형 변수, 나머지 값은 연속형 변수 입니다. . &#44208;&#52769;&#52824; &#50668;&#48512; . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 114 entries, 0 to 113 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 Species 114 non-null object 1 Island 114 non-null object 2 Clutch Completion 114 non-null object 3 Culmen Length (mm) 114 non-null float64 4 Culmen Depth (mm) 114 non-null float64 5 Flipper Length (mm) 114 non-null int64 6 Sex 111 non-null object 7 Delta 15 N (o/oo) 111 non-null float64 8 Delta 13 C (o/oo) 111 non-null float64 9 Body Mass (g) 114 non-null int64 dtypes: float64(4), int64(2), object(4) memory usage: 9.0+ KB . train[train[&#39;Sex&#39;].isnull()] . Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 6 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 42.0 | 20.2 | 190 | NaN | 9.13362 | -25.09368 | 4250 | . 8 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 34.1 | 18.1 | 193 | NaN | NaN | NaN | 3475 | . 70 Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 46.2 | 14.4 | 214 | NaN | 8.24253 | -26.81540 | 4650 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train[train[&#39;Delta 15 N (o/oo)&#39;].isnull()] . Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 8 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 34.1 | 18.1 | 193 | NaN | NaN | NaN | 3475 | . 18 Adelie Penguin (Pygoscelis adeliae) | Dream | No | 39.8 | 19.1 | 184 | MALE | NaN | NaN | 4650 | . 109 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 36.6 | 17.8 | 185 | FEMALE | NaN | NaN | 3700 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 우선 변수들의 결측치 여부를 info 함수를 이용해서 알아봤습니다. . 3개 행에서 Sex와 2개의 Delta 변수가 결측치로 관측되었습니다. . 6,8,70 번 관측치가 Sex변수가 NULL 값이고 8, 18, 109번 관측치가 2개의 Delta 변수에 대해서 NULL 값이 나왔습니다. . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 228 entries, 0 to 227 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Species 228 non-null object 1 Island 228 non-null object 2 Clutch Completion 228 non-null object 3 Culmen Length (mm) 228 non-null float64 4 Culmen Depth (mm) 228 non-null float64 5 Flipper Length (mm) 228 non-null float64 6 Sex 222 non-null object 7 Delta 15 N (o/oo) 219 non-null float64 8 Delta 13 C (o/oo) 220 non-null float64 dtypes: float64(5), object(4) memory usage: 16.2+ KB . 테스트 데이터에서도 위에 언급한 3개의 변수들이 일부 NULL 값을 가진걸 확인 했습니다. . 소규모 데이터이기 때문에 조금 신중하게 접근할 필요가 있겠습니다. . &#50672;&#49549;&#54805; &#48320;&#49688; &#44288;&#52272; . train[&#39;Body Mass (g)&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe29d33c710&gt; . 반응변수인 몸무게의 히스토그램입니다. 이상치가 있어보이진 않고, 정규분포와 꽤 유사해보입니다. . plt.figure(figsize=(20,15)) plt.suptitle(&quot;Histogram&quot;, fontsize=40) for i in range(len(continuous_names)): plt.subplot(2,3,i+1) plt.title(continuous_names[i]) plt.hist(train[continuous_names[i]]) . (데이콘 운영자님이 작성하신 코드를 일부 참고했습니다.) . 연속형 변수의 히스토그램을 그렸을 때 이상치도 크게 없어보이고 나름(?) 정규성을 근접하게라도 따르는 것 같습니다. . 하지만 각 변수 마다 스케일이 다르기 때문에 표준화가 필요할 것 같아요. . plt.figure(figsize=(15,10)) ax = sns.heatmap(train.drop(discrete_names, axis = 1).corr(), annot=True) plt.show() . 모든 변수가 반응변수(Body Mass)와 꽤 높은 상관관계를 가지고 있습니다. 딱히 버릴 변수는 없을 것 같아요. . 또한 설명변수끼리도 상관관계가 어느정도 있는 것 같아요. 추후에 차원축소(PCA)를 적용할 수도 있겠습니다. . &#51060;&#49328;&#54805; &#48320;&#49688; &#44288;&#52272; . plt.figure(figsize=(20,15)) for i in range(len(discrete_names)): plt.subplot(2,2,i+1) plt.xlabel(discrete_names[i]) plt.ylabel(&#39;Body Mass (g)&#39;) sns.violinplot(x= train[discrete_names[i]], y= train[&#39;Body Mass (g)&#39;]) plt.tight_layout(rect=[0, 0.03, 1, 0.95]) plt.show() . (데이콘 운영자님의 코드를 참고했습니다.) . 각각의 이산형 변수로 구분한 반응변수를 그래프로 나타냈습니다. 쉽게 얘기해서 이산형 변수와 반응변수의 관계를 알 수 있습니다. . 네 변수 모두 유의미하게 반응변수에 영향을 끼친다고 볼 수 있습니다. 즉 버릴변수는 없어보입니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . train.fillna(train.mean(), inplace = True) test.fillna(train.mean(), inplace = True) train = pd.get_dummies(train) test = pd.get_dummies(test) print(train.shape) print(test.shape) . (114, 16) (228, 15) . 두 Delta 변수 결측값은 연속형이기 때문에 평균값으로 채우기로 했습니다. . 그리고 이산형 변수는 모두 get_dummies 함수를 사용해서 원-핫 인코딩 처리 했습니다. . 라벨인코딩에 경우 간단하나, 이 변수들은 순서형 자료가 아닌 범주형 자료이기 때문에 사용이 부적절하다고 생각했습니다. . 이 경우 Sex 변수 결측값은 Sex_men, Sex_women 어느 경우에도 속하지 않기 때문에 두 행 전부 0으로 처리됩니다. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() train_scaler = scaler.fit_transform(train[continuous_names]) train[continuous_names] = pd.DataFrame(data=train_scaler, columns=continuous_names) test_scaler = scaler.transform(test[continuous_names]) test[continuous_names] = pd.DataFrame(data=test_scaler, columns=continuous_names) train[continuous_names].head() . Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Delta 15 N (o/oo) Delta 13 C (o/oo) . 0 1.016685 | -0.887255 | 1.161653 | -0.775548 | 0.630951 | . 1 0.922318 | 1.027037 | -0.209242 | 1.601553 | 1.629486 | . 2 0.091884 | -1.352893 | 0.476205 | -0.391149 | -1.533908 | . 3 -0.021357 | -1.197680 | 0.750384 | -0.962206 | -0.523568 | . 4 0.941191 | -0.525091 | 1.504376 | -0.635514 | -1.325731 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 앞서 말한대로 연속형 변수에 경우 변수간 스케일을 맞춰주기 위해 StandardScaler를 사용했습니다. . StandardScaler는 평균 0, 분산 1이 되도록 값을 바꿔주기 때문에 변수간 스케일이 동등해집니다. . 여기서 저는 &#39;테스트 데이터(test.csv)를 학습에 사용&#39; 하지 않기 위해 트레인 데이터만 스케일링에 사용했는데요. . 테스트 데이터를 스케일링에 사용된다면 엄밀히 말해서 data leakage에 해당할 수도 있을거 같습니다. . 이 부분은 제가 문의게시판에 올려보겠습니다. . (윗 코드 결측치를 평균 대치 한 부분도 train 평균 값만을 사용했는데 같은 이치입니다.) . train_label = train[&#39;Body Mass (g)&#39;] train.drop([&#39;Body Mass (g)&#39;], axis = 1, inplace = True) . 모델 적합을 위해 반응변수를 따로 label로 빼줍니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score alphas = [0,0.1,1,10,100] for alpha in alphas: ridge = Ridge(alpha=alpha) neg_mse_scores = cross_val_score(ridge, train, train_label, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 339.4875 alpha 값 0.1 일때 평균 rmse : 337.4481 alpha 값 1 일때 평균 rmse : 330.4727 alpha 값 10 일때 평균 rmse : 329.2807 alpha 값 100 일때 평균 rmse : 405.4389 . 릿지 회귀 모델을 사용하였습니다. 교차검증을 통해 최적의 알파(하이퍼파라미터)를 찾는 과정입니다. . 릿지 회귀를 간단히 설명하면 일반 선형 회귀에 응용으로 회귀계수 크기를 패널티로 부여한 모델입니다. . 전반적으로 회귀계수 크기가 수축하는 효과를 보이며 과적합을 방지할 수 있습니다. . 다만 릿지 회귀는 입력 특성 스케일에 민감하기에 앞서 스케일링 작업을 해주었습니다. . ridge = Ridge(alpha = 10) ridge.fit(train, train_label) sample_submission[&#39;Body Mass (g)&#39;] = ridge.predict(test) sample_submission.to_csv(&#39;Penguin_final_1.csv&#39;,index=False) . 앞서 교차검증으로 나온 최적의 알파 값은 10이기 때문에 이를 대입해서 모델을 만듭니다. . 이 모델에 Public MSE 값은 309.92 정도가 나옵니다. . from sklearn.linear_model import Lasso alphas = [0,0.1,1,10,100] for alpha in alphas: lasso = Lasso(alpha=alpha) neg_mse_scores = cross_val_score(lasso, train, train_label, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 339.5967 alpha 값 0.1 일때 평균 rmse : 338.9466 alpha 값 1 일때 평균 rmse : 335.2101 alpha 값 10 일때 평균 rmse : 326.6552 alpha 값 100 일때 평균 rmse : 396.6299 . lasso = Lasso(alpha = 10) lasso.fit(train, train_label) sample_submission[&#39;Body Mass (g)&#39;] = lasso.predict(test) sample_submission.to_csv(&#39;Penguin_final_2.csv&#39;,index=False) . from sklearn.linear_model import ElasticNet alphas = [0,0.1,1,10,100] #alphas = [0.07,0.1,0.5,1,3] for alpha in alphas: elasticNet = ElasticNet(alpha=alpha) neg_mse_scores = cross_val_score(elasticNet, train, train_label, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 339.5967 alpha 값 0.1 일때 평균 rmse : 327.6324 alpha 값 1 일때 평균 rmse : 358.7528 alpha 값 10 일때 평균 rmse : 573.6431 alpha 값 100 일때 평균 rmse : 754.224 . elasticNet = ElasticNet(alpha = 0.1) elasticNet.fit(train, train_label) sample_submission[&#39;Body Mass (g)&#39;] = elasticNet.predict(test) sample_submission.to_csv(&#39;Penguin_final_3.csv&#39;,index=False) .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/eda/ridge/scale/regression/2021/12/27/dacon_penguin.html",
            "relUrl": "/dacon/jupyter/eda/ridge/scale/regression/2021/12/27/dacon_penguin.html",
            "date": " • Dec 27, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "[처음 시작하는 딥러닝] 1. 신경망 기초 실습",
            "content": ". &#45336;&#54028;&#51060; . import numpy as np a = [1,2,3] b = [4,5,6] print(&#39;a+b:&#39;, a+b) try: print(a*b) except TypeError: print(&#39;불가능&#39;) print() a = np.array([1,2,3]) b = np.array([4,5,6]) print(&#39;a+b&#39;, a+b) print(&#39;a*b&#39;, a*b) . a+b: [1, 2, 3, 4, 5, 6] 불가능 a+b [5 7 9] a*b [ 4 10 18] . R의 벡터 자료형 처럼 파이썬의 리스트 자료형은 더하기, 곱하기가 자유롭지 않습니다. . 이를 해결하기 위해 C언어 기반 넘파이 패키지를 활용합니다. . a = np.array([[1,2],[3,4]]) print(a) print(&#39;axis =0&#39;, a.sum(axis = 0)) print(&#39;axis =1&#39;, a.sum(axis = 1)) . [[1 2] [3 4]] axis =0 [4 6] axis =1 [3 7] . 간단한 행렬 연산입니다. axis = 0과 1의 차이점을 나타냅니다. . a = np.array([[1,2,3],[4,5,6]]) b = np.array([10,20,30]) print(&#39;a+b: n&#39;, a+b) . a+b: [[11 22 33] [14 25 36]] . R과 마찬가지로 재활용 규칙이 사용됩니다. . &#49888;&#44221;&#47581; &#44592;&#52488; &#54665;&#47148; &#50672;&#49328; . from typing import Callable def sigmoid(x): return 1 / (1 + np.exp(-x)) def deriv(func, input_, delta = 0.001): return (func(input_+delta) - func(input_-delta)) / (2 * delta) def matrix_function_forward_sum(X,W,sigma): assert X.shape[1] == W.shape[0] # assert은 아래 코드가 다음조건이 맞을때만 성립한다는 것을 알려줍니다. N = np.dot(X,W) S = sigma(N) L = np.sum(S) return L def matrix_function_backward_sum_1(X,W,sigma): assert X.shape[1] == W.shape[0] # assert은 아래 코드가 다음조건이 맞을때만 성립한다는 것을 알려줍니다. N = np.dot(X,W) S = sigma(N) # 입력받은 함수를 적용합니다. L = np.sum(S) dLdS = np.ones_like(S) # S배열 차원크기 유지, 값은 모두 1로 dSdN = deriv(sigma, N) dLdN = dLdS * dSdN dNdX = np.transpose(W, (1,0)) dLdX = np.dot(dLdN, dNdX) return dLdX . 행렬 미분입니다. X는 입력값 행렬, W는 가중치행렬 입니다. . np.random.seed(190204) X = np.random.randn(3,3) W = np.random.randn(3,2) print(&#39;X:&#39;) print(X) print(&#39;L:&#39;) print(round(matrix_function_forward_sum(X,W,sigmoid), 4)) print() print(&#39;dLdX:&#39;) print(matrix_function_backward_sum_1(X,W,sigmoid)) . X: [[-1.57752816 -0.6664228 0.63910406] [-0.56152218 0.73729959 -1.42307821] [-1.44348429 -0.39128029 0.1539322 ]] L: 2.3755 dLdX: [[ 0.2488887 -0.37478057 0.01121962] [ 0.12604152 -0.27807404 -0.13945837] [ 0.22992798 -0.36623443 -0.02252592]] . X는 입력하는 행렬이고 L은 출력값 dLdX은 X 행렬이 변할 때 L값의 변화량, 즉 미분값입니다. . 이 미분값은 d(sigma)/d(u) (XW) (W)^T으로 구할 수 있습니다. (손으로 증명 가능) . X1 = X.copy() X1[0,0] += 0.001 print(round((matrix_function_forward_sum(X1,W,sigmoid) - matrix_function_forward_sum(X,W,sigmoid))/0.001, 4)) . 0.2489 . X[0,0] 값을 0.001 증가시켰을때 L 값의 변화량을 구했습니다. dLdX [0,0] 값과 유사한 것을 알 수 있어요. . &#49440;&#54805;&#54924;&#44480; &#54665;&#47148; &#50672;&#49328; . def forward_linear_regression(X_batch, y_batch, weights): # X는 행렬, y는 벡터, weight는 딕셔너리형태(W + B) assert X_batch.shape[0] == y_batch.shape[0] # 데이터 크기가 같은가 assert X_batch.shape[1] == weights[&#39;W&#39;].shape[0] # 가중치 개수가 맞는가.(행렬 연산이 가능한가) assert weights[&#39;B&#39;].shape[0] == weights[&#39;B&#39;].shape[1] == 1 N = np.dot(X_batch, weights[&#39;W&#39;]) P = N + weights[&#39;B&#39;] loss = np.mean(np.power(y_batch-P, 2)) forward_info = {} forward_info[&#39;X&#39;] = X_batch forward_info[&#39;N&#39;] = N # XW forward_info[&#39;P&#39;] = P # XW + B forward_info[&#39;y&#39;] = y_batch return loss, forward_info . 입력값(X) 파라미터(가중치, W)가 주어졌을때 선형 회귀 값을 구하는 함수입니다. . 하지만 우리가 원하는건 파라미터(가중치)를 추정하는 일이죠. . L = mean(Sigma(y(i) - yhat(i))) 을 최소로 하는 파라미터를 미분을 이용해서 찾아봅시다. . def loss_gradients(forward_info, weights): batch_size = forward_info[&#39;X&#39;].shape[0] dLdP = -2 * (forward_info[&#39;y&#39;] - forward_info[&#39;P&#39;]) dPdN = np.ones_like(forward_info[&#39;N&#39;]) dPdB = np.ones_like(weights[&#39;B&#39;]) dLdN = dLdP * dPdN dNdW = np.transpose(forward_info[&#39;X&#39;], (1,0)) dLdW = np.dot(dNdW, dLdN) # X^T가 행렬 곱법칙 때문에 먼저나옴. dLdB = (dLdP * dPdB).sum(axis = 0) loss_gradients = {} loss_gradients[&#39;W&#39;] = dLdW loss_gradients[&#39;B&#39;] = dLdB return loss_gradients . 앞서 구한 신경망 기초 행렬 연산식을 이용해 선형회귀 가중치의 도함수를 구했습니다. . &#48145;&#48148;&#48149;&#48512;&#53552; &#47564;&#46300;&#45716; &#49888;&#44221;&#47581; . def forward_loss(X, y, weights): M1 = np.dot(X, weights[&#39;W1&#39;]) N1 = M1 + weights[&#39;B1&#39;] O1 = sigmoid(N1) M2 = np.dot(O1, weights[&#39;W2&#39;]) P = M2 + weights[&#39;B2&#39;] loss = np.mean(np.power(y - P, 2)) forward_info = {} forward_info[&#39;X1&#39;] = X1 # 입력값 forward_info[&#39;M1&#39;] = M1 # X * W(1), 여러개의 선형결합 결과들 forward_info[&#39;N1&#39;] = N1 # 상수항 더하기 forward_info[&#39;O1&#39;] = O1 # 시그모이드 함수 씌우기 forward_info[&#39;M2&#39;] = M2 # 시그모이드 함수 씨운 13개 값 다시 선형결합 forward_info[&#39;P&#39;] = P # 다시 상수항 더한 값 최종 Y_HAT으로 생각 forward_info[&#39;y&#39;] = y . 간단한 신경망 구조입니다. 선형결합을 행 개수만큼 하고 나온 여러개의 결과값에 시그모이드 함수를 씌우고 다시 선형결합해서 결과를 냅니다. . def loss_gradients(forward_info, weights): &#39;&#39;&#39; 신경망의 각 파라미터에 대한 손실의 편미분을 계산 &#39;&#39;&#39; dLdP = -(forward_info[&#39;y&#39;] - forward_info[&#39;P&#39;]) dPdM2 = np.ones_like(forward_info[&#39;M2&#39;]) # P = M2 + B2 dLdM2 = dLdP * dPdM2 dPdB2 = np.ones_like(weights[&#39;B2&#39;]) dLdB2 = (dLdP * dPdB2).sum(axis=0) dM2dW2 = np.transpose(forward_info[&#39;O1&#39;], (1, 0)) # O1 * W2 dLdW2 = np.dot(dM2dW2, dLdP) dM2dO1 = np.transpose(weights[&#39;W2&#39;], (1, 0)) # 이게 중요 dLdO1 = np.dot(dLdM2, dM2dO1) dO1dN1 = sigmoid(forward_info[&#39;N1&#39;]) * (1- sigmoid(forward_info[&#39;N1&#39;])) dLdN1 = dLdO1 * dO1dN1 dN1dB1 = np.ones_like(weights[&#39;B1&#39;]) dN1dM1 = np.ones_like(forward_info[&#39;M1&#39;]) dLdB1 = (dLdN1 * dN1dB1).sum(axis=0) dLdM1 = dLdN1 * dN1dM1 dM1dW1 = np.transpose(forward_info[&#39;X&#39;], (1, 0)) dLdW1 = np.dot(dM1dW1, dLdM1) loss_gradients = {} loss_gradients[&#39;W2&#39;] = dLdW2 loss_gradients[&#39;B2&#39;] = dLdB2.sum(axis=0) loss_gradients[&#39;W1&#39;] = dLdW1 loss_gradients[&#39;B1&#39;] = dLdB1.sum(axis=0) return loss_gradients . 조금 복잡하긴 하지만 직접 합성함수 미분을 해봤습니다. . &#45712;&#45184;&#51216; . 코드 실습은 많지 않지만 공부시간이 상당히 오래걸렸습니다. . 다변량 벡터나 행렬을 사용한 합성함수를 미분하는게 여간 힘든게 아니였어요. 수학적 지식, 머신러닝 경험이 조금 필요합니다. . 직관적으로 이해하기위해 많이 노력했습니다. . 그래도 딥러닝이 어떤것인지 토대를 배웠는데 정말 흥미로웠어요. . 그동안 겉핥기 식으로 대충 모형만 보고 패키지만 갔다가 썼는데 밑바닥부터 구현하니 딥러닝의 기초 구조를 알수 있어서 좋았습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/deep%20learning/foundation/matrix/math/2021/12/26/FirstDeep1.html",
            "relUrl": "/book/jupyter/deep%20learning/foundation/matrix/math/2021/12/26/FirstDeep1.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "[Do it 자연어] 5. 개체명 인식하기 + 웹 실습",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 807 kB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 2.5 MB/s Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 9.7 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 11.1 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 36.0 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 45.3 MB/s Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 39.2 MB/s Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 19.7 MB/s Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 33.1 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 45.4 MB/s Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 501 kB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 41.0 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 4.5 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 44.3 MB/s Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 43.9 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 43.7 MB/s Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=8f50ce81c51e52b253729b577e873fce19ff0f756218e5f0555208db01a16494 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.ner import NERTrainArguments args = NERTrainArguments( pretrained_model_name = &#39;beomi/kcbert-base&#39;, downstream_corpus_name = &#39;ner&#39;, # 한국해양대학교 자연언어처리연구실 데이터 + 자체데이터 downstream_model_dir = &#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, # 모델 체크포인트 저장위치 batch_size = 32 if torch.cuda.is_available() else 4, learning_rate = 5e-5, max_seq_length = 64, epochs = 3, tpu_cores = 0 if torch.cuda.is_available() else 8, seed = 7, ) . 앞서 한것과 크게 다르지 않습니다. 프리 트레인 모델로 이준범님의 kcbert-base 모델을 사용했습니다. . ner, 한국해양대학교 데이터를 실습 데이터로 사용하겠습니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters NERTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;named-entity-recognition&#39;, downstream_corpus_name=&#39;ner&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) . set seed: 7 . 랜덤 시드를 고정했으며 로거를 설정했습니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . nlpbook.download_downstream_dataset(args) . Downloading: 100%|██████████| 17.9M/17.9M [00:00&lt;00:00, 33.8MB/s] Downloading: 100%|██████████| 1.13M/1.13M [00:00&lt;00:00, 27.2MB/s] . 말뭉치를 내려받습니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 토크나이저를 선언합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.ner import NERCorpus, NERDataset corpus = NERCorpus(args) train_dataset = NERDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39; ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/ner INFO:ratsnlp:loading train data... LOOKING AT /root/Korpora/ner/train.txt INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 이어 옆으로 움직여 김일성의 오른쪽에서 한 차례씩 두 번 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다. INFO:ratsnlp:target: 이어 옆으로 움직여 &lt;김일성:PER&gt;의 오른쪽에서 &lt;한 차례:NOH&gt;씩 &lt;두 번:NOH&gt; 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다. INFO:ratsnlp:tokens: [CLS] 이어 옆 ##으로 움직 ##여 김일성 ##의 오른 ##쪽에서 한 차례 ##씩 두 번 상 ##체를 굽 ##혀 조문 ##했 ##으며 이 ##윽 ##고 안 ##경을 벗고 손 ##수 ##건으로 눈 ##주 ##위를 닦 ##기도 했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O O O O B-PER O O O B-NOH I-NOH O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 10704, 2287, 7965, 10598, 4327, 10819, 4042, 11790, 17431, 3354, 16729, 4679, 917, 1530, 1801, 9678, 359, 4443, 23831, 4062, 9511, 2451, 5953, 4034, 2173, 19033, 19778, 1898, 4110, 29483, 721, 4043, 10327, 788, 8517, 9212, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 5, 4, 4, 4, 6, 16, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 제철과일리코타치즈샐러드는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다. INFO:ratsnlp:target: &lt;제철과일리코타치즈샐러드:POH&gt;는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다. INFO:ratsnlp:tokens: [CLS] 제 ##철 ##과 ##일 ##리 ##코 ##타 ##치 ##즈 ##샐 ##러 ##드는 직접 만든 쫀 ##쫀 ##한 치 ##즈 ##도 맛 ##있지만 , 영 ##귤 ##청 ##드 ##레 ##싱 ##이 상 ##큼 ##함을 더한 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] B-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2545, 4748, 4128, 4046, 4038, 4599, 4361, 4077, 4146, 7035, 4053, 8609, 9099, 8634, 2771, 6003, 4047, 2972, 4146, 4029, 1306, 25974, 15, 2282, 5376, 4190, 4273, 4306, 4097, 4017, 1801, 4582, 11091, 11554, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 정씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다. INFO:ratsnlp:target: &lt;정:PER&gt;씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다. INFO:ratsnlp:tokens: [CLS] 정 ##씨는 [UNK] 사고 예측 ##을 위한 빅 ##데이 ##터 ##나 전자 항 ##해 등 그동안 알지 못했 ##던 분야 ##에 대해 배울 수 있는 기회 ##였다 [UNK] 며 [UNK] 새로운 교육이 재 ##취업 ##에 많은 도움이 됐다 [UNK] 고 말했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2539, 10786, 1, 8472, 16843, 4027, 8717, 1665, 19545, 4025, 4136, 12116, 3370, 4032, 963, 8996, 10630, 22474, 4217, 16029, 4113, 9305, 17534, 1931, 8032, 8993, 9827, 1, 1363, 1, 10794, 21266, 2499, 21150, 4113, 8298, 11439, 14054, 1, 303, 19646, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: ―효진 역의 김환희(14)가 특히 인상적이었다. INFO:ratsnlp:target: ―&lt;효진:PER&gt; 역의 &lt;김환희:PER&gt;(&lt;14:NOH&gt;)가 특히 인상적이었다. INFO:ratsnlp:tokens: [CLS] [UNK] 효 ##진 역 ##의 김 ##환 ##희 ( 14 ) 가 특히 인상 ##적이 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O B-PER I-PER O O B-PER I-PER I-PER O B-NOH O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 1, 3476, 4153, 2270, 4042, 420, 4185, 4346, 11, 11524, 12, 197, 9250, 11662, 8805, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 5, 15, 4, 4, 5, 15, 15, 4, 6, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 전문가들은 미국 증시의 상승세가 유지되고 ‘트럼프노믹스’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다. INFO:ratsnlp:target: 전문가들은 &lt;미국:ORG&gt; 증시의 상승세가 유지되고 ‘&lt;트럼프노믹스:POH&gt;’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다. INFO:ratsnlp:tokens: [CLS] 전문가들 ##은 미국 증 ##시 ##의 상승 ##세가 유지 ##되고 [UNK] 트럼프 ##노 ##믹 ##스 [UNK] 에 대한 불 ##확 ##실 ##성이 걷 ##히면 국내 증 ##시도 박 ##스 ##권 탈출 ##을 시도 ##할 수 있다는 분석 ##을 내놓 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O B-ORG O O O O O O O O B-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 28067, 4057, 8057, 2680, 4039, 4042, 12360, 11279, 9846, 8593, 1, 8565, 4041, 5618, 4103, 1, 2255, 8014, 1616, 4277, 4353, 8361, 253, 15723, 8791, 2680, 15399, 1481, 4103, 4285, 12882, 4027, 13335, 4082, 1931, 9340, 14481, 4027, 11326, 8588, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/ner/cached_train_BertTokenizer_64_ner_named-entity-recognition [took 12.842 s] . NERCorpus가 넘겨준 데이터와 kcbert-base 데이터로 학습한 토크나이저를 사용해 4가지 출력값을 줍니다. . 마지막 출력값은 label_ids로 B-PER(시작-인명), I-PER(시작아님-인명) B-NOH(시작-기타수량표현) 등을 정수를 할당해 표현합니다. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . 학습 데이터 셋으로 로더를 구축했습니다. 배치크기만큼 인스턴스를 랜덤하게 뽑은 뒤 이를 합처 배치를 만듭니다. . &#54217;&#44032; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = NERDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;val&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), # 순서대로 추출 collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/ner INFO:ratsnlp:loading val data... LOOKING AT /root/Korpora/ner/val.txt INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다 INFO:ratsnlp:target: 결국 초연은 &lt;4년 반:DUR&gt;이 지난 후에 &lt;드레스덴:LOC&gt;에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다 INFO:ratsnlp:tokens: [CLS] 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O O O B-DUR I-DUR O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 14, 24, 4, 4, 4, 10, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다 INFO:ratsnlp:target: 한편 &lt;1840년:DAT&gt;부터 &lt;바그너:PER&gt;와 알고 지내던 &lt;리스트:PER&gt;가 잊혀져 있던 &lt;1악장:NOH&gt;을 부활시켜 &lt;1852년:DAT&gt;에 &lt;바이마르:LOC&gt;에서 연주했다 INFO:ratsnlp:tokens: [CLS] 한편 18 ##40 ##년 ##부터 바 ##그 ##너 ##와 알고 지내 ##던 리스트 ##가 잊혀 ##져 있던 1 ##악 ##장을 부활 ##시켜 18 ##5 ##2년 ##에 바이 ##마 ##르 ##에서 연 ##주 ##했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O B-DAT I-DAT I-DAT O B-PER I-PER I-PER O O O O B-PER O O O O B-NOH I-NOH I-NOH O O B-DAT I-DAT I-DAT O B-LOC I-LOC I-LOC O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 17332, 8601, 13422, 4482, 8042, 1480, 4313, 4538, 4196, 8297, 13683, 4217, 20899, 4009, 21343, 4413, 11759, 20, 4158, 8915, 13705, 8292, 8601, 4044, 13970, 4113, 25418, 4168, 4138, 7971, 2273, 4043, 8258, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 9, 19, 19, 4, 5, 15, 15, 4, 4, 4, 4, 5, 4, 4, 4, 4, 6, 16, 16, 4, 4, 9, 19, 19, 4, 10, 20, 20, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다 INFO:ratsnlp:target: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, &lt;제1바이올린:POH&gt;으로 더욱 명확하게 나타난다 INFO:ratsnlp:tokens: [CLS] 첫 부분 ##의 저 ##음 주제 ##는 주요 주제 ( 고 ##뇌 ##와 갈 ##망 동 ##기 , 청춘 ##의 사랑 동 ##기 ) 를 암 ##시하고 있으며 , 제1 ##바이 ##올린 ##으로 더욱 명확 ##하게 나타 ##난다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-POH I-POH I-POH O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2881, 11515, 4042, 2523, 4126, 14654, 4008, 16190, 14654, 11, 303, 4703, 4196, 204, 4227, 875, 4184, 15, 23061, 4042, 9004, 875, 4184, 12, 1265, 2183, 24730, 22886, 15, 12218, 18056, 22881, 7965, 10365, 14635, 8007, 10498, 8647, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 3편 (난키 방면 1편, 간사이 공항 방면 2편)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 4편 가까이의 편수가 운전되고 있다 INFO:ratsnlp:target: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 &lt;3편:NOH&gt; (&lt;난키:LOC&gt; 방면 &lt;1편:NOH&gt;, &lt;간사이 공항:LOC&gt; 방면 &lt;2편:NOH&gt;)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 &lt;4편:NOH&gt; 가까이의 편수가 운전되고 있다 INFO:ratsnlp:tokens: [CLS] 대 ##낮 시간 ##대 이후에 ##는 심 ##야 시간 ##대를 제외하고 ##는 시간 ##당 3 ##편 ( 난 ##키 방 ##면 1 ##편 , 간 ##사이 공항 방 ##면 2 ##편 ) 으로 비교 ##적 운행 편 ##수가 많아 승객 ##이 많은 시기에 ##는 임시 열 ##차도 운전 ##되기 때문에 시간 ##당 4 ##편 가까이 ##의 편 ##수가 운전 ##되고 있다 [SEP] [PAD] INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O B-NOH I-NOH O B-LOC I-LOC O O B-NOH I-NOH O B-LOC I-LOC I-LOC O O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O B-NOH I-NOH O O O O O O O [SEP] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 809, 4734, 8485, 4140, 22395, 4008, 2015, 4144, 8485, 10633, 23993, 4008, 8485, 4081, 22, 4393, 11, 591, 4379, 1497, 4063, 20, 4393, 15, 201, 13160, 13862, 1497, 4063, 21, 4393, 12, 10442, 8898, 4022, 23167, 3282, 8356, 9737, 25443, 4017, 8298, 14686, 4008, 18002, 2275, 21509, 9381, 17221, 8360, 8485, 4081, 23, 4393, 15238, 4042, 3282, 8356, 9381, 8593, 8120, 3, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 10, 20, 4, 4, 6, 16, 4, 10, 20, 20, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 1, 2]) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 덴노지 역 ~ 와카야마 역 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 한와 선 페이지에서는 생략되어 있다 INFO:ratsnlp:target: &lt;덴노지 역:LOC&gt; ~ &lt;와카야마 역:LOC&gt; 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 &lt;한와 선:ORG&gt; 페이지에서는 생략되어 있다 INFO:ratsnlp:tokens: [CLS] 덴 ##노 ##지 역 ~ 와 ##카 ##야 ##마 역 간에 ##서는 정 ##차 ##역 ##이 비교 ##적 적 ##기 때문에 시 ##판 ##되고 있는 시간 ##표 ##의 한 ##와 선 페 ##이지 ##에서는 생 ##략 ##되어 있다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: [CLS] B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:features: NERFeatures(input_ids=[2, 849, 4041, 4102, 2270, 95, 2320, 4024, 4144, 4168, 2270, 22590, 9666, 2539, 4495, 4119, 4017, 8898, 4022, 2524, 4184, 8360, 2002, 4448, 8593, 8032, 8485, 4302, 4042, 3354, 4196, 1846, 3272, 8067, 8652, 1821, 4873, 9079, 8120, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 10, 20, 20, 20, 4, 10, 20, 20, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 18, 18, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/ner/cached_val_BertTokenizer_64_ner_named-entity-recognition [took 0.824 s] . 입력받은 데이터를 토크나이저로 분리한 후 로더를 이용해 테스트용 배치를 만듭니다. . &#47784;&#45944; &#54617;&#49845; . from transformers import BertConfig, BertForTokenClassification pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = corpus.num_labels, ) # 프리트레인 BERT 모델 model = BertForTokenClassification.from_pretrained( args.pretrained_model_name, config = pretrained_model_config, ) # 프리트레인 모델 + 개체명 인식을 위한 태스크모듈 . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForTokenClassification: [&#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;] - This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForTokenClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . 윗 코드는 BERT 모델을 프리트레인 했고, 아랫 코드는 개체명 인식을 위한 태스크 모듈을 덧붙힌 모델입니다. . from ratsnlp.nlpbook.ner import NERTask task = NERTask(model, args) trainer = nlpbook.get_trainer(args) . /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Checkpoint directory /gdrive/My Drive/nlpbook/checkpoint-ner exists and is not empty. warnings.warn(*args, **kwargs) GPU available: True, used: True TPU available: False, using: 0 TPU cores . 앞서 만든 모델을 이용해 데이터를 학습시킵니다. NERTask는 아담 옵티마이저와 ExponentialLR 러닝레이트를 사용합니다. . 그 뒤 GPU/TPU 설정, 로그 및 체크포인트등을 하는 트레이너를 정의합니다. . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader, ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | model | BertForTokenClassification | 108 M -- 108 M Trainable params 0 Non-trainable params 108 M Total params 433.389 Total estimated model params size (MB) . 모델을 학습합니다. . &#44060;&#52404;&#47749; &#51064;&#49885; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . from ratsnlp.nlpbook.ner import NERDeployArguments args = NERDeployArguments( pretrained_model_name= &#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-ner&#39;, max_seq_length=64, ) . downstream_model_checkpoint_fpath: /gdrive/My Drive/nlpbook/checkpoint-ner/epoch=1-val_loss=0.20.ckpt downstream_model_labelmap_fpath: /gdrive/My Drive/nlpbook/checkpoint-ner/label_map.txt . 인퍼러스 설정을 다시 해줍니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 토크나이저도 초기화해줍니다. . import torch from transformers import BertConfig, BertForTokenClassification fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = fine_tuned_model_ckpt[&#39;state_dict&#39;][&#39;model.classifier.bias&#39;].shape.numel(), ) model = BertForTokenClassification(pretrained_model_config) . 체크포인트 로드, BERT 설정 로드, BERT 모델 초기화를 했습니다. . model.load_state_dict({k.replace(&#39;model.&#39;, &#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].items()}) model.eval() . BertForTokenClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30000, 768, padding_idx=0) (position_embeddings): Embedding(300, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=25, bias=True) ) . 체크포인트를 주입하고 모델을 평가모드로 바꿉니다. . labels = [label.strip() for label in open(args.downstream_model_labelmap_fpath, &quot;r&quot;).readlines()] id_to_label = {} for idx, label in enumerate(labels): if &quot;PER&quot; in label: label = &quot;인명&quot; elif &quot;LOC&quot; in label: label = &quot;지명&quot; elif &quot;ORG&quot; in label: label = &quot;기관명&quot; elif &quot;DAT&quot; in label: label = &quot;날짜&quot; elif &quot;TIM&quot; in label: label = &quot;시간&quot; elif &quot;DUR&quot; in label: label = &quot;기간&quot; elif &quot;MNY&quot; in label: label = &quot;통화&quot; elif &quot;PNT&quot; in label: label = &quot;비율&quot; elif &quot;NOH&quot; in label: label = &quot;기타 수량표현&quot; elif &quot;POH&quot; in label: label = &quot;기타&quot; else: label = label id_to_label[idx] = label id_to_label . {0: &#39;[CLS]&#39;, 1: &#39;[SEP]&#39;, 2: &#39;[PAD]&#39;, 3: &#39;[MASK]&#39;, 4: &#39;O&#39;, 5: &#39;인명&#39;, 6: &#39;기타 수량표현&#39;, 7: &#39;기타&#39;, 8: &#39;기관명&#39;, 9: &#39;날짜&#39;, 10: &#39;지명&#39;, 11: &#39;통화&#39;, 12: &#39;비율&#39;, 13: &#39;시간&#39;, 14: &#39;기간&#39;, 15: &#39;인명&#39;, 16: &#39;기타 수량표현&#39;, 17: &#39;기타&#39;, 18: &#39;기관명&#39;, 19: &#39;날짜&#39;, 20: &#39;지명&#39;, 21: &#39;통화&#39;, 22: &#39;비율&#39;, 23: &#39;시간&#39;, 24: &#39;기간&#39;} . 정수 인덱스를 레이블에 매핑하는 사전을 만듭니다. . def inference_fn(sentence): inputs = tokenizer( [sentence], max_length = args.mex_seq_length, padding = &#39;max_length&#39;, truncation = True, ) with torch.no_grad(): outputs = model(**{k : torch.tenser(v) for k, v in inputs.items()}), probs = outputs.logits[0].softmax(dim = 1), # 로짓에 소프트맥스를 취해 토큰이 어떤 개체명에 들어가나 확률 구하기. top_probs, preds = torch.topk(probs, dim = 1, k = 1), # 가장 높은 확률 값과 그 개체 인덱스 구하기 tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0]), # 토큰화된 것을 다시 읽을 수 있는 문자(한국어)로 복원 predicted_tags = [id_to_label[pred.item()] for pred in preds] # 개체 인덱스를 전에 라벨링한 문자로 바꿔주기.(PER =&gt; 인명) result = [] for token, predicted_tag, top_prob in zip(tokens, predicted_tags, top_probs): if token not in [tokenizer.pad_token, tokenizer.cls_token,tokenizer.seq_token]: # 이프문은 CLS, SEP, PAD를 제외하는 역활을 함. token_result = { &#39;token&#39;: token, # 단어 &#39;predicted_tag&#39;: predicted_tag, # 개체명 &#39;top_prob&#39; : str(round(top_prob[0].item(), 4)), # 그 확률 } result.append(token_result) return{&#39;sentence&#39; : sentence, &#39;result&#39; : result,} . 실제 입력값이 들어왓을때 처리 과정에 대한 함수입니다. 코드마다 설명을 상세히 했습니다. . from ratsnlp.nlpbook.ner import get_web_service_app app = get_web_service_app(inference_fn) app.run() . * Serving Flask app &#34;ratsnlp.nlpbook.ner.deploy&#34; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off . * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) . * Running on http://3f00-35-202-140-61.ngrok.io * Traffic stats available on http://127.0.0.1:4040 . 127.0.0.1 - - [24/Dec/2021 14:57:07] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [24/Dec/2021 14:57:07] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - . 웹 서비스로 구현합니다. . (이부분은 앞코드와 마찬가지로 따로 더 공부해야 오류없이 구동될것 같아요. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/24/Do_natural_language5.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/24/Do_natural_language5.html",
            "date": " • Dec 24, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "[시계열분석] 1. 시계열의 탐색적 자료분석",
            "content": ". EuStockMarkets &#45936;&#51060;&#53552;&#50640; &#45824;&#54644; . head(EuStockMarkets) . A matrix: 6 × 4 of type dbl DAXSMICACFTSE . 1628.75 | 1678.1 | 1772.8 | 2443.6 | . 1613.63 | 1688.5 | 1750.5 | 2460.2 | . 1606.51 | 1678.6 | 1718.0 | 2448.2 | . 1621.04 | 1684.1 | 1708.1 | 2470.4 | . 1618.16 | 1686.6 | 1723.1 | 2484.7 | . 1610.61 | 1671.6 | 1714.3 | 2466.8 | . EuStockMarkets은 1991년부터 1998년까지 유럽 4대 주가지수 일일 종가를 나타냅니다. . 이 데이터는 형식화 및 샘플링이 잘 되어 있어 이상치 조정 없이 바로 EDA가 가능합니다. . plot(EuStockMarkets) . 각 값을 개별 그래프로 그렸습니다. . class(EuStockMarkets) . &lt;ol class=list-inline&gt;&#39;mts&#39; | &#39;ts&#39; | &#39;matrix&#39; | &lt;/ol&gt; 다양한 시계열을 동시에 다루는 mts 객체를 사용합니다. . (ts는 단일 시계열을 다루는 객체입니다.) . ts &#44061;&#52404;&#51032; &#54632;&#49688; . frequency(EuStockMarkets) . 260 데이터의 연간 빈도를 출력하는 frequency 함수 입니다. . start(EuStockMarkets) . &lt;ol class=list-inline&gt;1991 | 130 | &lt;/ol&gt; end(EuStockMarkets) . &lt;ol class=list-inline&gt;1998 | 169 | &lt;/ol&gt; 데이터의 처음과 마지막 시간을 알아내는 start, end 함수 입니다. . window(EuStockMarkets, start = 1997, end = 1998) . A Time Series: 261 × 4 DAXSMICACFTSE . 1997.0002844.09 | 3869.8 | 2289.6 | 4092.5 | . 1997.0042844.09 | 3869.8 | 2289.6 | 4092.5 | . 1997.0082844.09 | 3869.8 | 2303.8 | 4092.5 | . 1997.0122859.22 | 3922.2 | 2307.0 | 4091.0 | . 1997.0152880.07 | 3948.3 | 2318.6 | 4115.7 | . 1997.0192880.07 | 3942.2 | 2315.7 | 4118.5 | . 1997.0232880.07 | 3942.2 | 2315.7 | 4118.5 | . 1997.0272820.81 | 3942.2 | 2257.0 | 4057.4 | . 1997.0312863.26 | 3940.1 | 2282.8 | 4089.5 | . 1997.0352890.20 | 3923.8 | 2306.7 | 4106.5 | . 1997.0382876.34 | 3922.9 | 2301.7 | 4078.8 | . 1997.0422904.08 | 3944.9 | 2331.6 | 4087.5 | . 1997.0462936.69 | 3966.2 | 2349.1 | 4087.0 | . 1997.0502915.81 | 3947.4 | 2327.5 | 4056.6 | . 1997.0542956.78 | 3975.5 | 2361.3 | 4107.3 | . 1997.0582978.84 | 3983.6 | 2402.1 | 4168.2 | . 1997.0622976.56 | 3979.6 | 2388.0 | 4158.9 | . 1997.0652996.12 | 4007.1 | 2407.8 | 4197.5 | . 1997.0693006.87 | 4019.9 | 2425.1 | 4207.7 | . 1997.0732999.19 | 4009.5 | 2406.1 | 4194.0 | . 1997.0773000.66 | 4023.1 | 2409.9 | 4195.5 | . 1997.0813026.63 | 4115.4 | 2442.5 | 4219.1 | . 1997.0853037.28 | 4161.0 | 2461.3 | 4219.1 | . 1997.0882982.63 | 4125.5 | 2430.3 | 4218.8 | . 1997.0922992.55 | 4127.3 | 2435.2 | 4212.0 | . 1997.0963028.27 | 4182.3 | 2482.8 | 4237.4 | . 1997.1002997.95 | 4169.7 | 2465.0 | 4207.5 | . 1997.1043018.58 | 4209.1 | 2503.1 | 4228.4 | . 1997.1083037.70 | 4272.2 | 2516.6 | 4275.8 | . 1997.1123064.70 | 4282.8 | 2508.6 | 4257.8 | . 1997.1153067.48 | 4296.5 | 2503.1 | 4260.9 | . 1997.1193114.73 | 4305.5 | 2541.3 | 4281.5 | . 1997.1233124.78 | 4309.8 | 2558.4 | 4265.9 | . 1997.1273161.36 | 4357.9 | 2597.5 | 4307.8 | . 1997.1313185.72 | 4384.3 | 2595.4 | 4307.7 | . 1997.1353191.45 | 4408.4 | 2582.1 | 4304.3 | . 1997.1383211.01 | 4444.1 | 2599.3 | 4304.3 | . 1997.1423256.86 | 4436.3 | 2628.4 | 4327.1 | . 1997.1463249.17 | 4464.2 | 2627.4 | 4341.0 | . 1997.1503260.30 | 4514.6 | 2634.5 | 4337.8 | . 1997.1543230.83 | 4490.7 | 2617.5 | 4332.3 | . 1997.1583209.04 | 4525.5 | 2594.8 | 4357.4 | . 1997.1623197.09 | 4530.8 | 2575.2 | 4356.1 | . 1997.1653203.79 | 4522.5 | 2562.8 | 4336.8 | . 1997.1693180.63 | 4463.2 | 2567.9 | 4331.1 | . 1997.1733233.34 | 4503.9 | 2607.7 | 4344.7 | . 1997.1773245.02 | 4539.0 | 2602.2 | 4329.3 | . 1997.1813272.58 | 4519.7 | 2629.4 | 4339.2 | . 1997.1853261.04 | 4487.6 | 2607.8 | 4308.3 | . 1997.1883258.74 | 4460.1 | 2600.3 | 4307.1 | . 1997.1923345.09 | 4513.7 | 2651.7 | 4357.7 | . 1997.1963375.45 | 4547.1 | 2666.2 | 4360.1 | . 1997.2003396.55 | 4605.2 | 2698.9 | 4399.3 | . 1997.2043419.51 | 4638.9 | 2708.3 | 4420.3 | . 1997.2083426.77 | 4684.4 | 2709.2 | 4437.4 | . 1997.2123430.95 | 4677.1 | 2686.2 | 4444.3 | . 1997.2153382.40 | 4676.2 | 2641.7 | 4422.5 | . 1997.2193367.82 | 4609.9 | 2632.1 | 4397.7 | . 1997.2233404.29 | 4636.2 | 2645.6 | 4424.3 | . 1997.2273337.11 | 4556.5 | 2588.4 | 4373.3 | . 1997.2313289.59 | 4519.9 | 2574.0 | 4356.8 | . 1997.2353305.72 | 4535.1 | 2596.8 | 4332.2 | . 1997.2383247.03 | 4442.9 | 2553.7 | 4258.1 | . 1997.2423288.52 | 4491.3 | 2587.1 | 4254.8 | . 1997.2463302.57 | 4497.3 | 2579.3 | 4214.8 | . 1997.2503374.93 | 4558.6 | 2624.3 | 4270.7 | . 1997.2543439.22 | 4620.5 | 2648.7 | 4301.5 | . 1997.2583407.83 | 4659.2 | 2656.7 | 4312.9 | . 1997.2623407.83 | 4659.2 | 2656.7 | 4312.9 | . 1997.2653407.83 | 4659.2 | 2656.7 | 4312.9 | . 1997.2693281.46 | 4501.7 | 2581.8 | 4248.1 | . 1997.2733210.94 | 4488.7 | 2530.3 | 4236.6 | . 1997.2773212.82 | 4463.9 | 2514.5 | 4214.6 | . 1997.2813235.35 | 4471.5 | 2518.0 | 4236.6 | . 1997.2853342.77 | 4588.0 | 2572.3 | 4271.7 | . 1997.2883328.13 | 4582.6 | 2579.0 | 4269.3 | . 1997.2923364.76 | 4634.9 | 2617.6 | 4292.3 | . 1997.2963352.58 | 4626.6 | 2608.0 | 4313.2 | . 1997.3003319.24 | 4604.2 | 2574.6 | 4270.7 | . 1997.3043297.52 | 4586.3 | 2566.1 | 4251.7 | . 1997.3083369.26 | 4643.4 | 2620.6 | 4286.8 | . 1997.3123347.54 | 4625.6 | 2621.0 | 4294.6 | . 1997.3153361.80 | 4665.7 | 2615.2 | 4298.9 | . 1997.3193361.20 | 4699.1 | 2547.6 | 4310.5 | . 1997.3233328.41 | 4740.1 | 2522.7 | 4328.7 | . 1997.3273348.90 | 4752.3 | 2514.7 | 4346.1 | . 1997.3313366.87 | 4781.1 | 2533.6 | 4387.7 | . 1997.3353396.49 | 4836.1 | 2539.8 | 4388.5 | . 1997.3383357.57 | 4772.3 | 2536.3 | 4369.7 | . 1997.3423372.96 | 4793.3 | 2550.3 | 4389.7 | . 1997.3463425.86 | 4855.1 | 2602.9 | 4433.2 | . 1997.3503438.09 | 4897.6 | 2639.5 | 4436.0 | . 1997.3543438.09 | 4897.6 | 2639.5 | 4445.0 | . 1997.3583491.08 | 4953.5 | 2655.3 | 4455.6 | . 1997.3623565.69 | 5029.6 | 2672.8 | 4455.6 | . 1997.3653548.52 | 4988.4 | 2651.9 | 4519.3 | . 1997.3693537.45 | 5016.0 | 2643.3 | 4537.5 | . 1997.3733537.45 | 5016.0 | 2643.3 | 4580.4 | . 1997.3773533.21 | 5004.7 | 2633.9 | 4630.9 | . 1997.3813593.14 | 5042.5 | 2693.1 | 4669.6 | . 1997.3853559.29 | 5084.2 | 2719.6 | 4691.0 | . 1997.3883588.57 | 5134.3 | 2774.6 | 4686.9 | . 1997.3923564.85 | 5141.7 | 2776.0 | 4681.2 | . 1997.3963569.26 | 5157.5 | 2784.3 | 4693.9 | . 1997.4003569.26 | 5157.5 | 2784.3 | 4645.2 | . 1997.4043516.20 | 5081.0 | 2751.1 | 4607.5 | . 1997.4083600.40 | 5178.6 | 2786.4 | 4642.0 | . 1997.4123575.44 | 5176.4 | 2741.7 | 4651.8 | . 1997.4153621.72 | 5181.0 | 2762.9 | 4661.8 | . 1997.4193669.31 | 5196.7 | 2654.7 | 4661.8 | . 1997.4233665.43 | 5190.0 | 2680.3 | 4681.6 | . 1997.4273626.60 | 5133.1 | 2583.2 | 4677.5 | . 1997.4313635.38 | 5132.1 | 2579.2 | 4672.3 | . 1997.4353562.73 | 5041.6 | 2583.9 | 4621.3 | . 1997.4383596.40 | 5150.0 | 2601.5 | 4562.8 | . 1997.4423655.59 | 5207.2 | 2624.5 | 4557.8 | . 1997.4463651.59 | 5238.5 | 2635.4 | 4557.1 | . 1997.4503684.60 | 5251.2 | 2690.9 | 4576.2 | . 1997.4543700.53 | 5320.0 | 2719.3 | 4645.0 | . 1997.4583668.61 | 5368.8 | 2686.2 | 4686.7 | . 1997.4623671.16 | 5361.9 | 2664.2 | 4739.6 | . 1997.4653671.87 | 5308.6 | 2696.2 | 4724.8 | . 1997.4693737.16 | 5364.2 | 2760.3 | 4757.4 | . 1997.4733752.37 | 5384.6 | 2808.5 | 4783.1 | . 1997.4773750.02 | 5362.0 | 2795.9 | 4745.1 | . 1997.4813721.18 | 5345.9 | 2762.6 | 4682.2 | . 1997.4853730.56 | 5405.0 | 2751.7 | 4657.0 | . 1997.4883777.56 | 5510.3 | 2739.7 | 4653.7 | . 1997.4923788.54 | 5561.8 | 2757.1 | 4593.9 | . 1997.4963748.79 | 5587.8 | 2762.2 | 4575.8 | . 1997.5003761.07 | 5576.1 | 2784.8 | 4596.3 | . 1997.5043819.52 | 5662.4 | 2867.4 | 4640.0 | . 1997.5083820.16 | 5669.9 | 2893.6 | 4657.9 | . 1997.5123809.92 | 5700.3 | 2891.0 | 4640.3 | . 1997.5153766.89 | 5620.6 | 2858.3 | 4604.6 | . 1997.5193834.84 | 5654.8 | 2944.0 | 4728.3 | . 1997.5233867.53 | 5674.3 | 2909.5 | 4751.4 | . 1997.5273939.73 | 5804.9 | 2937.0 | 4831.7 | . 1997.5313946.73 | 5846.5 | 2934.5 | 4812.8 | . 1997.5354003.35 | 5947.0 | 2947.7 | 4810.7 | . 1997.5384030.10 | 6012.6 | 2929.8 | 4758.5 | . 1997.5424026.97 | 5977.1 | 2950.6 | 4762.4 | . 1997.5464000.65 | 5885.4 | 2929.1 | 4767.8 | . 1997.5504074.30 | 5801.5 | 2941.6 | 4799.5 | . 1997.5544142.19 | 5845.8 | 2941.6 | 4857.4 | . 1997.5584139.68 | 5844.7 | 2950.7 | 4899.3 | . 1997.5624223.69 | 5927.5 | 2988.0 | 4964.2 | . 1997.5654203.91 | 5868.3 | 2958.6 | 4949.0 | . 1997.5694131.94 | 5737.1 | 2876.7 | 4877.2 | . 1997.5734139.96 | 5620.5 | 2874.1 | 4805.7 | . 1997.5774297.64 | 5677.1 | 2921.1 | 4846.7 | . 1997.5814384.82 | 5869.9 | 3003.5 | 4874.5 | . 1997.5854320.52 | 5849.2 | 2973.5 | 4862.9 | . 1997.5884368.54 | 5847.0 | 3025.9 | 4851.5 | . 1997.5924400.30 | 5888.0 | 3022.2 | 4862.6 | . 1997.5964377.70 | 5842.1 | 3023.6 | 4876.6 | . 1997.6004458.66 | 5929.5 | 3069.3 | 4927.3 | . 1997.6044405.52 | 5898.2 | 3075.7 | 4907.5 | . 1997.6084336.98 | 5898.2 | 3049.5 | 4899.3 | . 1997.6124302.50 | 5771.0 | 2992.4 | 4895.7 | . 1997.6154325.86 | 5765.2 | 2984.1 | 4960.6 | . 1997.6194364.25 | 5812.1 | 3037.1 | 5026.2 | . 1997.6234428.08 | 5922.1 | 3056.3 | 5086.8 | . 1997.6274342.31 | 5864.8 | 2996.3 | 5031.3 | . 1997.6314333.15 | 5825.6 | 2983.4 | 5031.9 | . 1997.6354377.51 | 5808.4 | 2998.6 | 5075.8 | . 1997.6384237.06 | 5682.1 | 2924.0 | 5003.6 | . 1997.6424195.53 | 5579.5 | 2921.8 | 4991.3 | . 1997.6464077.59 | 5498.5 | 2921.8 | 4865.8 | . 1997.6504080.55 | 5405.6 | 2870.1 | 4835.0 | . 1997.6544190.45 | 5580.1 | 2936.2 | 4914.2 | . 1997.6584251.93 | 5690.1 | 2979.3 | 4958.4 | . 1997.6624204.81 | 5668.8 | 2957.2 | 4978.0 | . 1997.6654090.14 | 5475.8 | 2904.2 | 4901.1 | . 1997.6694076.75 | 5473.9 | 2898.6 | 4901.1 | . 1997.6733993.70 | 5363.3 | 2869.3 | 4886.3 | . 1997.6773992.03 | 5409.6 | 2871.7 | 4906.9 | . 1997.6813897.43 | 5217.3 | 2828.4 | 4845.4 | . 1997.6853919.79 | 5216.7 | 2770.5 | 4817.5 | . 1997.6884001.81 | 5271.5 | 2805.8 | 4870.2 | . 1997.6924127.28 | 5447.5 | 2921.2 | 4952.2 | . 1997.6964062.13 | 5478.6 | 2918.0 | 4976.9 | . 1997.7004093.43 | 5478.1 | 2927.0 | 4991.3 | . 1997.7044073.71 | 5532.9 | 2924.5 | 4994.2 | . 1997.7084131.26 | 5505.3 | 2940.9 | 4985.2 | . 1997.7124104.57 | 5445.1 | 2919.7 | 4950.5 | . 1997.7154028.00 | 5356.7 | 2874.6 | 4905.2 | . 1997.7193890.24 | 5280.8 | 2843.6 | 4854.8 | . 1997.7233796.61 | 5281.9 | 2834.1 | 4848.2 | . 1997.7273869.53 | 5321.7 | 2898.6 | 4902.9 | . 1997.7313995.69 | 5417.8 | 2940.6 | 4976.4 | . 1997.7353970.44 | 5550.4 | 2944.0 | 5013.1 | . 1997.7384004.04 | 5629.0 | 2978.4 | 5046.2 | . 1997.7423983.06 | 5611.0 | 2977.2 | 5023.8 | . 1997.7464096.85 | 5705.1 | 3017.5 | 5075.7 | . 1997.7504091.77 | 5730.4 | 2997.2 | 5027.5 | . 1997.7544150.95 | 5732.5 | 3023.7 | 5077.2 | . 1997.7584104.93 | 5667.1 | 3005.4 | 5065.5 | . 1997.7624135.09 | 5716.6 | 2985.6 | 5226.3 | . 1997.7654116.52 | 5691.8 | 2989.0 | 5220.3 | . 1997.7694154.89 | 5673.6 | 3008.3 | 5244.2 | . 1997.7734262.98 | 5754.7 | 3054.9 | 5317.1 | . 1997.7774266.17 | 5825.0 | 3052.1 | 5296.1 | . 1997.7814266.17 | 5929.0 | 3094.0 | 5330.8 | . 1997.7854326.35 | 5897.4 | 3078.0 | 5300.0 | . 1997.7884311.13 | 5846.9 | 3064.4 | 5305.6 | . 1997.7924267.40 | 5822.3 | 3024.1 | 5262.1 | . 1997.7964179.92 | 5732.2 | 2960.7 | 5217.8 | . 1997.8004164.62 | 5699.5 | 2955.1 | 5227.3 | . 1997.8044225.27 | 5792.8 | 3002.9 | 5300.1 | . 1997.8084215.23 | 5836.3 | 3002.5 | 5298.9 | . 1997.8124168.62 | 5815.9 | 2992.2 | 5263.7 | . 1997.8154149.92 | 5806.8 | 2992.9 | 5287.9 | . 1997.8194049.16 | 5751.6 | 2958.0 | 5271.1 | . 1997.8234069.25 | 5777.2 | 2946.7 | 5211.0 | . 1997.8274172.47 | 5862.9 | 2989.9 | 5225.9 | . 1997.8314124.86 | 5803.2 | 2958.1 | 5148.8 | . 1997.8353976.38 | 5651.8 | 2856.9 | 4991.5 | . 1997.8383981.44 | 5689.5 | 2849.0 | 4970.2 | . 1997.8423871.39 | 5533.5 | 2769.6 | 4840.7 | . 1997.8463645.69 | 5279.7 | 2651.3 | 4755.4 | . 1997.8503806.66 | 5479.0 | 2818.0 | 4871.8 | . 1997.8543748.88 | 5370.9 | 2739.5 | 4801.9 | . 1997.8583753.66 | 5467.2 | 2739.3 | 4842.3 | . 1997.8623847.73 | 5581.6 | 2788.0 | 4906.4 | . 1997.8653784.80 | 5538.2 | 2774.9 | 4897.4 | . 1997.8693841.39 | 5601.6 | 2822.4 | 4908.3 | . 1997.8733813.88 | 5557.4 | 2781.8 | 4863.8 | . 1997.8773715.38 | 5438.6 | 2707.1 | 4764.3 | . 1997.8813728.37 | 5459.7 | 2707.1 | 4806.8 | . 1997.8853734.79 | 5483.9 | 2707.1 | 4793.7 | . 1997.8883697.48 | 5434.0 | 2694.5 | 4720.4 | . 1997.8923701.94 | 5418.2 | 2700.7 | 4711.0 | . 1997.8963676.65 | 5437.0 | 2698.9 | 4741.8 | . 1997.9003816.71 | 5565.0 | 2773.0 | 4867.0 | . 1997.9043844.14 | 5574.2 | 2782.6 | 4845.4 | . 1997.9083876.90 | 5571.7 | 2790.6 | 4830.1 | . 1997.9123931.81 | 5650.4 | 2821.2 | 4908.4 | . 1997.9153941.91 | 5725.5 | 2861.7 | 4985.8 | . 1997.9193832.10 | 5645.7 | 2802.5 | 4898.6 | . 1997.9233850.14 | 5666.3 | 2786.3 | 4863.5 | . 1997.9273926.93 | 5738.3 | 2811.7 | 4891.2 | . 1997.9313961.97 | 5772.4 | 2829.0 | 4889.0 | . 1997.9353972.08 | 5775.9 | 2854.4 | 4831.8 | . 1997.9384125.92 | 5875.1 | 2918.5 | 4921.8 | . 1997.9424096.40 | 5919.9 | 2913.1 | 4977.6 | . 1997.9464074.55 | 5922.7 | 2902.4 | 4970.7 | . 1997.9504159.72 | 5969.5 | 2914.5 | 5082.3 | . 1997.9544191.81 | 6009.0 | 2910.1 | 5142.9 | . 1997.9584208.14 | 6095.3 | 2932.5 | 5187.4 | . 1997.9624187.13 | 6103.2 | 2959.4 | 5177.1 | . 1997.9654116.70 | 6056.6 | 2932.2 | 5130.7 | . 1997.9694016.70 | 6021.8 | 2828.5 | 5035.9 | . 1997.9734061.91 | 6018.7 | 2830.3 | 5045.2 | . 1997.9774029.08 | 5986.6 | 2838.3 | 5121.8 | . 1997.9814150.31 | 6092.7 | 2912.2 | 5203.4 | . 1997.9854154.57 | 6122.1 | 2893.3 | 5190.8 | . 1997.9884162.92 | 6115.1 | 2894.5 | 5168.3 | . 1997.9924055.35 | 5989.9 | 2822.9 | 5020.2 | . 1997.9964125.54 | 6049.3 | 2869.7 | 5018.2 | . 1998.0004132.79 | 6044.7 | 2858.1 | 5049.8 | . 데이터에서 시간의 한 부분 범위를 얻을 수 있는 window 함수 입니다. . &#55176;&#49828;&#53664;&#44536;&#47016; . hist(EuStockMarkets[,&#39;SMI&#39;], 30) . 데이터의 히스토그램입니다. 특별한 인사이트가 없습니다. . hist(diff(EuStockMarkets[,&#39;SMI&#39;], 30)) . 인접한 데이터간 차이의 히스토그램 입니다. 정규분포와 유사한 모양을 띕니다. . 주가지수가 상승추세를 보이므로 차분(diff)은 양수쪽으로 미세하게 치우쳐 있습니다. . &#49328;&#51216;&#46020; . plot(EuStockMarkets[,&#39;SMI&#39;], EuStockMarkets[,&#39;DAX&#39;]) . 시간에 따른 두 주식의 가치를 보여주는 산점도 입니다. 강한 양의 상관관계를 보이네요. . plot(diff(EuStockMarkets[,&#39;SMI&#39;]), diff(EuStockMarkets[,&#39;DAX&#39;])) . 두 주식의 일일가치 변동을 보여주는 산점도입니다. 위 그림과 다르게 상관관계가 강하지 않습니다. . plot(lag(diff(EuStockMarkets[,&#39;SMI&#39;]),1), diff(EuStockMarkets[,&#39;DAX&#39;])) . 두 주식의 일일가치 변동이 참상관관계라도 실제로 적용시키는덴 무리가 있습니다. . 주식의 미래 가격을 알고 싶은건데, 같은 미래에 다른 주식의 가격하고 상관관계가 있는것은 의미가 없습니다. . 그보다 한 시점 전 타 주식 가격과 미래 주식 가격간의 연관이 있어야 유의미합니다. . lag 함수를 이용해서 한 주가를 1시점 앞으로 당기고 산점도를 구했습니다. . 두 주가의 상관관계가 사라졌다는 것을 산점도를 보고 알 수 있습니다. . &#47204;&#47553; &#50952;&#46020; . x &lt;- rnorm(n = 100, mean = 0, sd = 10) + 1:100 mn &lt;- function(n) rep(1/n, n) plot(x, type = &#39;l&#39;, lwd = 1) lines(filter(x, mn(5)), col = 2, lwd = 3, lty = 2) lines(filter(x, mn(50)), col = 3, lwd = 3, lty = 3) . filter(x, mn(5)) . A Time Series: &lt;ol class=list-inline&gt;&lt;NA&gt; | &lt;NA&gt; | -1.77288105461343 | 1.31992176123301 | 1.73981260314922 | 8.40487565556649 | 5.3748710170368 | 10.1065230240359 | 12.7346567822176 | 13.5586688980719 | 12.6921098544628 | 13.8435662061596 | 13.9730144970645 | 9.35900906311078 | 13.758956613448 | 15.6427630176612 | 20.4817849516833 | 22.3086556019964 | 25.3660016459804 | 22.1569653798478 | 25.1374316930229 | 18.9960252654437 | 19.6145257811429 | 19.2015080642112 | 24.7274005432306 | 21.6801410945616 | 28.8134751920388 | 30.3806401765035 | 30.2485143964029 | 26.9003444995948 | 28.4214160008667 | 30.6441878792136 | 28.7817189385494 | 32.4909977045378 | 31.6538379235585 | 30.4489153481993 | 28.5446624272632 | 31.6681220929798 | 37.6314633176833 | 38.4760566106296 | 45.8738959745044 | 45.4223532971025 | 47.5731596337499 | 47.2963569945348 | 47.7022098006225 | 46.3430642214953 | 47.909777607114 | 47.0691900552067 | 42.8364536442156 | 49.3877148972977 | 47.9736397149515 | 53.4000401255189 | 54.6497226664114 | 60.2655379215343 | 55.4698041274768 | 58.9018820051029 | 58.2373643951854 | 56.2450939372977 | 53.7826512222036 | 56.4110597160915 | 58.4204570491475 | 59.7460187553621 | 63.6126175676387 | 62.3517723657575 | 69.1501339271387 | 68.8062407224709 | 70.0341269606641 | 73.2089209563275 | 76.1726286991414 | 73.6915749387865 | 76.9208647715888 | 75.1541185829742 | 76.9942252691702 | 76.4561897047353 | 77.6567117512646 | 76.6029212106872 | 83.9077431791539 | 78.3277983013414 | 83.3466522428901 | 85.7994713218054 | 85.9370853983274 | 82.4936728886608 | 82.9567421627858 | 81.4989001340583 | 81.3373987045914 | 83.8820192148659 | 84.8431049407526 | 90.0661772264516 | 93.8484169700886 | 94.3840506066453 | 96.8521833747149 | 96.1322902206609 | 94.8408075569562 | 94.8032051094727 | 100.759551321651 | 101.59154547457 | 102.648280935003 | 106.438834514587 | &lt;NA&gt; | &lt;NA&gt; | &lt;/ol&gt; filter 함수를 이용해 이동평균 값을 구해서 그래프를 그렸습니다. . install.packages(&#39;zoo&#39;) library(zoo) f1 &lt;- rollapply(zoo(x), 20, function(w) min(w), align = &#39;left&#39;, partial = TRUE) f2 &lt;- rollapply(zoo(x), 20, function(w) min(w), align = &#39;right&#39;, partial = TRUE) plot(x, lwd =1, type = &#39;l&#39;) lines(f1, col = 2, lwd = 3, lty = 2) lines(f2, col = 3, lwd = 3, lty = 3) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . fitter 함수는 일차 선형 변환에 기반하므로 일차결합하지 않은 함수는 사용을 할 수 없습니다. . 그래서 zoo 패키지의 rollapply 함수를 사용했는데요. 사용형태로 rollapply(데이터, 원도 크기, 적용함수, 함수 적용 방향) 입니다. . 이때 ts 객체는 균등한 간격의 시계열을 가정하나 zoo 객체는 타임스테프를 색인 속성으로 저장하기에 주기적인 시계열을 요구하지 않습니다. . plot(x, type = &#39;l&#39;, lwd = 1) lines(cummax(x), col = 2, lwd = 3, lty = 2) # 최댓값 lines(cumsum(x)/1:length(x), col = 3, lwd = 3, lty = 3) # 평균 . 확장 윈도로 cummax, cumsum 함수를 사용했습니다. . plot(x, lwd =1, type = &#39;l&#39;) lines(rollapply(zoo(x), seq_along(x), function(w) max(w), partial = TRUE, align = &#39;right&#39;), col = 2, lwd = 3, lty = 2) lines(rollapply(zoo(x), seq_along(x), function(w) mean(w), partial = TRUE, align = &#39;right&#39;), col = 3, lwd = 3, lty = 3) . 윗 그림과 동일한 그림을 출력합니다. 이번엔 rollapply 함수를 사용했죠. . &#51088;&#44592; &#49345;&#44288; . x &lt;- 1:100 y &lt;- sin(x * pi / 3) plot(y, type = &#39;b&#39;) acf(y) . acf란 자기상관계수로 위 그림에서 x축인 Lag 값은 떨어진 정도를 의미합니다. . 더 자세히 설명하면 자기상관계수란 cor(Yt, Y(t-k))를 의미하는데 이때 k가 Lag 입니다. . install.packages(&#39;data.table&#39;) library(data.table) cor(y, shift(y,1), use = &#39;pairwise.complete.obs&#39;) cor(y, shift(y,2), use = &#39;pairwise.complete.obs&#39;) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . 0.500153115912332 -0.503715197153103 자기 상관 계수를 구하기 위해 data.table 패키지 내 shift 함수를 사용했습니다. . y &lt;- sin(x * pi / 3) plot(y[1:30], type = &#39;b&#39;) pacf(y) . pacf은 편자기상관 함수로 cor(et, e(t-k))을 나타냅니다. 즉 설명된 부분을 제외한 잔차의 상관계수를 나타내죠. . 다시 말해 k-1까지 설명된 정보 이외에 k번째 이전 변수가 현재 변수에 설명되는 것이 있는지를 나타낸 값이라고 할 수 있습니다. . 사인함수와 같이 일정 주기가 있는 경우 Lag(k) 값이 커질 수록 PACF 값이 0에 수렴합니다. . 다시말해 ACF와 달리 PACF는 불필요한 중복관계를 제거하는 역할도 합니다. . y1 &lt;- sin(x * pi / 3) plot(y1, type = &#39;b&#39;) acf(y1) pacf(y1) . y2 &lt;- sin(x * pi / 10) plot(y2, type = &#39;b&#39;) acf(y2) pacf(y2) . ACF와 PACF를 비교한 그래프들입니다. . 정상 데이터의 ACF는 빠르게 0으로 수렴해야합니다. 하지만 주기함수에서 ACF는 그렇지 못한 모습이죠. . y3 &lt;- y1 + y2 plot(y3, type = &#39;b&#39;) acf(y3) pacf(y3) . 두 계열을 더한 데이터의 ACF와 PACF를 구했습니다. . ACF는 앞서 구한 두 ACF를 합한 것과 같습니다. 반면 PACF는 단순히 합친것은 아닙니다. . noise1 &lt;- rnorm(100, sd = 0.05) noise2 &lt;- rnorm(100, sd = 0.05) y1 &lt;- y1 + noise1 y2 &lt;- y2 + noise2 y &lt;- y1 + y2 plot(y1, type = &#39;b&#39;) acf(y1) pacf(y1) . plot(y2, type = &#39;b&#39;) acf(y2) pacf(y2) . plot(y, type = &#39;b&#39;) acf(y) pacf(y) . 노이즈를 조금 추가했는데, 노이즈가 없는 것과 비슷한 결과가 나옵니다. . &#49884;&#44033;&#54868; . AirPassengers . A Time Series: 12 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1949112 | 118 | 132 | 129 | 121 | 135 | 148 | 148 | 136 | 119 | 104 | 118 | . 1950115 | 126 | 141 | 135 | 125 | 149 | 170 | 170 | 158 | 133 | 114 | 140 | . 1951145 | 150 | 178 | 163 | 172 | 178 | 199 | 199 | 184 | 162 | 146 | 166 | . 1952171 | 180 | 193 | 181 | 183 | 218 | 230 | 242 | 209 | 191 | 172 | 194 | . 1953196 | 196 | 236 | 235 | 229 | 243 | 264 | 272 | 237 | 211 | 180 | 201 | . 1954204 | 188 | 235 | 227 | 234 | 264 | 302 | 293 | 259 | 229 | 203 | 229 | . 1955242 | 233 | 267 | 269 | 270 | 315 | 364 | 347 | 312 | 274 | 237 | 278 | . 1956284 | 277 | 317 | 313 | 318 | 374 | 413 | 405 | 355 | 306 | 271 | 306 | . 1957315 | 301 | 356 | 348 | 355 | 422 | 465 | 467 | 404 | 347 | 305 | 336 | . 1958340 | 318 | 362 | 348 | 363 | 435 | 491 | 505 | 404 | 359 | 310 | 337 | . 1959360 | 342 | 406 | 396 | 420 | 472 | 548 | 559 | 463 | 407 | 362 | 405 | . 1960417 | 391 | 419 | 461 | 472 | 535 | 622 | 606 | 508 | 461 | 390 | 432 | . colors &lt;- c(&#39;green&#39;, &#39;red&#39;, &#39;pink&#39;, &#39;blue&#39;,&#39;yellow&#39;, &#39;lightsalmon&#39;, &#39;black&#39;, &#39;grey&#39;, &#39;cyan&#39;, &#39;lightblue&#39;, &#39;maroon&#39;, &#39;purple&#39;) matplot(matrix(AirPassengers, nrow = 12, ncol = 12), type = &#39;l&#39;, col = colors, lty = 1, lwd = 2.5, xaxt = &#39;n&#39;) legend(&#39;topleft&#39;, legend = 1949:1960, col = colors) . AirPassengers 자료를 시각화 했습니다. . install.packages(&#39;forecast&#39;) library(forecast) seasonplot(AirPassengers) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) also installing the dependencies ‘xts’, ‘TTR’, ‘quadprog’, ‘quantmod’, ‘fracdiff’, ‘lmtest’, ‘timeDate’, ‘tseries’, ‘urca’, ‘RcppArmadillo’ Registered S3 method overwritten by &#39;quantmod&#39;: method from as.zoo.data.frame zoo . forecast 패키지 내 seasonplot 함수로 비슷한 그림을 그렸습니다. . matplot(t(matrix(AirPassengers, nrow = 12, ncol = 12)), type = &#39;l&#39;, col = colors, lty = 1, lwd = 2.5, xaxt = &#39;n&#39;) . 이 그래프는 앞 그래프와 다르게 연도 시계열을 나타냅니다. . monthplot(AirPassengers) . 달 별로 값을 시각화 해주는 monthplot 함수를 사용했습니다. . &#45712;&#45184;&#51216; . 시계열 데이터를 분석하기 전 탐색적으로 자료분석 하는 여러가지 시각에 대해 공부했습니다. . 그 중 롤링 윈도와 자기 상관 부분이 앞으로 시계열 공부하는데 중요할 것 같네요. . 앞으로 공부할 내용이 더욱 기대됩니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/timeseries/r/eda/2021/12/23/TimeSeries1.html",
            "relUrl": "/book/jupyter/timeseries/r/eda/2021/12/23/TimeSeries1.html",
            "date": " • Dec 23, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "[Do it 자연어] 4. 문장 쌍 분류하기 + 웹 실습",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (0.0.9999) Requirement already satisfied: Korpora&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0) Requirement already satisfied: flask-ngrok&gt;=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25) Requirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Requirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4) Requirement already satisfied: flask-cors&gt;=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10) Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Requirement already satisfied: future&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.18.2) Requirement already satisfied: PyYAML&lt;=5.4.1,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (5.4.1) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.3.0) Requirement already satisfied: fsspec[http]&gt;=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2021.11.1) Requirement already satisfied: torchmetrics&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.2) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Requirement already satisfied: huggingface-hub&gt;=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.2.1) Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.0.46) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.10.3) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.8.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Requirement already satisfied: xlrd&gt;=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora&gt;=0.2.0-&gt;ratsnlp) (2.0.1) Requirement already satisfied: dataclasses&gt;=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora&gt;=0.2.0-&gt;ratsnlp) (0.6) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.2.0) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (5.2.0) Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.2.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.0.2) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.13.0) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.7.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.classification import ClassificationTrainArguments args = ClassificationTrainArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name = &#39;pair-classification&#39;, # 문장 쌍 분류를 할 예정이므로 downstream_corpus_name=&#39;klue-nli&#39;, # 업스테이지 기업이 공게한 KLUE-NLI 데이터로 파인튜닝 downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, batch_size = 32 if torch.cuda.is_available() else 4, learning_rate=5e-5, max_seq_length=64, epochs = 5, tpu_cores=0 if torch.cuda.is_available() else 8, seed = 7, ) . kcbert-base 모델을 klue-nli 데이터로 파인튜닝 합니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;pair-classification&#39;, downstream_corpus_name=&#39;klue-nli&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) . set seed: 7 . 랜덤 시드와 로거를 설정합니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . nlpbook.download_downstream_dataset(args) . Downloading: 100%|██████████| 12.3M/12.3M [00:00&lt;00:00, 37.5MB/s] Downloading: 100%|██████████| 1.47M/1.47M [00:00&lt;00:00, 35.4MB/s] . 말뭉치를 내려받습니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False, ) . 토크나이저를 구축합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.paircls import KlueNLICorpus from ratsnlp.nlpbook.classification import ClassificationDataset corpus = KlueNLICorpus() # json 파일형식의 KLUE-NLI 데이터를 문장(전제+가설)과 레이블(참 거짓 중립)으로 읽음 train_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39;, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/klue-nli INFO:ratsnlp:loading train data... LOOKING AT /root/Korpora/klue-nli/klue_nli_train.json INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 17.969 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다. INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다. INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다. INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다. INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다. INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 2.044 s] . train_dataset[0] . ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) . KlueNLICorpus에서 받은 문장들을 미리 설정한 토크나이저로 분리합니다. . 출력물은 input_ids, attention_mask, token_type_ids, label 총 4개가 나옵니다. . 이전과 같은 결과인데 짧게 설명하며 input_ids은 토큰 시퀀스를, attention_mask는 패딩 여부를 알려줍니다. . token_type_ids은 세그먼트 정보로 첫번째 문장은 0, 두번째 문장은 1, 나머지 패딩은 0을 줍니다. . label은 0일때 참, 1일때 거짓, 2일때 중립을 의미합니다. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), collate_fn = nlpbook.data_collator, # 뽑은 인스턴스를 배치로 바꿔줌(텐서 형태로) drop_last = False, num_workers = args.cpu_workers, ) . 학습 데이터 셋으로 로더를 구축했습니다. 배치크기만큼 인스턴스를 랜덤하게 뽑은 뒤 이를 합처 배치를 만듭니다. . &#54217;&#44032;&#50857; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;test&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), # 순서대로 추출 collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/klue-nli INFO:ratsnlp:loading test data... LOOKING AT /root/Korpora/klue-nli/klue_nli_dev.json INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 1.457 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기 불편함이 많았다. INFO:ratsnlp:tokens: [CLS] 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 만족 ##했다 . [SEP] 10명 ##이 함께 사용 ##하기 불편 ##함이 많았 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 14184, 8258, 17, 3, 21000, 4017, 9158, 9021, 8268, 10588, 11467, 14338, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 성인 10명이 함께 사용하기 불편함없이 없었다. INFO:ratsnlp:tokens: [CLS] 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 만족 ##했다 . [SEP] 성인 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 없었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: neutral INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 14184, 8258, 17, 3, 13246, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 12629, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기에 만족스러웠다. INFO:ratsnlp:tokens: [CLS] 10명 ##이 함께 사용 ##하기 불편 ##함 ##없이 만족 ##했다 . [SEP] 10명 ##이 함께 사용 ##하기에 만족 ##스러 ##웠다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: entailment INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 21000, 4017, 9158, 9021, 8268, 10588, 4421, 8281, 14184, 8258, 17, 3, 21000, 4017, 9158, 9021, 19956, 14184, 13378, 19996, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10층에 건물사람들만 이용하는 수영장과 썬베드들이 있구요. + 건물사람들은 수영장과 썬베드를 이용할 수 있습니다. INFO:ratsnlp:tokens: [CLS] 10 ##층 ##에 건물 ##사람들 ##만 이용하는 수영 ##장과 썬 ##베 ##드 ##들이 있 ##구요 . [SEP] 건물 ##사람들은 수영 ##장과 썬 ##베 ##드 ##를 이용할 수 있습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: entailment INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4491, 4113, 10828, 9390, 4049, 14502, 26770, 21758, 2060, 4155, 4273, 7967, 2469, 8875, 17, 3, 10828, 11249, 26770, 21758, 2060, 4155, 4273, 4180, 29861, 1931, 8982, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence A, B: 10층에 건물사람들만 이용하는 수영장과 썬베드들이 있구요. + 수영장과 썬베드는 9층에 있습니다. INFO:ratsnlp:tokens: [CLS] 10 ##층 ##에 건물 ##사람들 ##만 이용하는 수영 ##장과 썬 ##베 ##드 ##들이 있 ##구요 . [SEP] 수영 ##장과 썬 ##베 ##드는 9 ##층 ##에 있습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: contradiction INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4491, 4113, 10828, 9390, 4049, 14502, 26770, 21758, 2060, 4155, 4273, 7967, 2469, 8875, 17, 3, 26770, 21758, 2060, 4155, 8609, 28, 4491, 4113, 8982, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.293 s] . 입력받은 데이터를 토크나이저로 분리한 후 로더를 이용해 테스트용 배치를 만듭니다. . &#47784;&#45944; &#54617;&#49845; . from transformers import BertConfig, BertForSequenceClassification pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = corpus.num_labels, ) model = BertForSequenceClassification.from_pretrained( args.pretrained_model_name, config = pretrained_model_config ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . pretrained_model_config은 프리트레인을 마친 BERT모델을 기록한 형태입니다.분류할 라벨이 3인것까지 정보를 줍니다. . model은 윗 모델에 문서 분류용 태스크 모듈을 덧붙인 모델입니다. . from ratsnlp.nlpbook.classification import ClassificationTask task = ClassificationTask(model, args) . 앞서 정의한 모델을 학습시킵니다. ClassificationTask 내에는 옵티마이저와 러닝 레이트 스케줄러가 있습니다. . trainer = nlpbook.get_trainer(args) trainer . /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Checkpoint directory /gdrive/My Drive/nlpbook/checkpoint-paircls exists and is not empty. warnings.warn(*args, **kwargs) GPU available: True, used: True TPU available: False, using: 0 TPU cores . &lt;pytorch_lightning.trainer.trainer.Trainer at 0x7f92663bb710&gt; . 트레이너를 정의했습니다. 트레이너는 GPU/TPU 설정, 로그 및 체크포인트 등 귀찮은 설정을 알아서 해줍니다. . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | model | BertForSequenceClassification | 108 M -- 108 M Trainable params 0 Non-trainable params 108 M Total params 435.683 Total estimated model params size (MB) . &#51204;&#51228;&#50752; &#44032;&#49444;&#51012; &#44160;&#51613;&#54616;&#45716; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . from ratsnlp.nlpbook.classification import ClassificationDeployArguments args = ClassificationDeployArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-paircls&#39;, max_seq_length=64, ) . downstream_model_checkpoint_fpath: /gdrive/My Drive/nlpbook/checkpoint-paircls/epoch=1-val_loss=0.82-v1.ckpt . 인퍼런스 설정을 해줍니다. . import torch from transformers import BertConfig, BertForSequenceClassification fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) . 체크 포인트를 로드해줍니다. (이전에 만든 모델 업로드) . from transformers import BertConfig pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = 3 ) model = BertForSequenceClassification(pretrained_model_config) . BERT 설절을 로드하고 BERT 모델을 초기화합니다. . model.load_state_dict({k.replace(&#39;model.&#39;, &#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].items()}) model.eval() . BertForSequenceClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30000, 768, padding_idx=0) (position_embeddings): Embedding(300, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=3, bias=True) ) . 초기화한 BERT 모델에 체크 포인트를 주입합니다. 그 후 모델을 평가모드로 바꿉니다. . def inference_fn(premise, hypothesis): inputs = tokenizer( [(premise, hypothesis)], max_length = args.max_seq_length, padding = &#39;max_length&#39;, truncation = True # 잘린 값 처리 여부 ) with torch.no_grad(): outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()}) prob = outputs.logits.softmax(dim=1) entailment_prob = round(prob[0][0].item(), 2) contradiction_prob = round(prob[0][1].item(), 2) neutral_prob = round(prob[0][1].item(), 2) if torch.argmax(prob) == 0: pred = &#39;참&#39; elif torch.argmax(prob) == 1: pred = &#39;거짓&#39; else: pred = &#39;중립&#39; return { &#39;premise&#39; : premise, &#39;hypothesis&#39; : hypothesis, &#39;prediction&#39; : pred, &#39;entailment_data&#39;: f&quot;참 {entailment_prob}&quot;, &#39;contradiction_data&#39;: f&quot;거짓 {contradiction_prob}&quot;, &#39;neutral_data&#39;: f&quot;중립 {neutral_prob}&quot;, &#39;entailment_width&#39;: f&quot;{entailment_prob*100}%&quot;, &#39;contradiction_width&#39;: f&quot;{contradiction_prob*100}%&quot;, &#39;neutral_width&#39;: f&quot;{neutral_prob*100}%&quot;, } . 전제와 가설을 입력받아 각각 토큰화, 인덱싱을 수행한 것을 파이토치 텐서 자료형으로 변환한뒤 모델에 입력하는 함수를 만듭니다. . from ratsnlp.nlpbook.classification import get_web_service_app app = get_web_service_app(inference_fn) app.run() . * Serving Flask app &#34;ratsnlp.nlpbook.classification.deploy&#34; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off . * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) . * Running on http://8cea-35-225-219-137.ngrok.io * Traffic stats available on http://127.0.0.1:4040 . 127.0.0.1 - - [23/Dec/2021 12:52:08] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:52:09] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:52:28] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:52:28] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:54:25] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:54:25] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:54:44] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:54:44] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:55:19] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:55:19] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - 127.0.0.1 - - [23/Dec/2021 12:57:54] &#34;GET / HTTP/1.1&#34; 200 - 127.0.0.1 - - [23/Dec/2021 12:57:54] &#34;GET /favicon.ico HTTP/1.1&#34; 404 - . 패키지를 설치하여 주어진 모델을 웹에서 서비스하게 해줍니다. . 다만 오류가 뜨네요.. 이부분은 공부를 더 해야겠습니다. . &#45712;&#45184;&#51216; . 앞서 영화 감상평을 긍정, 부정으로 구분하는 모델, 이번에 전제와 가설이 일치하는지 여부를 판단하는 모델을 만들었습니다. . 자연어의 기초 이론을 배우고 적용해봤는데, 언어를 수리적인 인풋으로 바꿔서 이를 판단하는 모델을 만든다는 것 자체가 신기했습니다. . 다만, 제가 처음 자연어 처리를 떠올렸을때와 약간 다른 점은 사람이 개입할 여지가 조금 작다는 느낌이 들었습니다. . 그래도 현실에서 유용한 모델을 만들 수 있다는 점이 신기했고, 더욱 더 공부하고 싶은 생각이 듭니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/23/Do_natural_language4.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/web/classifier/2021/12/23/Do_natural_language4.html",
            "date": " • Dec 23, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "[DACON] 심장 질환 예측 경진대회",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) path = &#39;/content/drive/MyDrive/heart/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sample_submission.csv&#39;) train.head() . id age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 1 | 53 | 1 | 2 | 130 | 197 | 1 | 0 | 152 | 0 | 1.2 | 0 | 0 | 2 | 1 | . 1 2 | 52 | 1 | 3 | 152 | 298 | 1 | 1 | 178 | 0 | 1.2 | 1 | 0 | 3 | 1 | . 2 3 | 54 | 1 | 1 | 192 | 283 | 0 | 0 | 195 | 0 | 0.0 | 2 | 1 | 3 | 0 | . 3 4 | 45 | 0 | 0 | 138 | 236 | 0 | 0 | 152 | 1 | 0.2 | 1 | 0 | 2 | 1 | . 4 5 | 35 | 1 | 1 | 122 | 192 | 0 | 1 | 174 | 0 | 0.0 | 2 | 0 | 2 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#44592;&#48376; &#48320;&#49688; &#49444;&#47749; . sex : 성별(0은 여자, 1은 남자), cp : 가슴통증(0~3, 클수록 심한통증), trestbps : 휴식 중 혈압 . chol : 혈중 콜레스테롤, fbs : 공복 중 혈당(120이상시 1), restecg : 휴식 중 심전도 결과(0은 좌심실 비대, 1은 정상, 2는 ST-T파 이상) . thalach : 최대 심박수, exang : 활동으로 인한 협심증 여부(0은 정상, 1은 이상), oldpeak : 휴식 대비 운동으로 인한 ST 하강 . slope : 활동 ST 분절 피크의 기울기(0 하강, 1 보통, 2 상승), ca : 주요 혈관 수(0-3개, 4는 NULL값), thal : 지중해빈혈 여부(0 Null, 1 정상, 2~3 결함) . train[&#39;target&#39;].value_counts() . 1 83 0 68 Name: target, dtype: int64 . 반응변수는 target으로 심장질환 판단 여부를 나타냅니다. 1은 이상, 0은 정상입니다. . 테스트 데이터는 이상이 83개, 정상이 68개로 이상이 조금 더 많습니다. . &#48276;&#51452;&#54805; &#48320;&#49688;&#47484; &#49900;&#51109;&#51656;&#54872; &#50668;&#48512;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54616;&#44592; . import matplotlib.pyplot as plt import seaborn as sns train_0 = train[train[&#39;target&#39;]==0] train_1 = train[train[&#39;target&#39;]==1] def cat_plot(column): f, ax = plt.subplots(1, 2, figsize=(16, 6)) sns.countplot(x = column, data = train_0, ax = ax[0], order = train_0[column].value_counts().index) ax[0].tick_params(labelsize=12) ax[0].set_title(&#39;target = 0&#39;) ax[0].set_ylabel(&#39;count&#39;) ax[0].tick_params(rotation=50) sns.countplot(x = column, data = train_1, ax = ax[1], order = train_1[column].value_counts().index) ax[1].tick_params(labelsize=12) ax[1].set_title(&#39;target = 1&#39;) ax[1].set_ylabel(&#39;count&#39;) ax[1].tick_params(rotation=50) plt.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() cat_plot(&quot;sex&quot;) . test[&#39;sex&#39;].value_counts() . 1 104 0 48 Name: sex, dtype: int64 . 우선 전반적으로 sex가 1인 자료가 많습니다. 앞서 설명한대로 sex가 1인 자료는 남성입니다. 이는 테스트 자료도 유사합니다. . 왼쪽 그림은 심장병이 없는 데이터의 성별 별 개수, 오른쪽 그림은 심장병이 있는 데이터의 성별 별 개수 입니다. . 그래프로 보아 주어진 데이터 내 여성의 심장병 발생 확률이 높은 것을 보여줍니다. . cat_plot(&quot;cp&quot;) . 다음은 심장병이 있는 데이터와 없는 데이터를 가슴통증 유무 변수로 확인했습니다. . 0이 가슴통증이 없는 값인데, cp가 0인 데이터들은 대부분 심장병이 없습니다. . 나머지 변수들은(cp가 1~3) 모두 심장병이 있을 확률이 더 높습니다. . 특이한 점은 cp가 3인 경우 가슴통증이 더 심해서 심장병이 있을 확률이 제일 높을 것이라고 생각하는데 그렇지는 않습니다. . 오히려 cp가 2인 경우가 더 심장병이 있을 확률이 더 높습니다. . 즉 이 변수는 순서형으로 보면 안됩니다. . cat_plot(&quot;fbs&quot;) . 범주형 변수들을 계속 같은 패턴으로 분석할 것 입니다. . 그래프에서는 심장질환 유무를 판단하는데 fbs는 크게 유의미한 변수는 아닌 것 같습니다. . cat_plot(&quot;restecg&quot;) test[&#39;restecg&#39;].value_counts() . 1 77 0 72 2 3 Name: restecg, dtype: int64 . restecg, 휴식 중 심전도 변수입니다. 1이 정상 값이나 0값 대비 오히려 심장질환이 있을 확률이 높은 것을 알 수 있어요. . 이 변수는 처리하기 애매합니다. 또 2는 스몰 샘플이나 모두 심장질환이 없는데, 너무 스몰샘플이라 함부로 처리하면 안되겠습니다. . 그래서 저는 이 변수는 유의미 하지 않다고 판단, 제거하겠습니다. . cat_plot(&quot;exang&quot;) . exang, 활동으로 인한 협심증 여부를 판단하는 변수 입니다. 역시 0은 정상, 1은 이상으로 알고 있는데 이상합니다. . 0이 나왔을때가 심장 질환을 가질 확률이 높습니다. 잘 이해가 되진 않는데 차이가 눈에 띄게 유의미하니 이 변수는 사용해야겠습니다. . cat_plot(&quot;slope&quot;) . slope, 활동 ST 분절 피크의 기울기 변수입니다. 우선 0인 값은 절대적 개수도 적고 심장 질환이 있든 없든 분포가 비슷합니다. . 차이가 나는 것은 1과 2인데 1은 심장병이 없을 확률이, 2는 심장병이 있을 확률이 높아집니다. . cat_plot(&quot;ca&quot;) test[&#39;ca&#39;].value_counts() . 0 80 1 34 2 23 3 10 4 5 Name: ca, dtype: int64 . ca, 확인된 주요 혈관 수 변수 입니다. 0이 절대적으로 많으며 2,3은 개수는 적으나 대부분 심장질환이 없습니다. . 그래프를 관찰해보면 2,3은 심장질환이 없다고 판단할 수 있는 좋은 변수 입니다. . 0은 약 70%가 심장질환이 있는 변수, 1은 대부분이 심장질환이 없는 변수 입니다. . 특이사항은 테스트 데이터에만 NULL값을 의미하는 4가 있는데 처리를 고민해야겠습니다. . 2와 3은 심장질환이 없을 확률이 대단히 높으므로 두 칼럼을 병합하겠습니다. . cat_plot(&quot;thal&quot;) test[&#39;thal&#39;].value_counts() . 2 82 3 59 1 10 0 1 Name: thal, dtype: int64 . thal, 지중해빈혈 여부 입니다. 우선 데이터 내 2번에 비율이 꽤 높습니다. . 2는 대부분 심장질환이 있는 변수, 3은 대부분 심장질환이 없는 변수 입니다. . 1은 정상을 의미하는 변수이나 심장질환을 판단하기 쉽지 않은 변수입니다. . 0은 NULL 값이므로 이 변수에선 판단을 보류한다는 의미에서 1과 합쳐주겠습니다. . &#50672;&#49549;&#54805; &#48320;&#49688;&#47484; &#49900;&#51109;&#51656;&#54872; &#50668;&#48512;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . def num_plot(column): fig, axes = plt.subplots(1, 2, figsize=(16, 6)) sns.distplot(train_0[column], ax = axes[0]) axes[0].tick_params(labelsize=12) axes[0].set_title(&#39;target = 0&#39;) axes[0].set_ylabel(&#39;count&#39;) sns.distplot(train_1[column], ax = axes[1]) axes[1].tick_params(labelsize=12) axes[1].set_title(&#39;target = 1&#39;) axes[1].set_ylabel(&#39;count&#39;) plt.subplots_adjust(wspace=0.3, hspace=0.3) num_plot(&quot;trestbps&quot;) [(train_0[&#39;trestbps&#39;]).mean(), (train_1[&#39;trestbps&#39;]).mean()] . [134.4558823529412, 130.04819277108433] . trestbps, 휴식 중 혈압 변수 입니다. 사실 두 집단 간 유의미한 차이가 있는 것 같진 않아요. . num_plot(&quot;chol&quot;) [(train_0[&#39;chol&#39;]).mean(), (train_1[&#39;chol&#39;]).mean()] . [242.23529411764707, 246.40963855421685] . chol, 콜레스테롤 변수 입니다. 두 분포가 유의미하게 차이있진 않아요. . num_plot(&quot;thalach&quot;) [(train_0[&#39;thalach&#39;]).mean(), (train_1[&#39;thalach&#39;]).mean()] . [141.19117647058823, 158.36144578313252] . thalach, 최대 심박수 변수 입니다. 확실히 thalach 값이 크면 심장질환일 확률이 늘어나는 것 같아요. . num_plot(&quot;oldpeak&quot;) [(train_0[&#39;oldpeak&#39;]).mean(), (train_1[&#39;oldpeak&#39;]).mean()] . [1.4808823529411763, 0.563855421686747] . oldpeak, 운동으로 인한 ST 하강 변수 입니다. 이 변수의 값이 크면 심장질환이 아닐 확률이 높아집니다. . &#45936;&#51060;&#53080; &#48288;&#51060;&#49828;&#46972;&#51064;&#50640; &#51080;&#45716; &#50672;&#49549;&#54805; &#48320;&#49688; EDA . fig, axes = plt.subplots(5, 3, figsize=(25, 20)) fig.suptitle(&#39;feature distributions per quality&#39;, fontsize= 40) for ax, col in zip(axes.flat, train.columns[1:-1]): sns.violinplot(x= &#39;target&#39;, y= col, ax=ax, data=train) ax.set_title(col, fontsize=20) plt.tight_layout(rect=[0, 0.03, 1, 0.95]) plt.show() . 다음 코드를 참고했습니다. . https://dacon.io/competitions/official/235848/codeshare/3832?page=1&amp;dtype=recent . 한눈에 변수들을 살펴볼 수 있어서 좋은 것 같아요. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . train[&#39;thal&#39;][train[&#39;thal&#39;] == 0] = 1 test[&#39;thal&#39;][test[&#39;thal&#39;] == 0] = 1 train_label = train[&#39;target&#39;] train.drop([&#39;trestbps&#39;,&#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;target&#39;], axis = 1, inplace= True) test.drop([&#39;trestbps&#39;,&#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;], axis = 1, inplace= True) . 앞서 EDA 한 정보를 바탕으로 trestbps, chol, fbs, restecg 변수를 모델에서 제외했습니다. . test2 = (test[test[&#39;ca&#39;] == 4]).drop([&#39;ca&#39;], axis = 1) test2id = test2[&#39;id&#39;] . 또 ca가 4인 값은 트레인 데이터에서 없는 NULL 값입니다. . 따라서 이 값을 가진 테스트 데이터는 ca변수가 없는 별개의 모델에서 학습하도록 값을 조정해줍니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;target&#39;] = rf.predict(test) # ca가 4인 데이터는 cp를 제외한 모델에서 생성된 결과를 사용하기로 한다. rf2 = RandomForestClassifier(random_state = 0, n_estimators = 100) rf2.fit(train.drop([&#39;ca&#39;], axis = 1),train_label) pred2 = rf2.predict(test2) k = 0 for i in test2id: sample_submission[&#39;target&#39;][sample_submission[&#39;id&#39;] == i] = pred2[k] k += 1 sample_submission.to_csv(&#39;heart_final_3.csv&#39;,index=False) . 랜덤포레스트로 모델을 만들었습니다. . from xgboost import XGBClassifier xgb = XGBClassifier() xgb.fit(train,train_label) sample_submission[&#39;target&#39;] = xgb.predict(test) # ca가 4인 데이터는 cp를 제외한 모델에서 생성된 결과를 사용하기로 한다. xgb2 = XGBClassifier() xgb2.fit(train.drop([&#39;ca&#39;], axis = 1),train_label) pred2 = xgb2.predict(test2) k = 0 for i in test2id: sample_submission[&#39;target&#39;][sample_submission[&#39;id&#39;] == i] = pred2[k] k += 1 sample_submission.to_csv(&#39;heart_final_4.csv&#39;,index=False) . [14:00:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [14:00:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . xgb 모델을 만들었습니다. .",
            "url": "https://ksy1526.github.io/myblog/dacon/jupyter/eda/heart/xgboost/randomforest/classifier/2021/12/22/dacon_heart.html",
            "relUrl": "/dacon/jupyter/eda/heart/xgboost/randomforest/classifier/2021/12/22/dacon_heart.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "[Do it 자연어] 3. 문서에 꼬리표 달기 + 웹 실습",
            "content": ". &#47784;&#45944; &#54872;&#44221;&#49444;&#51221; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.4 MB/s Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 10.8 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 32.2 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 5.3 MB/s Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 35.1 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 42.8 MB/s Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 46.9 MB/s Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 45.5 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 504 kB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 31.9 MB/s Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 39.9 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 46.4 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 6.1 MB/s Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 46.4 MB/s Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 48.1 MB/s Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 36.6 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=1e7f96578c0a204626c5e0ce7352ca4a2a7b4da34d341e1960377950a7555852 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from torch.cuda import is_available import torch from ratsnlp.nlpbook.classification import ClassificationTrainArguments args = ClassificationTrainArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, # 프리트레인 마친 언어모델의 이름 downstream_corpus_name=&#39;nsmc&#39;, # 다운스트림 데이터 이름 downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-doccls&#39;, # 파인튜닝된 모델의 체크포인트가 저장될 위치. batch_size = 32 if torch.cuda.is_available() else 4, #배치 크기. learning_rate=5e-5, max_seq_length = 128, # 토큰 기준 입력 문장 최대 길이 epochs = 1, # 학습 데이터 3번 반복 tpu_cores=0 if torch.cuda.is_available() else 8, seed = 7, ) . kcbert-base 모델을 NSMC 데이터로 파인튜닝 합니다. . from ratsnlp import nlpbook nlpbook.set_seed(args) . set seed: 7 . 랜덤 시드를 설정합니다. . nlpbook.set_logger(args) . INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_task_name=&#39;document-classification&#39;, downstream_corpus_name=&#39;nsmc&#39;, downstream_corpus_root_dir=&#39;/root/Korpora&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-doccls&#39;, max_seq_length=128, save_top_k=1, monitor=&#39;min val_loss&#39;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0) . 각종 로그를 출력하는 로거를 설정합니다. . &#47568;&#47945;&#52824; &#45236;&#47140;&#48155;&#44592; . from Korpora import Korpora Korpora.fetch( corpus_name=args.downstream_corpus_name, # 다운스트림 데이터 이름 root_dir = args.downstream_corpus_root_dir, # 다운스트림 데이터를 내려받을 위치. force_download=True, ) . [nsmc] download ratings_train.txt: 14.6MB [00:00, 69.8MB/s] [nsmc] download ratings_test.txt: 4.90MB [00:00, 38.8MB/s] . 네이버 영화리뷰 데이터(NSMC)를 내려받습니다. . Korpora를 이용해 말뭉치를 특정장소에 저장합니다. . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( args.pretrained_model_name, do_lower_case = False ) . 토큰화를 수행하는 토크나이저를 선언합니다. . &#54617;&#49845; &#45936;&#51060;&#53552; &#44396;&#52629; . from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset corpus = NsmcCorpus() # csv 파일형식의 NSMC 데이터를 문장과 레이블(긍/부정)으로 읽어 들임. train_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode = &#39;train&#39;, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/nsmc INFO:ratsnlp:loading train data... LOOKING AT /root/Korpora/nsmc/ratings_train.txt INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 45.759 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 아 더빙.. 진짜 짜증나네요 목소리 INFO:ratsnlp:tokens: [CLS] 아 더 ##빙 . . 진짜 짜증나네 ##요 목소리 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 INFO:ratsnlp:tokens: [CLS] 흠 . . . 포 ##스터 ##보고 초딩 ##영화 ##줄 . . . . 오버 ##연기 ##조차 가볍 ##지 않 ##구나 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 1 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 3521, 17, 17, 17, 3294, 13069, 8190, 10635, 13796, 4006, 17, 17, 17, 17, 17613, 19625, 9790, 17775, 4102, 2175, 8030, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 너무재밓었다그래서보는것을추천한다 INFO:ratsnlp:tokens: [CLS] 너무 ##재 ##밓 ##었다 ##그래 ##서 ##보는 ##것을 ##추 ##천 ##한다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8069, 4089, 7847, 8217, 9791, 4072, 9136, 8750, 4142, 4244, 8008, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 INFO:ratsnlp:tokens: [CLS] 교도소 이야기 ##구먼 . . 솔직히 재미 ##는 없다 . . 평 ##점 조정 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 12164, 9089, 9828, 17, 17, 8876, 10827, 4008, 8131, 17, 17, 3288, 4213, 16612, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 INFO:ratsnlp:tokens: [CLS] 사이 ##몬 ##페 ##그 ##의 익 ##살 ##스런 연기 ##가 돋 ##보 ##였던 영화 ! 스파이 ##더 ##맨 ##에서 늙어 ##보이 ##기만 했던 커 ##스 ##틴 던 ##스트 ##가 너무나도 이뻐 ##보 ##였다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 1 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8538, 4880, 4335, 4313, 4042, 2452, 4471, 10670, 11219, 4009, 870, 4010, 13043, 9376, 5, 24034, 4356, 4617, 7971, 22878, 11980, 9235, 10129, 3010, 4103, 4713, 834, 8795, 4009, 22110, 23997, 4010, 9827, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/nsmc/cached_train_BertTokenizer_128_nsmc_document-classification [took 23.543 s] . 학습 데이터 셋을 구축했습니다. . 여기서 ClassificationDataset은 NsmcCorpus가 넘겨준 문장과 레이블을 각각 tokenizer을 활용해 가공합니다. . 이때 가공된 형태는 ClassificationFeatures 자료형으로 4가지 정보가 있습니다. . 첫번째로 input_ids로 토큰 시퀀스를 의미합니다. 두번째는 attention_mask로 패딩 토큰(0)을 구분합니다. . 세번째는 token_type_ids로 세그먼트 정보, 네번째는 label로 레이블 정보 입니다. . train_dataset[0] . ClassificationFeatures(input_ids=[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) . train_dataset[100] . ClassificationFeatures(input_ids=[2, 2005, 4024, 4017, 1293, 4599, 4775, 4042, 2478, 4075, 4196, 15, 1463, 4207, 4196, 8080, 4024, 10314, 11219, 4180, 12610, 10579, 832, 4140, 11414, 9827, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) . ClassificationFeatures 자료형을 개별 자료마다 확인했습니다. 4가지 정보가 있는걸 확인 할 수 있어요. . from torch.utils.data import DataLoader, RandomSampler train_dataloader = DataLoader( train_dataset, batch_size = args.batch_size, sampler = RandomSampler(train_dataset, replacement = False), # batch_size만큼 비복원 랜덤 추출. collate_fn = nlpbook.data_collator, # 뽑은 인스턴스를 배치로 만드는 역할. # 인스턴스들을 앞서 말한 4가지 정보별로 취합 후 파이토치가 요구하는 자료형인 텐서 형태로 바꿈. drop_last = False, num_workers = args.cpu_workers, ) . 학습 데이터 셋으로부터 로더를 구축했습니다. . 배치 크기만큼 인스턴스를 뽑아 이를 합처서 배치를 만듭니다. . &#54217;&#44032;&#50857; &#45936;&#51060;&#53552; &#44396;&#52629; . from torch.utils.data import SequentialSampler val_dataset = ClassificationDataset( args = args, corpus = corpus, tokenizer = tokenizer, mode=&#39;test&#39;, ) val_dataloader = DataLoader( val_dataset, batch_size = args.batch_size, sampler = SequentialSampler(val_dataset), collate_fn = nlpbook.data_collator, drop_last = False, num_workers = args.cpu_workers, ) . INFO:ratsnlp:Creating features from dataset file at /root/Korpora/nsmc INFO:ratsnlp:loading test data... LOOKING AT /root/Korpora/nsmc/ratings_test.txt INFO:ratsnlp:tokenize sentences, it could take a lot of time... INFO:ratsnlp:tokenize sentences [took 29.230 s] INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 굳 ㅋ INFO:ratsnlp:tokens: [CLS] 굳 ㅋ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 1 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 352, 192, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: GDNTOPCLASSINTHECLUB INFO:ratsnlp:tokens: [CLS] G ##D ##N ##TO ##P ##C ##L ##A ##S ##S ##I ##N ##T ##H ##E ##C ##L ##U ##B [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 40, 4452, 4581, 25144, 4579, 4881, 4450, 4580, 4985, 4985, 4506, 4581, 4850, 5121, 4451, 4881, 4450, 5167, 4756, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아 INFO:ratsnlp:tokens: [CLS] 뭐야 이 평 ##점 ##들은 . . . . 나쁘 ##진 않지만 10 ##점 짜리 ##는 더더욱 아니잖아 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 10691, 2451, 3288, 4213, 7977, 17, 17, 17, 17, 10476, 4153, 15426, 8240, 4213, 21394, 4008, 15616, 13439, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 지루하지는 않은데 완전 막장임... 돈주고 보기에는.... INFO:ratsnlp:tokens: [CLS] 지 ##루 ##하지는 않은데 완전 막장 ##임 . . . 돈주고 보기에 ##는 . . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2688, 4532, 16036, 20879, 8357, 15971, 4252, 17, 17, 17, 13900, 25253, 4008, 17, 17, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:*** Example *** INFO:ratsnlp:sentence: 3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠?? INFO:ratsnlp:tokens: [CLS] 3 ##D ##만 아니었 ##어도 별 다섯 개 줬 ##을텐데 . . 왜 3 ##D ##로 나와서 제 심 ##기를 불편 ##하게 하죠 ? ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] INFO:ratsnlp:label: 0 INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 22, 4452, 4049, 18851, 8194, 1558, 23887, 220, 2648, 9243, 17, 17, 2332, 22, 4452, 4091, 10045, 2545, 2015, 8313, 10588, 8007, 18566, 32, 32, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0) INFO:ratsnlp:Saving features into cached file, it could take a lot of time... INFO:ratsnlp:Saving features into cached file /root/Korpora/nsmc/cached_test_BertTokenizer_128_nsmc_document-classification [took 16.069 s] . 평가용 데이터는 인스턴스를 랜덤하게 뽑을 필요가 없습니다. . 그러므로 SequentialSampler을 사용해 인스턴스를 순서대로 추출합니다. . &#47784;&#45944; &#54617;&#49845; . args.pretrained_model_name . &#39;beomi/kcbert-base&#39; . from transformers import BertConfig, BertForSequenceClassification pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, # 프리트레인을 마친 BERT인 beomi/kcbert-base를 사용한다. num_labels = corpus.num_labels, # 긍정/부정을 분류하므로 2이다. ) model = BertForSequenceClassification.from_pretrained( args.pretrained_model_name, config = pretrained_model_config, ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.bias&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . BertForSequenceClassification 모델은 프리트레인을 마친 BERT 모델 위에 문서 분류용 태스크 모듈이 덧붙여진 형태입니다. . from ratsnlp.nlpbook.classification import ClassificationTask task = ClassificationTask(model, args) . ClassificationTask에는 옵티마이저(모델 파라미터를 업데이트 할때 최적의 방향/보폭 설정하는 도구), learning rata 스케줄러가 정의됩니다. . trainer = nlpbook.get_trainer(args) trainer . GPU available: True, used: True TPU available: False, using: 0 TPU cores . &lt;pytorch_lightning.trainer.trainer.Trainer at 0x7f5d99e99310&gt; . 트레이너를 정의했습니다. 트레이너는 GPU/TPU 설정, 로그 및 체크포인트 등 귀찮은 설정을 알아서 해줍니다. . trainer.fit( task, train_dataloader = train_dataloader, val_dataloaders = val_dataloader, ) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | model | BertForSequenceClassification | 108 M -- 108 M Trainable params 0 Non-trainable params 108 M Total params 435.680 Total estimated model params size (MB) /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown... warnings.warn(*args, **kwargs) . 1 에포크당 약 2시간?이 걸린다.. . &#50689;&#54868; &#47532;&#48624; &#44048;&#49457;&#48516;&#49437; &#50937; &#49436;&#48708;&#49828; &#47564;&#46308;&#44592; . # from google.colab import drive # drive.mount(&#39;/gdrive&#39;, force_remount=True) # from transformers import BertConfig, BertForSequenceClassification # from transformers import BertTokenizer # tokenizer = BertTokenizer.from_pretrained( # args.pretrained_model_name, # do_lower_case = False # ) . from ratsnlp.nlpbook.classification import ClassificationTrainArguments args = ClassificationTrainArguments( pretrained_model_name=&#39;beomi/kcbert-base&#39;, downstream_model_dir=&#39;/gdrive/My Drive/nlpbook/checkpoint-doccls&#39;, max_seq_length = 128, ) . 인퍼런스 설정을 해줍니다. 프리트레인 모델은 앞선 분석에서 쓴 모델과 같은 모델을 사용해야합니다. . import torch fine_tuned_model_ckpt = torch.load( args.downstream_model_checkpoint_fpath, map_location = torch.device(&#39;cpu&#39;), ) pretrained_model_config = BertConfig.from_pretrained( args.pretrained_model_name, num_labels = 2 ) model = BertForSequenceClassification.from_pretrained(pretrained_model_config) . AttributeError Traceback (most recent call last) &lt;ipython-input-29-6567a7cee6f5&gt; in &lt;module&gt;() 1 import torch 2 fine_tuned_model_ckpt = torch.load( -&gt; 3 args.downstream_model_checkpoint_fpath, 4 map_location = torch.device(&#39;cpu&#39;), 5 ) AttributeError: &#39;ClassificationTrainArguments&#39; object has no attribute &#39;downstream_model_checkpoint_fpath&#39; . 앞서 파인튜닝한 모델의 체크포인트를 읽어들입니다. . 그 뒤 BERT 모델을 초기화 해줍니다. . model.load_state_dict({k.replace(&#39;model.&#39;,&#39;&#39;): v for k, v in fine_tuned_model_ckpt[&#39;state_dict&#39;].items()}) model.eval() . 초기화한 BERT 모델에 체크포인트를 주입합니다. . 그 이후 모델을 평가모드로 전환합니다. 드롭아웃 등 학습 때만 사용하는 기능을 무력화 시키기 위해서 입니다. . (체크포인트란 모델이 사용하는 모든 매개변수에 대한 정확한 값을 기록하는 것) . from numpy.core.numeric import outer def inference_fn(sentence): inputs = tokenizer( [sentence], max_length = args.max_seq_length, padding = &#39;max_length&#39;, truncation = True, ) with torch.no_grad(): outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()}) # items : 키와 값 둘다 얻는 함수. prob = outputs.logits.softmax(dim = 1) # 로짓에 소프트맥스 취하기 positive_prob = round(prob[0][1].item(), 4) negative_prob = round(prob[0][0].item(), 4) pred = &#39;긍정&#39; if torch.argmax(prob) == 1 else &#39;부정&#39; return { &#39;sentence&#39; : sentence, &#39;prediction&#39; : pred, &#39;positive_data&#39;: f&quot;긍정 {positive_prob}&quot;, &#39;negative_data&#39;: f&quot;부정 {negative_prob}&quot;, &#39;positive_width&#39;: f&quot;{positive_prob * 100}%&quot;, &#39;negative_width&#39;: f&quot;{negative_prob * 100}%&quot;, } . 문장을 입력받으면 모델을 사용해 긍정/부정을 판단해주는 함수를 만듭니다. . 모델은 로짓값을 출력하므로 소프트맥스를 이용해 확률 값으로 나타냅니다. . with torch.no_grad()은 오차 역전파에 사용하는 계산량을 줄여서 처리 속도를 높입니다. . from ratsnlp.nlpbook.classification import get_web_service_app app = get_web_service_app(inference_fn) app.run() . 플라스크라는 파이썬 라이브러리의 도움을 받아 웹 서비스를 할 수 있습니다. . (딥러닝 모델에서 약 6시간.. 너무 오래 걸려서 오류 그대로 업로드 합니다.) .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/tokenizer/sentiment/web/classifier/2021/12/22/Do_natural_language3.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/tokenizer/sentiment/web/classifier/2021/12/22/Do_natural_language3.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "[Do it 자연어] 2. 숫자 세계로 떠난 자연어",
            "content": ". &#49472;&#54532; &#50612;&#53584;&#49440; &#46041;&#51089;&#50896;&#47532; . import torch x = torch.tensor([ [1.0, 0.0, 1.0, 0.0], [0.0, 2.0, 0.0, 2.0], [1.0, 1.0, 1.0, 1.0], ]) w_query = torch.tensor([ [1.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 1.0], ]) w_key = torch.tensor([ [0.0, 0.0, 1.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 1.0, 0.0], ]) w_value = torch.tensor([ [0.0, 2.0, 0.0], [0.0, 3.0, 0.0], [1.0, 0.0, 3.0], [1.0, 1.0, 0.0], ]) . 변수를 정의합니다. . keys = torch.matmul(x, w_key) querys = torch.matmul(x, w_query) values = torch.matmul(x, w_value) . 쿼리, 키, 벨류를 만듭니다. . attn_scores = torch.matmul(querys, keys.T) attn_scores . tensor([[ 2., 4., 4.], [ 4., 16., 12.], [ 4., 12., 10.]]) . 어텐션 스코어를 만듭니다. . import numpy as np from torch.nn.functional import softmax key_dim_sqrt = np.sqrt(keys.shape[-1]) attn_probs = softmax(attn_scores / key_dim_sqrt, dim = -1) attn_probs . tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01], [8.9045e-04, 9.0884e-01, 9.0267e-02], [7.4449e-03, 7.5471e-01, 2.3785e-01]]) . 소프트맥스 확률값을 만듭니다. . weighted_values = torch.matmul(attn_probs, values) weighted_values . tensor([[1.8639, 6.3194, 1.7042], [1.9991, 7.8141, 0.2735], [1.9926, 7.4796, 0.7359]]) . 소프트맥스 확률과 밸류를 가중합 하였습니다. 셀프 어텐션에 최종 출력값 입니다. . 셀프 어텐션은 가중치 행렬(w_..) 3개를 학습 대상으로 생각하고 태스크를 잘 수행하는 방향으로 업데이트 됩니다. . &#54588;&#46300;&#54252;&#50892;&#46300; &#45684;&#47092; &#45348;&#53944;&#50892;&#53356; &#44228;&#49328; &#50696;&#49884; . import torch x = torch.tensor([2,1]) w1 = torch.tensor([[3,2,-4],[2,-3,1]]) b1 = 1 w2 = torch.tensor([[-1,1],[1,2],[3,1]]) b2 = -1 . 변수를 입력합니다. . h_preact = torch.matmul(x, w1) + b1 h = torch.nn.functional.relu(h_preact) y = torch.matmul(h, w2) + b2 y . tensor([-8, 12]) . 결과 값 입니다. 여기서 w1, w2, b1, b2가 학습 대상이 됩니다. . 학습 대상은 태스크를 잘 수행하는 방향으로 업데이트 됩니다. . m = torch.nn.Dropout(p = 0.2) input = torch.randn(1,10) output = m(input) print(input) print(output) . tensor([[ 0.3678, 1.6664, 1.6091, 0.1445, 0.9486, 2.0537, 2.4539, 1.4420, 0.9701, -0.2877]]) tensor([[0.0000, 2.0830, 2.0113, 0.1806, 1.1858, 2.5672, 0.0000, 1.8025, 0.0000, -0.0000]]) . 간단한 드롭다웃 예제입니다. p 확률 만큼 뉴련을 0으로 대치해 계산에서 제외합니다. . &#47928;&#51109;&#51012; &#48289;&#53552;&#47196; &#48320;&#54872;&#54616;&#44592; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.6 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 15.9 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 6.4 MB/s Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 37.5 MB/s Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 56.2 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 59.2 MB/s Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 52.0 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 64.2 MB/s Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 44.7 MB/s Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 671 kB/s Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 48.7 MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 46.2 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 6.6 MB/s Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 69.4 MB/s Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 57.9 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 71.0 MB/s Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=3cd467807e542363544f5ce4ef26f212f5b2dd8812ec646fa206e9b85b0c7b48 Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained( &#39;beomi/kcbert-base&#39;, do_lower_case = False, ) . BERT(kcbert-base) 모델이 쓰는 토크나이저를 선언합니다. . from transformers import BertConfig, BertModel pretrained_model_config = BertConfig.from_pretrained( &#39;beomi/kcbert-base&#39; ) model = BertModel.from_pretrained( &#39;beomi/kcbert-base&#39;, config = pretrained_model_config, ) . Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;] - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). . BERT(kcbert-base) 모델을 읽어들입니다. . pretrained_model_config . BertConfig { &#34;_name_or_path&#34;: &#34;beomi/kcbert-base&#34;, &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;classifier_dropout&#34;: null, &#34;directionality&#34;: &#34;bidi&#34;, &#34;gradient_checkpointing&#34;: false, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 300, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;pooler_fc_size&#34;: 768, &#34;pooler_num_attention_heads&#34;: 12, &#34;pooler_num_fc_layers&#34;: 3, &#34;pooler_size_per_head&#34;: 128, &#34;pooler_type&#34;: &#34;first_token_transform&#34;, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.10.0&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 30000 } . pretrained_model_config은 BERT 모델을 프리트레인 할때 설정했던 내용이 있습니다. . 블록 수는 12개, 헤드 수는 12개, 어휘 집합 크기는 3만개 입니다. . sentences = [&#39;안녕하세요&#39;, &#39;하이!&#39;] features = tokenizer( sentences, max_length = 10, padding = &#39;max_length&#39;, truncation = True, ) features . {&#39;input_ids&#39;: [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 5, 3, 0, 0, 0, 0, 0, 0]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]} . BERT 모델의 입력값을 만듭니다. 앞서 배운 BERT 모델과 같이 3개의 변수가 나옵니다. . features = {k : torch.tensor(v) for k, v in features.items()} . 피처를 파이토치에 넣기 위해선 자료형이 텐서(tensor)이여야 하기 때문에 자료형을 변경했습니다. . outputs = model(**features) outputs . BaseModelOutputWithPoolingAndCrossAttentions([(&#39;last_hidden_state&#39;, tensor([[[-0.6969, -0.8248, 1.7512, ..., -0.3732, 0.7399, 1.1907], [-1.4803, -0.4398, 0.9444, ..., -0.7405, -0.0211, 1.3064], [-1.4299, -0.5033, -0.2069, ..., 0.1285, -0.2611, 1.6057], ..., [-1.4406, 0.3431, 1.4043, ..., -0.0565, 0.8450, -0.2170], [-1.3625, -0.2404, 1.1757, ..., 0.8876, -0.1054, 0.0734], [-1.4244, 0.1518, 1.2920, ..., 0.0245, 0.7572, 0.0080]], [[ 0.9371, -1.4749, 1.7351, ..., -0.3426, 0.8050, 0.4031], [ 1.6095, -1.7269, 2.7936, ..., 0.3100, -0.4787, -1.2491], [ 0.4861, -0.4569, 0.5712, ..., -0.1769, 1.1253, -0.2756], ..., [ 1.2362, -0.6181, 2.0906, ..., 1.3677, 0.8132, -0.2742], [ 0.5409, -0.9652, 1.6237, ..., 1.2395, 0.9185, 0.1782], [ 1.9001, -0.5859, 3.0156, ..., 1.4967, 0.1924, -0.4448]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)), (&#39;pooler_output&#39;, tensor([[-0.1594, 0.0547, 0.1101, ..., 0.2684, 0.1596, -0.9828], [-0.9221, 0.2969, -0.0110, ..., 0.4291, 0.0311, -0.9955]], grad_fn=&lt;TanhBackward0&gt;))]) . BERT 모델에 features를 적용했습니다. 두 개의 출력물 last_hidden_state, pooler_output이 나옵니다. . 전자를 단어수준 임베딩, 후자를 문장수준 임베딩이라고 부릅니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/bert/self-attention/tokenizer/2021/12/21/Do_natural_language2.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/bert/self-attention/tokenizer/2021/12/21/Do_natural_language2.html",
            "date": " • Dec 21, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "[Do it 자연어] 1. 문장을 작은 단위로 쪼개기",
            "content": ". &#54056;&#53412;&#51648; &#45796;&#50868;/&#44396;&#44544; &#50672;&#46041; . !pip install ratsnlp . Collecting ratsnlp Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.4 MB/s Collecting flask-ngrok&gt;=0.0.25 Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB) Collecting transformers==4.10.0 Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB) |████████████████████████████████| 2.8 MB 8.2 MB/s Collecting flask-cors&gt;=3.0.10 Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB) Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111) Collecting pytorch-lightning==1.3.4 Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB) |████████████████████████████████| 806 kB 48.6 MB/s Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4) Collecting Korpora&gt;=0.2.0 Downloading Korpora-0.2.0-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 4.9 MB/s Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.7.0) Collecting fsspec[http]&gt;=2021.4.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 31.4 MB/s Collecting pyDeprecate==0.3.0 Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB) Collecting torchmetrics&gt;=0.2.0 Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB) |████████████████████████████████| 332 kB 55.5 MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3) Collecting future&gt;=0.17.1 Downloading future-0.18.2.tar.gz (829 kB) |████████████████████████████████| 829 kB 43.3 MB/s Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.19.5) Collecting PyYAML&lt;=5.4.1,&gt;=5.1 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 47.3 MB/s Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 13.9 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20) Collecting huggingface-hub&gt;=0.0.12 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 451 kB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 58.6 MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.4.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.8.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3) Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0) Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 13.2 MB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1) Collecting xlrd&gt;=1.2.0 Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB) |████████████████████████████████| 96 kB 5.1 MB/s Collecting dataclasses&gt;=0.6 Downloading dataclasses-0.6-py3-none-any.whl (14 kB) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.6) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.12.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.42.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.0) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.1.1) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 57.5 MB/s Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB) Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 51.6 MB/s Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.8) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.2.0) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 55.2 MB/s Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0) Building wheels for collected packages: future Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=47a5461d5402fcdc2dcef6ae30181e838b5332b2f517b7570f63edff2f2ce53f Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0 Successfully built future Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: xlrd Found existing installation: xlrd 1.1.0 Uninstalling xlrd-1.1.0: Successfully uninstalled xlrd-1.1.0 Attempting uninstall: future Found existing installation: future 0.16.0 Uninstalling future-0.16.0: Successfully uninstalled future-0.16.0 Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2 . from google.colab import drive drive.mount(&#39;/gdrive&#39;, force_remount=True) . Mounted at /gdrive . from Korpora import Korpora nsmc = Korpora.load(&#39;nsmc&#39;, force_download=True) . Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을 손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다. 말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다. 해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고, 해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다. # Description Author : e9t@github Repository : https://github.com/e9t/nsmc References : www.lucypark.kr/docs/2015-pyconkr/#39 Naver sentiment movie corpus v1.0 This is a movie review dataset in the Korean language. Reviews were scraped from Naver Movies. The dataset construction is based on the method noted in [Large movie review dataset][^1] from Maas et al., 2011. [^1]: http://ai.stanford.edu/~amaas/data/sentiment/ # License CC0 1.0 Universal (CC0 1.0) Public Domain Dedication Details in https://creativecommons.org/publicdomain/zero/1.0/ . [nsmc] download ratings_train.txt: 14.6MB [00:00, 61.5MB/s] [nsmc] download ratings_test.txt: 4.90MB [00:00, 33.0MB/s] . NSMC는 네이버 영화 리뷰자료 입니다. . import os def write_lines(path, lines): with open(path, &#39;w&#39;, encoding = &#39;utf-8&#39;) as f: for line in lines: f.write(f&#39;{line} n&#39;) write_lines(&#39;/root/train.txt&#39;, nsmc.train.get_all_texts()) write_lines(&#39;/root/test.txt&#39;, nsmc.test.get_all_texts()) . 영화 리뷰들을 순수 텍스트 형태로 저장했습니다. . !head /root/train.txt . 아 더빙.. 진짜 짜증나네요 목소리 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 너무재밓었다그래서보는것을추천한다 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움. 원작의 긴장감을 제대로 살려내지못했다. 별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네 액션이 없는데도 재미 있는 몇안되는 영화 왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나? . 리뷰 앞 내용입니다. . GPT &#53664;&#53356;&#45208;&#51060;&#51200; &#44396;&#52629; . import os os.makedirs(&#39;/gdrive/My Drive/nlpbook/bbpe&#39;, exist_ok= True) . 저장용 디렉터리를 만들었습니다. . from tokenizers import ByteLevelBPETokenizer bytebpe_tokenizer = ByteLevelBPETokenizer() bytebpe_tokenizer.train( files=[&quot;/root/train.txt&quot;, &quot;/root/test.txt&quot;], # 학습 말뭉치를 리스트 형태로 vocab_size=10000, # 어휘 집합 크기 조절 special_tokens=[&quot;[PAD]&quot;] # 특수 토큰 추가 ) bytebpe_tokenizer.save_model(&quot;/gdrive/My Drive/nlpbook/bbpe&quot;) . [&#39;/gdrive/My Drive/nlpbook/bbpe/vocab.json&#39;, &#39;/gdrive/My Drive/nlpbook/bbpe/merges.txt&#39;] . 코드가 실행되면 vocab.json, merges.txt가 생성됩니다. . 전자는 바이트 수준 BPE 어휘집합, 후자는 바이그램 쌍과 병합우선순위가 있습니다. . !cat /gdrive/My Drive/nlpbook/bbpe/vocab.json . {&#34;[PAD]&#34;:0,&#34;!&#34;:1,&#34; &#34;&#34;:2,&#34;#&#34;:3,&#34;$&#34;:4,&#34;%&#34;:5,&#34;&amp;&#34;:6,&#34;&#39;&#34;:7,&#34;(&#34;:8,&#34;)&#34;:9,&#34;*&#34;:10,&#34;+&#34;:11,&#34;,&#34;:12,&#34;-&#34;:13,&#34;.&#34;:14,&#34;/&#34;:15,&#34;0&#34;:16,&#34;1&#34;:17,&#34;2&#34;:18,&#34;3&#34;:19,&#34;4&#34;:20,&#34;5&#34;:21,&#34;6&#34;:22,&#34;7&#34;:23,&#34;8&#34;:24,&#34;9&#34;:25,&#34;:&#34;:26,&#34;;&#34;:27,&#34;&lt;&#34;:28,&#34;=&#34;:29,&#34;&gt;&#34;:30,&#34;?&#34;:31,&#34;@&#34;:32,&#34;A&#34;:33,&#34;B&#34;:34,&#34;C&#34;:35,&#34;D&#34;:36,&#34;E&#34;:37,&#34;F&#34;:38,&#34;G&#34;:39,&#34;H&#34;:40,&#34;I&#34;:41,&#34;J&#34;:42,&#34;K&#34;:43,&#34;L&#34;:44,&#34;M&#34;:45,&#34;N&#34;:46,&#34;O&#34;:47,&#34;P&#34;:48,&#34;Q&#34;:49,&#34;R&#34;:50,&#34;S&#34;:51,&#34;T&#34;:52,&#34;U&#34;:53,&#34;V&#34;:54,&#34;W&#34;:55,&#34;X&#34;:56,&#34;Y&#34;:57,&#34;Z&#34;:58,&#34;[&#34;:59,&#34; &#34;:60,&#34;]&#34;:61,&#34;^&#34;:62,&#34;_&#34;:63,&#34;`&#34;:64,&#34;a&#34;:65,&#34;b&#34;:66,&#34;c&#34;:67,&#34;d&#34;:68,&#34;e&#34;:69,&#34;f&#34;:70,&#34;g&#34;:71,&#34;h&#34;:72,&#34;i&#34;:73,&#34;j&#34;:74,&#34;k&#34;:75,&#34;l&#34;:76,&#34;m&#34;:77,&#34;n&#34;:78,&#34;o&#34;:79,&#34;p&#34;:80,&#34;q&#34;:81,&#34;r&#34;:82,&#34;s&#34;:83,&#34;t&#34;:84,&#34;u&#34;:85,&#34;v&#34;:86,&#34;w&#34;:87,&#34;x&#34;:88,&#34;y&#34;:89,&#34;z&#34;:90,&#34;{&#34;:91,&#34;|&#34;:92,&#34;}&#34;:93,&#34;~&#34;:94,&#34;¡&#34;:95,&#34;¢&#34;:96,&#34;£&#34;:97,&#34;¤&#34;:98,&#34;¥&#34;:99,&#34;¦&#34;:100,&#34;§&#34;:101,&#34;¨&#34;:102,&#34;©&#34;:103,&#34;ª&#34;:104,&#34;«&#34;:105,&#34;¬&#34;:106,&#34;®&#34;:107,&#34;¯&#34;:108,&#34;°&#34;:109,&#34;±&#34;:110,&#34;²&#34;:111,&#34;³&#34;:112,&#34;´&#34;:113,&#34;µ&#34;:114,&#34;¶&#34;:115,&#34;·&#34;:116,&#34;¸&#34;:117,&#34;¹&#34;:118,&#34;º&#34;:119,&#34;»&#34;:120,&#34;¼&#34;:121,&#34;½&#34;:122,&#34;¾&#34;:123,&#34;¿&#34;:124,&#34;À&#34;:125,&#34;Á&#34;:126,&#34;Â&#34;:127,&#34;Ã&#34;:128,&#34;Ä&#34;:129,&#34;Å&#34;:130,&#34;Æ&#34;:131,&#34;Ç&#34;:132,&#34;È&#34;:133,&#34;É&#34;:134,&#34;Ê&#34;:135,&#34;Ë&#34;:136,&#34;Ì&#34;:137,&#34;Í&#34;:138,&#34;Î&#34;:139,&#34;Ï&#34;:140,&#34;Ð&#34;:141,&#34;Ñ&#34;:142,&#34;Ò&#34;:143,&#34;Ó&#34;:144,&#34;Ô&#34;:145,&#34;Õ&#34;:146,&#34;Ö&#34;:147,&#34;×&#34;:148,&#34;Ø&#34;:149,&#34;Ù&#34;:150,&#34;Ú&#34;:151,&#34;Û&#34;:152,&#34;Ü&#34;:153,&#34;Ý&#34;:154,&#34;Þ&#34;:155,&#34;ß&#34;:156,&#34;à&#34;:157,&#34;á&#34;:158,&#34;â&#34;:159,&#34;ã&#34;:160,&#34;ä&#34;:161,&#34;å&#34;:162,&#34;æ&#34;:163,&#34;ç&#34;:164,&#34;è&#34;:165,&#34;é&#34;:166,&#34;ê&#34;:167,&#34;ë&#34;:168,&#34;ì&#34;:169,&#34;í&#34;:170,&#34;î&#34;:171,&#34;ï&#34;:172,&#34;ð&#34;:173,&#34;ñ&#34;:174,&#34;ò&#34;:175,&#34;ó&#34;:176,&#34;ô&#34;:177,&#34;õ&#34;:178,&#34;ö&#34;:179,&#34;÷&#34;:180,&#34;ø&#34;:181,&#34;ù&#34;:182,&#34;ú&#34;:183,&#34;û&#34;:184,&#34;ü&#34;:185,&#34;ý&#34;:186,&#34;þ&#34;:187,&#34;ÿ&#34;:188,&#34;Ā&#34;:189,&#34;ā&#34;:190,&#34;Ă&#34;:191,&#34;ă&#34;:192,&#34;Ą&#34;:193,&#34;ą&#34;:194,&#34;Ć&#34;:195,&#34;ć&#34;:196,&#34;Ĉ&#34;:197,&#34;ĉ&#34;:198,&#34;Ċ&#34;:199,&#34;ċ&#34;:200,&#34;Č&#34;:201,&#34;č&#34;:202,&#34;Ď&#34;:203,&#34;ď&#34;:204,&#34;Đ&#34;:205,&#34;đ&#34;:206,&#34;Ē&#34;:207,&#34;ē&#34;:208,&#34;Ĕ&#34;:209,&#34;ĕ&#34;:210,&#34;Ė&#34;:211,&#34;ė&#34;:212,&#34;Ę&#34;:213,&#34;ę&#34;:214,&#34;Ě&#34;:215,&#34;ě&#34;:216,&#34;Ĝ&#34;:217,&#34;ĝ&#34;:218,&#34;Ğ&#34;:219,&#34;ğ&#34;:220,&#34;Ġ&#34;:221,&#34;ġ&#34;:222,&#34;Ģ&#34;:223,&#34;ģ&#34;:224,&#34;Ĥ&#34;:225,&#34;ĥ&#34;:226,&#34;Ħ&#34;:227,&#34;ħ&#34;:228,&#34;Ĩ&#34;:229,&#34;ĩ&#34;:230,&#34;Ī&#34;:231,&#34;ī&#34;:232,&#34;Ĭ&#34;:233,&#34;ĭ&#34;:234,&#34;Į&#34;:235,&#34;į&#34;:236,&#34;İ&#34;:237,&#34;ı&#34;:238,&#34;Ĳ&#34;:239,&#34;ĳ&#34;:240,&#34;Ĵ&#34;:241,&#34;ĵ&#34;:242,&#34;Ķ&#34;:243,&#34;ķ&#34;:244,&#34;ĸ&#34;:245,&#34;Ĺ&#34;:246,&#34;ĺ&#34;:247,&#34;Ļ&#34;:248,&#34;ļ&#34;:249,&#34;Ľ&#34;:250,&#34;ľ&#34;:251,&#34;Ŀ&#34;:252,&#34;ŀ&#34;:253,&#34;Ł&#34;:254,&#34;ł&#34;:255,&#34;Ń&#34;:256,&#34;Ġì&#34;:257,&#34;Ġë&#34;:258,&#34;ìĿ&#34;:259,&#34;ëĭ&#34;:260,&#34;íķ&#34;:261,&#34;ê°&#34;:262,&#34;..&#34;:263,&#34;ìĿ´&#34;:264,&#34;ëĭ¤&#34;:265,&#34;ëĬ&#34;:266,&#34;ìĹ&#34;:267,&#34;ê³&#34;:268,&#34;ì§&#34;:269,&#34;ëĬĶ&#34;:270,&#34;ìŀ&#34;:271,&#34;ë§&#34;:272,&#34;íĻ&#34;:273,&#34;ê³ł&#34;:274,&#34;ìł&#34;:275,&#34;íĻĶ&#34;:276,&#34;ĺģ&#34;:277,&#34;Ġê&#34;:278,&#34;ëı&#34;:279,&#34;ìķ&#34;:280,&#34;ãħ&#34;:281,&#34;ĺģíĻĶ&#34;:282,&#34;ìļ&#34;:283,&#34;ì§Ģ&#34;:284,&#34;íķĺ&#34;:285,&#34;ê°Ģ&#34;:286,&#34;ëĤ&#34;:287,&#34;ê²&#34;:288,&#34;ìĦ&#34;:289,&#34;Ġìŀ&#34;:290,&#34;¬ë&#34;:291,&#34;ê¸&#34;:292,&#34;Ġìķ&#34;:293,&#34;ëıĦ&#34;:294,&#34;Ġí&#34;:295,&#34;ëĵ&#34;:296,&#34;ë¦&#34;:297,&#34;ìĹĲ&#34;:298,&#34;ĠìĿ&#34;:299,&#34;íķľ&#34;:300,&#34;ĠìĺģíĻĶ&#34;:301,&#34;Īë&#34;:302,&#34;³´&#34;:303,&#34;ìĭ&#34;:304,&#34;ĸ´&#34;:305,&#34;ìĿĺ&#34;:306,&#34;ê¸°&#34;:307,&#34;ãħĭ&#34;:308,&#34;ĠìĹ&#34;:309,&#34;ìĿĢ&#34;:310,&#34;ë¡&#34;:311,&#34;ëį&#34;:312,&#34;ìĿĦ&#34;:313,&#34;Ŀ¼&#34;:314,&#34;ëĤĺ&#34;:315,&#34;ê²Į&#34;:316,&#34;ĠìĿ´&#34;:317,&#34;ìĦľ&#34;:318,&#34;Ġë§&#34;:319,&#34;ìļĶ&#34;:320,&#34;ìĬ&#34;:321,&#34;ìĸ´&#34;:322,&#34;ë¡ľ&#34;:323,&#34;ĠëĤ&#34;:324,&#34;ë§Į&#34;:325,&#34;ëĿ¼&#34;:326,&#34;ë¦¬&#34;:327,&#34;Ġìł&#34;:328,&#34;·¸&#34;:329,&#34;ëĭĪ&#34;:330,&#34;ëĵ¤&#34;:331,&#34;ë¥&#34;:332,&#34;ê±&#34;:333,&#34;ìķĦ&#34;:334,&#34;ëł&#34;:335,&#34;...&#34;:336,&#34;ë©&#34;:337,&#34;¬´&#34;:338,&#34;ìľ&#34;:339,&#34;Ġê°&#34;:340,&#34;ìĿ¸&#34;:341,&#34;ãħĭãħĭ&#34;:342,&#34;¯¸&#34;:343,&#34;ëį°&#34;:344,&#34;Ġì§&#34;:345,&#34;ëĦ&#34;:346,&#34;ĠìķĦ&#34;:347,&#34;ĮĢ&#34;:348,&#34;ëŁ&#34;:349,&#34;ìĺ&#34;:350,&#34;êµ&#34;:351,&#34;íķ´&#34;:352,&#34;Ġë³´&#34;:353,&#34;ë©´&#34;:354,&#34;ìĥ&#34;:355,&#34;ìĺģíĻĶ&#34;:356,&#34;Ġìĭ&#34;:357,&#34;Ġê·¸&#34;:358,&#34;ê¹&#34;:359,&#34;ë°&#34;:360,&#34;Ġëª&#34;:361,&#34;ìłĲ&#34;:362,&#34;ìĭľ&#34;:363,&#34;Īĺ&#34;:364,&#34;Ġëĭ&#34;:365,&#34;ë³´&#34;:366,&#34;ìĿĮ&#34;:367,&#34;ìĬ¤&#34;:368,&#34;£¼&#34;:369,&#34;ëŀ&#34;:370,&#34;Ġë°&#34;:371,&#34;ìľ¼&#34;:372,&#34;ëĦ¤&#34;:373,&#34;Ġìŀ¬ë&#34;:374,&#34;ë¥¼&#34;:375,&#34;ë§Ĳ&#34;:376,&#34;Ġì¢&#34;:377,&#34;!!&#34;:378,&#34;ë¶&#34;:379,&#34;ì¤&#34;:380,&#34;ĠìĤ&#34;:381,&#34;ê±°&#34;:382,&#34;ìĤ&#34;:383,&#34;ìĻ&#34;:384,&#34;ëĮĢ&#34;:385,&#34;ëĭĪëĭ¤&#34;:386,&#34;Īë¬´&#34;:387,&#34;ìŀĲ&#34;:388,&#34;ëĬĶëį°&#34;:389,&#34;ìĽ&#34;:390,&#34;Ġë³&#34;:391,&#34;ìłķ&#34;:392,&#34;ĠëĦ&#34;:393,&#34;ë§Ī&#34;:394,&#34;ê¹Į&#34;:395,&#34;ì²&#34;:396,&#34;ìĹĨ&#34;:397,&#34;ĠìĹĨ&#34;:398,&#34;ìĹĪ&#34;:399,&#34;ĠëĤĺ&#34;:400,&#34;Ġíķĺ&#34;:401,&#34;ìļ°&#34;:402,&#34;Ġë´&#34;:403,&#34;ì¹&#34;:404,&#34;ìķ¼&#34;:405,&#34;Ġì¢ĭ&#34;:406,&#34;ì£¼&#34;:407,&#34;ì§Ħ&#34;:408,&#34;Ġëĭ¤&#34;:409,&#34;ìĪĺ&#34;:410,&#34;íĸ&#34;:411,&#34;ë³&#34;:412,&#34;ë²&#34;:413,&#34;ìłģ&#34;:414,&#34;µľ&#34;:415,&#34;ìŀ¥&#34;:416,&#34;ìŀĪ&#34;:417,&#34;ìŀĳ&#34;:418,&#34;ìłĦ&#34;:419,&#34;ìĥģ&#34;:420,&#34;ëª&#34;:421,&#34;....&#34;:422,&#34;Ġìĥ&#34;:423,&#34;Ġìłķ&#34;:424,&#34;ì§ľ&#34;:425,&#34;ìĨ&#34;:426,&#34;°Į&#34;:427,&#34;ìľ¼ë¡ľ&#34;:428,&#34;Ġê²&#34;:429,&#34;ĠìŀĪ&#34;:430,&#34;ì§Ģë§Į&#34;:431,&#34;íĺ&#34;:432,&#34;ê°Ħ&#34;:433,&#34;ĠìĹ°&#34;:434,&#34;íķĺê³ł&#34;:435,&#34;ĠìĻ&#34;:436,&#34;¬ëŀ&#34;:437,&#34;ê³¼&#34;:438,&#34;Ĳëı&#34;:439,&#34;ìĺ¤&#34;:440,&#34;ĠìĬ&#34;:441,&#34;Ġëĵ&#34;:442,&#34;ëĤ´&#34;:443,&#34;Ġê¸&#34;:444,&#34;ıī&#34;:445,&#34;ãħł&#34;:446,&#34;ĠëĦĪë¬´&#34;:447,&#34;ëŁ°&#34;:448,&#34;ëħ&#34;:449,&#34;Ġìĸ´&#34;:450,&#34;Ġìĺ&#34;:451,&#34;Ġë§Į&#34;:452,&#34;íĥ&#34;:453,&#34;Ġìŀ¬ë¯¸&#34;:454,&#34;Ġì§Ģ&#34;:455,&#34;¹Ħ&#34;:456,&#34;ëĶ&#34;:457,&#34;ê·¸&#34;:458,&#34;ì°&#34;:459,&#34;íŀ&#34;:460,&#34;ëĥ&#34;:461,&#34;ìĹĲìĦľ&#34;:462,&#34;ĠëĤ´&#34;:463,&#34;ëĦ¤ìļĶ&#34;:464,&#34;ê±´&#34;:465,&#34;Ĳĺ&#34;:466,&#34;Ġíķľ&#34;:467,&#34;ëĵľ&#34;:468,&#34;Ġìĭľ&#34;:469,&#34;íĨ&#34;:470,&#34;Ġë¶&#34;:471,&#34;ìķĺ&#34;:472,&#34;íķł&#34;:473,&#34;ĠìĦ&#34;:474,&#34;ķĮ&#34;:475,&#34;ì¡&#34;:476,&#34;ìŀ¬ë&#34;:477,&#34;ìĹ°&#34;:478,&#34;Ġì§Ħ&#34;:479,&#34;Ġë´¤&#34;:480,&#34;ë£&#34;:481,&#34;Ġê°Ģ&#34;:482,&#34;ìļ´&#34;:483,&#34;ĠìĬ¤&#34;:484,&#34;ê³µ&#34;:485,&#34;Ġìµľ&#34;:486,&#34;ë´&#34;:487,&#34;ìĦ±&#34;:488,&#34;ìĻĢ&#34;:489,&#34;Ġëı&#34;:490,&#34;ë¯¸&#34;:491,&#34;Ġìļ&#34;:492,&#34;ìĹ¬&#34;:493,&#34;ê°ģ&#34;:494,&#34;ìĬµ&#34;:495,&#34;Ġì°&#34;:496,&#34;ê²ĥ&#34;:497,&#34; &#34; &#34;&#34;:498,&#34;íŀĪ&#34;:499,&#34;Ġëį&#34;:500,&#34;ìłľ&#34;:501,&#34;Ġ1&#34;:502,&#34;íĶ&#34;:503,&#34;ì¹ĺ&#34;:504,&#34;ì¶&#34;:505,&#34;ìĸ&#34;:506,&#34;ìļ©&#34;:507,&#34;Ġê¸°&#34;:508,&#34;íķĺëĬĶ&#34;:509,&#34;ĠëĮĢ&#34;:510,&#34;ĠìĨ&#34;:511,&#34;ë¶Ģ&#34;:512,&#34;ëł¤&#34;:513,&#34;ìĿ¼&#34;:514,&#34;ĠìĹ°ê¸°&#34;:515,&#34;íĨł&#34;:516,&#34;ëŀĺ&#34;:517,&#34;Ġê°Ĳëı&#34;:518,&#34;íĸĪ&#34;:519,&#34;íĮ&#34;:520,&#34;Ġìŀ¬ë°Į&#34;:521,&#34;ıīìłĲ&#34;:522,&#34;ë¬&#34;:523,&#34;ĠìĪĺ&#34;:524,&#34;Ħ°&#34;:525,&#34;êµ¬&#34;:526,&#34;Ġëª¨&#34;:527,&#34;ì¦&#34;:528,&#34;íķ¨&#34;:529,&#34;ë£¨&#34;:530,&#34;ìĤ¬&#34;:531,&#34;ìĸ´ìļĶ&#34;:532,&#34;Ġìłķë§Ĳ&#34;:533,&#34;ĠìłĦ&#34;:534,&#34;ĠìĤ¬ëŀ&#34;:535,&#34;Ŀê°ģ&#34;:536,&#34;êµŃ&#34;:537,&#34;Ġìľ&#34;:538,&#34;ãħİ&#34;:539,&#34;Ġì¤&#34;:540,&#34;ìĭł&#34;:541,&#34;íĦ°&#34;:542,&#34;Ġìŀĺ&#34;:543,&#34;ì¢&#34;:544,&#34;ì¤ĳ&#34;:545,&#34;ĠìķĬ&#34;:546,&#34;Ġë¬´&#34;:547,&#34;ë¶Ħ&#34;:548,&#34;íĬ&#34;:549,&#34;ëŁ¬&#34;:550,&#34;ìħ&#34;:551,&#34;ãħĭãħĭãħĭãħĭ&#34;:552,&#34;ê²ł&#34;:553,&#34;ìĬµëĭĪëĭ¤&#34;:554,&#34;ìĿ´ëĭ¤&#34;:555,&#34;íİ&#34;:556,&#34;ĠëįĶ&#34;:557,&#34;ìĦ¸&#34;:558,&#34;ĠìķĪ&#34;:559,&#34;íķĺëĭ¤&#34;:560,&#34;ĠëĬ&#34;:561,&#34;Ġì¡&#34;:562,&#34;ëłĪ&#34;:563,&#34;Ġê±&#34;:564,&#34;Ġì£¼&#34;:565,&#34;ê°Ļ&#34;:566,&#34;°ìļ°&#34;:567,&#34;ë¥´&#34;:568,&#34;ĵ°&#34;:569,&#34;ëķĮ&#34;:570,&#34;ĠìĽ&#34;:571,&#34;ìĨĮ&#34;:572,&#34;ê°ľ&#34;:573,&#34;~~&#34;:574,&#34;ĠëŃ&#34;:575,&#34;ìŀĦ&#34;:576,&#34;íĨłë¦¬&#34;:577,&#34;Ġëĵľ&#34;:578,&#34;ĠìĥĿê°ģ&#34;:579,&#34;ìĥĿ&#34;:580,&#34;Ġì§Ħì§ľ&#34;:581,&#34;ãħ¡&#34;:582,&#34;ê°Ĳ&#34;:583,&#34;Ġë§Ī&#34;:584,&#34;ëł¥&#34;:585,&#34;ëĵł&#34;:586,&#34;ëįĺ&#34;:587,&#34;ĠìĿ¸&#34;:588,&#34;ìŀħ&#34;:589,&#34;ìĭ¤&#34;:590,&#34;Ġê°Ļ&#34;:591,&#34;Ġìµľê³ł&#34;:592,&#34;íģ&#34;:593,&#34;Ġì²&#34;:594,&#34;Ġë§Ĳ&#34;:595,&#34;Ġêµ&#34;:596,&#34;Ġëª»&#34;:597,&#34;ê·&#34;:598,&#34;ëĤľ&#34;:599,&#34;ëĵ¯&#34;:600,&#34;ëĿ¼ë§Ī&#34;:601,&#34;ëĵ¤ìĿ´&#34;:602,&#34;ë¬´&#34;:603,&#34;Ġê°ľ&#34;:604,&#34;ĠìĹ¬&#34;:605,&#34;ëħĦ&#34;:606,&#34;ìķħ&#34;:607,&#34;íĴ&#34;:608,&#34;ãħłãħł&#34;:609,&#34;ĠìŀĲ&#34;:610,&#34;ëĶĶ&#34;:611,&#34;Ġìłľ&#34;:612,&#34;ĠëĬĲ&#34;:613,&#34;Ġëģ&#34;:614,&#34;ê¹Įì§Ģ&#34;:615,&#34;ê¸Ī&#34;:616,&#34;ë¦Ħ&#34;:617,&#34;ĠìĻľ&#34;:618,&#34;ëĥ¥&#34;:619,&#34;íİ¸&#34;:620,&#34;Ġê´&#34;:621,&#34;íĥĢ&#34;:622,&#34;íķĺê²Į&#34;:623,&#34;ë¹Ħ&#34;:624,&#34;ë³¸&#34;:625,&#34;Ġë³¸&#34;:626,&#34;Ġì¶&#34;:627,&#34;ëłĩ&#34;:628,&#34;ĳĲ&#34;:629,&#34;ìĺĢ&#34;:630,&#34;Ġìĸ&#34;:631,&#34;ĠìķĮ&#34;:632,&#34;ĠëĤ¨&#34;:633,&#34;íı&#34;:634,&#34;Ġíĺ&#34;:635,&#34;ìĿ´ëĿ¼&#34;:636,&#34;íĬ¸&#34;:637,&#34;Ġê²ĥ&#34;:638,&#34;ì¤Ģ&#34;:639,&#34;Ġìĺ¤&#34;:640,&#34;ĠíıīìłĲ&#34;:641,&#34;ë³´ëĭ¤&#34;:642,&#34;ìĹĪëĭ¤&#34;:643,&#34;Ġê·&#34;:644,&#34;½Ķ&#34;:645,&#34;Ġë³´ê³ł&#34;:646,&#34;ìħĺ&#34;:647,&#34;Ġë³¼&#34;:648,&#34;ĠìĿ´ëŁ°&#34;:649,&#34;ìľł&#34;:650,&#34;ê±¸&#34;:651,&#34;;;&#34;:652,&#34;ìłĢ&#34;:653,&#34;ìłķë§Ĳ&#34;:654,&#34;ĠìĨĮ&#34;:655,&#34;ë§ī&#34;:656,&#34;Ġë©&#34;:657,&#34;ëįĶ&#34;:658,&#34;ê´&#34;:659,&#34;??&#34;:660,&#34;ì²´&#34;:661,&#34;ë¹&#34;:662,&#34;ì§Ħì§ľ&#34;:663,&#34;ìĶ&#34;:664,&#34;Ġë¹&#34;:665,&#34;Ġë¹Ħ&#34;:666,&#34;ëģ&#34;:667,&#34;ĽĦ&#34;:668,&#34;ìĭ¬&#34;:669,&#34;Ġê°ĲëıĻ&#34;:670,&#34;Ġê¹&#34;:671,&#34;Ġë§İ&#34;:672,&#34;ĠíĿ&#34;:673,&#34;ĠìķĦëĭ&#34;:674,&#34;ĠìĬ¤íĨłë¦¬&#34;:675,&#34;ìŀ¬ë¯¸&#34;:676,&#34;ëłĪê¸°&#34;:677,&#34;íĴĪ&#34;:678,&#34;íķ´ìĦľ&#34;:679,&#34;ìķĪ&#34;:680,&#34;ìĽĲ&#34;:681,&#34;Ġë¯¸&#34;:682,&#34;ĠëĶ&#34;:683,&#34;ĠëĪ&#34;:684,&#34;Ġìŀĳ&#34;:685,&#34;ë²Ħ&#34;:686,&#34;ĵ°ëłĪê¸°&#34;:687,&#34;ìĪ&#34;:688,&#34;Ġìľł&#34;:689,&#34;Ġìŀ¥&#34;:690,&#34;Ĳľ&#34;:691,&#34;¡ľ&#34;:692,&#34;Ġë²&#34;:693,&#34;ëĦĪë¬´&#34;:694,&#34;ĠìĤ¬ëŀĮ&#34;:695,&#34;ëĥĲ&#34;:696,&#34;Ġë¶Ģ&#34;:697,&#34;ë©´ìĦľ&#34;:698,&#34;Ġë§Įëĵ¤&#34;:699,&#34;ĠìĿ¼&#34;:700,&#34;Ġíĸ&#34;:701,&#34;ë¨&#34;:702,&#34;ëĭ¨&#34;:703,&#34;ëĲĺ&#34;:704,&#34;ì¢ĭ&#34;:705,&#34;Ġãħĭãħĭ&#34;:706,&#34;ìµľ&#34;:707,&#34;ê³Ħ&#34;:708,&#34;Īëį&#34;:709,&#34;Ġì§Ģë£¨&#34;:710,&#34;ë¬¸&#34;:711,&#34;ë²Ī&#34;:712,&#34;ĠëĵľëĿ¼ë§Ī&#34;:713,&#34;ĠìķĪë&#34;:714,&#34;ìłģìĿ¸&#34;:715,&#34;ĠìĤ¬&#34;:716,&#34;Ġì¤ĳ&#34;:717,&#34;ëªħ&#34;:718,&#34;ìĦł&#34;:719,&#34;íĭ&#34;:720,&#34;Ġê³µ&#34;:721,&#34;ëłĩê²Į&#34;:722,&#34;ëĭ¤ëĬĶ&#34;:723,&#34;Ħë¡ľ&#34;:724,&#34;ĠëģĿ&#34;:725,&#34;ìĺģ&#34;:726,&#34;ãħľ&#34;:727,&#34;Ġë°°ìļ°&#34;:728,&#34;ì§ģ&#34;:729,&#34;ìĸµ&#34;:730,&#34;ì¶ľ&#34;:731,&#34;ëĭ¹&#34;:732,&#34;ĠëĤ´ìļ©&#34;:733,&#34;ë¦¬ê³ł&#34;:734,&#34;ë¦°&#34;:735,&#34;ë§Ŀ&#34;:736,&#34;ë¦¬ë&#34;:737,&#34;ìĽĮ&#34;:738,&#34;£½&#34;:739,&#34;ëŀĳ&#34;:740,&#34;ĠëĲĺ&#34;:741,&#34;Ġì¡°&#34;:742,&#34;íļ&#34;:743,&#34;ëıĻ&#34;:744,&#34;ë¯&#34;:745,&#34;Ġìļ°&#34;:746,&#34;Ġì¢Ģ&#34;:747,&#34;Ġíķ&#34;:748,&#34;Ġíķ´&#34;:749,&#34;,,&#34;:750,&#34;ĦìłĦ&#34;:751,&#34;Ġ10&#34;:752,&#34;ê¹Ŀ&#34;:753,&#34;ì¡°&#34;:754,&#34;^^&#34;:755,&#34;ĠëŃĲ&#34;:756,&#34;ĠëĨ&#34;:757,&#34;ë³´ê³ł&#34;:758,&#34;Ġìķł&#34;:759,&#34;íĤ&#34;:760,&#34;ãħİãħİ&#34;:761,&#34;ë´¤&#34;:762,&#34;ìŀ¬&#34;:763,&#34;¡ìħĺ&#34;:764,&#34;ì§Ģë§ī&#34;:765,&#34;Ġê°Ĳëıħ&#34;:766,&#34;Ġ2&#34;:767,&#34;ê°Ĳëı&#34;:768,&#34;ë¬¼&#34;:769,&#34;Ġìŀ¬ë¯¸ìŀĪ&#34;:770,&#34;ë¥¸&#34;:771,&#34;Ġë´Ĳ&#34;:772,&#34;Ġìĭ¶&#34;:773,&#34;Ġê°Ĳ&#34;:774,&#34;ëĨ&#34;:775,&#34;ìľ¼ë©´&#34;:776,&#34;ĠìĺģíĻĶë¥¼&#34;:777,&#34;ìŀ¬ë°Į&#34;:778,&#34;Ġì¹&#34;:779,&#34;ĠíĮ&#34;:780,&#34;ìĤ¬ëŀ&#34;:781,&#34;ê¸´&#34;:782,&#34;ëª¨&#34;:783,&#34;ë¦´&#34;:784,&#34;ìķł&#34;:785,&#34;ĠìĤ¬ëŀĳ&#34;:786,&#34;ĠìłĢ&#34;:787,&#34;íĺĦ&#34;:788,&#34;ìĨį&#34;:789,&#34;ĠíĹ&#34;:790,&#34;íĿ&#34;:791,&#34;íı¬&#34;:792,&#34;Ġëªħ&#34;:793,&#34;Ġê³ł&#34;:794,&#34;Ġëĺ&#34;:795,&#34;Ġë°ĺ&#34;:796,&#34;Ġì¢ĭìķĦ&#34;:797,&#34;Īëįĺ&#34;:798,&#34;ĠìĽĥ&#34;:799,&#34;ëĳĲ&#34;:800,&#34;Ġê±°&#34;:801,&#34;Ġìŀ¬ë¯¸ìĹĨ&#34;:802,&#34;Ġëħ&#34;:803,&#34;ìĹŃ&#34;:804,&#34;Ġì°¸&#34;:805,&#34;ì¤Ħ&#34;:806,&#34;ë°Ķ&#34;:807,&#34;Ġë³´ëĬĶ&#34;:808,&#34;ì²ĺ&#34;:809,&#34;Ġê·¸ëĥ¥&#34;:810,&#34;Ġë´¤ëĬĶëį°&#34;:811,&#34;¬¼&#34;:812,&#34;íķĺì§Ģ&#34;:813,&#34;ĠìĹŃ&#34;:814,&#34;ì¡±&#34;:815,&#34;íħ&#34;:816,&#34;Ġë°Ķ&#34;:817,&#34;Ġìĺģ&#34;:818,&#34;Ġìĥģ&#34;:819,&#34;ëĸ&#34;:820,&#34;ìľĦ&#34;:821,&#34;ëĵ¤ìĿĺ&#34;:822,&#34;ê»&#34;:823,&#34;Ġìĵ°ëłĪê¸°&#34;:824,&#34;ë°ķ&#34;:825,&#34;ĠìĹĨëĬĶ&#34;:826,&#34;íĶĦ&#34;:827,&#34;ãĦ&#34;:828,&#34;OO&#34;:829,&#34;ĠìŀĳíĴĪ&#34;:830,&#34;ëĤ¨&#34;:831,&#34;Ġëĭ¤ìĭľ&#34;:832,&#34;¥¼&#34;:833,&#34;ëĭĺ&#34;:834,&#34;ëħ¸&#34;:835,&#34;ìĿ¸ê³µ&#34;:836,&#34;ì§ĢëĬĶ&#34;:837,&#34;Ġë§¤&#34;:838,&#34;ëŁ½&#34;:839,&#34;ìŀħëĭĪëĭ¤&#34;:840,&#34;íĹ&#34;:841,&#34;ìļ¸&#34;:842,&#34;ì²Ń&#34;:843,&#34;Ġê²°&#34;:844,&#34;ìĭĿ&#34;:845,&#34;ĠìĺģíĻĶëĬĶ&#34;:846,&#34;ìĭ¶&#34;:847,&#34;íıīìłĲ&#34;:848,&#34;ĠëĬĲëĤ&#34;:849,&#34;Ġìĭ¤&#34;:850,&#34;ë§Īë&#34;:851,&#34;ëŃ&#34;:852,&#34;Ħ¤&#34;:853,&#34;ë°ĺ&#34;:854,&#34;!!!&#34;:855,&#34;¬ë¦&#34;:856,&#34;ìŀ¼&#34;:857,&#34;ĠíĻ&#34;:858,&#34;ê²½&#34;:859,&#34;ĠìŀĪëĬĶ&#34;:860,&#34;ì§Ī&#34;:861,&#34;Ġì¢ĭìĿĢ&#34;:862,&#34;©ëĭĪëĭ¤&#34;:863,&#34;Ġì¢ĭìķĺ&#34;:864,&#34;ê´Ģ&#34;:865,&#34;ëĭ¤ê³ł&#34;:866,&#34;ìī&#34;:867,&#34;Ġìĭľê°Ħ&#34;:868,&#34;ê¸¸&#34;:869,&#34;ëĿ¼ê³ł&#34;:870,&#34;ìĹĶ&#34;:871,&#34;ĠìĤ´&#34;:872,&#34;êµ°&#34;:873,&#34;Ġìĭł&#34;:874,&#34;ìĹ°ê¸°&#34;:875,&#34;ìĦ¤&#34;:876,&#34;ìķ¼ê¸°&#34;:877,&#34;ĠìĺģíĻĶê°Ģ&#34;:878,&#34;ëĭ¤ê°Ģ&#34;:879,&#34;ë°ľ&#34;:880,&#34;ĠíķĺëĤĺ&#34;:881,&#34;ìĬ¨&#34;:882,&#34;ºĲ&#34;:883,&#34;íļĮ&#34;:884,&#34;ìĹĨëĬĶ&#34;:885,&#34;Ġíĥ&#34;:886,&#34;ê°ĻìĿĢ&#34;:887,&#34;Ġì´&#34;:888,&#34;ìĸ´ìĦľ&#34;:889,&#34;ĠëķĮ&#34;:890,&#34;ì¶Ķ&#34;:891,&#34;Ġë¨&#34;:892,&#34;âĻ&#34;:893,&#34;ëŀĢ&#34;:894,&#34;ë´Ĳ&#34;:895,&#34;ĳľ&#34;:896,&#34;ĠìĹĨëĭ¤&#34;:897,&#34;Ġì²ĺ&#34;:898,&#34;ìĦ¸ìļĶ&#34;:899,&#34;ĠìĺĪ&#34;:900,&#34;ìĿ´ëŁ°&#34;:901,&#34;Ġíķł&#34;:902,&#34;ìłģìĿ´&#34;:903,&#34;ìµľê³ł&#34;:904,&#34;ìģ&#34;:905,&#34;Ġëħ¸&#34;:906,&#34;ĠíķĺëĬĶ&#34;:907,&#34;ìĿ¸ëį°&#34;:908,&#34;ëĲľ&#34;:909,&#34;ĠëĤĺìĺ¤&#34;:910,&#34;ëĭµ&#34;:911,&#34;!!!!&#34;:912,&#34;ĠëĦĺ&#34;:913,&#34;ĠìķĦëĭĪ&#34;:914,&#34;ì¦Ī&#34;:915,&#34;Ġì£½&#34;:916,&#34;ì¦Ŀ&#34;:917,&#34;ĠíĶ&#34;:918,&#34;ľì°&#34;:919,&#34;ĠìĿĺ&#34;:920,&#34;ìĹĲê²Į&#34;:921,&#34;ìĺĪ&#34;:922,&#34;Ġìķ¡ìħĺ&#34;:923,&#34;ëĵ¤ìĿĢ&#34;:924,&#34;ë¯¼&#34;:925,&#34;ìĽĢ&#34;:926,&#34;ãħ¡ãħ¡&#34;:927,&#34;ì½Ķ&#34;:928,&#34;Ġê¼&#34;:929,&#34;Ġì½Ķ&#34;:930,&#34;ìķĬ&#34;:931,&#34;ê·¹&#34;:932,&#34;Ġëª¨ë¥´&#34;:933,&#34;Ġíı&#34;:934,&#34;êµĲ&#34;:935,&#34;Ġëª°&#34;:936,&#34;ĠìķĦê¹Ŀ&#34;:937,&#34;Ġì¶Ķ&#34;:938,&#34;ìł¸&#34;:939,&#34;ìłģìľ¼ë¡ľ&#34;:940,&#34;ìĿ´ëĤĺ&#34;:941,&#34;ìĺ¨&#34;:942,&#34;ì¼&#34;:943,&#34;ĠíĽĦ&#34;:944,&#34;Ġ3&#34;:945,&#34;Ġê¸°ëĮĢ&#34;:946,&#34;ì²ľ&#34;:947,&#34;ĮìĿ´&#34;:948,&#34;ê²łëĭ¤&#34;:949,&#34;íĮĲ&#34;:950,&#34;Ġìµľê³łìĿĺ&#34;:951,&#34;ìŀĪëĬĶ&#34;:952,&#34;ê²¨&#34;:953,&#34;Ġíŀ&#34;:954,&#34;íķĻ&#34;:955,&#34;»Ķ&#34;:956,&#34;ë¶ĢíĦ°&#34;:957,&#34;íĸī&#34;:958,&#34;Ġëĸ&#34;:959,&#34;ë©°&#34;:960,&#34;ìĶ¨&#34;:961,&#34;ì´&#34;:962,&#34;ĠìĦ±&#34;:963,&#34;ľì°®&#34;:964,&#34;ëĮĢë¡ľ&#34;:965,&#34;ĠìĿ´íķ´&#34;:966,&#34;ĠëĺĲ&#34;:967,&#34;ê¸ī&#34;:968,&#34;íķľëĭ¤&#34;:969,&#34;ì°¨&#34;:970,&#34;ĪëĦ¤&#34;:971,&#34;ëĤł&#34;:972,&#34;ë¡Ŀ&#34;:973,&#34;íĭ°&#34;:974,&#34;íĸĪëĭ¤&#34;:975,&#34;ëĬĲ&#34;:976,&#34;ĠíĺĦ&#34;:977,&#34;ìĭľê°Ħ&#34;:978,&#34;íĽĦ&#34;:979,&#34;ĠíĬ&#34;:980,&#34;ĠìĹ°ì¶ľ&#34;:981,&#34;ĠíĸĪ&#34;:982,&#34;Ġëĵ¤&#34;:983,&#34;Ġë§Īì§Ģë§ī&#34;:984,&#34;Ġë¶Ī&#34;:985,&#34;ë°°ìļ°&#34;:986,&#34;ĠìĿ´ëłĩê²Į&#34;:987,&#34;ìŀĶ&#34;:988,&#34;Ġë¶Ħ&#34;:989,&#34;Ġë©ĭ&#34;:990,&#34;ì£&#34;:991,&#34;Ġì²ĺìĿĮ&#34;:992,&#34;Ġê´Ģ&#34;:993,&#34;ĠìĽĲ&#34;:994,&#34;Ġì§ľ&#34;:995,&#34;ĠìĿ´ìķ¼ê¸°&#34;:996,&#34;Ġê·¹&#34;:997,&#34;ìłĪ&#34;:998,&#34;ìłķëıĦ&#34;:999,&#34;íķ©ëĭĪëĭ¤&#34;:1000,&#34;íĺ¸&#34;:1001,&#34;íĶ¼&#34;:1002,&#34;Ġê¸°ìĸµ&#34;:1003,&#34;ĠìľĦ&#34;:1004,&#34;ĠëĶ°&#34;:1005,&#34;ì§ĢëıĦ&#34;:1006,&#34;ĠíĽ&#34;:1007,&#34;Ġíģ&#34;:1008,&#34;ì£¼ëĬĶ&#34;:1009,&#34;ê·¸ëĥ¥&#34;:1010,&#34;ì¹ľ&#34;:1011,&#34;ĠëıĦ&#34;:1012,&#34;ĠìĿ´ê±´&#34;:1013,&#34;ìĬ´&#34;:1014,&#34; &#34; &#34; &#34; &#34;&#34;:1015,&#34;ë§¨&#34;:1016,&#34;ìĤ´&#34;:1017,&#34;Ġìŀ¥ë©´&#34;:1018,&#34;ë¸&#34;:1019,&#34;âĻ¥&#34;:1020,&#34;íĤ¤&#34;:1021,&#34;ë¡ł&#34;:1022,&#34;Ġìµľìķħ&#34;:1023,&#34;Ġê°ķ&#34;:1024,&#34;íĺĢ&#34;:1025,&#34;Ġë§İìĿ´&#34;:1026,&#34;ë¥ĺ&#34;:1027,&#34;ë¬¸ìĹĲ&#34;:1028,&#34;ĠìĦ¸&#34;:1029,&#34;Ġíıī&#34;:1030,&#34;ĠíķľêµŃ&#34;:1031,&#34;Ġê·¸ë¦¬ê³ł&#34;:1032,&#34;Ġë§Įëĵł&#34;:1033,&#34;ëĤĺëĬĶ&#34;:1034,&#34;Ġë¬&#34;:1035,&#34;ãħĭãħĭãħĭ&#34;:1036,&#34;Ġëªħìŀĳ&#34;:1037,&#34;ĠìĪ&#34;:1038,&#34;ĠëĳĲ&#34;:1039,&#34;ĪëĿ¼&#34;:1040,&#34;ĠíĻĶ&#34;:1041,&#34;ĠëıĻ&#34;:1042,&#34;ĠìĻĦìłĦ&#34;:1043,&#34;ìĿ´ê±°&#34;:1044,&#34;ë¦¬ëĬĶ&#34;:1045,&#34;ë³¼&#34;:1046,&#34;ëĿ¼ëĬĶ&#34;:1047,&#34;ê¸Ģ&#34;:1048,&#34;Ġìŀ¬ë°Įê²Į&#34;:1049,&#34;ĠìķĦê¹Į&#34;:1050,&#34;ëŁ¼&#34;:1051,&#34;ë¨¸&#34;:1052,&#34;ì¢Ģ&#34;:1053,&#34;ëª»&#34;:1054,&#34;ê°Ĳëıħ&#34;:1055,&#34;ìĹĨëĭ¤&#34;:1056,&#34;ĠëĤľ&#34;:1057,&#34;Ġë³Ħë¡ľ&#34;:1058,&#34;ĦìļĶ&#34;:1059,&#34;Ġê·¸ëŁ°&#34;:1060,&#34;ìķĺëĭ¤&#34;:1061,&#34;ë°©&#34;:1062,&#34;ĠìķĦìī&#34;:1063,&#34;ĠìºĲ&#34;:1064,&#34;ìķĮ&#34;:1065,&#34;Ġë³´ìĹ¬&#34;:1066,&#34;ë³´ëĬĶ&#34;:1067,&#34;ĶìĿ´&#34;:1068,&#34;00&#34;:1069,&#34;ë§¤&#34;:1070,&#34;Īë¬¼&#34;:1071,&#34;ë³Ħ&#34;:1072,&#34;Ġê´ľì°®&#34;:1073,&#34;ĠìĿ´ê±°&#34;:1074,&#34;ê²°&#34;:1075,&#34;Ġë°°&#34;:1076,&#34;ĠìķĦëĭĮ&#34;:1077,&#34;¿Ĳ&#34;:1078,&#34;ëł¸&#34;:1079,&#34;ëĪ&#34;:1080,&#34;Ġíİ&#34;:1081,&#34;ê²ĥëıĦ&#34;:1082,&#34;Ġë¡ľ&#34;:1083,&#34;ëŃĲ&#34;:1084,&#34;íķĺëĤĺ&#34;:1085,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:1086,&#34;Ġë³Ħ&#34;:1087,&#34;ĠëıĪ&#34;:1088,&#34;ìĤ¬ëŀĮ&#34;:1089,&#34;ë¦Ń&#34;:1090,&#34;íĺķ&#34;:1091,&#34;ì©&#34;:1092,&#34;ë§ĮìĹĲ&#34;:1093,&#34;ìĥī&#34;:1094,&#34;ĠëĤ´ê°Ģ&#34;:1095,&#34;ì³&#34;:1096,&#34;ëģ¼&#34;:1097,&#34;ë¨¹&#34;:1098,&#34;ìĻľ&#34;:1099,&#34;ìŀĺ&#34;:1100,&#34;Ġìŀ¼&#34;:1101,&#34;ãĦ·&#34;:1102,&#34;ìĿĦê¹Į&#34;:1103,&#34;ê°ĢëĬĶ&#34;:1104,&#34;Ġê°ĻìĿĢ&#34;:1105,&#34;ëħĢ&#34;:1106,&#34;ìķĦìĦľ&#34;:1107,&#34;Ġìĸ´ëĸ&#34;:1108,&#34;ĠëŃĶ&#34;:1109,&#34;ĠìĹĨê³ł&#34;:1110,&#34;íħĮ&#34;:1111,&#34;Ġë°ľ&#34;:1112,&#34;ìĽł&#34;:1113,&#34;ê·¼&#34;:1114,&#34;Ġëĭ¹&#34;:1115,&#34;°ĭ&#34;:1116,&#34;ĠìķĦë¦Ħ&#34;:1117,&#34;ìĻĶ&#34;:1118,&#34;ê²©&#34;:1119,&#34;ë³µ&#34;:1120,&#34;Ġê·¸ëŀĺ&#34;:1121,&#34;ìĹĨìĿ´&#34;:1122,&#34;ê¹Ģ&#34;:1123,&#34;ë¦¼&#34;:1124,&#34;ìĪł&#34;:1125,&#34;ëª©&#34;:1126,&#34;........&#34;:1127,&#34;íĽ&#34;:1128,&#34;¬ë¦¬&#34;:1129,&#34;ìĹĪëĬĶëį°&#34;:1130,&#34;Ġì§Ģê¸Ī&#34;:1131,&#34;Ġê¹Ģ&#34;:1132,&#34;Ġìļ¸&#34;:1133,&#34;ĠíĬ¹&#34;:1134,&#34;ëĲ&#34;:1135,&#34;ìĿ¸ê°Ģ&#34;:1136,&#34;ìĬ¤íĨłë¦¬&#34;:1137,&#34;ĠìĹ¬ìŀĲ&#34;:1138,&#34;íķĺê¸°&#34;:1139,&#34;Ġíĳľ&#34;:1140,&#34;Ġê¼Ń&#34;:1141,&#34;ì½&#34;:1142,&#34;ĠëĬĲëĤĮ&#34;:1143,&#34;ëľ&#34;:1144,&#34;ìĽĥ&#34;:1145,&#34;Ġë§ī&#34;:1146,&#34;Ġíŀĺ&#34;:1147,&#34;ëĭĪë&#34;:1148,&#34;ëĤ´ìļ©&#34;:1149,&#34;Ġìĭ¬&#34;:1150,&#34;ãħī&#34;:1151,&#34;ĠìķĦê¹Ŀëĭ¤&#34;:1152,&#34;ë´ī&#34;:1153,&#34;ì§±&#34;:1154,&#34;ĠìĹĦ&#34;:1155,&#34;ê¼&#34;:1156,&#34;ìĹĩ&#34;:1157,&#34;íķĺë©´&#34;:1158,&#34;ãħľãħľ&#34;:1159,&#34;ìĺĢëĭ¤&#34;:1160,&#34;Ġë§ĪìĿĮ&#34;:1161,&#34;Ġãħłãħł&#34;:1162,&#34;ëĵľëĬĶ&#34;:1163,&#34;ĠíĹĪ&#34;:1164,&#34;Ġì¤Ħ&#34;:1165,&#34;ìĺģíĻĶëĬĶ&#34;:1166,&#34; &#34; &#34; &#34;&#34;:1167,&#34;ëĭĪê¹Į&#34;:1168,&#34;¬ëĿ¼&#34;:1169,&#34;ëĵľëĿ¼ë§Ī&#34;:1170,&#34;ìĪľ&#34;:1171,&#34;ëĤ´ê°Ģ&#34;:1172,&#34;Ġê·Ģ&#34;:1173,&#34;ê°ķ&#34;:1174,&#34;ì¹´&#34;:1175,&#34;ĠíĨ&#34;:1176,&#34;êµ¬ëĤĺ&#34;:1177,&#34;ĠìĿĮ&#34;:1178,&#34;íĮĮ&#34;:1179,&#34;ë§ĲìĿ´&#34;:1180,&#34;ì§ĳ&#34;:1181,&#34;ìĺģíĻĶê°Ģ&#34;:1182,&#34;ìĿ´ê²Į&#34;:1183,&#34;ĠìĦ¤&#34;:1184,&#34;Ġì£¼ìĿ¸ê³µ&#34;:1185,&#34;Ġë§Ŀ&#34;:1186,&#34;íķĺì§Ģë§Į&#34;:1187,&#34;Ġë³´ë©´&#34;:1188,&#34;ĠìĿ´ìĥģ&#34;:1189,&#34;Ġêµ¬&#34;:1190,&#34;ĠíıīìłĲìĿ´&#34;:1191,&#34;ĠìĹĲ&#34;:1192,&#34;Ġì°¨&#34;:1193,&#34;ĠëĲ&#34;:1194,&#34;ìķĦìļĶ&#34;:1195,&#34;Ġëĭ¨&#34;:1196,&#34;ĠìłĲ&#34;:1197,&#34;Ġë³´ê¸°&#34;:1198,&#34;ìĿ´ê³ł&#34;:1199,&#34;ĠìĺģíĻĶëĭ¤&#34;:1200,&#34;Ķì§ģ&#34;:1201,&#34;ê°ĲëıĻ&#34;:1202,&#34;ìĺģíĻĶë¥¼&#34;:1203,&#34;ëĵ±&#34;:1204,&#34;ë¹ł&#34;:1205,&#34;ë¯¸ëĶĶ&#34;:1206,&#34;ĠìĬ¬&#34;:1207,&#34;Ġì¡¸&#34;:1208,&#34;íģ¬&#34;:1209,&#34;ì»&#34;:1210,&#34;íĥľ&#34;:1211,&#34;Ġë¯&#34;:1212,&#34;ĠëĤ®&#34;:1213,&#34;ìĿ¸ì§Ģ&#34;:1214,&#34;Ġë§¤ëł¥&#34;:1215,&#34;ìĬ¤ëŁ½&#34;:1216,&#34;ĠìļĶ&#34;:1217,&#34;10&#34;:1218,&#34;Ġë§ŀ&#34;:1219,&#34;ĠíĺĦìĭ¤&#34;:1220,&#34;Ġê°Ģìŀ¥&#34;:1221,&#34;ëĬĶëĭ¤&#34;:1222,&#34;ëĬĶì§Ģ&#34;:1223,&#34;Ġê±¸&#34;:1224,&#34;ĠëĨĴ&#34;:1225,&#34;ìķĪë&#34;:1226,&#34;ì°¸&#34;:1227,&#34;į¼&#34;:1228,&#34;ĠìĨį&#34;:1229,&#34;íıī&#34;:1230,&#34;ìĥĿê°ģ&#34;:1231,&#34;ì¼Ģ&#34;:1232,&#34;ìĸ¸&#34;:1233,&#34;ĠìķĦìĿ´&#34;:1234,&#34;ìĸĳ&#34;:1235,&#34;Ġëª°ìŀħ&#34;:1236,&#34;ìĿ´ê±´&#34;:1237,&#34;ëĶ°&#34;:1238,&#34;Ġ.&#34;:1239,&#34;Ġë¹ł&#34;:1240,&#34;Ġë³¼ë§Į&#34;:1241,&#34;ìģĺ&#34;:1242,&#34;ì§Ģë£¨&#34;:1243,&#34;ë¦ŃíĦ°&#34;:1244,&#34;ĠìŀĪëĭ¤&#34;:1245,&#34;ĠìłķëıĦ&#34;:1246,&#34;ĠíĿ¥&#34;:1247,&#34;Ġì§ľì¦Ŀ&#34;:1248,&#34;íĮħ&#34;:1249,&#34;Ġë¦&#34;:1250,&#34;Ġ..&#34;:1251,&#34;Ġëĭ¤ë¥¸&#34;:1252,&#34;ì£¼ìĿ¸ê³µ&#34;:1253,&#34;Ġë¬´ìĬ¨&#34;:1254,&#34;ĠìĦł&#34;:1255,&#34;Ġìĸ¼&#34;:1256,&#34;ìĻĦìłĦ&#34;:1257,&#34;Ġë§Įëĵ¤ìĸ´&#34;:1258,&#34;ĠìŀĶ&#34;:1259,&#34;ìĶ¬&#34;:1260,&#34;Ġê³Ħ&#34;:1261,&#34;ìĹĪëįĺ&#34;:1262,&#34;Ġãħĭãħĭãħĭ&#34;:1263,&#34;ì£ł&#34;:1264,&#34;ìĹĲëĬĶ&#34;:1265,&#34;ĠëĮĢíķľ&#34;:1266,&#34;¨ìĸ´&#34;:1267,&#34;Ġìľłì¹ĺ&#34;:1268,&#34;Ġì¡´&#34;:1269,&#34;ĠëĪĪë¬¼&#34;:1270,&#34;ãħĩ&#34;:1271,&#34;ë°°&#34;:1272,&#34;ë°Ľ&#34;:1273,&#34;ìĬ¤ëŁ¬&#34;:1274,&#34;Ġì¶©&#34;:1275,&#34;ì§Ģê³ł&#34;:1276,&#34;ì§ĢìķĬ&#34;:1277,&#34;ĠìĿ´ê²Į&#34;:1278,&#34;ëĤĺìĦľ&#34;:1279,&#34;ìĤ¬ë&#34;:1280,&#34;Ġê³¼&#34;:1281,&#34;ëĶ©&#34;:1282,&#34;ĠìķĪëĲĺ&#34;:1283,&#34;Ġìłģ&#34;:1284,&#34;ĠìĹŃìĭľ&#34;:1285,&#34;ĠíķĦìļĶ&#34;:1286,&#34;ĠìĦľ&#34;:1287,&#34;ëĬ¥&#34;:1288,&#34;Ġãħİãħİ&#34;:1289,&#34;ìĤ°&#34;:1290,&#34;ĠìĥĿ&#34;:1291,&#34;ì²ĺëŁ¼&#34;:1292,&#34;Ġìļ°ë¦¬&#34;:1293,&#34;Ġëĵ¯&#34;:1294,&#34;ĠìłĦê°ľ&#34;:1295,&#34;ì±&#34;:1296,&#34;Ķì§ģíŀĪ&#34;:1297,&#34;Ġë°ĺìłĦ&#34;:1298,&#34;ëł¤ê³ł&#34;:1299,&#34;ìĵ°ëłĪê¸°&#34;:1300,&#34;ëĤĺìĺ¤&#34;:1301,&#34;íĽĪ&#34;:1302,&#34;Ġãħĭ&#34;:1303,&#34;ĠëĪĦ&#34;:1304,&#34;Ġê±´&#34;:1305,&#34;íĪ&#34;:1306,&#34;ë©Ķ&#34;:1307,&#34;ì¡Į&#34;:1308,&#34;ĠëĪĪ&#34;:1309,&#34;Ġê³µíı¬&#34;:1310,&#34;ĠëĤ¨ìŀĲ&#34;:1311,&#34;ëŀľ&#34;:1312,&#34;ĠìĬ¤ë¦´&#34;:1313,&#34;¾Į&#34;:1314,&#34;ì¦ĺ&#34;:1315,&#34;Ġë°©&#34;:1316,&#34;Ġì§ĳ&#34;:1317,&#34;ìŀ¬ë¯¸ìŀĪ&#34;:1318,&#34;ì³Ĳ&#34;:1319,&#34;ë§ģ&#34;:1320,&#34;ìĹĪìĿĮ&#34;:1321,&#34;Ġë°ķ&#34;:1322,&#34;ëĭ¤ëĭĪ&#34;:1323,&#34;ĠìĿ¸ìĥĿ&#34;:1324,&#34;Ġíı¬&#34;:1325,&#34;êµ¬ë&#34;:1326,&#34;êµ¿&#34;:1327,&#34;ĸìĹĲ&#34;:1328,&#34;ìŀ¡&#34;:1329,&#34;Ġì¦&#34;:1330,&#34;ĠëıĮ&#34;:1331,&#34;íģ¼&#34;:1332,&#34;Ġë»Ķ&#34;:1333,&#34;íĶĪ&#34;:1334,&#34;Ġì°¾&#34;:1335,&#34;ĠìķĦë¬´&#34;:1336,&#34;Ġíķĺì§Ģë§Į&#34;:1337,&#34;ĠìĿ¼ë³¸&#34;:1338,&#34;ĠìĪĺì¤Ģ&#34;:1339,&#34;ëĬĶê±°&#34;:1340,&#34;íĨµ&#34;:1341,&#34;ê»ĺ&#34;:1342,&#34;ë¶Ī&#34;:1343,&#34;ë§Īì§Ģë§ī&#34;:1344,&#34;ëĭ¤ìļ´&#34;:1345,&#34;íĥĦ&#34;:1346,&#34;ìĦĿ&#34;:1347,&#34;Ġê¸´&#34;:1348,&#34;Ġê°ĢìĬ´&#34;:1349,&#34;Ġë§İìĿĢ&#34;:1350,&#34;ìĤ¬ëŀĳ&#34;:1351,&#34;ìŁ&#34;:1352,&#34;Ġíķĺê³ł&#34;:1353,&#34;¸ìłľ&#34;:1354,&#34;ì¾Į&#34;:1355,&#34;ëĭ¬&#34;:1356,&#34;Ġãħ¡ãħ¡&#34;:1357,&#34;Ġë°Ľ&#34;:1358,&#34;ì¤ĳìĹĲ&#34;:1359,&#34;ĠëģĿê¹Įì§Ģ&#34;:1360,&#34;Ġìĸµ&#34;:1361,&#34;Īë°&#34;:1362,&#34;Ġì±&#34;:1363,&#34;ëĵ¤ìĿĦ&#34;:1364,&#34;ìĬ¤íĦ°&#34;:1365,&#34;į¨&#34;:1366,&#34;»ê²Į&#34;:1367,&#34;ë¦½&#34;:1368,&#34;Ġì§±&#34;:1369,&#34;ëĤĺëĿ¼&#34;:1370,&#34;ĠíĴ&#34;:1371,&#34;ĠìķĦëĭĪëĿ¼&#34;:1372,&#34;ĠìĿ¸ê°Ħ&#34;:1373,&#34;Ġìĺģìĥģ&#34;:1374,&#34;ĠOO&#34;:1375,&#34;ìĸ¼&#34;:1376,&#34;ìĿ´ì§Ģ&#34;:1377,&#34;Ĵ¤&#34;:1378,&#34;ê»´&#34;:1379,&#34;ë¦¬ì¦Ī&#34;:1380,&#34;ëįĶëĿ¼&#34;:1381,&#34;Ġíĭ&#34;:1382,&#34;Ġë´ĲëıĦ&#34;:1383,&#34;íĿ¬&#34;:1384,&#34;ĠëĲľ&#34;:1385,&#34;ê¸°ëıĦ&#34;:1386,&#34;Ġíĸī&#34;:1387,&#34;º¼&#34;:1388,&#34;ìĨ¡&#34;:1389,&#34;íĸĪëįĺ&#34;:1390,&#34;Ġëª¨ëĳĲ&#34;:1391,&#34;Ġì°į&#34;:1392,&#34;Ġê·¹ìŀ¥&#34;:1393,&#34;íĻ©&#34;:1394,&#34;ìŀ¥ë©´&#34;:1395,&#34;ĠìĪľ&#34;:1396,&#34;ì£½&#34;:1397,&#34;.....&#34;:1398,&#34;Ġíİ¸&#34;:1399,&#34;ĠíĶ¼&#34;:1400,&#34;ìĹŃìĭľ&#34;:1401,&#34;ëĲĺëĬĶ&#34;:1402,&#34;Ġìļķ&#34;:1403,&#34;ìłĳ&#34;:1404,&#34;Ġì¶Ķì²ľ&#34;:1405,&#34;ì£¼ê³ł&#34;:1406,&#34;ł¤&#34;:1407,&#34;Ġëª¨ëĵł&#34;:1408,&#34;ëĦĺ&#34;:1409,&#34;ê¶&#34;:1410,&#34;ë§İ&#34;:1411,&#34;Ġë´¤ëĭ¤&#34;:1412,&#34;ìŁģ&#34;:1413,&#34;ìĬ¤íĬ¸&#34;:1414,&#34;Ġê·¼&#34;:1415,&#34;ìĿĦëķĮ&#34;:1416,&#34;Ġìŀ¬&#34;:1417,&#34;ìķĦìĿ´&#34;:1418,&#34;Ġê°Ļëĭ¤&#34;:1419,&#34;ëĬĺ&#34;:1420,&#34;ĠìĿĮìķħ&#34;:1421,&#34;Ġìĭ¤ë§Ŀ&#34;:1422,&#34;Ġë¨¸&#34;:1423,&#34;ĪëĦ¤ìļĶ&#34;:1424,&#34;ë§Įëĵ¤&#34;:1425,&#34;ì¢ħ&#34;:1426,&#34;ë¿Ĳ&#34;:1427,&#34;Ġìĵ°&#34;:1428,&#34;ë¦¬ê°Ģ&#34;:1429,&#34;ìĹĲëıĦ&#34;:1430,&#34;ĠìĻ¸&#34;:1431,&#34;Ġëªĩ&#34;:1432,&#34;ĪëĮĢ&#34;:1433,&#34;ìĿ´ìĥģ&#34;:1434,&#34;ìĿ´ëŀĳ&#34;:1435,&#34;ê±´ì§Ģ&#34;:1436,&#34;ëĨĵ&#34;:1437,&#34;ìĹĨê³ł&#34;:1438,&#34;ĠìĻĦ&#34;:1439,&#34;ìĦŃ&#34;:1440,&#34;Ġê²Į&#34;:1441,&#34;Ġ&#39;&#34;:1442,&#34;ĠìķĦì§ģ&#34;:1443,&#34;ëĺ&#34;:1444,&#34;ĠíĤ&#34;:1445,&#34;ìĪĺë¡Ŀ&#34;:1446,&#34;ĠëĤĺìĺ¨&#34;:1447,&#34;ĠìłĦíĺĢ&#34;:1448,&#34;íĸĪëĬĶëį°&#34;:1449,&#34;ĵ¤&#34;:1450,&#34;ĠìĹ´&#34;:1451,&#34;Ġë¨¹&#34;:1452,&#34;ë´ĲëıĦ&#34;:1453,&#34;ìĻ¸&#34;:1454,&#34;Ġëª¨ìĬµ&#34;:1455,&#34;ĠìĹ°ê¸°ëł¥&#34;:1456,&#34;Ġíķľë²Ī&#34;:1457,&#34;Ġìĸµì§Ģ&#34;:1458,&#34;Ġì¤Ģ&#34;:1459,&#34;ëľ»&#34;:1460,&#34;Ġ4&#34;:1461,&#34;íĬ¹&#34;:1462,&#34;ìłĲëıĦ&#34;:1463,&#34;Ġê²½&#34;:1464,&#34;Ġ...&#34;:1465,&#34;ĠìºĲë¦ŃíĦ°&#34;:1466,&#34;ìŀ¥ê°Ĳ&#34;:1467,&#34;ìĿ´ëĿ¼ëĬĶ&#34;:1468,&#34;ëĵ¤ëıĦ&#34;:1469,&#34;Ŀ½&#34;:1470,&#34;ìĿ´ëĬĶ&#34;:1471,&#34;ĠëĤ´ëĤ´&#34;:1472,&#34;Ġê²°ë§Ĳ&#34;:1473,&#34;íį¼&#34;:1474,&#34;ëıħ&#34;:1475,&#34;ê¸°ëĬĶ&#34;:1476,&#34;ìĬ¹&#34;:1477,&#34;Ġì¢ĭëĭ¤&#34;:1478,&#34;ĠíĳľíĺĦ&#34;:1479,&#34;ëĮĢì²´&#34;:1480,&#34;Ġê³µê°Ĳ&#34;:1481,&#34;Ġë³´ì§Ģ&#34;:1482,&#34;ìĪĺê°Ģ&#34;:1483,&#34;ĠëĨĢ&#34;:1484,&#34;Ġì¹ĺ&#34;:1485,&#34;ĠìķĬê³ł&#34;:1486,&#34;Ġê³ĦìĨį&#34;:1487,&#34;©ĶìĿ´&#34;:1488,&#34;íĥĢìŀĦ&#34;:1489,&#34;Ġê°Ħ&#34;:1490,&#34;Ġíķ¨&#34;:1491,&#34;ëıĪ&#34;:1492,&#34;Ġì¶ľ&#34;:1493,&#34;ë§ŀ&#34;:1494,&#34;ê¸°ëĮĢ&#34;:1495,&#34;ìĸ´ìķ¼&#34;:1496,&#34;íķĺëĦ¤ìļĶ&#34;:1497,&#34;ìĹĪìĸ´ìļĶ&#34;:1498,&#34;Ġ8&#34;:1499,&#34;ĠìĹ¬ìļ´&#34;:1500,&#34;ëŁ¬ë&#34;:1501,&#34;Ġì½Ķë¯¸ëĶĶ&#34;:1502,&#34;ì¢ĭìĿĢ&#34;:1503,&#34;íĳľ&#34;:1504,&#34;ĠìĿ´ìłľ&#34;:1505,&#34;Ġë§ĲìĿ´&#34;:1506,&#34;ë¸Į&#34;:1507,&#34;ľ´&#34;:1508,&#34;ì¡´&#34;:1509,&#34;Ġ9&#34;:1510,&#34;íķĺëĦ¤&#34;:1511,&#34;ĠìĺģíĻĶìĹĲ&#34;:1512,&#34;ê¾&#34;:1513,&#34;ëĤ´ëĤ´&#34;:1514,&#34;ëķ&#34;:1515,&#34;ëĿ¼ëıĦ&#34;:1516,&#34;Ġëĵ±&#34;:1517,&#34;íĻĺ&#34;:1518,&#34;ìłĦìĹĲ&#34;:1519,&#34;Ġë²Ħ&#34;:1520,&#34;ĠëĮĢëĭ¨&#34;:1521,&#34;ëķĮë¬¸ìĹĲ&#34;:1522,&#34;Ġê¶&#34;:1523,&#34;Ġì»&#34;:1524,&#34;ĠìĹĦì²Ń&#34;:1525,&#34;íĹĪ&#34;:1526,&#34;ĠëĶ±&#34;:1527,&#34;ĠëĤĺìĺ¤ëĬĶ&#34;:1528,&#34;ê·¸ëŀĺ&#34;:1529,&#34;Ġëĭµ&#34;:1530,&#34;ëģĿ&#34;:1531,&#34;Ġ5&#34;:1532,&#34;ëĭ´&#34;:1533,&#34;ë»Ķ&#34;:1534,&#34;ê°Ŀ&#34;:1535,&#34;Ġì¦Ĳ&#34;:1536,&#34;ĠëĿ¼&#34;:1537,&#34;ĠìķĦëĭĪëĭ¤&#34;:1538,&#34;Ġëĸ¨ìĸ´&#34;:1539,&#34;ĠêµĲ&#34;:1540,&#34;ëĿ¼ë¦¬&#34;:1541,&#34;ĠìĹī&#34;:1542,&#34;ĠìķĬìĿĢ&#34;:1543,&#34;~~~&#34;:1544,&#34;ĠëĬĲê»´&#34;:1545,&#34;ìĿ´ê°Ģ&#34;:1546,&#34;ëŀĮ&#34;:1547,&#34;ë²Ķ&#34;:1548,&#34;íķľíħĮ&#34;:1549,&#34;ê¸°ìĹĲ&#34;:1550,&#34;Ġê°Ī&#34;:1551,&#34;Ł¬&#34;:1552,&#34;ìĸ´ëıĦ&#34;:1553,&#34;Ġ7&#34;:1554,&#34;ìķ¡ìħĺ&#34;:1555,&#34;ë¦¬ìĺ¤&#34;:1556,&#34;Ġê·¸ëłĩ&#34;:1557,&#34;ìķ½&#34;:1558,&#34;ë§Įíķľ&#34;:1559,&#34;Ġë¯¿&#34;:1560,&#34;ĠìĺģíĻĶìĿĺ&#34;:1561,&#34;ì´Ī&#34;:1562,&#34;ìłĲìĿ´&#34;:1563,&#34;Ġìŀħ&#34;:1564,&#34;Ġíĺ¸&#34;:1565,&#34;ëĭ¤ë©´&#34;:1566,&#34;íķ´ìļĶ&#34;:1567,&#34;ĠìķĦìī½&#34;:1568,&#34;íķĺë©´ìĦľ&#34;:1569,&#34;Ġìķ½&#34;:1570,&#34;ìĦ±ìĿ´&#34;:1571,&#34;Ġë§ĮëĵľëĬĶ&#34;:1572,&#34;ĠìĹ°ê¸°ê°Ģ&#34;:1573,&#34;ëĿ¼ë§Īë&#34;:1574,&#34;ìĵ°&#34;:1575,&#34;ì²ĺìĿĮ&#34;:1576,&#34;ë¥Ń&#34;:1577,&#34;ìĿ´ëĿ¼ê³ł&#34;:1578,&#34;ëĭ¤ìĭľ&#34;:1579,&#34;ìĹĦ&#34;:1580,&#34;ĠìŀĲì²´&#34;:1581,&#34;ëĿ½&#34;:1582,&#34;Ġê°ľë´ī&#34;:1583,&#34;ãĦ·ãĦ·&#34;:1584,&#34;ë³ĳ&#34;:1585,&#34;íģĲ&#34;:1586,&#34;ìŀĩ&#34;:1587,&#34;íĸ¥&#34;:1588,&#34;ĠìĨĲ&#34;:1589,&#34;ìĹħ&#34;:1590,&#34;Ġê¿&#34;:1591,&#34;ĶĶ&#34;:1592,&#34;łĪ&#34;:1593,&#34;ë§Įíģ¼&#34;:1594,&#34;ĠëŃĶê°Ģ&#34;:1595,&#34;ìµľê³łìĿĺ&#34;:1596,&#34;ĠíĮĮ&#34;:1597,&#34;Ġìĸ´ëĸ»ê²Į&#34;:1598,&#34;Ġëĭ´&#34;:1599,&#34;Ġëĸł&#34;:1600,&#34;Ġê¸°ë¶Ħ&#34;:1601,&#34;ĠìĽĲìŀĳ&#34;:1602,&#34;ĠíĿ¥ë¯¸&#34;:1603,&#34;ĠìĻĢ&#34;:1604,&#34;Ġë´Ĳìķ¼&#34;:1605,&#34;ìľ¼ëĤĺ&#34;:1606,&#34;ĠìķĦìī¬&#34;:1607,&#34;ê°Ļëĭ¤&#34;:1608,&#34;Ġìĭ¸&#34;:1609,&#34;Ġì¹ľ&#34;:1610,&#34;Įë¥Ń&#34;:1611,&#34;ë´¤ëĬĶëį°&#34;:1612,&#34;ìķĦëĭĪ&#34;:1613,&#34;ëįĺëį°&#34;:1614,&#34;Ġë¬¼&#34;:1615,&#34;ìĿ¸ëĵ¯&#34;:1616,&#34;Īë°ĺ&#34;:1617,&#34;Īë¡ľ&#34;:1618,&#34;ëĶ´&#34;:1619,&#34;ëĤ¬&#34;:1620,&#34;Ġìŀ¬ë¯¸ìŀĪê²Į&#34;:1621,&#34;ĳĺ&#34;:1622,&#34;ê¸°ê°Ģ&#34;:1623,&#34;ĠëĤĺìĻĶ&#34;:1624,&#34;ìŀĪëĭ¤&#34;:1625,&#34;ëĬĶê²Į&#34;:1626,&#34;íĺ¼&#34;:1627,&#34;Ġãħĭãħĭãħĭãħĭ&#34;:1628,&#34;ĠìµľìķħìĿĺ&#34;:1629,&#34;Ġê°Ģì¡±&#34;:1630,&#34;íķľêµŃ&#34;:1631,&#34;ì°½&#34;:1632,&#34;ìĿ´ëŀĢ&#34;:1633,&#34;Ġê³&#34;:1634,&#34;ĠìķĦì£¼&#34;:1635,&#34;Ġë¶Ģì¡±&#34;:1636,&#34;ìŀ¬ë°ĭ&#34;:1637,&#34;Ġ-&#34;:1638,&#34;Ġê·¸ëŀĺëıĦ&#34;:1639,&#34;ĠìĹ°ê¸°ëıĦ&#34;:1640,&#34;Ġìķħ&#34;:1641,&#34;ĠìłľìĿ¼&#34;:1642,&#34;Ġëıħ&#34;:1643,&#34;ëħĦëĮĢ&#34;:1644,&#34;ĠìķĦëĭĪê³ł&#34;:1645,&#34;Ġë¸&#34;:1646,&#34;ĠìĤ¶&#34;:1647,&#34;ëĤľëĭ¤&#34;:1648,&#34;ĠìķĦê¹Įìļ´&#34;:1649,&#34;ìĹĨìĿĮ&#34;:1650,&#34;ë³Ħë¡ľ&#34;:1651,&#34;ìģľ&#34;:1652,&#34;ìºĲ&#34;:1653,&#34;ì§Ģê¸Ī&#34;:1654,&#34;ĠìķĬëĬĶ&#34;:1655,&#34;ĠëĮĢë°ķ&#34;:1656,&#34;ë³´ë©´&#34;:1657,&#34;ĠìłľëĮĢë¡ľ&#34;:1658,&#34;Ġê·¼ëį°&#34;:1659,&#34;ĠìĹ¬ë&#34;:1660,&#34;ì¹ĺëĬĶ&#34;:1661,&#34;©´&#34;:1662,&#34;ê¿&#34;:1663,&#34;ìį¨&#34;:1664,&#34;ãħīãħī&#34;:1665,&#34;ì§Ĳ&#34;:1666,&#34;íķ´ëıĦ&#34;:1667,&#34;Įį&#34;:1668,&#34;Ġê½&#34;:1669,&#34;Ġìŀł&#34;:1670,&#34;Ġìĸ´ëĶĶ&#34;:1671,&#34;Ġãħ&#34;:1672,&#34;ĠìŀĬ&#34;:1673,&#34;ĠíĥĢ&#34;:1674,&#34;ĠìĶ&#34;:1675,&#34;íĮ¨&#34;:1676,&#34;ë¹Ļ&#34;:1677,&#34;Ġë³´ëĭ¤&#34;:1678,&#34;ĠëķĮë¬¸ìĹĲ&#34;:1679,&#34;Ġìŀ¬ë°ĭ&#34;:1680,&#34;ìĹ¬ìŀĲ&#34;:1681,&#34;íıīìłĲìĿ´&#34;:1682,&#34;ĠìĨĮìŀ¬&#34;:1683,&#34;Ġëª©&#34;:1684,&#34;êº¼&#34;:1685,&#34;ìĹ¬ë&#34;:1686,&#34;ìĺ¬&#34;:1687,&#34;íķłìĪĺ&#34;:1688,&#34;ĠìĿ´ìľł&#34;:1689,&#34;íĪ¬&#34;:1690,&#34;·¨&#34;:1691,&#34;ìĨĶì§ģíŀĪ&#34;:1692,&#34;Ġì¡°ê¸Ī&#34;:1693,&#34;ì¸&#34;:1694,&#34;ìĻķ&#34;:1695,&#34;Ġë©Ķ&#34;:1696,&#34;ĠëĤł&#34;:1697,&#34;ìĹĲìļĶ&#34;:1698,&#34;Ġì§Ī&#34;:1699,&#34;íģ´&#34;:1700,&#34;ëĤĺê³ł&#34;:1701,&#34;ëĤĺëıĦ&#34;:1702,&#34;ìµľìķħ&#34;:1703,&#34;ĠìĺģíĻĶëĿ¼ê³ł&#34;:1704,&#34;ë£¡&#34;:1705,&#34;ĠëĤ´ìļ©ìĿ´&#34;:1706,&#34;ĠëĲł&#34;:1707,&#34;ìĭ¶ëĭ¤&#34;:1708,&#34;ë²½&#34;:1709,&#34;ê°Ī&#34;:1710,&#34;ë¦¬ë¥¼&#34;:1711,&#34;ì§Ħëĭ¤&#34;:1712,&#34;~~~~&#34;:1713,&#34;Ġìĭ¶ìĿĢ&#34;:1714,&#34;ĵ¸&#34;:1715,&#34;ìĿ´ëłĩê²Į&#34;:1716,&#34;Ġë³´ê²Į&#34;:1717,&#34;ìķĺëĬĶëį°&#34;:1718,&#34;ë§Īëĭ¤&#34;:1719,&#34;Ġì¹´&#34;:1720,&#34;ëĤĺë¦¬ìĺ¤&#34;:1721,&#34;¬ë§ģ&#34;:1722,&#34;Ġë²ł&#34;:1723,&#34;Ĳëĭ¤&#34;:1724,&#34;ëĪĦ&#34;:1725,&#34;ìĿ´ëĦ¤&#34;:1726,&#34;ê¸°ë¥¼&#34;:1727,&#34;ĠëĤĺìĻĢ&#34;:1728,&#34;ìłģìĿ´ê³ł&#34;:1729,&#34;Ġëª¨ë¥´ê²ł&#34;:1730,&#34;íķĺëĬĶëį°&#34;:1731,&#34;êµ°ìļĶ&#34;:1732,&#34;ê¶Į&#34;:1733,&#34;ëıĮ&#34;:1734,&#34;ìĬ¤íĮħ&#34;:1735,&#34;ĠëĬĲëģ¼&#34;:1736,&#34;Ġë°°ìļ°ëĵ¤&#34;:1737,&#34;Ġê¸´ìŀ¥ê°Ĳ&#34;:1738,&#34;ê¾¸&#34;:1739,&#34;ìĿ´ìķ¼&#34;:1740,&#34;Ġìķŀ&#34;:1741,&#34;ëĤĺìļĶ&#34;:1742,&#34;ëŀľë§ĮìĹĲ&#34;:1743,&#34;ĠìĤ´ìķĦ&#34;:1744,&#34;Ġìŀ¬ë°Įëĭ¤&#34;:1745,&#34;ê³¤&#34;:1746,&#34;ìłĲìĿĢ&#34;:1747,&#34;ĠìĺģíĻĶìŀħëĭĪëĭ¤&#34;:1748,&#34;Ġíĺķ&#34;:1749,&#34;ìĬ¤ëŁ¬ìļ´&#34;:1750,&#34;ĠíĨµ&#34;:1751,&#34;ê²ģ&#34;:1752,&#34;ìĺģíĻĶìĿĺ&#34;:1753,&#34;ë°ĸìĹĲ&#34;:1754,&#34;ĠíĬ¹íŀĪ&#34;:1755,&#34;ĠìĺģíĻĶë¡ľ&#34;:1756,&#34;ë¸Ķ&#34;:1757,&#34;êµ´&#34;:1758,&#34;Ġìĭ¶ëĭ¤&#34;:1759,&#34;ĠìĿĺë¯¸&#34;:1760,&#34;ĠìĺģíĻĶìĺĢ&#34;:1761,&#34;ì¤ĺ&#34;:1762,&#34;ìĹĪìĬµëĭĪëĭ¤&#34;:1763,&#34;Ġìłķìĭł&#34;:1764,&#34;ìľ¼ë¡ľëıĦ&#34;:1765,&#34;Ġêµ¿&#34;:1766,&#34;Ġë§īìŀ¥&#34;:1767,&#34;Ľ°&#34;:1768,&#34;ìłķìĿ´&#34;:1769,&#34;ĠëĮĢìĤ¬&#34;:1770,&#34;Ġì£¼ëĬĶ&#34;:1771,&#34;Ġìĭ«&#34;:1772,&#34;Ġìĭľìŀĳ&#34;:1773,&#34;ìļ±&#34;:1774,&#34;ĠìłĪëĮĢ&#34;:1775,&#34;Ġì¢ĭê³ł&#34;:1776,&#34;Ġìłľìŀĳ&#34;:1777,&#34;âĻ¥âĻ¥&#34;:1778,&#34;ìķĦëĭ&#34;:1779,&#34;ê·Ģ&#34;:1780,&#34;ĠëĤ´ìļ©ëıĦ&#34;:1781,&#34;Ġëħ¸ëŀĺ&#34;:1782,&#34;Ġëį°&#34;:1783,&#34;ĠíĽĮë¥Ń&#34;:1784,&#34;Ġì²ľ&#34;:1785,&#34;Ġìĸ´ì©&#34;:1786,&#34;Ġìķ¼&#34;:1787,&#34;Ġê°ģ&#34;:1788,&#34;ëĦĪ&#34;:1789,&#34;ĠëĴ¤&#34;:1790,&#34;ĠëĲĺëĬĶ&#34;:1791,&#34;ĠìķĦë¦Ħëĭ¤ìļ´&#34;:1792,&#34;ëįķ&#34;:1793,&#34;ëł¨&#34;:1794,&#34;íķ´ìķ¼&#34;:1795,&#34;ĠëĤ¨ëĬĶ&#34;:1796,&#34;Ġ^^&#34;:1797,&#34;ĠíĽ¨&#34;:1798,&#34;ĠìĽĥê¸°&#34;:1799,&#34;íĬ¼&#34;:1800,&#34;ë¯¹&#34;:1801,&#34;ì§Ģê°Ģ&#34;:1802,&#34;ì§¸&#34;:1803,&#34;Ġë¶Ģë¶Ħ&#34;:1804,&#34;ëĨĪ&#34;:1805,&#34;ëĭĪë©ĶìĿ´&#34;:1806,&#34;Ġì¶ľìĹ°&#34;:1807,&#34;ĠìĪĺìŀĳ&#34;:1808,&#34;Ġì´Ī&#34;:1809,&#34;ĥĲ&#34;:1810,&#34;Ġë¦¬&#34;:1811,&#34;ëĤ¸&#34;:1812,&#34;ì¢ĭìķĦ&#34;:1813,&#34;ìĿ¸ìłģìľ¼ë¡ľ&#34;:1814,&#34;ĪëĶ&#34;:1815,&#34;ì¤Ģëĭ¤&#34;:1816,&#34;ëĭĪë©ĶìĿ´ìħĺ&#34;:1817,&#34;íķ©&#34;:1818,&#34;Ġëľ&#34;:1819,&#34;ìĭ¶ìĿĢ&#34;:1820,&#34;íľ´&#34;:1821,&#34;ëĤĺê²Į&#34;:1822,&#34;Ġë³µ&#34;:1823,&#34;ĠìĹĨìĿĮ&#34;:1824,&#34;???&#34;:1825,&#34;ë§ĪëĤĺ&#34;:1826,&#34;Ġì¤ĳê°Ħ&#34;:1827,&#34;ìĿ¸ìĿ´&#34;:1828,&#34;ë²ķ&#34;:1829,&#34;Ġãħł&#34;:1830,&#34;ìĬ¬&#34;:1831,&#34;ĠëłĪ&#34;:1832,&#34;Ģìĸ´&#34;:1833,&#34;ìĽĲìŀĳ&#34;:1834,&#34;íķľëį°&#34;:1835,&#34;Ġë´¤ìĬµëĭĪëĭ¤&#34;:1836,&#34;Ġë¹¼&#34;:1837,&#34;ĠëĶ°ëľ»&#34;:1838,&#34;ëŃĶ&#34;:1839,&#34;ì¦Į&#34;:1840,&#34;;;;&#34;:1841,&#34;_-&#34;:1842,&#34;ì¯&#34;:1843,&#34;Ġë´¤ëįĺ&#34;:1844,&#34;Ġë¡ľë§¨&#34;:1845,&#34;´Ĳ&#34;:1846,&#34;ìĮį&#34;:1847,&#34;ë°±&#34;:1848,&#34;ëĵĿ&#34;:1849,&#34;Ġìĭľë¦¬ì¦Ī&#34;:1850,&#34;íļ¨&#34;:1851,&#34;Ġ20&#34;:1852,&#34;ì§ĵ&#34;:1853,&#34;ìĿ¸ìĥĿ&#34;:1854,&#34;ìĦ±ëıĦ&#34;:1855,&#34;ê²ĥìĿ´&#34;:1856,&#34;ì¹ĺê³ł&#34;:1857,&#34;íħĲ&#34;:1858,&#34;ìĥĪ&#34;:1859,&#34;Ġë¬´ìĦľ&#34;:1860,&#34;Ġì¢ĭìķĺëĭ¤&#34;:1861,&#34;ìŀĳíĴĪ&#34;:1862,&#34;Ġë¬´ìĹĩ&#34;:1863,&#34;ê´ĳ&#34;:1864,&#34;ĠëĶĶ&#34;:1865,&#34;ìķĦê¹Ŀ&#34;:1866,&#34;Ġìĸ´ìĦ¤&#34;:1867,&#34;ĠìķĮë°Ķ&#34;:1868,&#34;ëĭ¥&#34;:1869,&#34;íķľê±°&#34;:1870,&#34;Ġì¢ĭìķĦíķĺëĬĶ&#34;:1871,&#34;íĹĺ&#34;:1872,&#34;ĠìĤ¼&#34;:1873,&#34;Ġìŀ¬ë¯¸ëıĦ&#34;:1874,&#34;Ġíĸīë³µ&#34;:1875,&#34;Ġë³´ëĭ¤ê°Ģ&#34;:1876,&#34;ì§Ŀ&#34;:1877,&#34;ìŀĸ&#34;:1878,&#34;ìĿ¼ë³¸&#34;:1879,&#34;ĠêµŃ&#34;:1880,&#34;oo&#34;:1881,&#34;ë©ĭ&#34;:1882,&#34;Ġìŀ¬ë¯¸ìĹĨëĭ¤&#34;:1883,&#34;ĠìĿ´íķ´ê°Ģ&#34;:1884,&#34;ĠìĤ°&#34;:1885,&#34;íĶĮ&#34;:1886,&#34;ìĹ´&#34;:1887,&#34;Ġê¹Ĭ&#34;:1888,&#34;ê²¬&#34;:1889,&#34;ĠìĿ´ê±¸&#34;:1890,&#34;ĠìĥĿê°ģìĿ´&#34;:1891,&#34;¬ë§ģíĥĢìŀĦ&#34;:1892,&#34;ì¶©&#34;:1893,&#34;ĠíĮĲ&#34;:1894,&#34;ëĶĶìĺ¤&#34;:1895,&#34;ìłĲìĿĦ&#34;:1896,&#34;Ġì§Ħìĭ¬&#34;:1897,&#34;ìķĦë¬´&#34;:1898,&#34;Ġë³Ģ&#34;:1899,&#34;ìĿ´ëĦ¤ìļĶ&#34;:1900,&#34;ìķĦê¹Į&#34;:1901,&#34;ë¶Ģë¶Ħ&#34;:1902,&#34;Ġìĭľê°ĦìĿ´&#34;:1903,&#34;ìļķ&#34;:1904,&#34;êµ¬ë§Į&#34;:1905,&#34;ĠíĻį&#34;:1906,&#34;,,,&#34;:1907,&#34;ĠìĿ¸ìĥģ&#34;:1908,&#34;ĠìĬ¤íĨłë¦¬ëıĦ&#34;:1909,&#34;ë¦¬ì§Ģ&#34;:1910,&#34;ĠëŃĺ&#34;:1911,&#34;ìĤ¼&#34;:1912,&#34;Ġìĸ´ìĥī&#34;:1913,&#34;Ġë¬´ìĦŃ&#34;:1914,&#34;ĠìłĲìĪĺ&#34;:1915,&#34;ĠìĹĨìĿ´&#34;:1916,&#34;ë§Ľ&#34;:1917,&#34;ëıĻìķĪ&#34;:1918,&#34;ìĹ½&#34;:1919,&#34;Ġíŀĺëĵ¤&#34;:1920,&#34;ìłĲëĮĢ&#34;:1921,&#34;Ġìŀĳê°Ģ&#34;:1922,&#34;Ġëĵ¤ìĸ´&#34;:1923,&#34;ĠìķĦì§ģëıĦ&#34;:1924,&#34;ĽëĤł&#34;:1925,&#34;ë²ł&#34;:1926,&#34;ĠìĬ¤íĨłë¦¬ê°Ģ&#34;:1927,&#34;Ġì¼&#34;:1928,&#34;ëŁ½ê²Į&#34;:1929,&#34;Ġëª»íķľ&#34;:1930,&#34;Ġíķ¨ê»ĺ&#34;:1931,&#34;ĠìĿ´ìĺģíĻĶ&#34;:1932,&#34;ìļ°ë¦¬&#34;:1933,&#34;Ġìłľëª©&#34;:1934,&#34;ĠìķłëĭĪ&#34;:1935,&#34;ëģĶ&#34;:1936,&#34;ëĤ¨ìŀĲ&#34;:1937,&#34;Ġê°ĳ&#34;:1938,&#34;ì¶ĺ&#34;:1939,&#34;ìĿĦëĵ¯&#34;:1940,&#34;Ġìŀ¡&#34;:1941,&#34;Ġê·¸ëŁ¬&#34;:1942,&#34;ĠìĹ¬ìļ´ìĿ´&#34;:1943,&#34;ĠìķĪë³´&#34;:1944,&#34;ĠíĻķ&#34;:1945,&#34;Ł¬ë&#34;:1946,&#34;ĠëĦ¤&#34;:1947,&#34;ĠìĤ¬ìĭ¤&#34;:1948,&#34;Ġëĳĺ&#34;:1949,&#34;Ġê¸¸&#34;:1950,&#34;Ġìĭľì²Ń&#34;:1951,&#34;Ġê¶ģ&#34;:1952,&#34;ĠìĨĮë¦Ħ&#34;:1953,&#34;ì¹¨&#34;:1954,&#34;Ġëĭµëĭµ&#34;:1955,&#34;ëħ¸ìŀ¼&#34;:1956,&#34;ĠìŀĲìĭł&#34;:1957,&#34;ëĵ¤ìĹĲê²Į&#34;:1958,&#34;Ġë³´ë©´ìĦľ&#34;:1959,&#34;Ġê°ĲëıħìĿĺ&#34;:1960,&#34;ìħ¨&#34;:1961,&#34;Ġìī&#34;:1962,&#34;Ġë¬¸ìłľ&#34;:1963,&#34;~!&#34;:1964,&#34;Ńë¹Ħ&#34;:1965,&#34;Ġì°¾ìķĦ&#34;:1966,&#34;ì¿&#34;:1967,&#34;Ġë¿Ĳ&#34;:1968,&#34;ĠëĽ°&#34;:1969,&#34;ìĿ´ì§Ģë§Į&#34;:1970,&#34;ëĺĲ&#34;:1971,&#34;ĠìĭłìĦł&#34;:1972,&#34;ìĿĮìķħ&#34;:1973,&#34;ìĽĶ&#34;:1974,&#34;íĺľ&#34;:1975,&#34;Ġë©ĭì§Ħ&#34;:1976,&#34;ìĿ´ìķ¼ê¸°&#34;:1977,&#34;Ġê¹¨&#34;:1978,&#34;Ġíĭ°&#34;:1979,&#34;ê±°ë¦¬&#34;:1980,&#34;ê±´ê°Ģ&#34;:1981,&#34;Ġíķĺì§Ģ&#34;:1982,&#34;Ġìĸ´ìĿ´&#34;:1983,&#34;Ġê´ľì°®ìĿĢ&#34;:1984,&#34;ê²¼&#34;:1985,&#34;ìŀĲê¸°&#34;:1986,&#34;ĠëĤĺë¦Ħ&#34;:1987,&#34;ëįĶëĭĪ&#34;:1988,&#34;ĪëįĶ&#34;:1989,&#34;Ġ6&#34;:1990,&#34;íķĺìĭľ&#34;:1991,&#34;Ġê°ĲëıħìĿ´&#34;:1992,&#34;Īë³´&#34;:1993,&#34;Ġë¹¨&#34;:1994,&#34;ìĬ·&#34;:1995,&#34;Ġë¶ĦìľĦ&#34;:1996,&#34;ĠíĹĪìłĳ&#34;:1997,&#34;ĠìĬ¤ë¦´ëŁ¬&#34;:1998,&#34;ĠìĹĶ&#34;:1999,&#34;ĠëĤļ&#34;:2000,&#34;ëĿ¼ìĿ´&#34;:2001,&#34;ìĨĮë¦¬&#34;:2002,&#34;ëĭ¤ë¥¸&#34;:2003,&#34;Īëģ&#34;:2004,&#34;êµ¬ìļĶ&#34;:2005,&#34;Ġê´Ģê°Ŀ&#34;:2006,&#34;ê²Ł&#34;:2007,&#34;ì£¼ìĿĺ&#34;:2008,&#34;Ġìļ©&#34;:2009,&#34;ëĤĺë§Ī&#34;:2010,&#34;ĠìĦ¸ìĥģ&#34;:2011,&#34;ĠC&#34;:2012,&#34;ì£Ħ&#34;:2013,&#34;Ġë³´ìĦ¸ìļĶ&#34;:2014,&#34;Ġë¯¸êµŃ&#34;:2015,&#34;ĠìķĪíĥĢ&#34;:2016,&#34;ĠìŀĶìŀĶ&#34;:2017,&#34;Ġìŀ¼ìŀĪ&#34;:2018,&#34;ëª°&#34;:2019,&#34;âĻ¡&#34;:2020,&#34;Ġíļ&#34;:2021,&#34;Ġ &#34; &#34; &#34; &#34;&#34;:2022,&#34;ëĳ&#34;:2023,&#34;Ġìŀ¬ë¯¸ê°Ģ&#34;:2024,&#34;Ġë¦¬ë&#34;:2025,&#34;ĠìĭľëĤĺë¦¬ìĺ¤&#34;:2026,&#34;Ġíģ°&#34;:2027,&#34;ìľ¤&#34;:2028,&#34;ì¤ĳê°Ħ&#34;:2029,&#34;Ġë§ĺ&#34;:2030,&#34;ë§Įëĵł&#34;:2031,&#34;Ġì¡¸ìŀĳ&#34;:2032,&#34;Īë²&#34;:2033,&#34;ĠíĿĲ&#34;:2034,&#34;ĠíĻ©&#34;:2035,&#34;ì»¤&#34;:2036,&#34;íķľìĺģíĻĶ&#34;:2037,&#34;íķĺë©°&#34;:2038,&#34;íķ¨ìĿĦ&#34;:2039,&#34;Ġ0&#34;:2040,&#34;ĠìļĶì¦ĺ&#34;:2041,&#34;ìłĦê°ľ&#34;:2042,&#34;ì¡¸&#34;:2043,&#34;ĠìĹ°ê¸°ëĬĶ&#34;:2044,&#34;ë¹Ī&#34;:2045,&#34;ì±ħ&#34;:2046,&#34;ë§ĪëĿ¼&#34;:2047,&#34;Ġê¸Ģ&#34;:2048,&#34;íĥĿ&#34;:2049,&#34;ê°ĻìĿ´&#34;:2050,&#34;Ġì°¨ëĿ¼ë¦¬&#34;:2051,&#34;ĠìŀĪìĹĪ&#34;:2052,&#34;Ġë§ĮíĻĶ&#34;:2053,&#34;ì°¬&#34;:2054,&#34;Ġìĸ¼ë§ĪëĤĺ&#34;:2055,&#34;ĠëıĮìķĦ&#34;:2056,&#34;ĠëĤĺëĬĶ&#34;:2057,&#34;ëłĪìĿ´&#34;:2058,&#34;Ġë°°ê²½&#34;:2059,&#34;ĠìĬ¬íĶĦ&#34;:2060,&#34;Ġìĸ¸ìłľ&#34;:2061,&#34;Ġê°ķì¶Ķ&#34;:2062,&#34;Ġì²¨&#34;:2063,&#34;ë§ĪëĶĶ&#34;:2064,&#34;ĠíĻĺ&#34;:2065,&#34;ĠíĽĦíļĮ&#34;:2066,&#34;Ġì©&#34;:2067,&#34;ì²Ļ&#34;:2068,&#34;ĠëĬĲëĤĮìĿ´&#34;:2069,&#34;Ġì¼Ģ&#34;:2070,&#34;ê¸¸ëŀĺ&#34;:2071,&#34;ĠìĻĦë²½&#34;:2072,&#34;ĠëĤ«&#34;:2073,&#34;ë³´ëĭ¨&#34;:2074,&#34;Ġíĺ¼&#34;:2075,&#34;!!!!!!!!&#34;:2076,&#34;Ġìĸ´ëĸ¤&#34;:2077,&#34;ĠìķĦë¦Ħëĭµ&#34;:2078,&#34;ê°Ģì§Ģ&#34;:2079,&#34;ëĮĢíķľ&#34;:2080,&#34;Ġê·ĢìĹ¬&#34;:2081,&#34;st&#34;:2082,&#34;Ġëª¸&#34;:2083,&#34;íĺĦìĭ¤&#34;:2084,&#34;ì±Ħ&#34;:2085,&#34;íĭ±&#34;:2086,&#34;ëĭ¹íŀĪ&#34;:2087,&#34;ĠìĺģíĻĶëĿ¼&#34;:2088,&#34;ëŀĦ&#34;:2089,&#34;ĠìķĬìķĺ&#34;:2090,&#34;Ġìĸĳ&#34;:2091,&#34;ê¹Ĭ&#34;:2092,&#34;ë³´ëĭĪ&#34;:2093,&#34;ëªĩ&#34;:2094,&#34;íĸĪìĿĮ&#34;:2095,&#34;Ġê·¸ìłĢ&#34;:2096,&#34;ìĻĦ&#34;:2097,&#34;ĠìĤ¬ëŀĮìĿ´&#34;:2098,&#34;íķĻêµĲ&#34;:2099,&#34;Ġë»Ķíķľ&#34;:2100,&#34;Ġì£¼ê³ł&#34;:2101,&#34;ĠìķĮê³ł&#34;:2102,&#34;ĠìĭĿ&#34;:2103,&#34;Ġê²ĥìĿ´&#34;:2104,&#34;ì¤Į&#34;:2105,&#34;Ġëĭ¤íģĲ&#34;:2106,&#34;ãħłãħłãħłãħł&#34;:2107,&#34;ĠìķłëĭĪë©ĶìĿ´ìħĺ&#34;:2108,&#34;ĠíĨł&#34;:2109,&#34;Ġë²Ķ&#34;:2110,&#34;ëĬĲëĤ&#34;:2111,&#34;ìŀĪê³ł&#34;:2112,&#34;íĥĪ&#34;:2113,&#34;ìķĪëĲĺ&#34;:2114,&#34;íħĲëį°&#34;:2115,&#34;ĠìŀĪê³ł&#34;:2116,&#34;ĠëĵľëĿ¼ë§Īë&#34;:2117,&#34;ìĿ¸ì¤Ħ&#34;:2118,&#34;Ġìŀ¬ë°ĮëĬĶ&#34;:2119,&#34;Ġìĸĺ&#34;:2120,&#34;ìĿ´ë²Ħ&#34;:2121,&#34;ĠìĺģíĻĶëıĦ&#34;:2122,&#34;ìľłì¹ĺ&#34;:2123,&#34;íıŃ&#34;:2124,&#34;Ġì´Ŀ&#34;:2125,&#34;Ġì²Ń&#34;:2126,&#34;ìŀ¬ë¯¸ìĹĨ&#34;:2127,&#34;Ġì¶Ķìĸµ&#34;:2128,&#34;Ġãħİ&#34;:2129,&#34;ĠìĽĥìĿĮ&#34;:2130,&#34;ìĽħ&#34;:2131,&#34;Ġëª¨ë¥´ê²łëĭ¤&#34;:2132,&#34;ĠíĿ¬&#34;:2133,&#34;Ġê¸°ìĸµìĹĲ&#34;:2134,&#34;ìĭ¸&#34;:2135,&#34;Ġìŀ¥ëĤľ&#34;:2136,&#34;ĠìĭľëĮĢ&#34;:2137,&#34;ĠìłĦìŁģ&#34;:2138,&#34;Ġê°ĻìĿ´&#34;:2139,&#34;Ġê·¸ëłĩê²Į&#34;:2140,&#34;ìĭľê¸¸&#34;:2141,&#34;ëªħìŀĳ&#34;:2142,&#34;Ġê¶ģê¸Ī&#34;:2143,&#34;ê°ĳ&#34;:2144,&#34;ê±°ì§Ģ&#34;:2145,&#34;ì¼ľ&#34;:2146,&#34;íı¬ë&#34;:2147,&#34;ìĽłëĭ¤&#34;:2148,&#34;ëĵ¤ê³¼&#34;:2149,&#34;Ġì¶©ë¶Ħ&#34;:2150,&#34;ëŁ´&#34;:2151,&#34;Ġíķľëĭ¤&#34;:2152,&#34;ëł¤ëĬĶ&#34;:2153,&#34;ĠìĹĶëĶ©&#34;:2154,&#34;Ġê°Ĳìłķ&#34;:2155,&#34;ê´ĢìĹĲìĦľ&#34;:2156,&#34;Ġê²°êµŃ&#34;:2157,&#34;ì½©&#34;:2158,&#34;ìĿ´ê±¸&#34;:2159,&#34;ì°į&#34;:2160,&#34;ë¬´ìĬ¨&#34;:2161,&#34;20&#34;:2162,&#34;Ġë´¤ìĸ´ìļĶ&#34;:2163,&#34;ê²ĥê°Ļ&#34;:2164,&#34;Ġê¹Į&#34;:2165,&#34;ëĭĿ&#34;:2166,&#34;ìĬ¤íĥĢ&#34;:2167,&#34;ì°°&#34;:2168,&#34;ë´Ħ&#34;:2169,&#34;ë¥´ëĬĶ&#34;:2170,&#34;ëł¥ìĿ´&#34;:2171,&#34;ĠìķĪëĲĺëĬĶ&#34;:2172,&#34;Ģìŀ¼&#34;:2173,&#34;ĠíĿĺ&#34;:2174,&#34;Ġì§Ģë£¨íķľ&#34;:2175,&#34;ìłķíķľ&#34;:2176,&#34;Ġë¹łìł¸&#34;:2177,&#34;ĠìĬ¤íĥĢ&#34;:2178,&#34;Ġì§ĳì¤ĳ&#34;:2179,&#34;Ġë¬¸&#34;:2180,&#34;Ġê¿Ī&#34;:2181,&#34;ìĿ¸ê°Ħ&#34;:2182,&#34;ìķ¼ì§Ģ&#34;:2183,&#34;ë²Ħë¦°&#34;:2184,&#34;Ġì£¼ìĹ°&#34;:2185,&#34;íķ¨ìĿ´&#34;:2186,&#34;Ġê°ľìĹ°&#34;:2187,&#34;ĠëĶ°ëĿ¼&#34;:2188,&#34;Ġìĸ´ë¦°&#34;:2189,&#34;ìłľëª©&#34;:2190,&#34;Ġë°°ìļ°ëĵ¤ìĿĺ&#34;:2191,&#34;°°ìļ°&#34;:2192,&#34;Ġíħ&#34;:2193,&#34;ëŁŃ&#34;:2194,&#34;ë°Ģ&#34;:2195,&#34;Ġì¢ħ&#34;:2196,&#34;ĪëĶ©&#34;:2197,&#34;íİĺ&#34;:2198,&#34;ê´´&#34;:2199,&#34;īìŀ¥&#34;:2200,&#34;Ġì£&#34;:2201,&#34;ê°Ķ&#34;:2202,&#34;ìĦ±ìĿĦ&#34;:2203,&#34;ĠíıŃ&#34;:2204,&#34;ëł¸ëĭ¤&#34;:2205,&#34;ĠíķĺëĤĺëıĦ&#34;:2206,&#34;íĻľ&#34;:2207,&#34;ĠìķĶ&#34;:2208,&#34;ĠìĺģíĻĶì¤ĳ&#34;:2209,&#34;Ġì¶©ê²©&#34;:2210,&#34;ë³Ģ&#34;:2211,&#34;ìŀ¬ë°Įê²Į&#34;:2212,&#34;ëĲł&#34;:2213,&#34;ĠìĺģíĻĶëĦ¤ìļĶ&#34;:2214,&#34;ĠíķĻ&#34;:2215,&#34;ĠíĶĦ&#34;:2216,&#34;íĸĪì§Ģë§Į&#34;:2217,&#34;ĠíĪ&#34;:2218,&#34;ëģĦ&#34;:2219,&#34;ĠìĺĪìĪł&#34;:2220,&#34;ë¡Ń&#34;:2221,&#34;íİ¸ìĿ´&#34;:2222,&#34;ĠìľĦíķ´&#34;:2223,&#34;ĠB&#34;:2224,&#34;ĠëģĮ&#34;:2225,&#34;ëĨĵê³ł&#34;:2226,&#34;ìĪĺì¤Ģ&#34;:2227,&#34;Ġì½Ķë¯¹&#34;:2228,&#34;ëĮĢë°ķ&#34;:2229,&#34;ìŀĲì²´&#34;:2230,&#34;Ġt&#34;:2231,&#34;Ġãħľãħľ&#34;:2232,&#34;ĠìķĬëĬĶëĭ¤&#34;:2233,&#34;ìĺģìĥģ&#34;:2234,&#34;Ġì§Ħë¶Ģ&#34;:2235,&#34;Ġìŀ¬ë°Įìĸ´ìļĶ&#34;:2236,&#34;íķĺíķĺ&#34;:2237,&#34;ĠìŀĦ&#34;:2238,&#34;ĠíĦ°&#34;:2239,&#34;......&#34;:2240,&#34;ĠëĽ°ìĸ´&#34;:2241,&#34;ìĭľëĮĢ&#34;:2242,&#34;ĠìĬ¤íĨł&#34;:2243,&#34;Ġëıĭ&#34;:2244,&#34;????&#34;:2245,&#34;Įĵ&#34;:2246,&#34;ì¹ł&#34;:2247,&#34;ëĿ¼ë©´&#34;:2248,&#34;ĠìľĦíķľ&#34;:2249,&#34;ìĿ´ìĺģíĻĶ&#34;:2250,&#34;ê¸°ë§Į&#34;:2251,&#34;íķ´ì§ĢëĬĶ&#34;:2252,&#34;ìĪĺìŀĪ&#34;:2253,&#34;ê³µíı¬&#34;:2254,&#34;Ġë¯¼&#34;:2255,&#34;ĠìĿ´ìģĺ&#34;:2256,&#34;íŀĪë&#34;:2257,&#34;Ġì§Ģë£¨íķĺê³ł&#34;:2258,&#34;Ġì»¤&#34;:2259,&#34;ĠìĻľìĿ´&#34;:2260,&#34;ìĽĮìĦľ&#34;:2261,&#34;ëĲľëĭ¤&#34;:2262,&#34;ë¦¬ìĬ¤&#34;:2263,&#34;Ġíĥľ&#34;:2264,&#34;ĠìĹĨìĹĪëĭ¤&#34;:2265,&#34;ĠëĤĺëıĦ&#34;:2266,&#34;ìī¬&#34;:2267,&#34;ìķĦì£¼&#34;:2268,&#34;ë³´ëĭ¤ê°Ģ&#34;:2269,&#34;ĠíĮ¨&#34;:2270,&#34;ê°ĢìĦľ&#34;:2271,&#34;ìĿ´ëĿ¼ëıĦ&#34;:2272,&#34;Ġë¹ĦìĬ·&#34;:2273,&#34;ĵ´&#34;:2274,&#34;ëĭ¹ìĭľ&#34;:2275,&#34;Ġëª°ìŀħëıĦ&#34;:2276,&#34;ìħĶ&#34;:2277,&#34;ľë¡ľ&#34;:2278,&#34;íĻį&#34;:2279,&#34;ĠìĿ´ë¦Ħ&#34;:2280,&#34;ì²ł&#34;:2281,&#34;ĠìĿ½&#34;:2282,&#34;Ġì±ħ&#34;:2283,&#34;ìłł&#34;:2284,&#34;ê²ĥìĿĦ&#34;:2285,&#34;Ġë§Įëĵ¤ìĹĪ&#34;:2286,&#34;ê·¸ë¦¬ê³ł&#34;:2287,&#34;ĠìĨĶì§ģíŀĪ&#34;:2288,&#34;Ġë°±&#34;:2289,&#34;íĺ¹&#34;:2290,&#34;Ġê±į&#34;:2291,&#34;ĠíĥĦ&#34;:2292,&#34;Ġíģ´&#34;:2293,&#34;Ġ,&#34;:2294,&#34;ê·¸ëŁ°&#34;:2295,&#34;ëĵ¯íķľ&#34;:2296,&#34;ĠìĤ¬ëŀĮëĵ¤&#34;:2297,&#34;ìĺ´&#34;:2298,&#34;Ġì§Ģë£¨íķĺëĭ¤&#34;:2299,&#34;ĠìºĲìĬ¤íĮħ&#34;:2300,&#34;ë³´ê¸°&#34;:2301,&#34;ì½Ķë¯¸ëĶĶ&#34;:2302,&#34;íĸĩ&#34;:2303,&#34;Ġê´´&#34;:2304,&#34;ëĨĢ&#34;:2305,&#34;Ġì£¼ìłľ&#34;:2306,&#34;Ġìľłì¾Į&#34;:2307,&#34;ĠíĮ¬&#34;:2308,&#34;ĠíĽ¨ìĶ¬&#34;:2309,&#34;ìĿ´ìłľ&#34;:2310,&#34;Ġë¨¼&#34;:2311,&#34;ìĺ¬ë&#34;:2312,&#34;Ġê·¸ë§Į&#34;:2313,&#34;Ġìĸ´ëĬĲ&#34;:2314,&#34;Ġë¹ĦêµĲ&#34;:2315,&#34;ãĦ±&#34;:2316,&#34;Ġë¶ĪìĮį&#34;:2317,&#34;Ġìļ°ë¦¬ëĤĺëĿ¼&#34;:2318,&#34;ì°Į&#34;:2319,&#34;Ġ!&#34;:2320,&#34;ìĿ¸ìĿĺ&#34;:2321,&#34;ë°ĺìłĦ&#34;:2322,&#34;Ġìĸ¼êµ´&#34;:2323,&#34;Ġìĥģìĥģ&#34;:2324,&#34;ìĨĮìŀ¬&#34;:2325,&#34;Ġëĺĳ&#34;:2326,&#34;Ġê³µíı¬ìĺģíĻĶ&#34;:2327,&#34;ĠOOO&#34;:2328,&#34;ìĸ´ëĸ&#34;:2329,&#34;Ġë³ĳ&#34;:2330,&#34;ë³´ì§Ģ&#34;:2331,&#34;Ġë¶Īë&#34;:2332,&#34;ê²łì§Ģë§Į&#34;:2333,&#34;íĿ¥&#34;:2334,&#34;Ġê±¸ìŀĳ&#34;:2335,&#34;ĠíĶĦë¡ľ&#34;:2336,&#34;ĠëĪĪë¬¼ìĿ´&#34;:2337,&#34;ĠëįĶë¹Ļ&#34;:2338,&#34;Ġìĺ¤ëŀĺ&#34;:2339,&#34;ìŀĶìŀĶ&#34;:2340,&#34;ĠëıĦëĮĢì²´&#34;:2341,&#34;¬ëĭ¤&#34;:2342,&#34;ê¼Ń&#34;:2343,&#34;Ġìµľê³łëĭ¤&#34;:2344,&#34;ì§Ģì§Ģ&#34;:2345,&#34;Ġë§Īì§Ģë§īìĹĲ&#34;:2346,&#34;Ġì±Ħ&#34;:2347,&#34;Ġê²ĥìĿĦ&#34;:2348,&#34;Ġê°ĲëıĻëıĦ&#34;:2349,&#34;ìĭľíĤ¤&#34;:2350,&#34;ĠìĹ°ê¸°ë¥¼&#34;:2351,&#34;íĥĢì§Ģ&#34;:2352,&#34;ĠìķĮìķĺ&#34;:2353,&#34;Ġë²Ī&#34;:2354,&#34;ĠìĦ¤ìłķ&#34;:2355,&#34;íķľë²Ī&#34;:2356,&#34;Ġìĥī&#34;:2357,&#34;Ġë³´ìĹ¬ì£¼ëĬĶ&#34;:2358,&#34;ľìĹĲ&#34;:2359,&#34;ìľ¼ëĭĪ&#34;:2360,&#34;ìłķëıĦë¡ľ&#34;:2361,&#34;ìĿ´ìĹĪëĭ¤&#34;:2362,&#34;ë¥ł&#34;:2363,&#34;¬ë§ģíĥĢìŀĦìļ©&#34;:2364,&#34;ĠìŀĪìĿĦ&#34;:2365,&#34;Ġê¸°ìĸµìĿ´&#34;:2366,&#34;ìļĶì¦ĺ&#34;:2367,&#34;Ġë°ĸìĹĲ&#34;:2368,&#34;ĠìĹĨìĸ´&#34;:2369,&#34;ë£Į&#34;:2370,&#34;íĶ½&#34;:2371,&#34;ëĤĺìĺ¨&#34;:2372,&#34;ë´£&#34;:2373,&#34;ê²łì§Ģ&#34;:2374,&#34;ê·¹ìŀ¥&#34;:2375,&#34;Ġë§¤ìļ°&#34;:2376,&#34;ĠìĿ´ëĶ´&#34;:2377,&#34;Ġê°ĸ&#34;:2378,&#34;Ġê²ĥëıĦ&#34;:2379,&#34;ãħĩãħĩ&#34;:2380,&#34;ĠìĹīìĦ±&#34;:2381,&#34;ë§īìŀ¥&#34;:2382,&#34;ì¯¤&#34;:2383,&#34;ëĶ±&#34;:2384,&#34;ìĹ°ì¶ľ&#34;:2385,&#34;ĠëĬĲëĤĦ&#34;:2386,&#34;£¨&#34;:2387,&#34;ë»&#34;:2388,&#34;ìĬ¤ëŁ½ëĭ¤&#34;:2389,&#34;íķĦìļĶ&#34;:2390,&#34;ood&#34;:2391,&#34;ê°ĢìĬ´&#34;:2392,&#34;ë³´ë©´ìĦľ&#34;:2393,&#34;ìĬ¤ë¦´&#34;:2394,&#34;ì§Ħìĭ¬&#34;:2395,&#34;ìĹĶëĶ©&#34;:2396,&#34;ìĬ¤ê°Ģ&#34;:2397,&#34;ë§ĪìĿĮ&#34;:2398,&#34;ìĿ´ëĿ¼ë©´&#34;:2399,&#34;ĠìķĮê²Į&#34;:2400,&#34;Ġê°ĲëıĻìłģìĿ´&#34;:2401,&#34;ë¨¼&#34;:2402,&#34;íģ¬ë&#34;:2403,&#34;Ġì¢ĭìķĦìļĶ&#34;:2404,&#34;ĠìĪľìĪĺ&#34;:2405,&#34;ê½&#34;:2406,&#34;ëŀĲ&#34;:2407,&#34;ĠìĥģíĻ©&#34;:2408,&#34;ĩ¼&#34;:2409,&#34;ì§ķ&#34;:2410,&#34;ãħħ&#34;:2411,&#34;ìĭľìłĪ&#34;:2412,&#34;ë§Ĳê³ł&#34;:2413,&#34;Ġê´ĳ&#34;:2414,&#34;Ġë§Ľ&#34;:2415,&#34;ĠëĮĢíķ´&#34;:2416,&#34;Ġìŀ¬ë°ĮìĿĮ&#34;:2417,&#34;ê°ľëıĦ&#34;:2418,&#34;ëħĦëıĦ&#34;:2419,&#34;ë²Ħì§Ģ&#34;:2420,&#34;ĠëĨĵ&#34;:2421,&#34;ĠëĭĪ&#34;:2422,&#34;ë´¤ëĭ¤&#34;:2423,&#34;ë§İìĿĢ&#34;:2424,&#34;ì¸ł&#34;:2425,&#34;ĠìŀĶìĿ¸&#34;:2426,&#34;ĠìĺģíĻĶìĺĢëĭ¤&#34;:2427,&#34;ë¡łê°Ģ&#34;:2428,&#34;ìĸ´ë¦´&#34;:2429,&#34;ê¹Ĳ&#34;:2430,&#34;ê²Ĳ&#34;:2431,&#34;ìłģìĿ´ëĭ¤&#34;:2432,&#34;Ġë§ĲëıĦ&#34;:2433,&#34;Ġì§Ģë£¨íķ¨&#34;:2434,&#34;ĠíĽĦë°ĺ&#34;:2435,&#34;Ġê½¤&#34;:2436,&#34;´¤&#34;:2437,&#34;ìķĹ&#34;:2438,&#34;ì§ľì¦Ŀ&#34;:2439,&#34;Ġêµ°&#34;:2440,&#34;ë¹¨&#34;:2441,&#34;ĠìķĬëĭ¤&#34;:2442,&#34;íķĻìĥĿ&#34;:2443,&#34;ª½&#34;:2444,&#34;Ġëķ&#34;:2445,&#34;ìľ¼ë©´ìĦľ&#34;:2446,&#34;ìĺ¤ëŀľë§ĮìĹĲ&#34;:2447,&#34;íİ¸ìĿĢ&#34;:2448,&#34;ĠìĨĮìŀ¬ë&#34;:2449,&#34;ìĵ¸&#34;:2450,&#34;Ġì£Ħ&#34;:2451,&#34;ë³´ê²Į&#34;:2452,&#34;Ġëĭ¹ìĭľ&#34;:2453,&#34;Ġê·¹ìŀ¥ìĹĲìĦľ&#34;:2454,&#34;´ĲëıĦ&#34;:2455,&#34;ëı¼&#34;:2456,&#34;Ġê·¸ëĤĺë§Ī&#34;:2457,&#34;ìĹĪê³ł&#34;:2458,&#34;Ġê°ĻìķĦìļĶ&#34;:2459,&#34;íĤ¨&#34;:2460,&#34;Ġë¸Ķ&#34;:2461,&#34;Ġì¢ĭê²ł&#34;:2462,&#34;Ġë°Ķë¡ľ&#34;:2463,&#34;ëĪĪ&#34;:2464,&#34;TV&#34;:2465,&#34;ëŀį&#34;:2466,&#34;ê°ĲìĿ´&#34;:2467,&#34;Ġìłľë°ľ&#34;:2468,&#34;;;;;&#34;:2469,&#34;ê±°ê°Ļ&#34;:2470,&#34;ĠìĿ¸ë¬¼&#34;:2471,&#34;Ġê°ĪìĪĺë¡Ŀ&#34;:2472,&#34;Ġë³´ëĬĶëĤ´ëĤ´&#34;:2473,&#34;ì³¤&#34;:2474,&#34;ìķĦê¹Ŀëĭ¤&#34;:2475,&#34;ìŀĸìķĦ&#34;:2476,&#34;Ġë³´ìķĺ&#34;:2477,&#34;ĠìĹŃìĤ¬&#34;:2478,&#34;ê´ľì°®&#34;:2479,&#34;ëĴ¤&#34;:2480,&#34;ĠìĻł&#34;:2481,&#34;ê±´ëį°&#34;:2482,&#34;Ġì·¨&#34;:2483,&#34;Ġë³´ëĭĪ&#34;:2484,&#34;Ġë¯¸ì¹ľ&#34;:2485,&#34;Ġëĺ¥&#34;:2486,&#34;Ġê°ĲëıĻìĿ´&#34;:2487,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿ´&#34;:2488,&#34;er&#34;:2489,&#34;íķ´ëĿ¼&#34;:2490,&#34;ëħĦìĿ´&#34;:2491,&#34;ê³¡&#34;:2492,&#34;ìŀ¬ë°Įìĸ´ìļĶ&#34;:2493,&#34;Ġ~&#34;:2494,&#34;ìĨĲ&#34;:2495,&#34;ìŀ¬ë¯¸ìĹĨëĭ¤&#34;:2496,&#34;íĭĢ&#34;:2497,&#34;Ġo&#34;:2498,&#34;ìĪĺë¥¼&#34;:2499,&#34;ìĭłìĿ´&#34;:2500,&#34;Ġë¡ľë§¨ìĬ¤&#34;:2501,&#34;ê°Ģì¡±&#34;:2502,&#34;ĠìĿ´íĽĦ&#34;:2503,&#34;ì§ľë¦¬&#34;:2504,&#34;ìĹ°íŀĪ&#34;:2505,&#34;ĠìķĦëĭĺ&#34;:2506,&#34;ëª¨ë¥´&#34;:2507,&#34;ìŀĲê°Ģ&#34;:2508,&#34;īìŀ¥íŀĪ&#34;:2509,&#34;ĠìĺģíĻĶìŀĦ&#34;:2510,&#34;ëĵ¤ìĸ´&#34;:2511,&#34;ìŀ¬ë°Įëĭ¤&#34;:2512,&#34;ìķĮë°Ķ&#34;:2513,&#34;ëĤĺìĺ¤ëĬĶ&#34;:2514,&#34;Ġíķ´ìĦľ&#34;:2515,&#34;ê°Ģê³ł&#34;:2516,&#34;Ġë§ĺìĹĲ&#34;:2517,&#34;ìķĦëĭĮ&#34;:2518,&#34;íĦ&#34;:2519,&#34;ê±°ìķ¼&#34;:2520,&#34;ëł¤ìĦľ&#34;:2521,&#34;ĠíĸĪëĬĶëį°&#34;:2522,&#34;ìľ¨&#34;:2523,&#34;ê°ľë´ī&#34;:2524,&#34;Ġë§ĪìĿĮìĿ´&#34;:2525,&#34;ĵľ&#34;:2526,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:2527,&#34;Ġìķ½ê°Ħ&#34;:2528,&#34;Ġê°ĲìĤ¬&#34;:2529,&#34;ê·¸ëŀĺëıĦ&#34;:2530,&#34;ĠìĿĢ&#34;:2531,&#34;ì£¼ìĦ¸ìļĶ&#34;:2532,&#34;ĠëĦĪë¬´ëĤĺ&#34;:2533,&#34;ĠëĬĺ&#34;:2534,&#34;ìĶ©&#34;:2535,&#34;ëł¹&#34;:2536,&#34;êµ³&#34;:2537,&#34;íĴį&#34;:2538,&#34;íķĦ&#34;:2539,&#34;íķłëķĮ&#34;:2540,&#34;Ġìŀĺëª»&#34;:2541,&#34;ĠìķĪëĲ&#34;:2542,&#34;ì´Īë°ĺ&#34;:2543,&#34;Ġíıīê°Ģ&#34;:2544,&#34;ĠìĥĿê°ģìĿĦ&#34;:2545,&#34;Ġë²Į&#34;:2546,&#34;Ġëĭ¬&#34;:2547,&#34;ĠìłĦë¶Ģ&#34;:2548,&#34;Ġìĺ¤ê¸Ģ&#34;:2549,&#34;Ġìĺ¤ëĬĺ&#34;:2550,&#34;ì¡Įëĭ¤&#34;:2551,&#34;,.&#34;:2552,&#34;ìĹ¬ìĦľ&#34;:2553,&#34;Ġì¤ĳêµŃ&#34;:2554,&#34;Ġë°ľìĹ°ê¸°&#34;:2555,&#34;Ġì³&#34;:2556,&#34;ëĤ®&#34;:2557,&#34;Ġëĭ¤ìĿĮ&#34;:2558,&#34;ĠìĹ¬ê¸°&#34;:2559,&#34;ê²¹&#34;:2560,&#34;Ġê·¸ëĮĢë¡ľ&#34;:2561,&#34;Ġì²«&#34;:2562,&#34;Ġê·¸ëŀĺìĦľ&#34;:2563,&#34;ĠëĨĴìĿĢ&#34;:2564,&#34;ĠíĴį&#34;:2565,&#34;Ġë¸Į&#34;:2566,&#34;ëĬĲëĤĮ&#34;:2567,&#34;ëļ&#34;:2568,&#34;ê±°ëĤĺ&#34;:2569,&#34;ĠìŀĪìĿĦê¹Į&#34;:2570,&#34;Ġì²ł&#34;:2571,&#34;®¤&#34;:2572,&#34;ìĽĮìļĶ&#34;:2573,&#34;ĠìłķëıĦë¡ľ&#34;:2574,&#34;ìĪĻ&#34;:2575,&#34;ëĲ¨&#34;:2576,&#34;íĶĶ&#34;:2577,&#34;ë¦°ëĭ¤&#34;:2578,&#34;Ġì´Īë°ĺ&#34;:2579,&#34;Ġìĺģìĥģë¯¸&#34;:2580,&#34;Ġìĸ´ìļ¸&#34;:2581,&#34;ĠìķĦìī½ëĭ¤&#34;:2582,&#34;ĠíĪ¬&#34;:2583,&#34;íķĺìŀĲ&#34;:2584,&#34;ëħĲ&#34;:2585,&#34;Ġì§ĢëĤĺ&#34;:2586,&#34;ëŃĶê°Ģ&#34;:2587,&#34;Ġëĭ¤ë¥´&#34;:2588,&#34;ê°Ħëĭ¤&#34;:2589,&#34;ĠíĿĶ&#34;:2590,&#34;ë¨¸ëĭĪ&#34;:2591,&#34;Ġê°ĻìĿĢëį°&#34;:2592,&#34;Ńìĥģ&#34;:2593,&#34;ĠìĿ´ë¯¸&#34;:2594,&#34;ìķĦìķ¼&#34;:2595,&#34;ë§ĪìĦ¸ìļĶ&#34;:2596,&#34;ĠìŀĪì§Ģë§Į&#34;:2597,&#34;Ġê·ĢìĹ½&#34;:2598,&#34;ĠíĤ¤&#34;:2599,&#34;ê·¸ëŀ&#34;:2600,&#34;Ġë´ĲìĦľ&#34;:2601,&#34;Ĳë¦¬&#34;:2602,&#34;Ġë§Īë&#34;:2603,&#34;Ġê¸°ëĭ¤&#34;:2604,&#34;ê·¼ëį°&#34;:2605,&#34;ë¶Ħëĵ¤&#34;:2606,&#34;ìĭľê°ĦìĿ´&#34;:2607,&#34;ĠìĺģíĻĶìĿ¸ëį°&#34;:2608,&#34;ìĺģíĻĶìĹĲ&#34;:2609,&#34;ìĹĲìĦľëıĦ&#34;:2610,&#34;Ġìĸ¸&#34;:2611,&#34;ĠëĶ¸&#34;:2612,&#34;ĠCG&#34;:2613,&#34;ãħĤ&#34;:2614,&#34;ëĤŃë¹Ħ&#34;:2615,&#34;ëĿ¼ìĦľ&#34;:2616,&#34;ë³´ëĭ¤ëĬĶ&#34;:2617,&#34;ë´¤ìĬµëĭĪëĭ¤&#34;:2618,&#34;Ġëĵ±ìŀ¥&#34;:2619,&#34;ĠëĤĺìĻĢìĦľ&#34;:2620,&#34;-_-&#34;:2621,&#34;ëĤĺìĻĢ&#34;:2622,&#34;Ġìĺ¨&#34;:2623,&#34;Ġìŀ¥ë¥´&#34;:2624,&#34;ĠìķĪëĲľ&#34;:2625,&#34;ê°ĲëıħìĿ´&#34;:2626,&#34;ĠêµĲíĽĪ&#34;:2627,&#34;ĠëĮĢì²´&#34;:2628,&#34;ìķĶ&#34;:2629,&#34;Ġë´¤ì§Ģë§Į&#34;:2630,&#34;Įĵê¸Ģ&#34;:2631,&#34;ìĺ¥&#34;:2632,&#34;Ġíķĺëĭ¤&#34;:2633,&#34;ìŀĳê°Ģ&#34;:2634,&#34;Ġìŀ¬ë°Įê³ł&#34;:2635,&#34;Ġc&#34;:2636,&#34;ìĿĦìĪĺ&#34;:2637,&#34;ìĿĦíħĲëį°&#34;:2638,&#34;Ġì¢ĭìĿĢëį°&#34;:2639,&#34;ĠìķĦë¦Ħëĭ¤&#34;:2640,&#34;Ġãħīãħī&#34;:2641,&#34;ìĺģíĻĶëĭ¤&#34;:2642,&#34;ĠìĪ¨&#34;:2643,&#34;................&#34;:2644,&#34;¬ë¦¬ìĬ¤&#34;:2645,&#34;ìĤ¬ë¥¼&#34;:2646,&#34;ĠìłĦì²´&#34;:2647,&#34;Ġë³´ê³łìĭ¶&#34;:2648,&#34;ĠëĤ´ìļ©ìĿĢ&#34;:2649,&#34;ì·¨&#34;:2650,&#34;Ġìı&#34;:2651,&#34;ìĹī&#34;:2652,&#34;ìŀĪëĦ¤ìļĶ&#34;:2653,&#34;ìĹĪì§Ģë§Į&#34;:2654,&#34;Ġë³¼ìĪĺ&#34;:2655,&#34;Ġì¤Ģëĭ¤&#34;:2656,&#34;ĠìłĪ&#34;:2657,&#34;ìķĦë¦Ħ&#34;:2658,&#34;ìķĦì§ģ&#34;:2659,&#34;ĠìĤ¬íļĮ&#34;:2660,&#34;ãĦ´&#34;:2661,&#34;ë§ĪëĶĶë¡ľ&#34;:2662,&#34;Ġb&#34;:2663,&#34;ìķŀ&#34;:2664,&#34;ĠìĥĪë¡ľ&#34;:2665,&#34;Ġê°ĲëıĻìĿĦ&#34;:2666,&#34;íķĺëĥĲ&#34;:2667,&#34;ë©ĺ&#34;:2668,&#34;ìĽĥìĿĮ&#34;:2669,&#34;Ġì§ģ&#34;:2670,&#34;íĥķ&#34;:2671,&#34;Ġê°ľìĿ¸ìłģìľ¼ë¡ľ&#34;:2672,&#34;Ġë¹ĦëĶĶìĺ¤&#34;:2673,&#34;ìĿ´íķ´&#34;:2674,&#34;¬ë¦¬ë&#34;:2675,&#34;ì§Ģë¥¼&#34;:2676,&#34;ĪëĥĲ&#34;:2677,&#34;ê·ľ&#34;:2678,&#34;¸Į&#34;:2679,&#34;ìĺĢìĿĮ&#34;:2680,&#34;Ġë©į&#34;:2681,&#34;¥´&#34;:2682,&#34;âĺ&#34;:2683,&#34;ê³łëĬĶ&#34;:2684,&#34;Ġìŀĥ&#34;:2685,&#34;Ġìķķ&#34;:2686,&#34;ĠìĦ¹&#34;:2687,&#34;ë°°ìļ°ëĵ¤&#34;:2688,&#34;Ġì«&#34;:2689,&#34;Ġë´£&#34;:2690,&#34;ìĹĲìĦľëĬĶ&#34;:2691,&#34;ĠëįĶìļ±&#34;:2692,&#34;ĠíķľêµŃìĺģíĻĶ&#34;:2693,&#34;Ġì¡´ìŀ¬&#34;:2694,&#34;ìĭŃ&#34;:2695,&#34;Ġê±°ìĿĺ&#34;:2696,&#34;Ġêµ¬ìĦ±&#34;:2697,&#34;Ġ?&#34;:2698,&#34;ëŁī&#34;:2699,&#34;ëŀĳìĬ¤&#34;:2700,&#34;ĠìĻķ&#34;:2701,&#34;ëĭ¤ëĬĶê²Į&#34;:2702,&#34;Ġëĭ¤ìļ´&#34;:2703,&#34;ìĤ¬ê°Ģ&#34;:2704,&#34;Ġíĺ¼ìŀĲ&#34;:2705,&#34;¶Ģ&#34;:2706,&#34;ìĹĲìĦł&#34;:2707,&#34;ëĪĪë¬¼&#34;:2708,&#34;ĠS&#34;:2709,&#34;ë¡ľìļ´&#34;:2710,&#34;ìĭľë¦¬ì¦Ī&#34;:2711,&#34;Ġê°Ģì¹ĺ&#34;:2712,&#34;íĮ¬&#34;:2713,&#34;ìĭ¤ë§Ŀ&#34;:2714,&#34;Ġìŀ¬ë¯¸ìĹĨìĿĮ&#34;:2715,&#34;ì¶Ķì²ľ&#34;:2716,&#34;ìĺĪìļĶ&#34;:2717,&#34;ĠëĤ®ìĿĢ&#34;:2718,&#34;ĠëĪĦê°Ģ&#34;:2719,&#34;Ġíı¬ìĬ¤íĦ°&#34;:2720,&#34;ĩĮ&#34;:2721,&#34;ĠìŀĪìĸ´ìĦľ&#34;:2722,&#34;Ġìŀ¥ë©´ìĿ´&#34;:2723,&#34;Ġê°ĢìĬ´ìĿ´&#34;:2724,&#34;ìĿ´íĮħ&#34;:2725,&#34;íĭ´&#34;:2726,&#34;ëĨĶ&#34;:2727,&#34;ìĿ¼ìĿ´&#34;:2728,&#34;ĠíĬ¸&#34;:2729,&#34;ìĭľëĭ¤&#34;:2730,&#34;ìĪ¨&#34;:2731,&#34;ĠíĹĪë¬´&#34;:2732,&#34;Ġs&#34;:2733,&#34;ëĵ¬&#34;:2734,&#34;ìĤ¬íļĮ&#34;:2735,&#34;ĠìĹ¬ì£¼ìĿ¸ê³µ&#34;:2736,&#34;Ġì½&#34;:2737,&#34;ìĺ¤ëĬĺ&#34;:2738,&#34;Ġëĵ£&#34;:2739,&#34;ë§İìĿ´&#34;:2740,&#34;íĮĮìĿ´&#34;:2741,&#34;ë²Īì§¸&#34;:2742,&#34;ëıĦëĮĢì²´&#34;:2743,&#34;íķľê²Į&#34;:2744,&#34;ĠëĤĺìĺ¬&#34;:2745,&#34;Ġê°ĲëıĻìłģìĿ¸&#34;:2746,&#34;ĠìłĢëŁ°&#34;:2747,&#34;~!!&#34;:2748,&#34;ìĿ´íĬ¸&#34;:2749,&#34;Ġê·¸ë¦¬&#34;:2750,&#34;ĠìĥĿê°ģíķĺê²Į&#34;:2751,&#34;good&#34;:2752,&#34;ëĦĲ&#34;:2753,&#34;ìĨįìĹĲ&#34;:2754,&#34;Ġë§ĪìĿĮìĹĲ&#34;:2755,&#34;ĠìķĦë¬´ë¦¬&#34;:2756,&#34;ìĹĲê²Ĳ&#34;:2757,&#34;ĠìĿ´ê²ĥ&#34;:2758,&#34;ìĿ¸ëĭ¤&#34;:2759,&#34;ìĿ´ëĿ¼ëĭĪ&#34;:2760,&#34;ĠìĹĦë§Ī&#34;:2761,&#34;ĠBê¸ī&#34;:2762,&#34;Ġìĵ¸&#34;:2763,&#34;íĻķ&#34;:2764,&#34;Ġë°©ìĨ¡&#34;:2765,&#34;ĠìĿ´ìłķëıĦ&#34;:2766,&#34;ìĸ´ë¦°&#34;:2767,&#34;ì¢ĭëĭ¤&#34;:2768,&#34;ë©ĶëĶĶ&#34;:2769,&#34;ĠìŀĲì²´ê°Ģ&#34;:2770,&#34;ë§ĮíĻĶ&#34;:2771,&#34;ĠìŀĪìĸ´&#34;:2772,&#34;ê²°ë§Ĳ&#34;:2773,&#34;Ġëĭ¨ìĪľ&#34;:2774,&#34;ëłµ&#34;:2775,&#34;ĠìŀĪìĹĪëĭ¤&#34;:2776,&#34;ìķĺìĿĮ&#34;:2777,&#34;Ġë©ľë¡ľ&#34;:2778,&#34;Ġë´ĲëĿ¼&#34;:2779,&#34;Īëģ¼&#34;:2780,&#34;ìĹ¬ìļ´&#34;:2781,&#34;ìłĢíŀĪ&#34;:2782,&#34;,,,,&#34;:2783,&#34;ë³´ëĬĶëĤ´ëĤ´&#34;:2784,&#34;Ġìį¨&#34;:2785,&#34;ëĿ¼ëĭĪ&#34;:2786,&#34;ë¶ģ&#34;:2787,&#34;ĠìĦ¸ê³Ħ&#34;:2788,&#34;ìĿµ&#34;:2789,&#34;ê¸°ê³ł&#34;:2790,&#34;Ġë¹µ&#34;:2791,&#34;Ġìķłëĵ¤&#34;:2792,&#34;ĠìĺĪìģĺ&#34;:2793,&#34;Ġ30&#34;:2794,&#34;Ġãħ¡&#34;:2795,&#34;ê¸Īë&#34;:2796,&#34;Ġëĭ¤ëĵ¤&#34;:2797,&#34;Ġë´¤ìĿĮ&#34;:2798,&#34;Ġëª»íķĺëĬĶ&#34;:2799,&#34;ìĭ«&#34;:2800,&#34;ìĺģíĻĶì¤ĳ&#34;:2801,&#34;Ġë´Ħ&#34;:2802,&#34;ì½ľ&#34;:2803,&#34;ì¥&#34;:2804,&#34;ê¸°ìĸµ&#34;:2805,&#34;íĸĪìĸ´ìļĶ&#34;:2806,&#34;ëŁ¬ìļ´&#34;:2807,&#34;ëĤĺë¦Ħ&#34;:2808,&#34;Ġ100&#34;:2809,&#34;ĠìĺĪìĥģ&#34;:2810,&#34;ĠíĸĪëĭ¤&#34;:2811,&#34;ëĤ´ìļ©ìĿ´&#34;:2812,&#34;Ġê°ģë³¸&#34;:2813,&#34;íķĺëĭĪ&#34;:2814,&#34;ĠìŀĪëĬĶëį°&#34;:2815,&#34;ĠìĺĽëĤł&#34;:2816,&#34;Ġë§Įëĵľ&#34;:2817,&#34;ëħĦìłĦ&#34;:2818,&#34;Ġë¹Ľ&#34;:2819,&#34;ëĲĺê³ł&#34;:2820,&#34;Ġì¤ĳìļĶ&#34;:2821,&#34;íķĺì§ĢìķĬ&#34;:2822,&#34;¬ë¦°&#34;:2823,&#34;ĠíĻĶëł¤&#34;:2824,&#34;ì§Ģê²Į&#34;:2825,&#34;ìĸ´ê°Ģ&#34;:2826,&#34;Ġì°©&#34;:2827,&#34;ì°¨ëĿ¼ë¦¬&#34;:2828,&#34;Ġê·¸ëŁ°ì§Ģ&#34;:2829,&#34;ìĬ¤ëŁ½ê³ł&#34;:2830,&#34;Ġì¹ľêµ¬&#34;:2831,&#34;ë¦¬íĭ°&#34;:2832,&#34;ìĬ¤ìĿĺ&#34;:2833,&#34;ĠìĥĿê°ģíķ´&#34;:2834,&#34;Ġê°ľê·¸&#34;:2835,&#34;ĠëģĿëĤĺ&#34;:2836,&#34;ĠìŀĲìĹ°&#34;:2837,&#34;ìľłìĿĺ&#34;:2838,&#34;ĠìķĦëĭĲ&#34;:2839,&#34;·°&#34;:2840,&#34;Ġìŀ¬ë¯¸ìŀĪëĬĶ&#34;:2841,&#34;ĠìĤ¼ë¥ĺ&#34;:2842,&#34;Ġì³Ĳ&#34;:2843,&#34;ê³¨&#34;:2844,&#34;ëĵ¤ìķĦ&#34;:2845,&#34;ìļ°ëĵľ&#34;:2846,&#34;Ġê°Ģì§Ģê³ł&#34;:2847,&#34;ĠíıīìłĲìĿĦ&#34;:2848,&#34;ìĸµì§Ģ&#34;:2849,&#34;ĠìĬ¬íĶĪ&#34;:2850,&#34;Ġ(&#34;:2851,&#34;Ġíĺ¹&#34;:2852,&#34;Ġìĥģëĭ¹íŀĪ&#34;:2853,&#34;Ġëıĭë³´&#34;:2854,&#34;ëıĦë¡Ŀ&#34;:2855,&#34;ĠìĹĨìĸ´ìĦľ&#34;:2856,&#34;ĠìŀĲê¸°&#34;:2857,&#34;Ġê·¹ìŀ¥íĮĲ&#34;:2858,&#34;99&#34;:2859,&#34;ĠìĻĶ&#34;:2860,&#34;Ġë¶Īíİ¸&#34;:2861,&#34;Ġíĭ°ë¹Ħ&#34;:2862,&#34;ìĿ´ì½Ķ&#34;:2863,&#34;ìķĺëįĺ&#34;:2864,&#34;Ġíĸ¥&#34;:2865,&#34;Ġê²°ë§ĲìĿ´&#34;:2866,&#34;Ġìį&#34;:2867,&#34;¬ëį&#34;:2868,&#34;ì§Ħì§Ħ&#34;:2869,&#34;ìŀ¥ëĤľ&#34;:2870,&#34;ê°ľìĿ¸ìłģìľ¼ë¡ľ&#34;:2871,&#34;Ġë¹Ħíķ´&#34;:2872,&#34;Ġíķ´ëıĦ&#34;:2873,&#34;Ġë³´ìĹ¬ì£¼&#34;:2874,&#34;Ġ....&#34;:2875,&#34;íı¬ìĬ¤íĦ°&#34;:2876,&#34;Ġì¦Ĳê±°&#34;:2877,&#34;Ġìĸ´ìĦ¤íĶĪ&#34;:2878,&#34;ìĿĳ&#34;:2879,&#34;¬ëł¤&#34;:2880,&#34;ìĿ¸ë¬¼&#34;:2881,&#34;Ġìĵ°ëłĪê¸°ìĺģíĻĶ&#34;:2882,&#34;Ġì´ĪëĶ©&#34;:2883,&#34;Ġë¬ĺ&#34;:2884,&#34;ë§Īì§Ģë§īìĹĲ&#34;:2885,&#34;Ġê·¸ëŁ¬ëĤĺ&#34;:2886,&#34;ëıĭ&#34;:2887,&#34;íĸĪê³ł&#34;:2888,&#34;Ġêµ³&#34;:2889,&#34;Ġë²Ĺ&#34;:2890,&#34;ĠìĤ¬ëŀĮìĿĢ&#34;:2891,&#34;Ġíķ©ëĭĪëĭ¤&#34;:2892,&#34;ì¡°ê±´&#34;:2893,&#34;ĠíŀĪ&#34;:2894,&#34;ĠìķĦëĭĮê°Ģ&#34;:2895,&#34;in&#34;:2896,&#34;ë¯¼êµŃ&#34;:2897,&#34;Ġì¡´ëĤĺ&#34;:2898,&#34;ħëĭĪëĭ¤&#34;:2899,&#34;ìľ¼ë©°&#34;:2900,&#34;ìłĦìŁģ&#34;:2901,&#34;ëĭĺìĿĺ&#34;:2902,&#34;Ġìĭ¤íĻĶ&#34;:2903,&#34;ìĺģíĻĶë¡ľ&#34;:2904,&#34;Ġìĭ¤ìłľ&#34;:2905,&#34;Ġì¤ĳê°ĦìĹĲ&#34;:2906,&#34;íĺĳ&#34;:2907,&#34;ìĦ¸ìĥģ&#34;:2908,&#34;Ġìĺ¤ëŀľë§ĮìĹĲ&#34;:2909,&#34;ëĭ¹íķľ&#34;:2910,&#34;Ġìŀ¬ë¯¸ìĹĨëĬĶ&#34;:2911,&#34;Īë¡&#34;:2912,&#34;ëķĮë§Īëĭ¤&#34;:2913,&#34;Ġë°°ìļ°ëĵ¤ìĿ´&#34;:2914,&#34;íķĺì§ĢëıĦ&#34;:2915,&#34;Ġë³¼ë§Įíķľ&#34;:2916,&#34;ê±°ëĭ¤&#34;:2917,&#34;ëª¨ëĵł&#34;:2918,&#34;Ġëħ¸ìŀ¼&#34;:2919,&#34;ì¿ł&#34;:2920,&#34;Ġêº¼&#34;:2921,&#34;ìĸ´ëł¸&#34;:2922,&#34;ë¯¸êµŃ&#34;:2923,&#34;Ġê²ĥìĿĢ&#34;:2924,&#34;íĭ°ë¹Ħ&#34;:2925,&#34;ĠìķĦê¹ĮìĽĢ&#34;:2926,&#34;ĳ¸&#34;:2927,&#34;ìŀĪê²Į&#34;:2928,&#34;íŀĺ&#34;:2929,&#34;ìĿ¼ê¹Į&#34;:2930,&#34;ëįĺê°Ģ&#34;:2931,&#34;ë¹µ&#34;:2932,&#34;ë¦¿&#34;:2933,&#34;ĠìĤ¬ë&#34;:2934,&#34;ë´¤ìĸ´ìļĶ&#34;:2935,&#34;ë§Įìľ¼ë¡ľëıĦ&#34;:2936,&#34;ë¦¬ìĿĺ&#34;:2937,&#34;ë³¼ë§Į&#34;:2938,&#34;ëİ&#34;:2939,&#34;ìľ¡&#34;:2940,&#34;ê¹¨&#34;:2941,&#34;ëĵľëĿ¼ë§Īë&#34;:2942,&#34;ĠëĬ¥&#34;:2943,&#34;íŀĪëł¤&#34;:2944,&#34;ìķĦëıĦ&#34;:2945,&#34;ĠíĴĢìĸ´&#34;:2946,&#34;ë»Ķíķľ&#34;:2947,&#34;ĠìĺģíĻĶê´ĢìĹĲìĦľ&#34;:2948,&#34;íİ¸ìĿĺ&#34;:2949,&#34;ëķĲ&#34;:2950,&#34;ĠìĹ¬ë°°ìļ°&#34;:2951,&#34;ĠíĮĲíĥĢì§Ģ&#34;:2952,&#34;ê±į&#34;:2953,&#34;ĠìĹĨëĦ¤&#34;:2954,&#34;ìĹĪìľ¼ë©´&#34;:2955,&#34;ì°©&#34;:2956,&#34;ĠìĪĺê°Ģ&#34;:2957,&#34;ìº&#34;:2958,&#34;Ġêµīìŀ¥íŀĪ&#34;:2959,&#34;ì¡°ì°¨&#34;:2960,&#34;ìĸ´ì§ĢëĬĶ&#34;:2961,&#34;ëĤ´ëĬĶ&#34;:2962,&#34;ĠìĬ¤íĨłë¦¬ëĬĶ&#34;:2963,&#34;ĠíĿ¥íĸī&#34;:2964,&#34;ĠëıħíĬ¹&#34;:2965,&#34;ĠìķĪë³&#34;:2966,&#34;ìķĦë¬´ë¦¬&#34;:2967,&#34;¥ľ&#34;:2968,&#34;¬ìĺģ&#34;:2969,&#34;íķĺê¸´&#34;:2970,&#34;ë§ĮìĿĺ&#34;:2971,&#34;ëłĪë&#34;:2972,&#34;ê²ĥê°Ļëĭ¤&#34;:2973,&#34;ìłľê°Ģ&#34;:2974,&#34;Ġìĥģìĺģ&#34;:2975,&#34;ë¦½ëĭĪëĭ¤&#34;:2976,&#34;Ġê°ĳìŀĲê¸°&#34;:2977,&#34;ĠíĥĦíĥĦ&#34;:2978,&#34;Ġíķĺê²Į&#34;:2979,&#34;Ġìļ´&#34;:2980,&#34;ìłĢìĶ¨&#34;:2981,&#34;ĠìĦłíĥĿ&#34;:2982,&#34;Ġê¶Į&#34;:2983,&#34;ĠëĦ¤ìĿ´ë²Ħ&#34;:2984,&#34;ĠìĹĲë¡ľ&#34;:2985,&#34;Ġì§Ħíĸī&#34;:2986,&#34;Ġê°ĲìĦ±&#34;:2987,&#34;Ġìĸ´ì©Į&#34;:2988,&#34;Ġìĭľì²Ńë¥ł&#34;:2989,&#34;ĠíļĮ&#34;:2990,&#34;ë§ĮìĿ´&#34;:2991,&#34;ê²ĥìĿĢ&#34;:2992,&#34;ĠíķŃìĥģ&#34;:2993,&#34;ĠìĪľê°Ħ&#34;:2994,&#34;ë¿&#34;:2995,&#34;Ġìĭľì¦Į&#34;:2996,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿĢ&#34;:2997,&#34;Ġìŀ¬ë¯¸ìŀĪëĭ¤&#34;:2998,&#34;Ġì¶©ë¶ĦíŀĪ&#34;:2999,&#34;Īëĭ¤&#34;:3000,&#34;ìĸ´ëĤĺ&#34;:3001,&#34;Ġëĵł&#34;:3002,&#34;Ġìĸ´ë¥¸&#34;:3003,&#34;Ġê°ĢëĬĶ&#34;:3004,&#34;Ġë¹Į&#34;:3005,&#34;ìŀ¬ë¯¸ëıĦ&#34;:3006,&#34;ĠëĲľëĭ¤&#34;:3007,&#34;íķĺëĭ¤ê³ł&#34;:3008,&#34;Ġê·¸ëħĢ&#34;:3009,&#34;ì¤¬&#34;:3010,&#34;ĠëĦĪ&#34;:3011,&#34;ì²¨&#34;:3012,&#34;Ġë§Įì¡±&#34;:3013,&#34;ĠëĬĲê¼&#34;:3014,&#34;ĠíķĦ&#34;:3015,&#34;ëĭĺìĿ´&#34;:3016,&#34;ì½¤&#34;:3017,&#34;ĠíħĮ&#34;:3018,&#34;íķĺìĹ¬&#34;:3019,&#34;Ġê²ģ&#34;:3020,&#34;ĠìĭľìłĪ&#34;:3021,&#34;íĸĪìĬµëĭĪëĭ¤&#34;:3022,&#34;Ġì¢ĭê²łëĭ¤&#34;:3023,&#34;Ġì¢Ģë¹Ħ&#34;:3024,&#34;ĠìĹīë§Ŀ&#34;:3025,&#34;Ġìĸĺê¸°&#34;:3026,&#34;ê³³&#34;:3027,&#34;ìŀĪëĦ¤&#34;:3028,&#34;Ġë³´ëĬĶëį°&#34;:3029,&#34;ĠíĿĳ&#34;:3030,&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;&#34;:3031,&#34;ëĤ´ìļ©ëıĦ&#34;:3032,&#34;Ġë¶ĦìľĦê¸°&#34;:3033,&#34;ìĹĺ&#34;:3034,&#34;ëŀ¬&#34;:3035,&#34;ë³į&#34;:3036,&#34;Ġìī½&#34;:3037,&#34;.......&#34;:3038,&#34;ìĭ¤íŀĪ&#34;:3039,&#34;ĠìĿĺëıĦ&#34;:3040,&#34;Ġë¨¸ë¦¬&#34;:3041,&#34;ĠìĿ´ë²Ī&#34;:3042,&#34;Ġê¸ī&#34;:3043,&#34;íĶĦë¡ľ&#34;:3044,&#34;ĠíĹ¤&#34;:3045,&#34;ĠíĽĪ&#34;:3046,&#34;ê°ķì¶Ķ&#34;:3047,&#34;ëķľìĹĲ&#34;:3048,&#34;ìĹĦì²Ń&#34;:3049,&#34;Ġë¦¬ìĸ¼&#34;:3050,&#34;íĢ&#34;:3051,&#34;Ġìĵ´&#34;:3052,&#34;ìĿ´ëĥĲ&#34;:3053,&#34;ìļ°ëĬĶ&#34;:3054,&#34;Ġìŀ¬ë¯¸ëĬĶ&#34;:3055,&#34;Ġìŀĺë§Įëĵ¤&#34;:3056,&#34;ĠëĪĪë&#34;:3057,&#34;Ġíķ´ìķ¼&#34;:3058,&#34;//&#34;:3059,&#34;ì§Ģëĭ¤&#34;:3060,&#34;ëł¬&#34;:3061,&#34;ĠëĨĪ&#34;:3062,&#34;íĿĲ&#34;:3063,&#34;ì´Ŀ&#34;:3064,&#34;ĠíĽĮë¥Ńíķľ&#34;:3065,&#34;ìļ°ë¦¬ëĤĺëĿ¼&#34;:3066,&#34;ĠíĻĶë©´&#34;:3067,&#34;Ġë¯¿ê³ł&#34;:3068,&#34;Ġìĸ´ì©Ķ&#34;:3069,&#34;ĠìĥĪë¡ľìļ´&#34;:3070,&#34;Ġê°Ķ&#34;:3071,&#34;ìĭľê³ł&#34;:3072,&#34;Ġì¢ĭëĦ¤ìļĶ&#34;:3073,&#34;ĠìķĪíĥĢê¹Ŀ&#34;:3074,&#34;ìĿ´íķĺ&#34;:3075,&#34;ĠìĺģíĻĶìĿ¸&#34;:3076,&#34;Ġê·¸ê²Į&#34;:3077,&#34;ĠëĤĺìĿĺ&#34;:3078,&#34;Ġê²Ģ&#34;:3079,&#34;íķĺëĬĶê±°&#34;:3080,&#34;Ġëª¨ë¥´ê³ł&#34;:3081,&#34;ë°°ìļ°ëĵ¤ìĿĺ&#34;:3082,&#34;Ġë¹¨ë¦¬&#34;:3083,&#34;ĠM&#34;:3084,&#34;Ġìĸ´ëł¤&#34;:3085,&#34;ìłľë°ľ&#34;:3086,&#34;Ġìŀ¬ë°ĮëĦ¤ìļĶ&#34;:3087,&#34;ë³¸ëĭ¤&#34;:3088,&#34;ì¢ĭìķĦìļĶ&#34;:3089,&#34;ĠìĤ´ìĿ¸&#34;:3090,&#34;ìĺĽëĤł&#34;:3091,&#34;ìĺ¤ëĬĶ&#34;:3092,&#34;ìĦ¸ê³Ħ&#34;:3093,&#34;ìłĢëıĦ&#34;:3094,&#34;ĠìķĦëĭĪë&#34;:3095,&#34;Ġì°¸ê³ł&#34;:3096,&#34;ë´Ĳìķ¼&#34;:3097,&#34;ìĬ¤ëŁ°&#34;:3098,&#34;ĠìŀĪìĿĮ&#34;:3099,&#34;ê·ł&#34;:3100,&#34;ëħĦìĹĲ&#34;:3101,&#34;ĠëŃĲê°Ģ&#34;:3102,&#34;Ġíŀĺëĵł&#34;:3103,&#34;ëª°ìŀħ&#34;:3104,&#34;Ġì§Ŀ&#34;:3105,&#34;ìĺģíĻĶëıĦ&#34;:3106,&#34;ĠìĹ¬ìłĦ&#34;:3107,&#34;ĠëŃĲëĥĲ&#34;:3108,&#34;Ġë¬»&#34;:3109,&#34;íĺķìłģìĿ¸&#34;:3110,&#34;ĠìķĦìī¬ìļ´&#34;:3111,&#34;ĠìķĪíĥĢê¹Į&#34;:3112,&#34;ê·¸ëŀ¨&#34;:3113,&#34;ĠO&#34;:3114,&#34;ìĭľëĬĶ&#34;:3115,&#34;ë²Īë&#34;:3116,&#34;ê°Ħë§ĮìĹĲ&#34;:3117,&#34;Ġì§Ħìłķíķľ&#34;:3118,&#34;Ġë§ĪìĿ´&#34;:3119,&#34;ëĨĴ&#34;:3120,&#34;ê¿Ģìŀ¼&#34;:3121,&#34;ìµľìķħìĿĺ&#34;:3122,&#34;ìĩ¼&#34;:3123,&#34;ĳìĹĲ&#34;:3124,&#34;Ġë§¨&#34;:3125,&#34;ëĤ´ê³ł&#34;:3126,&#34;Ġìľ¼&#34;:3127,&#34;Ġê´ľ&#34;:3128,&#34;Ġê²ĥìĿ´ëĭ¤&#34;:3129,&#34;íĶĦëĭ¤&#34;:3130,&#34;Ġê·¸ëķĮ&#34;:3131,&#34;ê²łëĦ¤&#34;:3132,&#34;ĠëĮĢëĭ¨íķľ&#34;:3133,&#34;ĠëŁ¬&#34;:3134,&#34;ìĿ´ë©´&#34;:3135,&#34;ìĸ´ë¡ľ&#34;:3136,&#34;ìŀĲìĿĺ&#34;:3137,&#34;OOO&#34;:3138,&#34;Ġë©ĭìŀĪ&#34;:3139,&#34;Ġíģ¬&#34;:3140,&#34;Ġìĸ´ëĶĶìĦľ&#34;:3141,&#34;ìĹĪëĦ¤ìļĶ&#34;:3142,&#34;ĠìĨĮìĦ¤&#34;:3143,&#34;ëĲĺìĸ´&#34;:3144,&#34;Ħ¤ìļĶ&#34;:3145,&#34;ĠíĢ&#34;:3146,&#34;ĠìŀĪëĭ¤ëĬĶ&#34;:3147,&#34;ĠìĿ´ìĺģíĻĶë¥¼&#34;:3148,&#34;ĠìĬ¹&#34;:3149,&#34;Ġíķľìĭ¬&#34;:3150,&#34;íķ¨ê³¼&#34;:3151,&#34;ìłľìŀĳ&#34;:3152,&#34;Ġê°Ĳìĥģ&#34;:3153,&#34;ìĻł&#34;:3154,&#34;ê·¸ëŁ¬&#34;:3155,&#34;Ġë¨¹ë¨¹&#34;:3156,&#34;íĦ´&#34;:3157,&#34;ìĬ¤ë¥¼&#34;:3158,&#34;ĠëĦ£&#34;:3159,&#34;Ġë´¤ëĦ¤ìļĶ&#34;:3160,&#34;ĠìĨĮì¤ĳ&#34;:3161,&#34;ĠëŃĲì§Ģ&#34;:3162,&#34;ìĤ¬ëŀĮìĿ´&#34;:3163,&#34;ĠìĦ±ìļ°&#34;:3164,&#34;Ġëĭ¹ìĭł&#34;:3165,&#34;ìĬ¤íĨłë¦¬ê°Ģ&#34;:3166,&#34;íıīê°Ģ&#34;:3167,&#34;ĠíĤ¬ë§ģíĥĢìŀĦìļ©&#34;:3168,&#34;ĠìķĪëĲľëĭ¤&#34;:3169,&#34;¬ê¸Ī&#34;:3170,&#34;ĠìĹĨìĿĦ&#34;:3171,&#34;ĠìĹĨëĦ¤ìļĶ&#34;:3172,&#34;ë²Į&#34;:3173,&#34;ĠìĨ¡&#34;:3174,&#34;ĠëĲĺìĸ´&#34;:3175,&#34;ĠëĬĲëģ¼ê²Į&#34;:3176,&#34;Ġê°ľìĹ°ìĦ±&#34;:3177,&#34;Ġì°Į&#34;:3178,&#34;ìĿ´ë¸Ķ&#34;:3179,&#34;ê³łìĭ¶&#34;:3180,&#34;ëłĮ&#34;:3181,&#34;ê²ĥìĿ´ëĭ¤&#34;:3182,&#34;ĠìķĮëł¤&#34;:3183,&#34;Ġíıīë²Ķ&#34;:3184,&#34;ëľ¨&#34;:3185,&#34;ìłķìĭł&#34;:3186,&#34;ìħĪ&#34;:3187,&#34;ë¦¬ë¡ľ&#34;:3188,&#34;ìĽĢìĿ´&#34;:3189,&#34;Ġë§Ŀìŀĳ&#34;:3190,&#34;Ġë¬¼ë¡ł&#34;:3191,&#34;ìºĲë¦ŃíĦ°&#34;:3192,&#34;ĠãĦ·ãĦ·&#34;:3193,&#34;¬ëŀĳ&#34;:3194,&#34;Ġê°ĢëĬ¥&#34;:3195,&#34;ĠëŃĲìķ¼&#34;:3196,&#34;Ġíİ¸ì§ĳ&#34;:3197,&#34;Ġíľ´&#34;:3198,&#34;Ġë°Ģ&#34;:3199,&#34;ìĹĨìĸ´&#34;:3200,&#34;Ġíķĺë©´&#34;:3201,&#34;ìķłëĭĪ&#34;:3202,&#34;Ġê´ĢëŀĮ&#34;:3203,&#34;ëŀĻ&#34;:3204,&#34;Ġìľ¤&#34;:3205,&#34;ìĺĢëĬĶëį°&#34;:3206,&#34;ãĦ¹&#34;:3207,&#34;íĹĲ&#34;:3208,&#34;ĠìłĲìłĲ&#34;:3209,&#34;ĠìļĶìĨĮ&#34;:3210,&#34;Ġë§ĮìłĲ&#34;:3211,&#34;íŀĮ&#34;:3212,&#34;ĠìŀĲê·¹&#34;:3213,&#34;ëħ¸ëŀĺ&#34;:3214,&#34;Ġ-_-&#34;:3215,&#34;Ġìĺ¤ê¸Ģê±°&#34;:3216,&#34;ST&#34;:3217,&#34;ëĢ&#34;:3218,&#34;ìª½&#34;:3219,&#34;Ġëĥ&#34;:3220,&#34;ìŀĲëĵ¤&#34;:3221,&#34;Ġì¤ĺ&#34;:3222,&#34;ìĸ´ëĸ»ê²Į&#34;:3223,&#34;Ġm&#34;:3224,&#34;ìĦ¼&#34;:3225,&#34;íķĺëĬĶê²Į&#34;:3226,&#34;ãħĭãħĭãħĭãħĭãħĭ&#34;:3227,&#34;Ġë°ĶëĿ¼&#34;:3228,&#34;ìĺĪìĤ°&#34;:3229,&#34;ì»¬&#34;:3230,&#34;ª¨&#34;:3231,&#34;íĻĶê°Ģ&#34;:3232,&#34;Ġê¹Ķ&#34;:3233,&#34;Ġ200&#34;:3234,&#34;ĠìĤ¬ëŀĳìĿĦ&#34;:3235,&#34;Ġì£½ëĬĶ&#34;:3236,&#34;Ġë³ĦìłĲ&#34;:3237,&#34;Ġëĵ¯íķľ&#34;:3238,&#34;--&#34;:3239,&#34;12&#34;:3240,&#34;ê¸°ìĹĶ&#34;:3241,&#34;Ġë²ķ&#34;:3242,&#34;Ġë§Įëĵ¤ìĹĪëĭ¤&#34;:3243,&#34;ì©Į&#34;:3244,&#34;ĠëĤ«ëĭ¤&#34;:3245,&#34;Ġì¿&#34;:3246,&#34;Ġë½&#34;:3247,&#34;ìł¤&#34;:3248,&#34;ĠëĤĺìģľ&#34;:3249,&#34;Ġì¢ĭìĿĮ&#34;:3250,&#34;ĠìĥĪ&#34;:3251,&#34;Ġìŀ¬ë°ĮìĹĪëĭ¤&#34;:3252,&#34;ë¹¼&#34;:3253,&#34;ìĭ¬ìĿĦ&#34;:3254,&#34;Ġëª¨ë¥´ëĬĶ&#34;:3255,&#34;ĠìłĦê°ľê°Ģ&#34;:3256,&#34;ĠìĭĿìĥģ&#34;:3257,&#34;ìį&#34;:3258,&#34;ìĿ´ëĶ´&#34;:3259,&#34;íķľëĵ¯&#34;:3260,&#34;íİĳ&#34;:3261,&#34;ĠíıīìłĲìĿĢ&#34;:3262,&#34;ìķĪíķĺê³ł&#34;:3263,&#34;ĠìŀħëĭĪëĭ¤&#34;:3264,&#34;Ġë§¥&#34;:3265,&#34;ĠëģĿëĤ´&#34;:3266,&#34;Ġìŀ¬ë¯¸ìŀĪìĸ´ìļĶ&#34;:3267,&#34;ê°ĻìĿĢëį°&#34;:3268,&#34;ìµľê³łëĭ¤&#34;:3269,&#34;ĠëĲĲ&#34;:3270,&#34;ĠìĤ¬ëĿ¼&#34;:3271,&#34;ìĹĨëĦ¤&#34;:3272,&#34;ĠëĤĺìĿ´&#34;:3273,&#34;íĴĭ&#34;:3274,&#34;ĠìŀĲê¾¸&#34;:3275,&#34;ĠìĿ´ìķ¼ê¸°ë¥¼&#34;:3276,&#34;ĠíĿ¥ë¯¸ì§Ħì§Ħ&#34;:3277,&#34;ĠìĶ¨&#34;:3278,&#34;ìİ&#34;:3279,&#34;ìı&#34;:3280,&#34;ë³´ìĦ¸ìļĶ&#34;:3281,&#34;ĠìĹ¬ì£¼&#34;:3282,&#34;ëŀ«&#34;:3283,&#34;ĠìķĮìķĺëĭ¤&#34;:3284,&#34;íıĲ&#34;:3285,&#34;ĠíĭĢ&#34;:3286,&#34;Ġ!!&#34;:3287,&#34;Ġìĺ¤íŀĪëł¤&#34;:3288,&#34;ê»ı&#34;:3289,&#34;Ġê¼¬&#34;:3290,&#34;ìĽłëįĺ&#34;:3291,&#34;Ġë²Ķì£Ħ&#34;:3292,&#34;ĠìĨĮë¦¬&#34;:3293,&#34;Ġìĭ«ìĸ´&#34;:3294,&#34;ëŀĦê¹Į&#34;:3295,&#34;ìķĪëĲĺëĬĶ&#34;:3296,&#34;ĠëĮĵê¸Ģ&#34;:3297,&#34;ëĭ¿&#34;:3298,&#34;ëĤĺë©´&#34;:3299,&#34;ìķĦìķĦ&#34;:3300,&#34;ãħłãħłãħł&#34;:3301,&#34;ìĭ¬ìĿ´&#34;:3302,&#34;Ġë§Įëĵ¤ì§Ģ&#34;:3303,&#34;ìĭĿìĿ´&#34;:3304,&#34;Ġì¢ĭìķĺìĸ´ìļĶ&#34;:3305,&#34;ĠíĿĺëŁ¬&#34;:3306,&#34;âĢ&#34;:3307,&#34;ì°¾&#34;:3308,&#34;Ġì°½&#34;:3309,&#34;Ġë¯¸íĻĶ&#34;:3310,&#34;Ġãħħ&#34;:3311,&#34;ĠìĿ´ë¦¬&#34;:3312,&#34;ìĸ´ìļ¸&#34;:3313,&#34;ĠëĤĺëĿ¼&#34;:3314,&#34;Ġì§Ħì§Ģ&#34;:3315,&#34;íİ¸ìĿĦ&#34;:3316,&#34;ĠìĽĥê¸´&#34;:3317,&#34;ë©ĶëĿ¼&#34;:3318,&#34;Ġëª¨ìĬµìĿ´&#34;:3319,&#34;ìĹĪëĦ¤&#34;:3320,&#34;ĠìķĮìķĺëĬĶëį°&#34;:3321,&#34;ê¼¬&#34;:3322,&#34;êµ¿êµ¿&#34;:3323,&#34;ë³´ëĬĶëį°&#34;:3324,&#34;ĠìĺģìĽħ&#34;:3325,&#34;ë´ĲìĦľ&#34;:3326,&#34;ĠìĦ¤ëªħ&#34;:3327,&#34;ìºĲìĬ¤íĮħ&#34;:3328,&#34;Ġë£¨&#34;:3329,&#34;Ġëĭ¤íĸī&#34;:3330,&#34;ìĦ¸ê°Ģ&#34;:3331,&#34;ë²ĪìĿĦ&#34;:3332,&#34;Ġìŀ¥ë©´ìĿĢ&#34;:3333,&#34;Ġì£¼ìĿ¸ê³µìĿ´&#34;:3334,&#34;êµ¬ëĵ¤&#34;:3335,&#34;ì§Īë&#34;:3336,&#34;ĠìłĬ&#34;:3337,&#34;ìķĦë¨¹&#34;:3338,&#34;Ġê²¨&#34;:3339,&#34;ìĸĺ&#34;:3340,&#34;ìĨĮìĦ¤&#34;:3341,&#34;Ġë§Ĳê³ł&#34;:3342,&#34;ëĺ¥&#34;:3343,&#34;Ġë©įì²Ń&#34;:3344,&#34;Ġa&#34;:3345,&#34;Ġê»&#34;:3346,&#34;Ġê·¸ëŁ´&#34;:3347,&#34;ìĥģìĿĦ&#34;:3348,&#34;ìī½&#34;:3349,&#34;Ġìĭłê¸°&#34;:3350,&#34;ĠìĹĦì²ŃëĤľ&#34;:3351,&#34;ĠìķĦë¹ł&#34;:3352,&#34;ìłĲì¤Ģ&#34;:3353,&#34;ĠìĦ±ë£¡&#34;:3354,&#34;Ġë©ĶìĦ¸&#34;:3355,&#34;he&#34;:3356,&#34;Ġê¾&#34;:3357,&#34;ëª½&#34;:3358,&#34;Ġë¬´ì¡°ê±´&#34;:3359,&#34;Ġì¤ĳìĹĲ&#34;:3360,&#34;ìĹ°ê¸°ëıĦ&#34;:3361,&#34;ĠíĸīëıĻ&#34;:3362,&#34;ìĸ´ëĶĶ&#34;:3363,&#34;Ġì§§&#34;:3364,&#34;ĠìłĦê¸°&#34;:3365,&#34;íĶĦê³ł&#34;:3366,&#34;Ġë³´ìĹ¬ì¤Ģ&#34;:3367,&#34;ìĭ±&#34;:3368,&#34;ë§Įíķĺë©´&#34;:3369,&#34;ìĬ¤ë¡ľ&#34;:3370,&#34;ê·¸ìłĢ&#34;:3371,&#34;ì¤ĦìķĮ&#34;:3372,&#34;ì¹ľêµ¬&#34;:3373,&#34;ĠíĻĶìĿ´íĮħ&#34;:3374,&#34;Ġ90&#34;:3375,&#34;ĠìĺģíĻĶìĺĢìĬµëĭĪëĭ¤&#34;:3376,&#34;¶Ħ&#34;:3377,&#34;Ġì¥&#34;:3378,&#34;ê¸°ëĭ¤&#34;:3379,&#34;ìĭľì¦Į&#34;:3380,&#34;íĤ¹&#34;:3381,&#34;ĠìĹ°ì¶ľëł¥&#34;:3382,&#34;ê°ĲëıħìĿĺ&#34;:3383,&#34;ĠìĬ¤íĥĢìĿ¼&#34;:3384,&#34;ìŀł&#34;:3385,&#34;ìķĻ&#34;:3386,&#34;ìĸ´ìĿ´&#34;:3387,&#34;ë¦¬ìķĦ&#34;:3388,&#34;Ġê·¸ë¦¼&#34;:3389,&#34;ĠëĮĢìŀĳ&#34;:3390,&#34;Ġì£½ìĿĮ&#34;:3391,&#34;?)&#34;:3392,&#34;ë½&#34;:3393,&#34;íķĺê²ł&#34;:3394,&#34;ëĤĺìĿ´&#34;:3395,&#34;ë¦¬ê²Į&#34;:3396,&#34;íĮĶ&#34;:3397,&#34;ìĨĮíķľ&#34;:3398,&#34;Ġëģ¼&#34;:3399,&#34;Ĵ·&#34;:3400,&#34;Ġë®¤&#34;:3401,&#34;ìĿ¸ìĿĦ&#34;:3402,&#34;Ġê·¸ëĭ¥&#34;:3403,&#34;ĠíķĺëĬĶëį°&#34;:3404,&#34;Ġìĸ´ë¦´&#34;:3405,&#34;ĠìŀĳíĴĪìĿ´&#34;:3406,&#34;Ġëħ¸ëł¥&#34;:3407,&#34;ĠìŀĬíĺĢ&#34;:3408,&#34;200&#34;:3409,&#34;ìłķìĿĦ&#34;:3410,&#34;Ġê°ĻëĦ¤ìļĶ&#34;:3411,&#34;Ġë©´&#34;:3412,&#34;Ġìľłë¨¸&#34;:3413,&#34;ë¦¬ëĿ¼&#34;:3414,&#34;ìŀ¬ë°ĮìĿĮ&#34;:3415,&#34;Ġê´Ģê³Ħ&#34;:3416,&#34;ĠìŀĶìŀĶíķľ&#34;:3417,&#34;ĠíĿ¬ë§Ŀ&#34;:3418,&#34;ê¸Īë´ĲëıĦ&#34;:3419,&#34;ĠìĿ´ìĸ´&#34;:3420,&#34;Ġë§Īë¬´&#34;:3421,&#34;ìĿ¸ê±°&#34;:3422,&#34;Ġëįķ&#34;:3423,&#34;ìĤ¬ëĬĶ&#34;:3424,&#34;Ġíķ©&#34;:3425,&#34;Ġê²°íĺ¼&#34;:3426,&#34;Ġì¢ĭìķĺê³ł&#34;:3427,&#34;ĠìĺģíĻĶìĹĲìĦľ&#34;:3428,&#34;ë§Ĳë¡ľ&#34;:3429,&#34;ìŀĲëĬĶ&#34;:3430,&#34;ìŀĪìĸ´ìļĶ&#34;:3431,&#34;íĥģ&#34;:3432,&#34;Ġê°ĢëĵĿ&#34;:3433,&#34;ì¹ĺê²Į&#34;:3434,&#34;ĠíĹĲë¦¬&#34;:3435,&#34;ĠìĽĥê²¨&#34;:3436,&#34;ë°©ìĨ¡&#34;:3437,&#34;Ġë¬´ìĦľìļ´&#34;:3438,&#34;ĩ´&#34;:3439,&#34;Ġë§Įíģ¼&#34;:3440,&#34;ìĹ¬ì£¼&#34;:3441,&#34;ë¶ĦìĿ´&#34;:3442,&#34;Ġì°¸ìĭł&#34;:3443,&#34;ĠëıĪì£¼ê³ł&#34;:3444,&#34;ĠìķĦë¬´ê²ĥëıĦ&#34;:3445,&#34;©ĶìĿ´íģ¬&#34;:3446,&#34;ìĿ¸ìĿĢ&#34;:3447,&#34;Ġëª»ë³´&#34;:3448,&#34;ìĤ¬ëŀĮëĵ¤&#34;:3449,&#34;íķĺë©´ìĦľëıĦ&#34;:3450,&#34;Ġê·¸ëł¤&#34;:3451,&#34;ê·¸ëĿ¼&#34;:3452,&#34;Ġ19&#34;:3453,&#34;Ġì¢ĭìķĺìĬµëĭĪëĭ¤&#34;:3454,&#34;ĠëĺĲíķľ&#34;:3455,&#34;ĠíĬ¹ìľłìĿĺ&#34;:3456,&#34;ĠíĽĪíĽĪ&#34;:3457,&#34;íī&#34;:3458,&#34;íķľê±´&#34;:3459,&#34;ìĭľìŀĳ&#34;:3460,&#34;Ġìĺ¬&#34;:3461,&#34;ìĺĢìĬµëĭĪëĭ¤&#34;:3462,&#34;ĠìĬ¤íĨłë¦¬ìĹĲ&#34;:3463,&#34;ê¸´ìŀ¥ê°Ĳ&#34;:3464,&#34;ĠìķĦëĭĪì§Ģë§Į&#34;:3465,&#34;ìŀ¬ë¯¸ìŀĪê²Į&#34;:3466,&#34;Ġìĸµì§Ģë¡ľ&#34;:3467,&#34;Ġì¦Ĳê²ģ&#34;:3468,&#34;ê±¸ê¹Į&#34;:3469,&#34;ë´ĲëĿ¼&#34;:3470,&#34;Ġì½Ķë©ĶëĶĶ&#34;:3471,&#34;ì½ĺ&#34;:3472,&#34;ĠëĨĢëĿ¼&#34;:3473,&#34;ĠëĿ¼ëĬĶ&#34;:3474,&#34;ìķĦëĭĪê³ł&#34;:3475,&#34;Ġë¡ľë§¨íĭ±&#34;:3476,&#34;Ġë³ĳë§Ľ&#34;:3477,&#34;ĠTV&#34;:3478,&#34;ĠìĿ´ëĶ°&#34;:3479,&#34;íķ´ì£¼&#34;:3480,&#34;ìŀĲë¥¼&#34;:3481,&#34;ë¶ĦìĹĲ&#34;:3482,&#34;ëĦĪë¬´ëĦĪë¬´&#34;:3483,&#34;ĠìĻĦìĦ±ëıĦ&#34;:3484,&#34;Ġê·¸ëłĩê³ł&#34;:3485,&#34;ĠìĻĦë²½íķľ&#34;:3486,&#34;ëĮ&#34;:3487,&#34;ìĹ¼&#34;:3488,&#34;ìłĲì¤Ģëĭ¤&#34;:3489,&#34;ĠëĤĺìģĺ&#34;:3490,&#34;Ġì§Ħìĭ¤&#34;:3491,&#34;ë¥´ê³ł&#34;:3492,&#34;Ġìķ¡ìħĺëıĦ&#34;:3493,&#34;Ġëª¨ìĬµìĿĦ&#34;:3494,&#34;Ġê¿Ģìŀ¼&#34;:3495,&#34;²Ħ&#34;:3496,&#34;ì¦Īë&#34;:3497,&#34;ĠìķĬìĿĦ&#34;:3498,&#34;ĠìĥĿê°ģë³´ëĭ¤&#34;:3499,&#34;ìķłëĵ¤&#34;:3500,&#34;ëŃĲìķ¼&#34;:3501,&#34;âĺħ&#34;:3502,&#34;ĠìķĦëĭĲê¹Į&#34;:3503,&#34;~^^&#34;:3504,&#34;ìŀĪëĬĶëį°&#34;:3505,&#34;ĠìĥĿê°ģíķľëĭ¤&#34;:3506,&#34;Ġëħ¸ì¶ľ&#34;:3507,&#34;Ġê¸°ëĮĢíķĺê³ł&#34;:3508,&#34;ĠìĦ±ìŀ¥&#34;:3509,&#34;ëĵ±íķĻêµĲ&#34;:3510,&#34;Ġê¹ĬìĿĢ&#34;:3511,&#34;íĻĶë¥¼&#34;:3512,&#34;ëĵľê°Ģ&#34;:3513,&#34;ê°ĲìĿĦ&#34;:3514,&#34;ĠëıĦìłĢíŀĪ&#34;:3515,&#34;Ġë³µìĪĺ&#34;:3516,&#34;ë»Ĳ&#34;:3517,&#34;ëĳĺ&#34;:3518,&#34;ê³łíİ¸&#34;:3519,&#34;íķĺëįĺ&#34;:3520,&#34;ê³¼ëĬĶ&#34;:3521,&#34;Ġë¶ģ&#34;:3522,&#34;ĠìķĬìķĺëĭ¤&#34;:3523,&#34;ìĺĢëįĺ&#34;:3524,&#34;ãħİãħİãħİ&#34;:3525,&#34;Ġê°ĲëıħìĿĢ&#34;:3526,&#34;ìł¸ìĦľ&#34;:3527,&#34;ëĤ¬ëĭ¤&#34;:3528,&#34;ĠíĽĦë°ĺë¶Ģ&#34;:3529,&#34;ĠìĿ¸ìłķ&#34;:3530,&#34;Ġë§Ĳíķĺê³ł&#34;:3531,&#34;ĠíĮĶ&#34;:3532,&#34;Ġíıīë¡łê°Ģ&#34;:3533,&#34;Ġê·¸ëŀĺíĶ½&#34;:3534,&#34;Ġë¶Ģë¶ĦìĿ´&#34;:3535,&#34;ī´&#34;:3536,&#34;ìĿ´ìĹĲìļĶ&#34;:3537,&#34;ĠìĺģíĻĶì¤ĳìĹĲ&#34;:3538,&#34;ĠìķĦë²Ħì§Ģ&#34;:3539,&#34;íķĺëĬĶì§Ģ&#34;:3540,&#34;ëł¤ë©´&#34;:3541,&#34;Ġë³¸ëĭ¤&#34;:3542,&#34;ì¢ĭê³ł&#34;:3543,&#34;ĠíĻįì½©&#34;:3544,&#34;íķľëĭ¤ëĬĶ&#34;:3545,&#34;ëĤĺìĻĶ&#34;:3546,&#34;ĠëįĶëŁ½ê²Į&#34;:3547,&#34;ĠìķĪë§ŀ&#34;:3548,&#34;Ġëª»íķĺê³ł&#34;:3549,&#34;Ġìľłëªħ&#34;:3550,&#34;ìģ¨&#34;:3551,&#34;ĠëıĮëł¤&#34;:3552,&#34;Ġê³¨&#34;:3553,&#34;ĠëĶ°ëľ»íķľ&#34;:3554,&#34;Ġë¹ĦìĬ·íķľ&#34;:3555,&#34;ìķĦë¦Ħëĭ¤ìļ´&#34;:3556,&#34;ëĭ«&#34;:3557,&#34;ìĽ¨&#34;:3558,&#34;ìĹĪëĤĺ&#34;:3559,&#34;ĠìķĬìĿĮ&#34;:3560,&#34;Ġìłľê°Ģ&#34;:3561,&#34;Ġíİĺ&#34;:3562,&#34;ì§Ģë£¨íķĺê³ł&#34;:3563,&#34;Ġëľ¬ê¸Ī&#34;:3564,&#34;ì§Ģëª»&#34;:3565,&#34;¬ëĵ¤&#34;:3566,&#34;ĠìĺģíĻĶìĿ´ëĭ¤&#34;:3567,&#34;Ġìłĳ&#34;:3568,&#34;ìŀ¥ìĿ´&#34;:3569,&#34;ĠìķĪëĤĺìĺ¤&#34;:3570,&#34;ë¯¿&#34;:3571,&#34;Ġì´Į&#34;:3572,&#34;Ġë¶Ħëªħ&#34;:3573,&#34;Ġê³¼ê±°&#34;:3574,&#34;or&#34;:3575,&#34;ìłĪëĮĢ&#34;:3576,&#34;ì§ĢëĦ¤ìļĶ&#34;:3577,&#34;íķ´ì£¼ëĬĶ&#34;:3578,&#34;ëģĮ&#34;:3579,&#34;íĿĳ&#34;:3580,&#34;íĹ¤&#34;:3581,&#34;ĠìĹ°ì¶ľìĿ´&#34;:3582,&#34;ìĪľê°Ħ&#34;:3583,&#34;Ġë¨¼ìłĢ&#34;:3584,&#34;Ģë¡ľ&#34;:3585,&#34;ìĹ¿&#34;:3586,&#34;ë¶Īë&#34;:3587,&#34;ìĪĺìŀĪëĬĶ&#34;:3588,&#34;ìĿ¼ë¿Ĳ&#34;:3589,&#34;ĠìŀĺìĥĿ&#34;:3590,&#34;ë§¹&#34;:3591,&#34;ìĪĺëıĦ&#34;:3592,&#34;ĠìĺĪë&#34;:3593,&#34;ì¶ķ&#34;:3594,&#34;íĸĪìľ¼ë©´&#34;:3595,&#34;Ġê°ĻìĬµëĭĪëĭ¤&#34;:3596,&#34;ëįĶêµ°ìļĶ&#34;:3597,&#34;Ġíķ´ì£¼ëĬĶ&#34;:3598,&#34;Ġê±°ì§Ģ&#34;:3599,&#34;Ġë°Ķë³´&#34;:3600,&#34;ëĸ¨ìĸ´&#34;:3601,&#34;Ġì´¬ìĺģ&#34;:3602,&#34;ëĭµëĭµ&#34;:3603,&#34;ĠìĦľë¡ľ&#34;:3604,&#34;ê¸Īë³´&#34;:3605,&#34;ìŀĪìĿĮ&#34;:3606,&#34;ĠìĬĪ&#34;:3607,&#34;ĠìķĮê²ł&#34;:3608,&#34;Ġë¹Ī&#34;:3609,&#34;ëģĿê¹Įì§Ģ&#34;:3610,&#34;ĠìķĦëĭĪë©´&#34;:3611,&#34;ê°ĢìļĶ&#34;:3612,&#34;ëĵłëĭ¤&#34;:3613,&#34;ĠìĤ¬ê±´&#34;:3614,&#34;ĠëŃĶì§Ģ&#34;:3615,&#34;ĠëĤĺìĺ¨ëĭ¤&#34;:3616,&#34;ìĹĦë§Ī&#34;:3617,&#34;ĠSF&#34;:3618,&#34;ìŀī&#34;:3619,&#34;ìĥĪëģ¼&#34;:3620,&#34;íķłëĵ¯&#34;:3621,&#34;ìĦ±ìķł&#34;:3622,&#34;Ġë°°ìļ°ê°Ģ&#34;:3623,&#34;ìĹ°ê¸°ê°Ģ&#34;:3624,&#34;ìłĪíķľ&#34;:3625,&#34;ëĲĲ&#34;:3626,&#34;Ġãħľ&#34;:3627,&#34;..?&#34;:3628,&#34;ëĮĢìĤ¬&#34;:3629,&#34;íģ°&#34;:3630,&#34;ëįĶë¹Ļ&#34;:3631,&#34;ìĶĢ&#34;:3632,&#34;Ġê¹ľ&#34;:3633,&#34;ìĬ¤íĨłë¦¬ëıĦ&#34;:3634,&#34;ĠíĶ¼íķ´&#34;:3635,&#34;íĬ¹íŀĪ&#34;:3636,&#34;Īë²½&#34;:3637,&#34;ĠëŁ¬ë&#34;:3638,&#34;ìĿĺë¯¸&#34;:3639,&#34;ë°ĭ&#34;:3640,&#34;ìłĲì§ľë¦¬&#34;:3641,&#34;ĠìŀĪìĹĪëįĺ&#34;:3642,&#34;ìĺ¤ëŀĺ&#34;:3643,&#34;ëĤ´ìļĶ&#34;:3644,&#34;ëĶ¸&#34;:3645,&#34;ĠëĤ¨ìķĦ&#34;:3646,&#34;ì§ĢëıĦìķĬ&#34;:3647,&#34;ĠìĿ¸ê°ĦìĿĺ&#34;:3648,&#34;ëĨĵìĿĢ&#34;:3649,&#34;ĠìĤ¶ìĿĦ&#34;:3650,&#34;ĠíĶĦëŀĳìĬ¤&#34;:3651,&#34;ìĬ¤ë¦´ëŁ¬&#34;:3652,&#34;Ģëĭ¤&#34;:3653,&#34;ĠìĺģíĻĶë³´ëĭ¤&#34;:3654,&#34;ë§ĪìĿ´&#34;:3655,&#34;ì¶°&#34;:3656,&#34;Ġë°ĶëŀĮ&#34;:3657,&#34;ĠìĺģìĽĲ&#34;:3658,&#34;Ġì¢ĭìķĺìĿĮ&#34;:3659,&#34;ĠëĬĲê»´ì§ĢëĬĶ&#34;:3660,&#34;Ġë¬´ìĹĩë³´ëĭ¤&#34;:3661,&#34;ëĤĺìĺ¨ëĭ¤&#34;:3662,&#34;ìĸ´ìĦ¤&#34;:3663,&#34;ĠëĤŃë¹Ħ&#34;:3664,&#34;ë§Įìľ¼ë¡ľ&#34;:3665,&#34;ëĮĢëĭ¨&#34;:3666,&#34;ìķĺìĸ´ìļĶ&#34;:3667,&#34;ë¬ĺ&#34;:3668,&#34;ĠìłĢì§Ī&#34;:3669,&#34;Ġì¢ĭìĿĢìĺģíĻĶ&#34;:3670,&#34;Ġëª°ëŀĲ&#34;:3671,&#34;Ġíģ¬ë¦¬ìĬ¤&#34;:3672,&#34;Ġì§Ģê¸ĪëıĦ&#34;:3673,&#34;ĠìĿ´ìĥģíķľ&#34;:3674,&#34;Ġëĭ¤ìļ´ë°Ľ&#34;:3675,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:3676,&#34;íķ´ë³´&#34;:3677,&#34;ìĬ¤ëĬĶ&#34;:3678,&#34;ìĺ¤ê¸Ģ&#34;:3679,&#34;Ġì§ĢëĤľ&#34;:3680,&#34;ìĦ±ë£¡&#34;:3681,&#34;íķĺëĭ¤ëĬĶ&#34;:3682,&#34;ìĨĮëħĦ&#34;:3683,&#34;ë¦¬ëĵ¤&#34;:3684,&#34;íĹĮ&#34;:3685,&#34;ìĤ¬ëĵ¤&#34;:3686,&#34;Ġd&#34;:3687,&#34;ĠìĿ´ìĺģíĻĶëĬĶ&#34;:3688,&#34;Ġê·¸ìĿĺ&#34;:3689,&#34;Ġëĭ¤ê°Ģ&#34;:3690,&#34;ĠìłĦìĦ¤&#34;:3691,&#34;Ġê°Ĳëıħëĭĺ&#34;:3692,&#34;íĿł&#34;:3693,&#34;ìĸ¸ìłľ&#34;:3694,&#34;Ġì¦Ŀ&#34;:3695,&#34;Ġê²½ì°°&#34;:3696,&#34;Ġë¶Ģì¡±íķľ&#34;:3697,&#34;Ġì§Īì§Ī&#34;:3698,&#34;Ġì²ľìŀ¬&#34;:3699,&#34;ĠíĻĺìĥģ&#34;:3700,&#34;ëĭĿíĥĢìŀĦ&#34;:3701,&#34;ĠìĻľìĿ´ë¦¬&#34;:3702,&#34;ìĸ´ë¦´ëķĮ&#34;:3703,&#34;¹Ľ&#34;:3704,&#34;ëĭ¤íģĲ&#34;:3705,&#34;ĠëĤĺë¨¸&#34;:3706,&#34;ĠíĸĪëįĺ&#34;:3707,&#34;ëª¨ìĬµ&#34;:3708,&#34;Ġê·¸ëŁ°ëį°&#34;:3709,&#34;ìĬ¤ëŁ½ê²Į&#34;:3710,&#34;80&#34;:3711,&#34;ìĭľì¼ľ&#34;:3712,&#34;Ġìĸ»&#34;:3713,&#34;ĠìĦ±ê³µ&#34;:3714,&#34;Ġë§ĪìĿĮìĿĦ&#34;:3715,&#34;ĠìĿ¸ìĥģê¹Ĭ&#34;:3716,&#34;tv&#34;:3717,&#34;ľĺ&#34;:3718,&#34;ë¦¬ëĭ¤&#34;:3719,&#34;Ġì§ĵ&#34;:3720,&#34;ìĤ¶&#34;:3721,&#34;íı°&#34;:3722,&#34;Ġë¹Ħì¶Ķ&#34;:3723,&#34;ìĭ¬íŀĪ&#34;:3724,&#34;Ġë¯¸ìĨĮ&#34;:3725,&#34;ĠìĹŃëĮĢ&#34;:3726,&#34;Ġì¢ĭìķĺëĬĶëį°&#34;:3727,&#34;ìĽĢìĿĦ&#34;:3728,&#34;ĠíĸĪì§Ģë§Į&#34;:3729,&#34;ĠìĿ´ìķ¼ê¸°ê°Ģ&#34;:3730,&#34;êµ¬ëł¤&#34;:3731,&#34;Ġëŀ&#34;:3732,&#34;ìĦľëĬĶ&#34;:3733,&#34;ìĸ´ëĿ¼&#34;:3734,&#34;Ġìĸ´ëłµ&#34;:3735,&#34;ìłĢëŁŃ&#34;:3736,&#34;ê³ĦìĨį&#34;:3737,&#34;ĠìĻĢëĭ¿&#34;:3738,&#34;Ġ:&#34;:3739,&#34;íķĺìĦ¸ìļĶ&#34;:3740,&#34;ê¸°ë¡ľ&#34;:3741,&#34;ë¦¬ìĹĲ&#34;:3742,&#34;ìķĦëĿ¼&#34;:3743,&#34;ëĦ¤ìĹ¬&#34;:3744,&#34;ìĹĪìĿĦ&#34;:3745,&#34;Ġë¶Ģëª¨&#34;:3746,&#34;ëĵ±íķĻìĥĿ&#34;:3747,&#34;ĠìĹ¬ëŁ¬&#34;:3748,&#34;Ġëĵłëĭ¤&#34;:3749,&#34;íĺģ&#34;:3750,&#34;ëıĦëĬĶ&#34;:3751,&#34;ëĤĺê°Ģ&#34;:3752,&#34;ìĬĪ&#34;:3753,&#34;ìķĦê°Ģ&#34;:3754,&#34;ê¹Ķ&#34;:3755,&#34;Ġíķľê³Ħ&#34;:3756,&#34;ĠëĵľëĬĶ&#34;:3757,&#34;ë¹ĦëĶĶìĺ¤&#34;:3758,&#34;Ġì¡°ìĹ°&#34;:3759,&#34;ĠëĤĺìĺ¤ê³ł&#34;:3760,&#34;ĠëªħìŀĳìĿ´ëĭ¤&#34;:3761,&#34;ĠìķĦëĭĮëį°&#34;:3762,&#34;ĠíĹĪìĪł&#34;:3763,&#34;ìĻ¸ë¡ľ&#34;:3764,&#34;ĠíĶĦë¡ľê·¸ëŀ¨&#34;:3765,&#34;Ĳë§ģ&#34;:3766,&#34;ìĿ´ìľł&#34;:3767,&#34;ìķĺìĬµëĭĪëĭ¤&#34;:3768,&#34;Ġíķ´ì¤Ģ&#34;:3769,&#34;Ġìĭ¤íĮ¨&#34;:3770,&#34;ĠëĦĺìĸ´&#34;:3771,&#34;Ġì²ĺìĿĮìĿ´ëĭ¤&#34;:3772,&#34;ë°°ê²½&#34;:3773,&#34;©°&#34;:3774,&#34;ĠìĺģíĻĶì§Ģë§Į&#34;:3775,&#34;ëĮĢê°Ģ&#34;:3776,&#34;Ġê°ĻìĿĮ&#34;:3777,&#34;Ġì¶¤&#34;:3778,&#34;Ġë©Ģ&#34;:3779,&#34;ĠëıĪìĿ´&#34;:3780,&#34;ì§ĢìķĬëĬĶ&#34;:3781,&#34;ì¡´ëĤĺ&#34;:3782,&#34;Ġ;;&#34;:3783,&#34;ë¦¬ëıĦ&#34;:3784,&#34;ìĹ¬ì£¼ìĿ¸ê³µ&#34;:3785,&#34;ĠíĿł&#34;:3786,&#34;ë¦¬ë©´&#34;:3787,&#34;Ġë§ŀëĬĶ&#34;:3788,&#34;ê°ĪìĪĺë¡Ŀ&#34;:3789,&#34;ê·¹ìŀ¥ìĹĲìĦľ&#34;:3790,&#34;Ġost&#34;:3791,&#34;ĠëĴ·&#34;:3792,&#34;ìĿ´ìĿĺ&#34;:3793,&#34;ëĤĺìĺ´&#34;:3794,&#34;ìĦľìķ¼&#34;:3795,&#34;ëģĪ&#34;:3796,&#34;ĠëĵľëĿ¼ë§Īê°Ģ&#34;:3797,&#34;ëªħìĿ´&#34;:3798,&#34;ĠëıĻíĻĶ&#34;:3799,&#34;ìķĦì§ģëıĦ&#34;:3800,&#34;Ħë¦¬íĭ°&#34;:3801,&#34;ìĿ´ìĬ¤&#34;:3802,&#34;ìķĦëĤĺ&#34;:3803,&#34;ìŀ¬ë¯¸ìĹĨìĿĮ&#34;:3804,&#34;ĠìĽĲëŀĺ&#34;:3805,&#34;Ġãħİãħİãħİ&#34;:3806,&#34;Ġê³³&#34;:3807,&#34;Ġëª©ìĨĮë¦¬&#34;:3808,&#34;(?)&#34;:3809,&#34;14&#34;:3810,&#34;ĠìķĦëĬĶ&#34;:3811,&#34;ë§ĲëıĦ&#34;:3812,&#34;ìĹ°ê±¸&#34;:3813,&#34;Ġì§Ģë£¨íķ´&#34;:3814,&#34;ë³´ê³łìĭ¶&#34;:3815,&#34;ĠìĤ¬ëŀĳìĿ´&#34;:3816,&#34;ë°ľìĿ´&#34;:3817,&#34;íĽĦë°ĺ&#34;:3818,&#34;íķĺê¸°ë§Į&#34;:3819,&#34;ĠìĽĲìŀĳìĿĦ&#34;:3820,&#34;Ġì²Ńì¶ĺ&#34;:3821,&#34;ëŁŃìłĢëŁŃ&#34;:3822,&#34;¥ĺ&#34;:3823,&#34;ĠìĦ¬&#34;:3824,&#34;ĠìŀĲìľł&#34;:3825,&#34;Ġë³´ê³łìĭ¶ìĿĢ&#34;:3826,&#34;Ġíķ´íĶ¼&#34;:3827,&#34;Ġê°ĲíĥĦ&#34;:3828,&#34;ĠíĶĮ&#34;:3829,&#34;ĠëĤľëĭ¤&#34;:3830,&#34;Ġê²ĮìŀĦ&#34;:3831,&#34;Ġê°Ħë§ĮìĹĲ&#34;:3832,&#34;íĳ¸&#34;:3833,&#34;ĠìĿ´ëŁ°ê±°&#34;:3834,&#34;Ġë¯¸ëŀĺ&#34;:3835,&#34;Ġìŀ¬ë¯¸ìĹĨê³ł&#34;:3836,&#34;ëŁ½ëĭ¤&#34;:3837,&#34;ìĭĿìľ¼ë¡ľ&#34;:3838,&#34;Ġëľ¨&#34;:3839,&#34;ĠëĵľëĿ¼ë§Īë¥¼&#34;:3840,&#34;ëıĦê°Ģ&#34;:3841,&#34;Ġë§ĪëĿ¼&#34;:3842,&#34;ë§ĮëĵľëĬĶ&#34;:3843,&#34;ìĺ¤ë¹ł&#34;:3844,&#34;ĠìĬ¤ì¼Ģ&#34;:3845,&#34;ìĦ±ìĿĢ&#34;:3846,&#34;ĠìĹ°ê¸°ìĹĲ&#34;:3847,&#34;ĠìĽĮ&#34;:3848,&#34;Ġíķ´ì£¼&#34;:3849,&#34;íħĶ&#34;:3850,&#34;ê´Ģê°Ŀ&#34;:3851,&#34;ì¶Ķìĸµ&#34;:3852,&#34;ëĶ°ëľ»&#34;:3853,&#34;Ġìł¤&#34;:3854,&#34;ëĵ¤ë§Į&#34;:3855,&#34;Ġë©Ķìĭľ&#34;:3856,&#34;Ġìķŀìľ¼ë¡ľ&#34;:3857,&#34;Ġì·¨íĸ¥&#34;:3858,&#34;ëļ±&#34;:3859,&#34;ëĩĮ&#34;:3860,&#34;Ġë¶Ļ&#34;:3861,&#34;íĴĢ&#34;:3862,&#34;ìĺĢìĸ´ìļĶ&#34;:3863,&#34;ë©´ìĦľëıĦ&#34;:3864,&#34;ĠìĿ¼ìĸ´&#34;:3865,&#34;Ġê³łë¯¼&#34;:3866,&#34;Ġë°ĶëĢ&#34;:3867,&#34;Ġì²ĺìĿĮë¶ĢíĦ°&#34;:3868,&#34;ìĥĿê°ģë³´ëĭ¤&#34;:3869,&#34;ĠëĪĪìĿĦ&#34;:3870,&#34;Ġëĸ¨ìĸ´ì§ĢëĬĶ&#34;:3871,&#34;Ġë¶Īë¥ľ&#34;:3872,&#34;Ġíį¼&#34;:3873,&#34;ĠìĿĳ&#34;:3874,&#34;ĠìĿ´íķĺ&#34;:3875,&#34;ê±°ëĥĲ&#34;:3876,&#34;ìĹĨìĸ´ìĦľ&#34;:3877,&#34;ì¦Ĳ&#34;:3878,&#34;Ġìŀĺë§Įëĵł&#34;:3879,&#34;Ġê·Ģìĭł&#34;:3880,&#34;ĠìĨĮìŀ¬ë¥¼&#34;:3881,&#34;ĠD&#34;:3882,&#34;Ġë³´ìĿ´ëĬĶ&#34;:3883,&#34;ë°¥&#34;:3884,&#34;ìĭľì²Ń&#34;:3885,&#34;ê·¸ëłĩ&#34;:3886,&#34;ĠìłĦë¬¸&#34;:3887,&#34;ë¬´ìĦŃ&#34;:3888,&#34;Ġê°ĲìłķìĿ´&#34;:3889,&#34;Ġëħ¼&#34;:3890,&#34;ĠìĿ´ìľłê°Ģ&#34;:3891,&#34;Ġëļ&#34;:3892,&#34;ë§¥&#34;:3893,&#34;ìĥ¤&#34;:3894,&#34;ìĻĢìĦľ&#34;:3895,&#34;íĸĪìĿĦ&#34;:3896,&#34;ĠëĬĻ&#34;:3897,&#34;ĠëĤ¨ëĬĶëĭ¤&#34;:3898,&#34;ĠìĬ¤íĨłë¦¬ìĻĢ&#34;:3899,&#34;Ġë¯¸ìķĪ&#34;:3900,&#34;ĠìķĪë´¤&#34;:3901,&#34;Ġê³µíı¬ë&#34;:3902,&#34;Ġë³´ëĬĶê²Į&#34;:3903,&#34;ĠìķĦê¹ĮìĽĮ&#34;:3904,&#34;ĠëĪĪìĿ´&#34;:3905,&#34;Ġì°įìĿĢ&#34;:3906,&#34;ê²Łëĭ¤&#34;:3907,&#34;90&#34;:3908,&#34;ë©ĶìĿ´&#34;:3909,&#34;ĠìķĦë§Ī&#34;:3910,&#34;Ġìĸ´ì°Į&#34;:3911,&#34;ĠëģĬ&#34;:3912,&#34;ë¹Ħê°Ģ&#34;:3913,&#34;ĠëĮĢíķľë¯¼êµŃ&#34;:3914,&#34;ĠëĪĦêµ°&#34;:3915,&#34;Ġë¡&#34;:3916,&#34;ê°Ģìŀ¥&#34;:3917,&#34;ëĵ¤ê³ł&#34;:3918,&#34;ìķĦë²Ħì§Ģ&#34;:3919,&#34;ĠëĤĺìĦľ&#34;:3920,&#34;ëł¤ëĭ¤&#34;:3921,&#34;ĠìĻľìĿ´ëłĩê²Į&#34;:3922,&#34;ĠìĤ¬ëŀĮìĿĦ&#34;:3923,&#34;ĠíķĺëĤĺíķĺëĤĺ&#34;:3924,&#34;Ġê·¸ëŁ°ê°Ģ&#34;:3925,&#34;ëŃĲì§Ģ&#34;:3926,&#34;ĠìŀĪëĭ¤ë©´&#34;:3927,&#34;Ġìľłì¹ĺíķľ&#34;:3928,&#34;ìŀ¬ë¯¸ìŀĪìĸ´ìļĶ&#34;:3929,&#34;ĠìĹ¬ëŁ¬ë&#34;:3930,&#34;ĠíĨµíķ´&#34;:3931,&#34;¬ë¡ľ&#34;:3932,&#34;³´ëĭ¤&#34;:3933,&#34;ĠìĿ´ëŀĺ&#34;:3934,&#34;ìľ¼ë¡ł&#34;:3935,&#34;Ġê¸Ī&#34;:3936,&#34;ì¤ĳêµŃ&#34;:3937,&#34;ìłĢëĬĶ&#34;:3938,&#34;Ġë§¤ëł¥ìĿ´&#34;:3939,&#34;ëĳ¥&#34;:3940,&#34;Ġë¦¬ë©ĶìĿ´íģ¬&#34;:3941,&#34;Ġê°ĲìĤ¬íķ©ëĭĪëĭ¤&#34;:3942,&#34;Ġ+&#34;:3943,&#34;ëĭ¤ì§Ģ&#34;:3944,&#34;ëĬĶê°Ģ&#34;:3945,&#34;ìĺģíĻĶëĿ¼&#34;:3946,&#34;ìĺģíĻĶëĿ¼ê³ł&#34;:3947,&#34;Ġê°ĢìĦľ&#34;:3948,&#34;ìĹ°ê¸°ëł¥&#34;:3949,&#34;ĠíĻĶê°Ģ&#34;:3950,&#34;Ġì¤Ħê±°ë¦¬&#34;:3951,&#34;Ġìłģëĭ¹&#34;:3952,&#34;ĠëĤĺìĻĶìľ¼ë©´&#34;:3953,&#34;ĠëŁ¬ë¸Į&#34;:3954,&#34;11&#34;:3955,&#34;ĠìĺģíĻĶëĥĲ&#34;:3956,&#34;ëĦ¤ìĿ´ë²Ħ&#34;:3957,&#34;Ġë´¤ëĭ¤ê°Ģ&#34;:3958,&#34;ìĹ¬ê¸°&#34;:3959,&#34;ĠìķĮìķĦ&#34;:3960,&#34;ĠìĿ´ëŁ°ìĺģíĻĶ&#34;:3961,&#34;Ġìŀ¬ë¯¸ìŀĪê³ł&#34;:3962,&#34;Ġëħ¹&#34;:3963,&#34;ì§±ì§±&#34;:3964,&#34;ê·¸ëŀĺìĦľ&#34;:3965,&#34;Ġëıħë¦½&#34;:3966,&#34;ĠìĬ¤íĨłë¦¬ë¥¼&#34;:3967,&#34;ìĸ´ëł¸ìĿĦëķĮ&#34;:3968,&#34;ë³įê²Į&#34;:3969,&#34;ĠìĹ¬ìłĦíŀĪ&#34;:3970,&#34;ìİĦ&#34;:3971,&#34;ì¨&#34;:3972,&#34;íķľì§Ģ&#34;:3973,&#34;ëĿ¼ìĬ¤&#34;:3974,&#34;ëŀĲëĭ¤&#34;:3975,&#34;Ġìĺ¬ëĿ¼&#34;:3976,&#34;ìĻĢë¥´&#34;:3977,&#34;Ġíķµ&#34;:3978,&#34;ĠìłĢëĬĶ&#34;:3979,&#34;ĠìĹŃíķł&#34;:3980,&#34;ìľĦíķ´&#34;:3981,&#34;ê²°êµŃ&#34;:3982,&#34;ĠìķĪëĲĺê³ł&#34;:3983,&#34;Ġ80&#34;:3984,&#34;íļ¨ê³¼&#34;:3985,&#34;ĠìķĪë³¸&#34;:3986,&#34;µĿ&#34;:3987,&#34;ìĵ´&#34;:3988,&#34;ìĿ´íĦ°&#34;:3989,&#34;ëĭĪìķĦ&#34;:3990,&#34;Ġë³´ëŁ¬&#34;:3991,&#34;íķłê¹Į&#34;:3992,&#34;Ġëįĺ&#34;:3993,&#34;ĠëĤ´ìļ©ìĿĦ&#34;:3994,&#34;Ġìļ°ìļ¸&#34;:3995,&#34;Ġìĭľê°ĦëĤŃë¹Ħ&#34;:3996,&#34;Ġì²ĺìĿĮìľ¼ë¡ľ&#34;:3997,&#34;Ġíķľíİ¸&#34;:3998,&#34;ĠìĥĿê°ģíķ©ëĭĪëĭ¤&#34;:3999,&#34;ìĿ´ëĿ¼ìĦľ&#34;:4000,&#34;ĠëģĿëĤĺê³ł&#34;:4001,&#34;ìĹŃìĤ¬&#34;:4002,&#34;íķľëĭ¤ë©´&#34;:4003,&#34;ìĤ´ëĭ¤&#34;:4004,&#34;ë¨¹ê³ł&#34;:4005,&#34;ì¾Įíķľ&#34;:4006,&#34;ìķĦëĭĪëĿ¼&#34;:4007,&#34;ĠìŀĲìĭłìĿĺ&#34;:4008,&#34;Ġì²łíķĻ&#34;:4009,&#34;bb&#34;:4010,&#34;ê¸°ë¶Ħ&#34;:4011,&#34;Ġìłķì¹ĺ&#34;:4012,&#34;ĠìĭľìĦł&#34;:4013,&#34;Ġë´¤ìĹĪëĬĶëį°&#34;:4014,&#34;Ġê°Ģì§Ħ&#34;:4015,&#34;ìĿ¼ëĭ¨&#34;:4016,&#34;Ġìŀ¬ë°Įìĸ´&#34;:4017,&#34;ĠìłĦëĭ¬&#34;:4018,&#34;ĠìķĪíķĺê³ł&#34;:4019,&#34;ë¥¸ëĭ¤&#34;:4020,&#34;Ġë³´ê¸°ìĹĶ&#34;:4021,&#34;Ġìľłì¹ĺíķĺê³ł&#34;:4022,&#34;ì§ĢìķĬìĿĢ&#34;:4023,&#34;íĳľíĺĦ&#34;:4024,&#34;ë©ĺíĦ°&#34;:4025,&#34;ìĹ¬ìļ´ìĿ´&#34;:4026,&#34;íīģ&#34;:4027,&#34;an&#34;:4028,&#34;ĵ¨&#34;:4029,&#34;Ġë¥ĺ&#34;:4030,&#34;ìĿ´ìŀĲ&#34;:4031,&#34;ëłī&#34;:4032,&#34;ìĹĪìĸ´&#34;:4033,&#34;ì£¼ìĸ¼&#34;:4034,&#34;íĨ¤&#34;:4035,&#34;ê±¸ë¡ľ&#34;:4036,&#34;ìĽĲìĿ´&#34;:4037,&#34;ìĶ¨ê°Ģ&#34;:4038,&#34;ĠëĪĦêµ¬&#34;:4039,&#34;Ġê´ľíŀĪ&#34;:4040,&#34;**&#34;:4041,&#34;100&#34;:4042,&#34;30&#34;:4043,&#34;ou&#34;:4044,&#34;įëĭĪëĭ¤&#34;:4045,&#34;ìĿ´ë¥¼&#34;:4046,&#34;íķľë§ĪëĶĶë¡ľ&#34;:4047,&#34;ë²¤&#34;:4048,&#34;ĠëĦĪë¬´ëĦĪë¬´&#34;:4049,&#34;ë¯¸ì¹ľ&#34;:4050,&#34;Ġìŀĺë´¤ìĬµëĭĪëĭ¤&#34;:4051,&#34;ëłĪìĬ¤&#34;:4052,&#34;ëŃĺ&#34;:4053,&#34;Ġë³Ħë£¨&#34;:4054,&#34;Ġì§ľì¦ĿëĤĺ&#34;:4055,&#34;ĠíĴĢ&#34;:4056,&#34;ĠëĮĢëĭ¨íķĺëĭ¤&#34;:4057,&#34;ĠìķĪëĲ¨&#34;:4058,&#34;Ġcg&#34;:4059,&#34;ëĤĺìĿĺ&#34;:4060,&#34;ì£¼ìĹ°&#34;:4061,&#34;ëĤ´ìĿ¸ìĥĿ&#34;:4062,&#34;ìĹĲìĦľìĿĺ&#34;:4063,&#34;ĠëĮĢì¶©&#34;:4064,&#34;ëĶĶìĸ´&#34;:4065,&#34;íĬ¸ê°Ģ&#34;:4066,&#34;ĠëĵľëĿ¼ë§ĪëĬĶ&#34;:4067,&#34;ĠëĲĺìĹĪëĭ¤&#34;:4068,&#34;ĠìĹ¬ìŀĲê°Ģ&#34;:4069,&#34;ëĤ´ìļ©ìĿĢ&#34;:4070,&#34;ê¶ģ&#34;:4071,&#34;íĹĪìłĳ&#34;:4072,&#34;15&#34;:4073,&#34;ģìĵ¸&#34;:4074,&#34;íķĺëł¤ëĬĶ&#34;:4075,&#34;ĠìĹĨì§Ģë§Į&#34;:4076,&#34;Ġì°¬&#34;:4077,&#34;Ġì¤ĳë°ĺ&#34;:4078,&#34;ì¶ľìĹ°&#34;:4079,&#34;ëĨ¨&#34;:4080,&#34;Ġìŀ¬ë¯¸ìĹĨìĸ´&#34;:4081,&#34;ĠìłĦê¸°ìĦ¸ê°Ģ&#34;:4082,&#34;ê¸°ìĿĺ&#34;:4083,&#34;ìĭľìķĦ&#34;:4084,&#34;ê¹Įì§Ħ&#34;:4085,&#34;êµ¬íķĺê³ł&#34;:4086,&#34;Ġëª¨ìŀĲ&#34;:4087,&#34;ìĭłìĦł&#34;:4088,&#34;ì¡°ê¸Ī&#34;:4089,&#34;Ġë°Ķê¿&#34;:4090,&#34;Ġê¼´&#34;:4091,&#34;ì´ĪëĶ©&#34;:4092,&#34;íıīë¡łê°Ģ&#34;:4093,&#34;ĠíĻ©ëĭ¹&#34;:4094,&#34;íĿ¥ë¯¸&#34;:4095,&#34;ì§Īëģ&#34;:4096,&#34;ãħģ&#34;:4097,&#34;ĠìķĦìłĢìĶ¨&#34;:4098,&#34;ĠëĤĺìĹĲê²Į&#34;:4099,&#34;ìłĦíĺĢ&#34;:4100,&#34;Ġìŀ¬ë¯¸ë¥¼&#34;:4101,&#34;íİ¸ë³´ëĭ¤&#34;:4102,&#34;ë²Ħëł¸ëĭ¤&#34;:4103,&#34;ê»Ħ&#34;:4104,&#34;ĠìĺĪìģľ&#34;:4105,&#34;ì©Ķ&#34;:4106,&#34;Ġì§Ģê¸Īê¹Įì§Ģ&#34;:4107,&#34;ìĽĥê¸°&#34;:4108,&#34;ìŀĬ&#34;:4109,&#34;Ġë³´ìĭľê¸¸&#34;:4110,&#34;ëĮĢëĬĶ&#34;:4111,&#34;Ġëĭ¤ë§Į&#34;:4112,&#34;ìŀĪìĸ´&#34;:4113,&#34;ĠìĥĪë¡&#34;:4114,&#34;Ġë³´ê³łëĤĺìĦľ&#34;:4115,&#34;ĠìĤ¬ëĬĶ&#34;:4116,&#34;ê²°ë§ĲìĿ´&#34;:4117,&#34;ĠíĿ¥ë¯¸ë¡Ń&#34;:4118,&#34;Ġê·¸ëŁ¼&#34;:4119,&#34;ìĭľëĤĺë¦¬ìĺ¤&#34;:4120,&#34;ĠìŀĪìĬµëĭĪëĭ¤&#34;:4121,&#34;ì°Ŀ&#34;:4122,&#34;ĠìĺģíĻĺ&#34;:4123,&#34;ĠìµľìķħìĿ´ëĭ¤&#34;:4124,&#34;Ġëª°ìŀħìĿ´&#34;:4125,&#34;ĠìĹ°ê¸°ëł¥ìĿ´&#34;:4126,&#34;¬ëł&#34;:4127,&#34;!!!!!&#34;:4128,&#34;ĠëĤĺë¥¼&#34;:4129,&#34;ĠëĤĺíĥĢ&#34;:4130,&#34;ìĪĺëĬĶ&#34;:4131,&#34;ĠëĦĪë¬´ëĤĺëıĦ&#34;:4132,&#34;ëĵľëĿ¼&#34;:4133,&#34;Ġìŀ¬ë°ĮìĹĪìĸ´ìļĶ&#34;:4134,&#34;ĠìĹ¬ìĦ±&#34;:4135,&#34;ëįĶë§Į&#34;:4136,&#34;ë§¤ëł¥&#34;:4137,&#34;Ġê¸°ë¶ĦìĿ´&#34;:4138,&#34;Ġìĸ´ìĥīíķľ&#34;:4139,&#34;ì§Ģì»¬&#34;:4140,&#34;Ġê·¸ëĵ¤ìĿĺ&#34;:4141,&#34;ìĹĪì§Ģ&#34;:4142,&#34;ë²¼&#34;:4143,&#34;ĠìĹ°ê¸°ìĻĢ&#34;:4144,&#34;ĠìĿ¼ëĭ¨&#34;:4145,&#34;ëĲĺì§Ģ&#34;:4146,&#34;ì¦ĪëĭĪ&#34;:4147,&#34;ĠíķĦìļĶìĹĨëĬĶ&#34;:4148,&#34;Ġìłľìŀĳì§Ħ&#34;:4149,&#34;Ġê·ĢìĹ¬ìļ´&#34;:4150,&#34;ĠíķĻêµĲ&#34;:4151,&#34;ĠìłĦì²´ìłģìľ¼ë¡ľ&#34;:4152,&#34;Ġì§ł&#34;:4153,&#34;Ġëĭ¤ìĨĮ&#34;:4154,&#34;ëŀĺê¸°&#34;:4155,&#34;ë¬´ìĦľ&#34;:4156,&#34;ìĤ¬ëŀĮìĿĢ&#34;:4157,&#34;ĠìĻ¸êµŃ&#34;:4158,&#34;ìŀĶìŀĶíķľ&#34;:4159,&#34;ê°ĢëĬ¥&#34;:4160,&#34;ë³´ìĿ´&#34;:4161,&#34;Ġë°Ŀ&#34;:4162,&#34;ë´ĩ&#34;:4163,&#34;íĸĪëĭ¤ëĬĶ&#34;:4164,&#34;ê°ĻìĿĮ&#34;:4165,&#34;Ġë§Įëĵ¤ìĸ´ì§Ħ&#34;:4166,&#34;ĠìĪĺì¤ĢìĿ´&#34;:4167,&#34;Ġì©Ķ&#34;:4168,&#34;Ġê¾¸&#34;:4169,&#34;ĠìĺĪë»&#34;:4170,&#34;ìĿ´ê²ĥëıĦ&#34;:4171,&#34;ìĸ´ì§Ħ&#34;:4172,&#34;ë¡ľëĬĶ&#34;:4173,&#34;ë¡ľë§¨&#34;:4174,&#34;ìķĦëĨĶ&#34;:4175,&#34;ê±°ìļ´&#34;:4176,&#34;Ġëĭ¤ìĭľë´ĲëıĦ&#34;:4177,&#34;ìķĪë³´&#34;:4178,&#34;Ġíı¬ìŀ¥&#34;:4179,&#34;ëıĮìķĦ&#34;:4180,&#34;ĳ¹&#34;:4181,&#34;ĠìĿ´ìłł&#34;:4182,&#34;ìłĲì£¼ëĬĶ&#34;:4183,&#34;ê±°ë¦¬ëĬĶ&#34;:4184,&#34;ë§ĪìłĢ&#34;:4185,&#34;ìĥģìĿ´&#34;:4186,&#34;Ġëı¼&#34;:4187,&#34;Ġëįľ&#34;:4188,&#34;Ġê¸°ë³¸&#34;:4189,&#34;ĠìĪĺëıĦ&#34;:4190,&#34;ĠìłĦìĹĲ&#34;:4191,&#34;Ġê°ľíĮĲ&#34;:4192,&#34;Ġìļ°ìĹ°íŀĪ&#34;:4193,&#34;Ġê¼½&#34;:4194,&#34;Ġë³´ì§Ģë§Ī&#34;:4195,&#34;Ġë³´ì§Ģë§ĪìĦ¸ìļĶ&#34;:4196,&#34;Ġë¦¬ë·°&#34;:4197,&#34;ĠíĿĲë¦Ħ&#34;:4198,&#34;µĿìĺ¤&#34;:4199,&#34;ë¢&#34;:4200,&#34;ë¡¯&#34;:4201,&#34;ìĿ¸ëıĦ&#34;:4202,&#34;ìĭľíĤ¨&#34;:4203,&#34;ĠìŀĪëĤĺ&#34;:4204,&#34;ê³µê°Ĳ&#34;:4205,&#34;ĠìĤ¬ìĿ´&#34;:4206,&#34;ĠìĤ¬ê·¹&#34;:4207,&#34;ĠìłĢëłĩê²Į&#34;:4208,&#34;ì½Ķë¯¹&#34;:4209,&#34;Ġìĭ¬ìĭ¬&#34;:4210,&#34;ìķĦê¹Įìļ´&#34;:4211,&#34;Ġëª¸ë§¤&#34;:4212,&#34;ĠìĨĮìŀ¬ë¡ľ&#34;:4213,&#34;ë§ĺ&#34;:4214,&#34;ì§ĢìĿĺ&#34;:4215,&#34;Ġìŀĩ&#34;:4216,&#34;Ġì¢ĭìĿĦ&#34;:4217,&#34;ìľ¼ë¡ľìĦľ&#34;:4218,&#34;Ġìĸ´ìĿ´ê°Ģ&#34;:4219,&#34;ĠëŃī&#34;:4220,&#34;ëĵ¯ìĿ´&#34;:4221,&#34;Ġìĺ¤ëĿ½&#34;:4222,&#34;ĠìľłìĿ¼&#34;:4223,&#34;ëĦĪë¬´ëĤĺ&#34;:4224,&#34;ĠìłĢëıĦ&#34;:4225,&#34;ĠíĹĽ&#34;:4226,&#34;ê´Ģê³Ħ&#34;:4227,&#34;ĠìķĦëĭĮëĵ¯&#34;:4228,&#34;ìĽłìĿĮ&#34;:4229,&#34;Ġë°ĺìłĦìĿ´&#34;:4230,&#34;Ġê·¸ëłĩëĭ¤&#34;:4231,&#34;ĠíĹĪìłĳíķľ&#34;:4232,&#34;ĠìķĦë¦Ħëĭµê³ł&#34;:4233,&#34;?!&#34;:4234,&#34;ìĿ´ë¦¬&#34;:4235,&#34;ë§ĮìłĲ&#34;:4236,&#34;Ġë°¤&#34;:4237,&#34;ëĮĢìĿĺ&#34;:4238,&#34;ĠíķĺëĦ¤ìļĶ&#34;:4239,&#34;ëª¸&#34;:4240,&#34;Ġìĺ¤ê·¸ëĿ¼&#34;:4241,&#34;Ġë¶ĢíĦ°&#34;:4242,&#34;ĠëĲĺê³ł&#34;:4243,&#34;Ġê³łìĥĿ&#34;:4244,&#34;ëĳĲê³ł&#34;:4245,&#34;¬ë¦Ħ&#34;:4246,&#34;ìķĪëĲ&#34;:4247,&#34;ëĶ°ëĿ¼&#34;:4248,&#34;Ġíı¬ê¸°&#34;:4249,&#34;Ġì±Ļ&#34;:4250,&#34;Ġê³µê°ĲìĿ´&#34;:4251,&#34;Ġì¹´ë©ĶëĿ¼&#34;:4252,&#34;Ġtv&#34;:4253,&#34;Ġì§Ħë¶Ģíķľ&#34;:4254,&#34;ìĺ¬ëķĮ&#34;:4255,&#34;ĪëķĮ&#34;:4256,&#34;ë¡±&#34;:4257,&#34;¬ëŀĢ&#34;:4258,&#34;Ġíķľë§ĪëĶĶë¡ľ&#34;:4259,&#34;Ġë´¤ìľ¼ë©´&#34;:4260,&#34;Ġê°ĢëģĶ&#34;:4261,&#34;Ġ15&#34;:4262,&#34;Ġê°ĻìķĦ&#34;:4263,&#34;ĠëĶ´&#34;:4264,&#34;íĤ¬ë§ģíĥĢìŀĦìļ©&#34;:4265,&#34;ĠíĻľ&#34;:4266,&#34;ĠíĥĪ&#34;:4267,&#34;ĠëĳĲë²Ī&#34;:4268,&#34;Ġë§¤ëł¥ìłģìĿ¸&#34;:4269,&#34;êµ¬ë¥¼&#34;:4270,&#34;ĠìĨĮë¦Ħëıĭ&#34;:4271,&#34;Ġìļ©ìĦľ&#34;:4272,&#34;Ġê´´ë¬¼&#34;:4273,&#34;íķľê²ĥ&#34;:4274,&#34;ĠìĿ´ê¸°&#34;:4275,&#34;íķ´ì¤Ģ&#34;:4276,&#34;Ġë³´ìĭľ&#34;:4277,&#34;ë²Īë³´&#34;:4278,&#34;ĠìĽĶ&#34;:4279,&#34;Ġë¶ĢëģĦ&#34;:4280,&#34;ìĨįìĹĲìĦľ&#34;:4281,&#34;ëĬĲëĿ¼&#34;:4282,&#34;ë¥ĺìĿĺ&#34;:4283,&#34;Ġì§ľì¦ĿëĤĺëĬĶ&#34;:4284,&#34;ĠìĿ´ìľłëĬĶ&#34;:4285,&#34;Ġìĸ´ìĿ´ìĹĨëĬĶ&#34;:4286,&#34;ĠëĪĪë¹Ľ&#34;:4287,&#34;ë¢°&#34;:4288,&#34;Ġë§¡&#34;:4289,&#34;ëĦ£&#34;:4290,&#34;ĠìĹĨëĤĺ&#34;:4291,&#34;ë£¬&#34;:4292,&#34;ìĦ±ìĹĲ&#34;:4293,&#34;ĠìĨĮëħĢ&#34;:4294,&#34;ë´¤ìĿĮ&#34;:4295,&#34;ìŀ¬ë°ĮëĬĶëį°&#34;:4296,&#34;ĠíıĲ&#34;:4297,&#34;ë°°ìļ°ê°Ģ&#34;:4298,&#34;ê¼½&#34;:4299,&#34;ì§Ģë£¨íķ¨&#34;:4300,&#34;Ġì¶Ķì²ľíķ©ëĭĪëĭ¤&#34;:4301,&#34;ê´ľì°®ìĿĢ&#34;:4302,&#34;ê²¹ëĭ¤&#34;:4303,&#34;ĠOST&#34;:4304,&#34;ìĿ´ëŀĺ&#34;:4305,&#34;ìķĦëĭĺ&#34;:4306,&#34;ê¹Įì§ĢëĬĶ&#34;:4307,&#34;ì§ĢìķĬê³ł&#34;:4308,&#34;ê¾¼&#34;:4309,&#34;íķĺìĭľëĬĶ&#34;:4310,&#34;ĠìĬ¬íĶĦëĭ¤&#34;:4311,&#34;ĠìłĬìĿĢ&#34;:4312,&#34;ìŀĪëįĺ&#34;:4313,&#34;ãħĪ&#34;:4314,&#34;ëįľ&#34;:4315,&#34;ìĺ¹&#34;:4316,&#34;Ġë°¥&#34;:4317,&#34;ìŀĲëĭ¨&#34;:4318,&#34;ĠëģĿëĤĺëĬĶ&#34;:4319,&#34;ìĹ¬ë°°ìļ°&#34;:4320,&#34;ëĪĦê°Ģ&#34;:4321,&#34;ì¤ĺìĦľ&#34;:4322,&#34;ĠíĿ¬ìĥĿ&#34;:4323,&#34;ĠìĻłì§Ģ&#34;:4324,&#34;ëıĦìĹĨê³ł&#34;:4325,&#34;ìŀĲê³ł&#34;:4326,&#34;Ġíķĺë£¨&#34;:4327,&#34;ìĻĢëĬĶ&#34;:4328,&#34;ìĭłìĿĺ&#34;:4329,&#34;íķĺëĭ¤ê°Ģ&#34;:4330,&#34;ĠìĥĿê°ģëĤĺëĬĶ&#34;:4331,&#34;ë²Ħëł¸&#34;:4332,&#34;Ġì¹¼&#34;:4333,&#34;ĠìĤ´ì§Ŀ&#34;:4334,&#34;ĠíĻķìĭ¤íŀĪ&#34;:4335,&#34;ì¤ĳê°ĦìĹĲ&#34;:4336,&#34;ĠìķĦë¦Ħëĭµëĭ¤&#34;:4337,&#34;ĠìĪľìĪĺíķľ&#34;:4338,&#34;ì§Ģë§Ī&#34;:4339,&#34;Ġë³´ìĿ´&#34;:4340,&#34;ìĺģíĻĶìŀĦ&#34;:4341,&#34;ë³´ìĹ¬&#34;:4342,&#34;ìŀ¥ë¥´&#34;:4343,&#34;ìĤ¬ìĭ¤&#34;:4344,&#34;Ġë§Īì¹ĺ&#34;:4345,&#34;ëħĦìłĦìĹĲ&#34;:4346,&#34;ìķĪìĹĲ&#34;:4347,&#34;Ġë§Įëĵ¤ê³ł&#34;:4348,&#34;Ġìĥģì²ĺ&#34;:4349,&#34;Ġì¢ĭìķĺëįĺ&#34;:4350,&#34;ë³ĦìłĲ&#34;:4351,&#34;Ġíİĳ&#34;:4352,&#34;Ġíĳľìłķ&#34;:4353,&#34;Ġê±´ì§Ģ&#34;:4354,&#34;Ġë´Ĳìķ¼íķł&#34;:4355,&#34;ĠìĶ¬&#34;:4356,&#34;ê°Ģê°Ģ&#34;:4357,&#34;ê¸°ëįķ&#34;:4358,&#34;ìķĦìĺ¤&#34;:4359,&#34;ĠìŀĪì§Ģ&#34;:4360,&#34;íŀĪëĬĶ&#34;:4361,&#34;Ġë§ĲìĿĦ&#34;:4362,&#34;Ġìĭ¶ìĸ´&#34;:4363,&#34;Ġì¹¨&#34;:4364,&#34;ë°°ìļ°ëĵ¤ìĿ´&#34;:4365,&#34;ĠíıīìĥĿ&#34;:4366,&#34;ĠíķĦìļĶìĹĨëĭ¤&#34;:4367,&#34;ìŀ¥ë©´ìĿ´&#34;:4368,&#34;ĠëĨĢëŀį&#34;:4369,&#34;ĠëĸłëĤĺ&#34;:4370,&#34;ĠìĦ¹ìĭľ&#34;:4371,&#34;°©&#34;:4372,&#34;ĠìĺģíĻĶê´Ģ&#34;:4373,&#34;ëĵ¤ìĹĲ&#34;:4374,&#34;Ġê²©&#34;:4375,&#34;ĠìŀĪê²Į&#34;:4376,&#34;ëįĺì§Ģ&#34;:4377,&#34;Ġë¹Ħê·¹&#34;:4378,&#34;Ġê°ĲëıĻìłģ&#34;:4379,&#34;Ġìŀ¬ë¯¸ìĹĨëĦ¤&#34;:4380,&#34;ìĺĪìĪł&#34;:4381,&#34;ìĺĪìłĦìĹĲ&#34;:4382,&#34;Ġì¡¸ëĿ¼&#34;:4383,&#34;ëĭ¤ìĭľë´ĲëıĦ&#34;:4384,&#34;Ġìĭ¸ìĿ´ì½Ķ&#34;:4385,&#34;Ġìĭ«ëĭ¤&#34;:4386,&#34;ê²¼ëĭ¤&#34;:4387,&#34;Ġìŀ¼ìŀĪê²Į&#34;:4388,&#34;Ġíģ¬ê²Į&#34;:4389,&#34;ĠìĿ´ëŁ¬&#34;:4390,&#34;Ġì§Ĳ&#34;:4391,&#34;ìĺģíĻĶë³´ëĭ¤&#34;:4392,&#34;ĠëĤĺìĿĢ&#34;:4393,&#34;ĠìĺĢ&#34;:4394,&#34;Ġìĺ¤ëŀľ&#34;:4395,&#34;ĠíıīìłĲìĹĲ&#34;:4396,&#34;Ġì§Ģë£¨íķĺì§Ģ&#34;:4397,&#34;ëģ¼ë¦¬&#34;:4398,&#34;ĠìĹŃìĭľëĤĺ&#34;:4399,&#34;Ġìŀ¬ë°ĭê²Į&#34;:4400,&#34;19&#34;:4401,&#34;Ġìº&#34;:4402,&#34;ìĿ´íĽĦ&#34;:4403,&#34;ãħı&#34;:4404,&#34;ëłģ&#34;:4405,&#34;íķ´ì§Ħ&#34;:4406,&#34;ì²«&#34;:4407,&#34;ìĪĺìĹĨëĬĶ&#34;:4408,&#34;íĬ¸ì½¤&#34;:4409,&#34;ĠìĤ¬ê¸°&#34;:4410,&#34;ìķłëĵ¤ìĿ´&#34;:4411,&#34;ìĦ¤ìłķ&#34;:4412,&#34;Ġë¶Ħëħ¸&#34;:4413,&#34;Ġê¸´ìŀ¥&#34;:4414,&#34;ì²ĺìĿĮë¶ĢíĦ°&#34;:4415,&#34;ìĽĲìŀĳìĿĦ&#34;:4416,&#34;ĠìĤ°ë§Į&#34;:4417,&#34;Ġíļ¨&#34;:4418,&#34;ìĨĮìŀ¬ëĬĶ&#34;:4419,&#34;Ġê¹ĶëģĶ&#34;:4420,&#34;ëĬĶê±´&#34;:4421,&#34;ĠìĺģíĻĶë³´ê³ł&#34;:4422,&#34;ìĺģíĻĶê´ĢìĹĲìĦľ&#34;:4423,&#34;Ġê·¸ê²ĥ&#34;:4424,&#34;Ġê·¸ë¦°&#34;:4425,&#34;ìŀĲìĹ°&#34;:4426,&#34;ìĪĺìĿĺ&#34;:4427,&#34;ĠëĵľëŁ¬&#34;:4428,&#34;ëł¥ìĿĦ&#34;:4429,&#34;ĠíĽĦìĨį&#34;:4430,&#34;ĠíķĦìļĶíķľ&#34;:4431,&#34;ĠëĿ¼ê³ł&#34;:4432,&#34;ĠíĻįë³´&#34;:4433,&#34;Ġê·ĢìĹ½ê³ł&#34;:4434,&#34;on&#34;:4435,&#34;ĥ¥&#34;:4436,&#34;ëĤļ&#34;:4437,&#34;ìĭľìĹĲ&#34;:4438,&#34;ê°ĦìĿĺ&#34;:4439,&#34;êµ¬ëĭĪ&#34;:4440,&#34;ì¤ĳìĿĺ&#34;:4441,&#34;ĠìķĬê²Į&#34;:4442,&#34;Ġêµ¬ë&#34;:4443,&#34;Ġìĺ¤ë²Ħ&#34;:4444,&#34;Ġìķłëĵ¤ìĿ´&#34;:4445,&#34;ìŀĶìĿ¸&#34;:4446,&#34;ê°Ĳëıħëĭĺ&#34;:4447,&#34;ìĿ¸ê°ĢìļĶ&#34;:4448,&#34;Ġíĺķíİ¸&#34;:4449,&#34;ĠìĨĮë¦Ħëģ¼&#34;:4450,&#34;Ġê¹Įì§Ģ&#34;:4451,&#34;ad&#34;:4452,&#34;Ġë±&#34;:4453,&#34;Ġë¿&#34;:4454,&#34;ìĿ´ë©°&#34;:4455,&#34;íķĺëł¤&#34;:4456,&#34;ìĦ¯&#34;:4457,&#34;ĠìĿ´ë£¨&#34;:4458,&#34;ìĬ¤íĨł&#34;:4459,&#34;ĠìĹ°ìķł&#34;:4460,&#34;ëªħìĿĺ&#34;:4461,&#34;Ġìĭľê°ĦìĿĦ&#34;:4462,&#34;ìĬ¤íĨłë¦¬ëĬĶ&#34;:4463,&#34;íķĺê¸°ëıĦ&#34;:4464,&#34;ëįĶëĿ¼êµ¬ìļĶ&#34;:4465,&#34;ëıħêµĲ&#34;:4466,&#34;Ġì£Ħëĭ¤&#34;:4467,&#34;ĠíĻĶëł¤íķľ&#34;:4468,&#34;ëĭ¤ìĿĮ&#34;:4469,&#34;Ġìŀ¤&#34;:4470,&#34;ìĿĦê±°&#34;:4471,&#34;Ġê·¸ë¦½&#34;:4472,&#34;ë°ĳìĹĲ&#34;:4473,&#34;Ġíķĺë©´ìĦľ&#34;:4474,&#34;ĠìĹ°ìĺĪ&#34;:4475,&#34;ìĭ¤íĻĶ&#34;:4476,&#34;Ġë³´ê³łìĭ¶ëĭ¤&#34;:4477,&#34;Ġë°ĺìĦ±&#34;:4478,&#34;ĠìĿĺë¬¸&#34;:4479,&#34;Ġëª°ëĿ¼ëıĦ&#34;:4480,&#34;ĠíĬ¹ë³Ħ&#34;:4481,&#34;Ġì£¼ìĿ¸ê³µìĿĺ&#34;:4482,&#34;ë¶Īê°Ģ&#34;:4483,&#34;ĠãħħãħĤ&#34;:4484,&#34;re&#34;:4485,&#34;ĠìĭŃ&#34;:4486,&#34;Ġê·¸ì§Ģ&#34;:4487,&#34;ìĭĿìĿĦ&#34;:4488,&#34;ĠíĺĦìŀ¬&#34;:4489,&#34;íĶ¼ìĨĮ&#34;:4490,&#34;ì£¼ìĿ¸ê³µìĿ´&#34;:4491,&#34;ìŀĲì²´ê°Ģ&#34;:4492,&#34;Ġw&#34;:4493,&#34;ìĿ´ëĭĪ&#34;:4494,&#34;ê²Ģ&#34;:4495,&#34;ìķĦëĭĪëĭ¤&#34;:4496,&#34;ĠìłķìĦľ&#34;:4497,&#34;ĠìŀĪìĸ´ìļĶ&#34;:4498,&#34;ê·¸ìĿ¸&#34;:4499,&#34;Ġì§Ħìłķ&#34;:4500,&#34;Ġê°Ģê¹Į&#34;:4501,&#34;ĠìĬ¤íĬ¸&#34;:4502,&#34;ë¶Ģì¡±&#34;:4503,&#34;Ġìŀ¬ë°ĮëĬĶëį°&#34;:4504,&#34;Ġì¶ķ&#34;:4505,&#34;Ġì¡°íıŃ&#34;:4506,&#34;ĠìĤ¬ëŀĳìĹĲ&#34;:4507,&#34;Ġë¶Īêµ¬íķĺê³ł&#34;:4508,&#34;ì¹ľëĭ¤&#34;:4509,&#34;ĠìĦłìĥĿ&#34;:4510,&#34;Ġê³¼ìĹ°&#34;:4511,&#34;ĠíĢĦë¦¬íĭ°&#34;:4512,&#34;íķĺìĭł&#34;:4513,&#34;ê°ĢëĿ½&#34;:4514,&#34;ìĹĲë§Į&#34;:4515,&#34;Ġëĭ¬ëĿ¼&#34;:4516,&#34;ĠìĭľìĽĲ&#34;:4517,&#34;Ġë¬´ìĭľ&#34;:4518,&#34;ĠìĽ¬&#34;:4519,&#34;Ġë¯¸ìĬ¤&#34;:4520,&#34;ĠìĤ¬ëŀĳíķĺëĬĶ&#34;:4521,&#34;Ġì¢ĭìķĦìĦľ&#34;:4522,&#34;Ġë¶Ħëĵ¤&#34;:4523,&#34;Ġë°ĽìķĦ&#34;:4524,&#34;ĠìķħìĹŃ&#34;:4525,&#34;ì¤ĺìķ¼&#34;:4526,&#34;Ġê¹¨ëĭ«&#34;:4527,&#34;ê°ĢìĬ´ìĿ´&#34;:4528,&#34;Ġë¸ĶëŀĻ&#34;:4529,&#34;ë¿Ķ&#34;:4530,&#34;Ġìī½ê²Į&#34;:4531,&#34;Ġíķ´íĶ¼ìĹĶëĶ©&#34;:4532,&#34;ë§ĪìĬ¤&#34;:4533,&#34;ĠëĤĺìĺ´&#34;:4534,&#34;ì§Ħë¶Ģ&#34;:4535,&#34;ìĤ¬ìĿ´&#34;:4536,&#34;ĠëģĦ&#34;:4537,&#34;Ġíı¬ë&#34;:4538,&#34;ë§ĲìĿ´íķĦìļĶ&#34;:4539,&#34;êµ¬ë¡ľ&#34;:4540,&#34;Ġê²ģëĤĺ&#34;:4541,&#34;ë§Ļ&#34;:4542,&#34;Ġê°ľìĿ¸&#34;:4543,&#34;íİ¸ìĹĲ&#34;:4544,&#34;ë¦¬ëĦ¤&#34;:4545,&#34;ĠìĹ°ì¶ľëıĦ&#34;:4546,&#34;ë¸Ĳ&#34;:4547,&#34;ĠìĦ¤ëłĪ&#34;:4548,&#34;ĠëĤ®ìķĦìĦľ&#34;:4549,&#34;ĠìĹ´ìĭ¬íŀĪ&#34;:4550,&#34;ĠëĸłëĤĺìĦľ&#34;:4551,&#34;ĠOOOê¸°&#34;:4552,&#34;Ġê·¸ê±¸&#34;:4553,&#34;ìłĲì£¼&#34;:4554,&#34;Ġìĸ´ëł¸&#34;:4555,&#34;Ġìŀ¬ë°ĮìĹĪëĬĶëį°&#34;:4556,&#34;Ġìŀ¬ë°ĮìĹĪìĿĮ&#34;:4557,&#34;ĠìķĬìķĦ&#34;:4558,&#34;ĠëĤ¨ìĿĦ&#34;:4559,&#34;ĠìĿ´ëŁ°ê±¸&#34;:4560,&#34;Ġìķ¡ìħĺìĿ´&#34;:4561,&#34;ìł¸ìļĶ&#34;:4562,&#34;Ġê¸°ëĮĢë¥¼&#34;:4563,&#34;íķĻëħĦ&#34;:4564,&#34;ĠëĳĲê³ł&#34;:4565,&#34;ĠíĺĦìĭ¤ìĿĦ&#34;:4566,&#34;ê¿Ī&#34;:4567,&#34;Ġëľ»&#34;:4568,&#34;ìķĦê¹ĮìĽĮ&#34;:4569,&#34;Ġëĳĺëĭ¤&#34;:4570,&#34;Ġìľłì¾Įíķľ&#34;:4571,&#34;ìĸ´ëĸ¤&#34;:4572,&#34;¥´ëħ¸&#34;:4573,&#34;Ġêµ³ìĿ´&#34;:4574,&#34;Ġìħ&#34;:4575,&#34;ìĿ´ë¯¸&#34;:4576,&#34;ì§Ģë§ĪëĿ¼&#34;:4577,&#34;íķĺìħ¨&#34;:4578,&#34;ĠìĿ´ê²ĥëıĦ&#34;:4579,&#34;ĠìĿ´ìģľ&#34;:4580,&#34;ìĿ¸ëĵ¤&#34;:4581,&#34;ìĭľëĤĺ&#34;:4582,&#34;ëŀµ&#34;:4583,&#34;Ġì°¡&#34;:4584,&#34;ìħī&#34;:4585,&#34;Ġëª»íĸĪëĭ¤&#34;:4586,&#34;ĠìŀĲë§ī&#34;:4587,&#34;ë´¤ëįĺ&#34;:4588,&#34;íĿĶ&#34;:4589,&#34;Ġê²°ë¡ł&#34;:4590,&#34;ĠëĬĲëĤĢ&#34;:4591,&#34;Ġë°ľìłĦ&#34;:4592,&#34;ĠëĬĲëĤĮìĿĦ&#34;:4593,&#34;ì»¨&#34;:4594,&#34;Ġãħĭãħĭãħĭãħĭãħĭ&#34;:4595,&#34;Ġìį©&#34;:4596,&#34;CG&#34;:4597,&#34;íĵ¨&#34;:4598,&#34;íķľëĭ¤ê³ł&#34;:4599,&#34;ĠìĿ´ë»Ĳ&#34;:4600,&#34;ëł·&#34;:4601,&#34;...?&#34;:4602,&#34;ĠìķĦìĺ¤&#34;:4603,&#34;ĠëĤĺê°Ģ&#34;:4604,&#34;ìļ°ê³ł&#34;:4605,&#34;Ġìµľê·¼&#34;:4606,&#34;Ġë¬´ì§Ģ&#34;:4607,&#34;ë¬´ë¹Ħ&#34;:4608,&#34;Ġê°ĲëıĻìłģìĿ´ê³ł&#34;:4609,&#34;Ġê±°ì§ĵ&#34;:4610,&#34;ê²¨ìļ´&#34;:4611,&#34;ĠìĿ´íķ´íķł&#34;:4612,&#34;ìŀ¡íķľ&#34;:4613,&#34;ë¶Īìĸ´&#34;:4614,&#34;ëįĶëĿ¼ëıĦ&#34;:4615,&#34;Ġê²°ë§ĲëıĦ&#34;:4616,&#34;ĠìĺģíĻĶìĹĲìļĶ&#34;:4617,&#34;ëŀįëĭĪëĭ¤&#34;:4618,&#34;ĠìĬ¤ì¼ĢìĿ¼&#34;:4619,&#34;ê°Ģì§Ģê³ł&#34;:4620,&#34;ëĤ«&#34;:4621,&#34;ëĤ¼&#34;:4622,&#34;ìķĦìī½&#34;:4623,&#34;Ġìĭ¬ë¦¬&#34;:4624,&#34;Ġê·¸ëŀ¬&#34;:4625,&#34;ìŀĳìĿ´&#34;:4626,&#34;ì°®&#34;:4627,&#34;íĸĪëĬĶì§Ģ&#34;:4628,&#34;ìĭłìĿĢ&#34;:4629,&#34;ê²łìĿĮ&#34;:4630,&#34;Ġë§ĪìĦ¸ìļĶ&#34;:4631,&#34;ĠëĲĺì§Ģ&#34;:4632,&#34;Ġê°Ĳëªħ&#34;:4633,&#34;íĮĲìĹĲ&#34;:4634,&#34;ĠëıĻìĥĿ&#34;:4635,&#34;Ġë³Ħë¡ľëĭ¤&#34;:4636,&#34;ìĤ¬ëŀĮëĵ¤ìĿ´&#34;:4637,&#34;ì§Ģë£¨íķĺëĭ¤&#34;:4638,&#34;ĠíĴĭ&#34;:4639,&#34;Ġê¹ĬìĿ´&#34;:4640,&#34;ì¤Įë§Ī&#34;:4641,&#34;is&#34;:4642,&#34;íķĺëĿ¼&#34;:4643,&#34;íķĺëł¤ê³ł&#34;:4644,&#34;ĠìĹĳ&#34;:4645,&#34;ĠìĿ´ìĺģíĻĶê°Ģ&#34;:4646,&#34;ë§Įëĵľ&#34;:4647,&#34;ìĹĪëĬĶì§Ģ&#34;:4648,&#34;ê²łëĦ¤ìļĶ&#34;:4649,&#34;Ġë¹Ħì£¼ìĸ¼&#34;:4650,&#34;ëĭ¨ìĪľ&#34;:4651,&#34;Ġë°°ìļ°ìĿĺ&#34;:4652,&#34;Ġì¢ĭìķĦíķł&#34;:4653,&#34;íıīìłĲìĹĲ&#34;:4654,&#34;¬ë¦¼&#34;:4655,&#34;Ġìĭłê²½&#34;:4656,&#34;ĠëķĮë¬¸&#34;:4657,&#34;Ġë³¼ë§Įíķ¨&#34;:4658,&#34;ìĵ°ëłĪê¸°ìĺģíĻĶ&#34;:4659,&#34;ì¢ĭìĿĢìĺģíĻĶ&#34;:4660,&#34;ĠìĬ¬íĶĦê³ł&#34;:4661,&#34;ë±&#34;:4662,&#34;Ġìĩ¼&#34;:4663,&#34;Ġëĭ¬ë¦¬&#34;:4664,&#34;Ġìĥ¤&#34;:4665,&#34;Ġê²ĮìĿ´&#34;:4666,&#34;ĠìŀĪìĸ´ìķ¼&#34;:4667,&#34;ê³¼ìĿĺ&#34;:4668,&#34;Ġíķľëį°&#34;:4669,&#34;Ġì¤Į&#34;:4670,&#34;íİ¸ëıĦ&#34;:4671,&#34;Ġë§İëĭ¤&#34;:4672,&#34;ĠìłĢìĺĪìĤ°&#34;:4673,&#34;ë´ĲìļĶ&#34;:4674,&#34;ì¢ĭìķĦíķĺëĬĶ&#34;:4675,&#34;Ġíıīë²Ķíķľ&#34;:4676,&#34;. &#34; &#34; &#34;&#34;:4677,&#34;¦ê²Į&#34;:4678,&#34;ìĿ´ìħĺ&#34;:4679,&#34;ìķ¤&#34;:4680,&#34;ì§ĢëĤĺ&#34;:4681,&#34;ĠìŀĪëĦ¤ìļĶ&#34;:4682,&#34;Ġíĳ¹&#34;:4683,&#34;ĠìĹĨëĬĶëį°&#34;:4684,&#34;ìķĺê³ł&#34;:4685,&#34;ì¤ĳë°ĺ&#34;:4686,&#34;ĠëįĶìĿ´ìĥģ&#34;:4687,&#34;íķĺëĭ¤ëĭĪ&#34;:4688,&#34;ĠìĽĢ&#34;:4689,&#34;ĠëĤ¨ì£¼&#34;:4690,&#34;ĠìĿ¼ìĥģ&#34;:4691,&#34;ĠëĲĺê²Į&#34;:4692,&#34;ì²ĺêµ¬ëĭĪ&#34;:4693,&#34;ĠíŀĲë§ģ&#34;:4694,&#34;007&#34;:4695,&#34;ëŃĲëĥĲ&#34;:4696,&#34;Ġê·¸ëłĩëĭ¤ê³ł&#34;:4697,&#34;Ġìķħëĭ¹&#34;:4698,&#34;Ġìī¬&#34;:4699,&#34;ë§ĪìĿĮìĿ´&#34;:4700,&#34;Ġ&lt;&#34;:4701,&#34;Ġíĩ´&#34;:4702,&#34;ĠìĹĨëĭ¤ëĬĶ&#34;:4703,&#34;ĠëĤĺì¤ĳìĹĲ&#34;:4704,&#34;íĸĪëĦ¤&#34;:4705,&#34;ĠìĹ°ê²°&#34;:4706,&#34;Ġë³¼ìĪĺë¡Ŀ&#34;:4707,&#34;Ġë³¼ëķĮë§Īëĭ¤&#34;:4708,&#34;ê³ĦìĿĺ&#34;:4709,&#34;Ġìŀ¬ë¯¸ìŀĪìĿĮ&#34;:4710,&#34;Ġìĭ¶ëĦ¤ìļĶ&#34;:4711,&#34;íĽĮë¥Ń&#34;:4712,&#34;ë©ĭì§Ħ&#34;:4713,&#34;ìķĦê¹ĮìĽĢ&#34;:4714,&#34;ëĤĺìĻĢìĦľ&#34;:4715,&#34;Bê¸ī&#34;:4716,&#34;at&#34;:4717,&#34;ŀĳ&#34;:4718,&#34;Ġëº&#34;:4719,&#34;ëĬĶìĺģíĻĶ&#34;:4720,&#34;¬ë°&#34;:4721,&#34;Ġì§ķ&#34;:4722,&#34;ìľ¼ë¡ľìį¨&#34;:4723,&#34;íķ¨ìĹĲ&#34;:4724,&#34;ìĤ¬ìĿĺ&#34;:4725,&#34;ĠìłĦíĺķìłģìĿ¸&#34;:4726,&#34;Ġë§Ĳíķł&#34;:4727,&#34;ĠëĿ¼ìĿ´&#34;:4728,&#34;Ġëª©ìĨĮ&#34;:4729,&#34;ê½¤&#34;:4730,&#34;Īëł&#34;:4731,&#34;ë¡ľëıĦ&#34;:4732,&#34;ìļ°ìĻĢ&#34;:4733,&#34;Ġìĭľê°ģ&#34;:4734,&#34;Ġì°Ŀ&#34;:4735,&#34;ĠìĥĿê°ģíķĺê³ł&#34;:4736,&#34;ë¬´ì§Ģ&#34;:4737,&#34;ìŀ¬ë¯¸ê°Ģ&#34;:4738,&#34;ĠìĤ¬ëŀĮìĿĺ&#34;:4739,&#34;ëª¨ëĳĲ&#34;:4740,&#34;ĠíķĺëĬĶì§Ģ&#34;:4741,&#34;ìĺ¨ëĭ¤&#34;:4742,&#34;íĭ°ë¸Į&#34;:4743,&#34;ĠìĬ¬íĶĶ&#34;:4744,&#34;ĠìĤ°ìľ¼ë¡ľ&#34;:4745,&#34;Ġë°ĶëĢĮ&#34;:4746,&#34;Ġíİĳíİĳ&#34;:4747,&#34;ì§Ħìłķíķľ&#34;:4748,&#34;ìłĦì²´&#34;:4749,&#34;ìĦ±ìļ°&#34;:4750,&#34;ĠëĤ¨ëħĢ&#34;:4751,&#34;íĿ¡&#34;:4752,&#34;ê´ĢëŀĮ&#34;:4753,&#34;ĠìĥĿìĥĿ&#34;:4754,&#34;Ġë°ĺìłĦëıĦ&#34;:4755,&#34;ìķĦìĿ´ëĵ¤ìĿ´&#34;:4756,&#34;¬ëįĺ&#34;:4757,&#34;BS&#34;:4758,&#34;ĠëĩĮ&#34;:4759,&#34;ê²ĮìŀĦ&#34;:4760,&#34;Ġë³´ìķĺëĭ¤&#34;:4761,&#34;ë°¤&#34;:4762,&#34;Ġì¢ĭìĬµëĭĪëĭ¤&#34;:4763,&#34;Ġëª»íķł&#34;:4764,&#34;íļį&#34;:4765,&#34;ëŀĢëĭ¤&#34;:4766,&#34;Ġìķ¡ìħĺìĿĢ&#34;:4767,&#34;Ġë§Įëĵłëĭ¤&#34;:4768,&#34;Ġì§±ì§±&#34;:4769,&#34;Ġìŀ¬ëĤľ&#34;:4770,&#34;ĠìĿĮìķħëıĦ&#34;:4771,&#34;Ġì©Į&#34;:4772,&#34;Ġê´ĳê³ł&#34;:4773,&#34;ĠT&#34;:4774,&#34;ì§Ģë§Ĳ&#34;:4775,&#34;íķĺêµ¬&#34;:4776,&#34;ëį©&#34;:4777,&#34;ëĤĺë§Į&#34;:4778,&#34;ê±°ë©´&#34;:4779,&#34;ëĮĢë¥¼&#34;:4780,&#34;ĠìŀĪìľ¼ë©´&#34;:4781,&#34;ê·¸ë¦¬&#34;:4782,&#34;ìĦ±ìĿĺ&#34;:4783,&#34;ĠëĮĢíĳľ&#34;:4784,&#34;ĠìķĬì§Ģë§Į&#34;:4785,&#34;ĠíĿ¡&#34;:4786,&#34;ìĪŃ&#34;:4787,&#34;ĠëĨį&#34;:4788,&#34;ëŁ½ê³ł&#34;:4789,&#34;Ġê°ķëł¬&#34;:4790,&#34;ì§ĳëĭĪëĭ¤&#34;:4791,&#34;ìĶ¬ìĿĢ&#34;:4792,&#34;Ġì¡´ê²½&#34;:4793,&#34;ĠëĪĪë¬¼ìĿĦ&#34;:4794,&#34;ĠìĿ´ìłľìķ¼&#34;:4795,&#34;Ġë¹¼ê³ł&#34;:4796,&#34;âĻ¡âĻ¡&#34;:4797,&#34;ìĬ¤íĥĢìĿ¼&#34;:4798,&#34;ìħĶìĦľ&#34;:4799,&#34;ë´£ëĬĶëį°&#34;:4800,&#34;Ġë´£ëĬĶëį°&#34;:4801,&#34;ĠA&#34;:4802,&#34;Ġìª½&#34;:4803,&#34;ĠìĺģíĻĶëĿ¼ëĬĶ&#34;:4804,&#34;ë¡ľìĦľ&#34;:4805,&#34;ĠìķĦì¹¨&#34;:4806,&#34;íģ¬ë¦¬ìĬ¤&#34;:4807,&#34;Ġì²´&#34;:4808,&#34;ĠìĤ´ê³ł&#34;:4809,&#34;ĠìĺĪìłĦ&#34;:4810,&#34;Ġìĭ¬ê°ģ&#34;:4811,&#34;ĠìĬ¬íİ&#34;:4812,&#34;ĠëĨĴëĭ¤&#34;:4813,&#34;ĠìĥĿìķł&#34;:4814,&#34;ìĸ¼êµ´&#34;:4815,&#34;Ġì»¨&#34;:4816,&#34;ì§Ģê¸Īê¹Įì§Ģ&#34;:4817,&#34;Ġì¢ħêµĲ&#34;:4818,&#34;ĠìĿ´ìģĺê³ł&#34;:4819,&#34;ĠìĮ&#34;:4820,&#34;ìĿ´ê¸°&#34;:4821,&#34;ìĿ¸íĬ¸&#34;:4822,&#34;ì£¼ìłľ&#34;:4823,&#34;íŀĲ&#34;:4824,&#34;ë¬´ìĹĩ&#34;:4825,&#34;ëıĻìĥĿ&#34;:4826,&#34;Ġì¢ĢëįĶ&#34;:4827,&#34;Ġìķ¡ìħĺìĺģíĻĶ&#34;:4828,&#34;ĠëıĻë¬¼&#34;:4829,&#34;ar&#34;:4830,&#34;ëĭ¬ëĿ¼&#34;:4831,&#34;íķµ&#34;:4832,&#34;ê°ĸ&#34;:4833,&#34;ìķķ&#34;:4834,&#34;ì§Ģìĸ´&#34;:4835,&#34;ĠìķĦëł¨&#34;:4836,&#34;ìľ¼ëĭĪê¹Į&#34;:4837,&#34;ê±°ìŀĦ&#34;:4838,&#34;ë²¨&#34;:4839,&#34;ĠìĬ¤íı¬&#34;:4840,&#34;ë¯¸ê°Ģ&#34;:4841,&#34;ĠìĹ°ê¸°ìŀĲ&#34;:4842,&#34;Ġìŀ¬ë°ĮìĬµëĭĪëĭ¤&#34;:4843,&#34;Ġë¬´íĺĳ&#34;:4844,&#34;ë¶ĦìľĦ&#34;:4845,&#34;íģ¬ë¦°&#34;:4846,&#34;ĠìĿ¼ë¶Ģ&#34;:4847,&#34;ìµľê·¼&#34;:4848,&#34;ìĹŃëĮĢ&#34;:4849,&#34;ìĿ´ëŁ°ìĺģíĻĶ&#34;:4850,&#34;Ġê·¹ì¹ĺ&#34;:4851,&#34;íĺ¸ê°Ģ&#34;:4852,&#34;Ġëĭ¨ìĹ°&#34;:4853,&#34;ìĦĿìĿ´&#34;:4854,&#34;Ġëªĩë²Ī&#34;:4855,&#34;Ġê½Ŀ&#34;:4856,&#34;Ġì§Ħìĭ¬ìľ¼ë¡ľ&#34;:4857,&#34;ĠìłĲìĪĺë¥¼&#34;:4858,&#34;ĠëĽ°ìĸ´ëĤľ&#34;:4859,&#34;Ġíĥľìĸ´&#34;:4860,&#34;ê½ĥ&#34;:4861,&#34;Ġìĵ¸ëį°&#34;:4862,&#34;Ġë½ĳ&#34;:4863,&#34;Ġë§Īë¬´ë¦¬&#34;:4864,&#34;ìĿ´ìłķëıĦ&#34;:4865,&#34;ĠìĺģíĻĶìĿ¸ëĵ¯&#34;:4866,&#34;Ġëĭ®&#34;:4867,&#34;ĠìĹ°ìĨį&#34;:4868,&#34;Ġì§Ģê¸Īë´ĲëıĦ&#34;:4869,&#34;ê·¸ëłĩê²Į&#34;:4870,&#34;Ġìĭľë¦¬&#34;:4871,&#34;ìķĺì§Ģë§Į&#34;:4872,&#34;Ġê°Ģìł¸&#34;:4873,&#34;ĠìľĮ&#34;:4874,&#34;Ġë¬´ë¹Ħ&#34;:4875,&#34;ĠëĤ¨ê²¨&#34;:4876,&#34;íĬ¸ë§¨&#34;:4877,&#34;Ġì§Ģë£¨íķ´ìĦľ&#34;:4878,&#34;ëĤłëķĮ&#34;:4879,&#34;ì©¡&#34;:4880,&#34;ĠíĳľìłĪ&#34;:4881,&#34;ê¼´&#34;:4882,&#34;ìĸĳìĿ´&#34;:4883,&#34;ĠìĿ¸ìĥĿìĿĦ&#34;:4884,&#34;ĠìķĦìī¬ìĽĢ&#34;:4885,&#34;Ġìĭ¸êµ¬ëł¤&#34;:4886,&#34;ì¡¸ìŀĳ&#34;:4887,&#34;ê³µíı¬ìĺģíĻĶ&#34;:4888,&#34;ìıĺ&#34;:4889,&#34;ver&#34;:4890,&#34;ìĿ´ìĺģíĻĶë¥¼&#34;:4891,&#34;ëĵ£&#34;:4892,&#34;ìĹĲëĮĢíķľ&#34;:4893,&#34;ëĤĺëĦ¤&#34;:4894,&#34;Ġë³´ëĭĪê¹Į&#34;:4895,&#34;ë§ĲìĿĦ&#34;:4896,&#34;ìĻĵ&#34;:4897,&#34;Ġê²ī&#34;:4898,&#34;íĮ©&#34;:4899,&#34;ìĨĮëħĢ&#34;:4900,&#34;Ġê°ĻìķĦìĦľ&#34;:4901,&#34;Ġë§ĲíķĺëĬĶ&#34;:4902,&#34;Ġë³¸ëĭ¤ë©´&#34;:4903,&#34;ĠìĨĮìŀ¥&#34;:4904,&#34;Ġë¯¸ì¹ĺ&#34;:4905,&#34;ë¦¬ë©°&#34;:4906,&#34;Ġê³łìłĦ&#34;:4907,&#34;Ġê³łíĨµ&#34;:4908,&#34;Ġê±°ê¸°&#34;:4909,&#34;ĠìŀĳíĴĪìĿ´ëĭ¤&#34;:4910,&#34;ìĶ¨ìĿĺ&#34;:4911,&#34;ĠìĨįìĹĲ&#34;:4912,&#34;ĠìĿĮìķħìĿ´&#34;:4913,&#34;Ġëĸłìĺ¤&#34;:4914,&#34;ĠìĤ¶ìĿĺ&#34;:4915,&#34;ĠìĶģìĵ¸&#34;:4916,&#34;~~~~~~~~&#34;:4917,&#34;ĠìĿĺë¯¸ë¥¼&#34;:4918,&#34;it&#34;:4919,&#34;Ġ^&#34;:4920,&#34;Ġgood&#34;:4921,&#34;ê°Ĵ&#34;:4922,&#34;ë§ĮìĿĦ&#34;:4923,&#34;ëĦ·&#34;:4924,&#34;ĠìĦŀ&#34;:4925,&#34;ìŀ¬ë¯¸ëĬĶ&#34;:4926,&#34;ĠìĤ¬ëŀĳê³¼&#34;:4927,&#34;ĠìķĦê¹Ŀì§Ģ&#34;:4928,&#34;Ġë©ĭì§Ģëĭ¤&#34;:4929,&#34;ê°ĢëĬĶì¤Ħ&#34;:4930,&#34;Ġì§Ģê¸ĪìĿĢ&#34;:4931,&#34;ëĲ¬&#34;:4932,&#34;ĠìķĦìĿ´ëĵ¤ìĿ´&#34;:4933,&#34;ìĬ¤ëŁ¬ìĽĢ&#34;:4934,&#34;ĠíĳľíĺĦíķľ&#34;:4935,&#34;ĠìĺģíĻĶìĺĢìĸ´ìļĶ&#34;:4936,&#34;ìĻĦë²½&#34;:4937,&#34;Ġëĵ±ìŀ¥ìĿ¸ë¬¼&#34;:4938,&#34;ing&#34;:4939,&#34;Ġì¥Ĳ&#34;:4940,&#34;ĠìĬĪíį¼&#34;:4941,&#34;ll&#34;:4942,&#34;ħ¸&#34;:4943,&#34;ê°Ģë¥¼&#34;:4944,&#34;ĠëĤ´ëł¤&#34;:4945,&#34;ê²łìĸ´ìļĶ&#34;:4946,&#34;ĠìķĪëı¼&#34;:4947,&#34;Ġë³¸ìĺģíĻĶ&#34;:4948,&#34;Ġë¹¡&#34;:4949,&#34;ĠëģĿëĤľ&#34;:4950,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪëĭ¤&#34;:4951,&#34;Ġë°ĺë³µ&#34;:4952,&#34;ĠìŀĳíĴĪìĿĦ&#34;:4953,&#34;ê´ĢìĿ´&#34;:4954,&#34;Ġìŀ¥ë©´ìĹĲìĦľ&#34;:4955,&#34;ê°ĲëıħìĿĢ&#34;:4956,&#34;ë³Ħë£¨&#34;:4957,&#34;ĠìĹĲíĶ¼ìĨĮ&#34;:4958,&#34;ìĥĿê°ģìĹĨìĿ´&#34;:4959,&#34;ĠìķĦìĿ´ëĵ¤&#34;:4960,&#34;Ġëª¨ëĵłê²Į&#34;:4961,&#34;Ġì¹ľêµ¬ëĵ¤&#34;:4962,&#34;Ġìķķê¶Į&#34;:4963,&#34;ì¥Ĳ&#34;:4964,&#34;Ġìĸ´ëł¤ìļ´&#34;:4965,&#34;ie&#34;:4966,&#34;¨¸&#34;:4967,&#34;ĠìĮį&#34;:4968,&#34;ĠìĺģíĻĶëĵ¤&#34;:4969,&#34;ê±°ê°Ļëĭ¤&#34;:4970,&#34;ìĨĶ&#34;:4971,&#34;Ġì§Ģì¼ľ&#34;:4972,&#34;ĠëįĶë¶Īìĸ´&#34;:4973,&#34;Ġë³¼ê²Į&#34;:4974,&#34;ìĺģìĿ´&#34;:4975,&#34;íĿ¬ë&#34;:4976,&#34;Ġê±°ìĬ&#34;:4977,&#34;Ġëĭ¤ìĭľíķľë²Ī&#34;:4978,&#34;ìĭľê°ĦìĹĲ&#34;:4979,&#34;Ġìļ¸ê³ł&#34;:4980,&#34;Ġìļ¸ìĹĪëĭ¤&#34;:4981,&#34;ĠìĻ¸ëª¨&#34;:4982,&#34;Ġë¨¹ê³ł&#34;:4983,&#34;Ġê²½íĹĺ&#34;:4984,&#34;ĠêµŃë¯¼&#34;:4985,&#34;Ġìĸ¸ìłľëĤĺ&#34;:4986,&#34;¬ëįĶ&#34;:4987,&#34;ĠëĬ¥ëł¥&#34;:4988,&#34;ĠëıħíĬ¹íķľ&#34;:4989,&#34;Ġê²Ģìĥī&#34;:4990,&#34;44&#34;:4991,&#34;bs&#34;:4992,&#34;ë¤&#34;:4993,&#34;ìĿ´ìĺģíĻĶëĬĶ&#34;:4994,&#34;ëĭ¤ëĵ¤&#34;:4995,&#34;íķĺì§Ħ&#34;:4996,&#34;ĠìŀĪëįĺ&#34;:4997,&#34;ëĤĺëĭ¤&#34;:4998,&#34;ìķĦë¹ł&#34;:4999,&#34;ìĥģíĻ©&#34;:5000,&#34;ĠëĮĢë¶Ģë¶Ħ&#34;:5001,&#34;ĠìłĦíĪ¬&#34;:5002,&#34;ê°ľê·¸&#34;:5003,&#34;ê°ĲëıĦ&#34;:5004,&#34;ĠìĿ´ëŁ°ê²Į&#34;:5005,&#34;ĠíĿī&#34;:5006,&#34;ìĽĲëŀĺ&#34;:5007,&#34;ëĲĺìĦľ&#34;:5008,&#34;Ġì¡°íķ©&#34;:5009,&#34;ìĹ°ê¸°ëĬĶ&#34;:5010,&#34;Ġë°ľê²¬&#34;:5011,&#34;Ġãħłãħłãħł&#34;:5012,&#34;Ġë§Ŀì³Ĳ&#34;:5013,&#34;ĠìĬ¤ë¦´ëŁ¬ë&#34;:5014,&#34;íĺ¼ìŀĲ&#34;:5015,&#34;Ġëª¨ë¥´ê²łê³ł&#34;:5016,&#34;ĠíıŃëł¥&#34;:5017,&#34;ĠëģĮìĸ´&#34;:5018,&#34;Ġë°Ķê¿Ķ&#34;:5019,&#34;Ġì°Ŀì°Ŀ&#34;:5020,&#34;ĠìĿµ&#34;:5021,&#34;ìļ°ê¸°&#34;:5022,&#34;Ġìŀ¬ë¯¸ìŀĪëĦ¤ìļĶ&#34;:5023,&#34;Ġíķľì°¸&#34;:5024,&#34;ĠìĬ¤ìĬ¤ë¡ľ&#34;:5025,&#34;ë¯¸ìķĪ&#34;:5026,&#34;ìĨĮë¦Ħ&#34;:5027,&#34;ìĺĢì§Ģë§Į&#34;:5028,&#34;Ġê°ĲëıĻê³¼&#34;:5029,&#34;Ġë¯¸ëª¨&#34;:5030,&#34;Ġì§Ģë£¨íķł&#34;:5031,&#34;ê¸´ëĭ¤&#34;:5032,&#34;ĠìĿĺìĭ¬&#34;:5033,&#34;ĠìĬ¬íį¼&#34;:5034,&#34;ĠëĨĴìķĦ&#34;:5035,&#34;Ġê²Įëĭ¤ê°Ģ&#34;:5036,&#34;ĠìķĦìī½ëĦ¤ìļĶ&#34;:5037,&#34;ĠëĬĲëģ¼ëĬĶ&#34;:5038,&#34;âĻ¥âĻ¥âĻ¥âĻ¥&#34;:5039,&#34;Ġë¹łìł¸ëĵ¤&#34;:5040,&#34;ĠìĦ¬ìĦ¸&#34;:5041,&#34;ìĸ´ë¥¸&#34;:5042,&#34;ìłĦìŀĳ&#34;:5043,&#34;ìĤ¬ê±´&#34;:5044,&#34;ĠìĻľì¼Ģ&#34;:5045,&#34;ë²Ħë¦¬ê³ł&#34;:5046,&#34;ĠìĿ¼ë°ĺ&#34;:5047,&#34;ìĺģíĻĺ&#34;:5048,&#34;ĠíĹĲ&#34;:5049,&#34;íĿĺ&#34;:5050,&#34;Ġì²ĺëŁ¼&#34;:5051,&#34;ĠíĺĦëĮĢ&#34;:5052,&#34;ë°©ìĤ¬&#34;:5053,&#34;Ġë°ķìĪĺë¥¼&#34;:5054,&#34;ĠìĻ¸ê³Ħ&#34;:5055,&#34;ĠìĽĥê¸°ê³ł&#34;:5056,&#34;Ġë¹¼ê³łëĬĶ&#34;:5057,&#34;ĠëĶ°ëľ»íķ´ì§ĢëĬĶ&#34;:5058,&#34;Ġëĭ¤íģĲë©ĺíĦ°&#34;:5059,&#34;ìŀ¬ë¯¸ìĹĨìĸ´&#34;:5060,&#34;ì³¤ëĭ¤&#34;:5061,&#34;ĠìĿ´íĽĦë¡ľ&#34;:5062,&#34;ĠíĴĭíĴĭ&#34;:5063,&#34;ë®¤&#34;:5064,&#34;íĻĶëł¤&#34;:5065,&#34;íķĺê²łëĭ¤&#34;:5066,&#34;ĠìķĦë¥ĺ&#34;:5067,&#34;ê±°ëĿ¼&#34;:5068,&#34;ìĹĪìĿĦê¹Į&#34;:5069,&#34;Ġëĭ¤ë£¬&#34;:5070,&#34;Ġê²¬&#34;:5071,&#34;ëĵľìĿĺ&#34;:5072,&#34;ìĦ±ì¹ĺ&#34;:5073,&#34;ê²ĥë§Į&#34;:5074,&#34;ĠìĿ¸íĦ°&#34;:5075,&#34;ìľłì¾Į&#34;:5076,&#34;Ġíķłê¹Į&#34;:5077,&#34;ĠëıĻìķĪ&#34;:5078,&#34;ĠëĤľíķ´&#34;:5079,&#34;ë§¤ìļ°&#34;:5080,&#34;íķĺëĤĺëıĦ&#34;:5081,&#34;ĠëĲ¨&#34;:5082,&#34;ĠìĿ¸ìĥĿìĿĺ&#34;:5083,&#34;Ġ40&#34;:5084,&#34;ìłĦê°ľê°Ģ&#34;:5085,&#34;Ġë¯¼ë§Ŀ&#34;:5086,&#34;Ġëĺĳê°Ļ&#34;:5087,&#34;ìĸ´ìĦ¤íĶĪ&#34;:5088,&#34;ĠëŃīíģ´&#34;:5089,&#34;íĮĢ&#34;:5090,&#34;ë¡Ģ&#34;:5091,&#34;Ġìĭ±&#34;:5092,&#34;ìłķìĿĢ&#34;:5093,&#34;íĸĪëĦ¤ìļĶ&#34;:5094,&#34;ëĤ´ìĥĿ&#34;:5095,&#34;ê·¸ë§Į&#34;:5096,&#34;ëĥĪ&#34;:5097,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:5098,&#34;Ġë§Ĳë¡ľ&#34;:5099,&#34;Ġëª»íķ´&#34;:5100,&#34;Ġìłľë¡ľ&#34;:5101,&#34;ĠëģĶ&#34;:5102,&#34;ë²ĦíĬ¸&#34;:5103,&#34;ë¬¸ìłľ&#34;:5104,&#34;ìŀ¬ë°Įìĸ´&#34;:5105,&#34;ĠìĤ¬ëŀĳìĿ´ìķ¼ê¸°&#34;:5106,&#34;íĶĦëŀĳìĬ¤&#34;:5107,&#34;Ġì¢ĭìķĺìĿĦ&#34;:5108,&#34;ìĪľìĪĺ&#34;:5109,&#34;ĠëĪĪìĹĲ&#34;:5110,&#34;ì³ĲìĦľ&#34;:5111,&#34;ĠíĭĢìĸ´&#34;:5112,&#34;ĠìķĦì§ģê¹Įì§Ģ&#34;:5113,&#34;Ġì¤ĳê°Ħì¤ĳê°Ħ&#34;:5114,&#34;ĠìĦ¸ìĥģìĹĲ&#34;:5115,&#34;ìĭľíĤ¤ëĬĶ&#34;:5116,&#34;ì«&#34;:5117,&#34;ìĿ¸ëĵ¤ìĿ´&#34;:5118,&#34;êµ¬ëŀĳ&#34;:5119,&#34;Ġê·¸ëŁ¬ë&#34;:5120,&#34;Ġê·¸ëŁŃìłĢëŁŃ&#34;:5121,&#34;ì¹¸&#34;:5122,&#34;ì¹¼&#34;:5123,&#34;Ġì¢ĭì§Ģë§Į&#34;:5124,&#34;Ġìĺ¬ëł¤&#34;:5125,&#34;Ġì£¼ë³Ģ&#34;:5126,&#34;ë¹Į&#34;:5127,&#34;Ġê³µë¶Ģ&#34;:5128,&#34;ëĸł&#34;:5129,&#34;ë¨¸ë¦¬&#34;:5130,&#34;ëľ©&#34;:5131,&#34;ĠìĨįíİ¸&#34;:5132,&#34;Ġê°Īëĵ±&#34;:5133,&#34;Ġë¯¿ìĿĦ&#34;:5134,&#34;Ġë¶ģíķľ&#34;:5135,&#34;Ġë»&#34;:5136,&#34;íķŃ&#34;:5137,&#34;ê°ĵ&#34;:5138,&#34;ìĿ´ëŁ¬&#34;:5139,&#34;ìł¼&#34;:5140,&#34;ë¦¬ê¸°&#34;:5141,&#34;ìľĦë¡ľ&#34;:5142,&#34;Ġë³´ê¸¸&#34;:5143,&#34;Ġë°ĭ&#34;:5144,&#34;ìŀ¥ìĹĲìĦľ&#34;:5145,&#34;Ġë´¤ëįĶëĭĪ&#34;:5146,&#34;ê²ĥëĵ¤&#34;:5147,&#34;íĮį&#34;:5148,&#34;íİĻ&#34;:5149,&#34;ê°ľìĹ°&#34;:5150,&#34;ëįĶëŁ½ê²Į&#34;:5151,&#34;Ġë§İìķĦ&#34;:5152,&#34;Ġë°°ìļ°ëĵ¤ëıĦ&#34;:5153,&#34;Ġì¢ĭìķĦíķĺëĬĶëį°&#34;:5154,&#34;ìĭĿìĿĺ&#34;:5155,&#34;íĤ¤ëĬĶ&#34;:5156,&#34;íħĮìĿ¼&#34;:5157,&#34;Ġê²°ë§ĲìĿĢ&#34;:5158,&#34;ĠëĴ¤ë¡ľ&#34;:5159,&#34;ì¯§&#34;:5160,&#34;ãħłãħłãħłãħłãħłãħłãħłãħł&#34;:5161,&#34;Ġì¦Ĳê²ģê²Į&#34;:5162,&#34;ãĢ&#34;:5163,&#34;ĠìķĪë¬´&#34;:5164,&#34;ëĤĺëĦ¤ìļĶ&#34;:5165,&#34;ĠëĤ³&#34;:5166,&#34;ìŀĲëĤĺ&#34;:5167,&#34;ĠìĭľíĬ¸ì½¤&#34;:5168,&#34;Ġìłķë§Ĳë¡ľ&#34;:5169,&#34;ìĭłìĿĦ&#34;:5170,&#34;ĠìķĬëĤĺ&#34;:5171,&#34;Ġë¹ĦíĺĦìĭ¤&#34;:5172,&#34;ìŀ¬ë°Įê³ł&#34;:5173,&#34;ĠëĤĺìĺ¤ë©´&#34;:5174,&#34;Ġëĵ¤ìĹĪëĭ¤&#34;:5175,&#34;ê°ĲëıĻìłģìĿ´&#34;:5176,&#34;ĠíĺĦìĭ¤ìłģìĿ¸&#34;:5177,&#34;ì§Ģë£¨íķľ&#34;:5178,&#34;ĠìķĪë³´ê³ł&#34;:5179,&#34;!!!!!!!!!!!!!!!!&#34;:5180,&#34;Ńī&#34;:5181,&#34;ê°±&#34;:5182,&#34;ìĦ¹&#34;:5183,&#34;ìĸ´ìłľ&#34;:5184,&#34;ëĭĪê¹Ĳ&#34;:5185,&#34;Ġëª½&#34;:5186,&#34;ìłĦíĺķìłģìĿ¸&#34;:5187,&#34;ìĻĢìļ°&#34;:5188,&#34;ĠìĥĿê°ģíķĺëĬĶ&#34;:5189,&#34;ê¸´íķľëį°&#34;:5190,&#34;Ġëª°ìŀħíķ´ìĦľ&#34;:5191,&#34;Ġë°©ë²ķ&#34;:5192,&#34;Ġë»Ķíķĺê³ł&#34;:5193,&#34;Ġë¬¸íĻĶ&#34;:5194,&#34;Ġìĥģìĥģëł¥&#34;:5195,&#34;ĠëĬĺìĸ´&#34;:5196,&#34;13&#34;:5197,&#34;ov&#34;:5198,&#34;Ġf&#34;:5199,&#34;ĠìĹĺ&#34;:5200,&#34;êµ¬ëĿ¼&#34;:5201,&#34;Ġì¢Į&#34;:5202,&#34;ĠìĹĨìĹĪëįĺ&#34;:5203,&#34;ì¹Ļ&#34;:5204,&#34;ìłĦëĵľ&#34;:5205,&#34;ê·¸ëĮĢë¡ľ&#34;:5206,&#34;Ġì¢ĭìķĺì§Ģë§Į&#34;:5207,&#34;ĠìĺĪìłĦìĹĲ&#34;:5208,&#34;ĠìĺĪê³łíİ¸&#34;:5209,&#34;Ġìļ°ë¦¬ê°Ģ&#34;:5210,&#34;Ġëª¨ë¥´ê²łì§Ģë§Į&#34;:5211,&#34;ìĤ¼ë¥ĺ&#34;:5212,&#34;Ġíŀĺëĵ¤ëĭ¤&#34;:5213,&#34;ê°Ķëĭ¤&#34;:5214,&#34;ĠìķĶíĬ¼&#34;:5215,&#34;Ġì±ĦëĦĲ&#34;:5216,&#34;Ġë°ĭë°ĭ&#34;:5217,&#34;Ġ!!!&#34;:5218,&#34;ĠìĹħ&#34;:5219,&#34;ìķĦìī¬&#34;:5220,&#34;ĠìķĦëĵ¤&#34;:5221,&#34;íķ´ì§Ģ&#34;:5222,&#34;Ġë³´ê¸´&#34;:5223,&#34;Ġë³´ìĿ¸ëĭ¤&#34;:5224,&#34;ìłĲìĪĺ&#34;:5225,&#34;ìłķìĿĺ&#34;:5226,&#34;ìłģìĿĢ&#34;:5227,&#34;Ġìĭľìĭľ&#34;:5228,&#34;íķ¨ìĿĢ&#34;:5229,&#34;ĠìłĦìŀĳ&#34;:5230,&#34;ĠìķĬìĿĦê¹Į&#34;:5231,&#34;Ġë¬´ì²Ļ&#34;:5232,&#34;ìĭ¤íķľ&#34;:5233,&#34;Ġë§İê³ł&#34;:5234,&#34;Īëį°&#34;:5235,&#34;Ġì§Ģë£¨íķĺê²Į&#34;:5236,&#34;ë³´ê³łìĭ¶ìĿĢ&#34;:5237,&#34;ãħİãħİãħİãħİ&#34;:5238,&#34;ë¬¼ìĿ´&#34;:5239,&#34;Ġì¢ĭìķĺìľ¼ëĤĺ&#34;:5240,&#34;Ġê¸°ëĮĢíĸĪëĬĶëį°&#34;:5241,&#34;ìĤ¬ëŀĮëĵ¤ìĿĢ&#34;:5242,&#34;ĠíĬ¹ìĪĺ&#34;:5243,&#34;Ġíİ¸ê²¬&#34;:5244,&#34;ĠíĥĢìŀĦ&#34;:5245,&#34;ĠìĽĥìĿĮìĿ´&#34;:5246,&#34;Ġê°ĲìłķìĿĦ&#34;:5247,&#34;VD&#34;:5248,&#34;ìĩ&#34;:5249,&#34;Ġìį¼&#34;:5250,&#34;ìŀŃ&#34;:5251,&#34;ìļĶìĿ¼&#34;:5252,&#34;Ġë³´ë©°&#34;:5253,&#34;ìļ©ìľ¼ë¡ľ&#34;:5254,&#34;íĸĪëĤĺ&#34;:5255,&#34;íĸĪìĸ´&#34;:5256,&#34;Ġëª¨ë¥¼&#34;:5257,&#34;ìħĢ&#34;:5258,&#34;ê²łìĬµëĭĪëĭ¤&#34;:5259,&#34;Ġëª»íķľëĭ¤&#34;:5260,&#34;Ġë©ĺ&#34;:5261,&#34;Ġìļ°ì£¼&#34;:5262,&#34;ìŀ¬ë°ĮëĦ¤ìļĶ&#34;:5263,&#34;ĠìĽĥê¹Ģ&#34;:5264,&#34;ìĹĩëĭ¤&#34;:5265,&#34;ĠíĨ°&#34;:5266,&#34;ë°ĽìĿĢ&#34;:5267,&#34;ëĺĳ&#34;:5268,&#34;ëıħíĬ¹&#34;:5269,&#34;ĠìŀĬìĿĦ&#34;:5270,&#34;ĠìĿ´ìľłë¥¼&#34;:5271,&#34;ì¸µ&#34;:5272,&#34;ĠëłĪìĿ´&#34;:5273,&#34;ĪëįĶëĭĪ&#34;:5274,&#34;Ġì¼ĢìĿ´ë¸Ķ&#34;:5275,&#34;Ġìĸ´ë¦°ìĿ´&#34;:5276,&#34;Ġë²Įìį¨&#34;:5277,&#34;ì¥¬&#34;:5278,&#34;Ġëįķë¶ĦìĹĲ&#34;:5279,&#34;ĠíĹĲë¦¬ìļ°ëĵľ&#34;:5280,&#34;Ġëī´&#34;:5281,&#34;ãħ£&#34;:5282,&#34;.........&#34;:5283,&#34;ĠìĭľëıĦ&#34;:5284,&#34;ë²Ħê·¸&#34;:5285,&#34;ë²Ħë¦¬ëĬĶ&#34;:5286,&#34;ë²ĦìĬ¤íĦ°&#34;:5287,&#34;ë³´ê³łìĭ¶ëĭ¤&#34;:5288,&#34;ĠìĭłíĮĮ&#34;:5289,&#34;Ġì£½ìĿĦ&#34;:5290,&#34;Ġê´Ģíķľ&#34;:5291,&#34;ë¶ĪìĮį&#34;:5292,&#34;Ġìłķìĭłë³ĳ&#34;:5293,&#34;ĠìŀĬíĺĢì§Ģì§Ģ&#34;:5294,&#34;Ġì±Ļê²¨&#34;:5295,&#34;ĠìĽĢì§ģ&#34;:5296,&#34;ĻëĭĪëĭ¤&#34;:5297,&#34;ìĿ¸ê³¼&#34;:5298,&#34;Ġëĭ¤ìĸĳ&#34;:5299,&#34;íķĺê³łìŀĲ&#34;:5300,&#34;ĠëĮĢíķ´ìĦľ&#34;:5301,&#34;ĠëĬĲìĻĢë¥´&#34;:5302,&#34;ì¢ĭìĿĮ&#34;:5303,&#34;Ġëĸ¡&#34;:5304,&#34;ëĬĲëĭĪ&#34;:5305,&#34;0000&#34;:5306,&#34;Ġë¡ľê·¸ìĿ¸&#34;:5307,&#34;ìŀ¬ë¯¸ìŀĪëĭ¤&#34;:5308,&#34;ëŁ¬ëĶĶ&#34;:5309,&#34;ìĸ´ë¦´ìłģ&#34;:5310,&#34;ëİĢ&#34;:5311,&#34;Ġë§¨ëĤł&#34;:5312,&#34;ìĸĺê¸°&#34;:5313,&#34;ë¯¿ê³ł&#34;:5314,&#34;ĠëŁ&#34;:5315,&#34;ìĿ´ë¡ľ&#34;:5316,&#34;ìĹĲë¡ľ&#34;:5317,&#34;ìĸ´ì©Į&#34;:5318,&#34;ìĹĨëĦ¤ìļĶ&#34;:5319,&#34;ìĥģìĺģ&#34;:5320,&#34;ìĺ¤ìĺ¤&#34;:5321,&#34;ĠìĦ¼&#34;:5322,&#34;Ġë´¤ìĿĦëķĮ&#34;:5323,&#34;Ġìŀ¬ë°Įëĭ¤ê³ł&#34;:5324,&#34;ë£¨ìĬ¤&#34;:5325,&#34;Ġë¬´ìĸ¸&#34;:5326,&#34;Ġì²Ļ&#34;:5327,&#34;ì²´ê°Ģ&#34;:5328,&#34;Ġë¹ĦíĮĲ&#34;:5329,&#34;ìķĪíĥĢ&#34;:5330,&#34;Ġì¢ĭìķĦíķ´ìĦľ&#34;:5331,&#34;ĠìŀĳíĴĪìĿĢ&#34;:5332,&#34;ĠëĦĺì¹ĺëĬĶ&#34;:5333,&#34;íĸīë³µ&#34;:5334,&#34;Ġë³´ìĹ¬ì£¼ê³ł&#34;:5335,&#34;ìĸ¸ëĭĪ&#34;:5336,&#34;Ġë³¼ë§Įíķĺëĭ¤&#34;:5337,&#34;ĠëĮĢìĤ¬ê°Ģ&#34;:5338,&#34;ë¦¬ìĬ¤ë§Ī&#34;:5339,&#34;Ġì¢ĭê²łìĸ´ìļĶ&#34;:5340,&#34;Ġì§ģìłĳ&#34;:5341,&#34;ĠìĥĪë¡Ń&#34;:5342,&#34;om&#34;:5343,&#34;ªħ&#34;:5344,&#34;ìĿ´íģ¬&#34;:5345,&#34;¬ëŁ¬&#34;:5346,&#34;ë¦¬íı¬&#34;:5347,&#34;ìĺ¬ë¦¬&#34;:5348,&#34;ìķ¼ë§Ĳë¡ľ&#34;:5349,&#34;Ġì¢ĭê²Į&#34;:5350,&#34;ìŀĪëĤĺ&#34;:5351,&#34;¬ëŀĺ&#34;:5352,&#34;ëħ¼&#34;:5353,&#34;ë£©&#34;:5354,&#34;ìĻĢìĿĺ&#34;:5355,&#34;ëł¤ìļĶ&#34;:5356,&#34;ìĺĢê³ł&#34;:5357,&#34;ĠìĿ¼íĴĪ&#34;:5358,&#34;ĠìĤ¬ëŀĳìĿĺ&#34;:5359,&#34;ĠìĥģìĹħ&#34;:5360,&#34;ëĸĦ&#34;:5361,&#34;Ġê²°ê³¼&#34;:5362,&#34;ëĭĪë¡ľ&#34;:5363,&#34;Ġì§ĳìĹĲìĦľ&#34;:5364,&#34;ĠìĭłìĦłíķľ&#34;:5365,&#34;ĠìĹīìĦ±íķľ&#34;:5366,&#34;,.,.&#34;:5367,&#34;ĠìķĪíĥĢê¹Ŀëĭ¤&#34;:5368,&#34;ĠëĿ&#34;:5369,&#34;¬ëĵľ&#34;:5370,&#34;ĠìĿ´ìļ©&#34;:5371,&#34;ìļĶìĨĮ&#34;:5372,&#34;ìķĦëĤ´&#34;:5373,&#34;ìłĲìĿ´ëĭ¤&#34;:5374,&#34;ìłķìļ°&#34;:5375,&#34;ëĶĺ&#34;:5376,&#34;ìłľëĮĢë¡ľ&#34;:5377,&#34;Ġìĺ¤ë°Ķ&#34;:5378,&#34;Ġìĵ°ëłĪê¸°ëĭ¤&#34;:5379,&#34;ì¢Ģë¹Ħ&#34;:5380,&#34;ĠëĬĲê»´ì§Ħëĭ¤&#34;:5381,&#34;ì¸¡&#34;:5382,&#34;ëĳĳ&#34;:5383,&#34;Ġì«Į&#34;:5384,&#34;Ġíĭ°ë¹ĦìĹĲìĦľ&#34;:5385,&#34;Ġë¹Įëł¤&#34;:5386,&#34;ĠëŁ¬ëĭĿíĥĢìŀĦ&#34;:5387,&#34;Ġë®¤ì§Ģì»¬&#34;:5388,&#34;ì¦Īë¥¼&#34;:5389,&#34;ĠI&#34;:5390,&#34;ìĿ´ë¦Ħ&#34;:5391,&#34;ê¸°ìĹĲëĬĶ&#34;:5392,&#34;ĠìķĦíĶĶ&#34;:5393,&#34;Ġ12&#34;:5394,&#34;ĠìĨĶì§ģ&#34;:5395,&#34;Ġëª¨ë¥´ê²Į&#34;:5396,&#34;ĠëĶ°ë¡ľ&#34;:5397,&#34;ë¨¹ëĬĶ&#34;:5398,&#34;ĠìĹ°ê¸°ëł¥ëıĦ&#34;:5399,&#34;Ġê·¸ëłĩì§Ģ&#34;:5400,&#34;ëĦĪìĬ¤&#34;:5401,&#34;ĠìŀĪìĹĪì§Ģë§Į&#34;:5402,&#34;Ġthe&#34;:5403,&#34;Ġëĥī&#34;:5404,&#34;¨¼&#34;:5405,&#34;..^^&#34;:5406,&#34;ãħĹ&#34;:5407,&#34;íķĺëįĺëį°&#34;:5408,&#34;ìĦľëıĦ&#34;:5409,&#34;ìłĲìłķëıĦ&#34;:5410,&#34;ìĤ¬ëĿ¼&#34;:5411,&#34;ĠìĹĨì§Ģ&#34;:5412,&#34;ĠëĤĺë§Į&#34;:5413,&#34;ì§ĦìĿ´&#34;:5414,&#34;Ġíķľë§ĪëĶĶ&#34;:5415,&#34;Ġê¸°ìĪł&#34;:5416,&#34;ëł¤ëĤĺ&#34;:5417,&#34;íĸĪëĭ¤ê³ł&#34;:5418,&#34;ĠëįĶëŁ¬ìļ´&#34;:5419,&#34;ĠëĵľëŁ½ê²Į&#34;:5420,&#34;Ġëĵľë¦½ëĭĪëĭ¤&#34;:5421,&#34;ìĥĿíĻľ&#34;:5422,&#34;ĠìĹ¬íĸī&#34;:5423,&#34;ĠìŀĳìĿĢ&#34;:5424,&#34;ë°Ķë¡ľ&#34;:5425,&#34;Ġì£½ìĿ´ê³ł&#34;:5426,&#34;ĠíıīìĿ´&#34;:5427,&#34;ĠëĪĦêµ¬ëĤĺ&#34;:5428,&#34;ĠíĤ¬ë§ģíĥĢìŀĦ&#34;:5429,&#34;ĠãħĪ&#34;:5430,&#34;Ġì§Īì§Īëģ&#34;:5431,&#34;Ġë²łìĬ¤íĬ¸&#34;:5432,&#34;Ġë¬´ìĦŃëĭ¤&#34;:5433,&#34;Ġê¹¨ëĭ¬&#34;:5434,&#34;Ġë¶ĪëŁ¬&#34;:5435,&#34;Ġì¦Ĳê±°ìļ´&#34;:5436,&#34;íķ´ì§Ħëĭ¤&#34;:5437,&#34;ĠìĹĨìĸ´ìļĶ&#34;:5438,&#34;ìļ´ëĵľ&#34;:5439,&#34;Ġ18&#34;:5440,&#34;Ġìŀĺíķĺê³ł&#34;:5441,&#34;ìŀĦìĿĦ&#34;:5442,&#34;ë³¸ìĺģíĻĶ&#34;:5443,&#34;íı´&#34;:5444,&#34;ĠìĿ¼ìĿ´&#34;:5445,&#34;ëªħìĿĦ&#34;:5446,&#34;Ġì£½ìĿĢ&#34;:5447,&#34;íĽĦíļĮ&#34;:5448,&#34;ì»¥&#34;:5449,&#34;ĠëĤ¨ìŀĲê°Ģ&#34;:5450,&#34;Ġíĺ¸ëŁ¬&#34;:5451,&#34;ëĨĪëĵ¤&#34;:5452,&#34;Īë²ķ&#34;:5453,&#34;Ġë¬ĺìĤ¬&#34;:5454,&#34;zz&#34;:5455,&#34;ĠìĿ´ëķĮ&#34;:5456,&#34;ìĸ´ëĬĲ&#34;:5457,&#34;ìķĦëĬĶ&#34;:5458,&#34;ë³´ëĦ¤&#34;:5459,&#34;íĨłë¡Ŀ&#34;:5460,&#34;ĠìĿ¸íķ´&#34;:5461,&#34;ĠìŀĲì£¼&#34;:5462,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿĺ&#34;:5463,&#34;ì¼ĵ&#34;:5464,&#34;Ġë¶Īì¾Į&#34;:5465,&#34;ĠìĻĦìłĦíŀĪ&#34;:5466,&#34;Ġë°ľë¡ľ&#34;:5467,&#34;Ġìĵ°ëŀĺê¸°&#34;:5468,&#34;Ġìŀĳê°Ģê°Ģ&#34;:5469,&#34;ĠìĿ´ìģĺëĭ¤&#34;:5470,&#34;ì·¨íĸ¥&#34;:5471,&#34;ë½ķ&#34;:5472,&#34;22&#34;:5473,&#34;©ĺ&#34;:5474,&#34;Ġãħĩãħĩ&#34;:5475,&#34;İģ&#34;:5476,&#34;ìĹĳ&#34;:5477,&#34;ê¸ĭ&#34;:5478,&#34;ìĸ´ìĥī&#34;:5479,&#34;ë©ľë¡ľ&#34;:5480,&#34;ìŀĪìĸ´ìĦľ&#34;:5481,&#34;ĠìĺĨ&#34;:5482,&#34;ë´ħëĭĪëĭ¤&#34;:5483,&#34;ìłľë¡ľ&#34;:5484,&#34;Ġê¸°ê°Ģ&#34;:5485,&#34;ìŀĦìĬ¤&#34;:5486,&#34;ê°ĲìĿĢ&#34;:5487,&#34;Ġë³¼ìĪĺìŀĪ&#34;:5488,&#34;Ġìĭ¶ìĿĦ&#34;:5489,&#34;ìķłê°Ģ&#34;:5490,&#34;ĠìĤ¬ëŀĳìĬ¤ëŁ¬ìļ´&#34;:5491,&#34;ìĨįìĿĺ&#34;:5492,&#34;Ġê³łë§Ī&#34;:5493,&#34;Ġë°Ķê¾¸&#34;:5494,&#34;ĠíŀĺìĿ´&#34;:5495,&#34;Ġê°ĢìĬ´ìķĦ&#34;:5496,&#34;ìŀ¥ë©´ìĿĢ&#34;:5497,&#34;Ġê°Ħëĭ¤&#34;:5498,&#34;ĠìĭłìĦłíķĺê³ł&#34;:5499,&#34;Ġì¶ĶìĸµìĿĺ&#34;:5500,&#34;Ġë³´ìķĺìĬµëĭĪëĭ¤&#34;:5501,&#34;Ġê°Ģì¹ĺê°Ģ&#34;:5502,&#34;ĠìĺĪë»Ĳ&#34;:5503,&#34;ë°©ìĤ¬ìĪĺ&#34;:5504,&#34;Ġëł&#34;:5505,&#34;ëĭī&#34;:5506,&#34;ìĿ´ë²Ī&#34;:5507,&#34;ìĹĪëĥĲ&#34;:5508,&#34;ĠìĺģíĻĶëĦ¤&#34;:5509,&#34;ĠìķĦíĶĦ&#34;:5510,&#34;Ġê·¸ëĭ¤ì§Ģ&#34;:5511,&#34;Ġìĸ´ì²ĺêµ¬ëĭĪ&#34;:5512,&#34;Ġë§ĮëĤĺ&#34;:5513,&#34;íİľ&#34;:5514,&#34;ìĦ¸ê¸°&#34;:5515,&#34;Ġëª»ìĥĿ&#34;:5516,&#34;ëħĦë§ĮìĹĲ&#34;:5517,&#34;ëĲĺëĦ¤ìļĶ&#34;:5518,&#34;ë³´ê³łëĤĺìĦľ&#34;:5519,&#34;íħĿ&#34;:5520,&#34;ëĤ¨ëĬĶ&#34;:5521,&#34;ê²¨ì§Ħ&#34;:5522,&#34;ĠìĦ±ìĿ¸&#34;:5523,&#34;ëĲ©ëĭĪëĭ¤&#34;:5524,&#34;Ġê³¼ìŀ¥&#34;:5525,&#34;ĠëĶ±íŀĪ&#34;:5526,&#34;ì²ĺìĿĮìĹĶ&#34;:5527,&#34;Ġë¶Ģì¡±íķĺëĭ¤&#34;:5528,&#34;Ġìĭ¶ìĿĢëį°&#34;:5529,&#34;Ġë¬¸ìłľê°Ģ&#34;:5530,&#34;Ġë¿ĲìĿ´ëĭ¤&#34;:5531,&#34;ĠìĻľìĿ´ëŀĺ&#34;:5532,&#34;ĠìĦ¹ìĬ¤&#34;:5533,&#34;ìĸ´ëł¸ìĿĦ&#34;:5534,&#34;ĠëĬĲê¼Ī&#34;:5535,&#34;ì§ĢëıĦìķĬê³ł&#34;:5536,&#34;íľĺ&#34;:5537,&#34;ĦìĥĪ&#34;:5538,&#34;ŃëĭĪëĭ¤&#34;:5539,&#34;ìĹĲíľ´&#34;:5540,&#34;Ġê·¸ê±°&#34;:5541,&#34;ìłĲì¤Į&#34;:5542,&#34;ìĬ¤íĭ°&#34;:5543,&#34;ìĬ¤ì¼Ģ&#34;:5544,&#34;Ġë´¤ìĿĦ&#34;:5545,&#34;íķ¨ìĿĺ&#34;:5546,&#34;Ġì¤¬&#34;:5547,&#34;ìĭłë¶Ħ&#34;:5548,&#34;Ġëª»íķ¨&#34;:5549,&#34;Ġê°ľë¿Ķ&#34;:5550,&#34;Ġë©Ī&#34;:5551,&#34;ìĽĲìĿĺ&#34;:5552,&#34;ëĲĺìĹĪëĭ¤&#34;:5553,&#34;ë¦¬ëį&#34;:5554,&#34;ìķłëĭĪë©ĶìĿ´ìħĺ&#34;:5555,&#34;ĠëªħíĴĪ&#34;:5556,&#34;ĠíķĺëĤĺìĿĺ&#34;:5557,&#34;ë³¼ë§Įíķľ&#34;:5558,&#34;Ġë³´ìĹ¬ì¤Ģëĭ¤&#34;:5559,&#34;Ġëĭ¨ì§Ģ&#34;:5560,&#34;ëĬ¥ëł¥&#34;:5561,&#34;ë§Įëĵ¤ìĸ´&#34;:5562,&#34;ëıħë¦½&#34;:5563,&#34;ĠìĨĮìŀ¬ê°Ģ&#34;:5564,&#34;Ġê¸´ìŀ¥ê°ĲëıĦ&#34;:5565,&#34;ĪëĶ§&#34;:5566,&#34;Ġë¬´ìĹĩìĿĦ&#34;:5567,&#34;ì¶©ê²©&#34;:5568,&#34;ê¹Ĭê²Į&#34;:5569,&#34;ĠíķĻìĥĿ&#34;:5570,&#34;ĠíŀĪìĸ´ë¡ľ&#34;:5571,&#34;Ġë©įì²Ńíķľ&#34;:5572,&#34;ìĿ½&#34;:5573,&#34;íĻĢ&#34;:5574,&#34;ëĤĻ&#34;:5575,&#34;ëį¤&#34;:5576,&#34;ĠìĿ´ëģĮ&#34;:5577,&#34;ë¡ľëĵľ&#34;:5578,&#34;ëĦ¹&#34;:5579,&#34;ë°ĸ&#34;:5580,&#34;ë¶ķ&#34;:5581,&#34;ĠìĹĨìĬµëĭĪëĭ¤&#34;:5582,&#34;ĠìĥĪëģ¼&#34;:5583,&#34;Ġìĺ·&#34;:5584,&#34;ìĦ±ê³¼&#34;:5585,&#34;Ġìŀ¬ë°ĮëĦ¤&#34;:5586,&#34;ĠìĥĿê°ģìĹĨìĿ´&#34;:5587,&#34;ê´ľ&#34;:5588,&#34;Ġë¯¸ëĵľ&#34;:5589,&#34;ë²Ħë¦¼&#34;:5590,&#34;ĠìŀĪëĬĶì§Ģ&#34;:5591,&#34;ĠíķĺëĬĶê±°&#34;:5592,&#34;ì´Į&#34;:5593,&#34;ĠìľĦíķ´ìĦľ&#34;:5594,&#34;ĠìĨįìĹĲìĦľ&#34;:5595,&#34;ĠìłķëıĦìĿĺ&#34;:5596,&#34;ëıĪì£¼ê³ł&#34;:5597,&#34;ĠìĿ¸ìĥģìłģìĿ´&#34;:5598,&#34;Ġê¿ĪìĿĦ&#34;:5599,&#34;ĠëĽ°ìĸ´ëĦĺ&#34;:5600,&#34;ĠìĿ´ìłķëıĦë©´&#34;:5601,&#34;ĠìķĮê²łëĬĶëį°&#34;:5602,&#34;Ġë°ĳ&#34;:5603,&#34;ìĹ°ìĿ´&#34;:5604,&#34;ĠìĥĿê°ģíķ¨&#34;:5605,&#34;ëħĦìĿĺ&#34;:5606,&#34;Ġë¹Ħíķĺë©´&#34;:5607,&#34;ĠìķĪë´Ĳ&#34;:5608,&#34;Ġìĥģê´Ģ&#34;:5609,&#34;ì¶Ķê¸°&#34;:5610,&#34;ê²¨ìĦľ&#34;:5611,&#34;ĠìĿ´íķ´íķĺê¸°&#34;:5612,&#34;ĠëĨĴê²Į&#34;:5613,&#34;ë²Ķì£Ħ&#34;:5614,&#34;ê²ģëĭĪëĭ¤&#34;:5615,&#34;ĠëĶĶì¦ĪëĭĪ&#34;:5616,&#34;Ġì²ŃìĨĮëħĦ&#34;:5617,&#34;ê²ĥê°ĻìķĦìļĶ&#34;:5618,&#34;ĠíĴįê²½&#34;:5619,&#34;ĠíĪ¬ìŀĲ&#34;:5620,&#34;Ġì§ĿìĿ´&#34;:5621,&#34;Ġì°Įì§Ī&#34;:5622,&#34;Ġìĸ´ë¦´ëķĮ&#34;:5623,&#34;ĠìĿĳìĽĲ&#34;:5624,&#34;.,&#34;:5625,&#34;¬ĺ&#34;:5626,&#34;ëĮĵê¸Ģ&#34;:5627,&#34;Ġì¯&#34;:5628,&#34;ĠìµĿìĺ¤&#34;:5629,&#34;ëĭĮ&#34;:5630,&#34;ëĤĺìĺ¬&#34;:5631,&#34;ĠíķĺìĿ´&#34;:5632,&#34;ĠìŀĪêµ¬ëĤĺ&#34;:5633,&#34;ê·¸ëĤĺë§Ī&#34;:5634,&#34;Ġê°Ģë³įê²Į&#34;:5635,&#34;íĮĶìĿ´&#34;:5636,&#34;ëŁ¬ìĽĮ&#34;:5637,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:5638,&#34;Ġë§ĲìķĦë¨¹&#34;:5639,&#34;Ġíķ´ëĿ¼&#34;:5640,&#34;ĠìķĦê¹Įìļ¸&#34;:5641,&#34;ì¼ĢìĿ´ë¸Ķ&#34;:5642,&#34;ìĶ¬ìĿ´&#34;:5643,&#34;Ġëª¨ë¥´ê²łìĿĮ&#34;:5644,&#34;ĠìıŁ&#34;:5645,&#34;Ġì¤ĳìļĶíķľ&#34;:5646,&#34;ĠíĥĦíĥĦíķľ&#34;:5647,&#34;Ġê¹ľì§Ŀ&#34;:5648,&#34;ìĿ´ëıĦ&#34;:5649,&#34;ëĭĪëĭ¹&#34;:5650,&#34;ìĭľìĽĲ&#34;:5651,&#34;ìĿĮìĿ´&#34;:5652,&#34;ëª¬&#34;:5653,&#34;ĠìłĦë°ĺ&#34;:5654,&#34;Ġìļ°ëł¤&#34;:5655,&#34;Ġìŀ¬ë¯¸ìĹĨê²Į&#34;:5656,&#34;ë°ľìĹ°ê¸°&#34;:5657,&#34;ĠìĦ±ê²©&#34;:5658,&#34;Ġë©ĭìŀĪëĭ¤&#34;:5659,&#34;000&#34;:5660,&#34;ëĵľëĿ¼ë§Īê°Ģ&#34;:5661,&#34;Ġì§ľì¦ĿëĤľëĭ¤&#34;:5662,&#34;ĠìĪĺì¤ĢìĿĺ&#34;:5663,&#34;Ġì¹ĺê³ł&#34;:5664,&#34;ëķħ&#34;:5665,&#34;ĠêµĲìľ¡&#34;:5666,&#34;ĠíĥĢìĿ´&#34;:5667,&#34;Ġìłľìŀĳë¹Ħ&#34;:5668,&#34;ìĿ¸ê°ĦìĿĺ&#34;:5669,&#34;âĺĨ&#34;:5670,&#34;¬¸&#34;:5671,&#34;Ńìłľ&#34;:5672,&#34;ĠëĮ&#34;:5673,&#34;ìĿ´ìģĺ&#34;:5674,&#34;Ġìķ¤&#34;:5675,&#34;ê¸°ì§Ģ&#34;:5676,&#34;ë§ĮìĿĢ&#34;:5677,&#34;Ġë³´ëł¤ê³ł&#34;:5678,&#34;ì£¼ê°Ģ&#34;:5679,&#34;ĠìłķìĿĺ&#34;:5680,&#34;Ġìĸ´ìļ°&#34;:5681,&#34;ĠëĤ´ë©´&#34;:5682,&#34;ìķĺìĿĦ&#34;:5683,&#34;ìŀĦìĥĪ&#34;:5684,&#34;Ġë³¼ê±°&#34;:5685,&#34;Ġíķ´ê²°&#34;:5686,&#34;Ġíķłë§ĲìĿ´&#34;:5687,&#34;Ġê´Ģìĭ¬&#34;:5688,&#34;ìĤ´ìĿ¸&#34;:5689,&#34;íĪ¬ë&#34;:5690,&#34;ĠëĤ¨ìŀĲìĿĺ&#34;:5691,&#34;Ġë°ĽìĿĦ&#34;:5692,&#34;ĠìºĲë¦ŃíĦ°ê°Ģ&#34;:5693,&#34;ìĹ¬ëŁ¬ë&#34;:5694,&#34;ê·ĢìĹ½&#34;:5695,&#34;ĠìĨĮë¦ĦìĿ´&#34;:5696,&#34;ëĮĢíķľë¯¼êµŃ&#34;:5697,&#34;Ġê²¨ìļ°&#34;:5698,&#34;Ġê¹¨ëĭ«ê²Į&#34;:5699,&#34;ë·&#34;:5700,&#34;ĪĦ&#34;:5701,&#34;ìĿ´ëĵ¤&#34;:5702,&#34;ê°Ģë©´&#34;:5703,&#34;ìĿ¸ê²ĥ&#34;:5704,&#34;ê¹ģ&#34;:5705,&#34;ê±°ìĿĺ&#34;:5706,&#34;ìŀĲëĵ¤ìĿ´&#34;:5707,&#34;ë¶Ģìŀĳ&#34;:5708,&#34;ĠìŀĲìĤ´&#34;:5709,&#34;Ġìļ°ìłķ&#34;:5710,&#34;Ġëħ¸ëĭµ&#34;:5711,&#34;ê°ķíĺ¸&#34;:5712,&#34;Ġë°©ìĺģ&#34;:5713,&#34;Ġì°įìĸ´&#34;:5714,&#34;Ġ50&#34;:5715,&#34;ìŀ¬ë°ĭëĭ¤&#34;:5716,&#34;Ġíĸīë³µíķľ&#34;:5717,&#34;Ġê°ľìĹ°ìĦ±ìĿ´&#34;:5718,&#34;Ġê°ľìĹ°ìĦ±ëıĦ&#34;:5719,&#34;ìĺģìĥģë¯¸&#34;:5720,&#34;Ġëĺĳê°ĻìĿĢ&#34;:5721,&#34;Īë²½ìĹĲ&#34;:5722,&#34;.^^&#34;:5723,&#34;ĠìĹ¿&#34;:5724,&#34;ë¦¬ì¹´&#34;:5725,&#34;ëłĲ&#34;:5726,&#34;Ġë³´ìŀĲ&#34;:5727,&#34;ĠìŀĪìĹĪëĬĶëį°&#34;:5728,&#34;íĸĪìľ¼ëĤĺ&#34;:5729,&#34;ĠìķĬëĦ¤ìļĶ&#34;:5730,&#34;ĠìĿ´ëŁ°ìĺģíĻĶê°Ģ&#34;:5731,&#34;ëĲĺê²Į&#34;:5732,&#34;Ġëĭ¹ìĹ°&#34;:5733,&#34;ĠíĹĪìĦ¸&#34;:5734,&#34;ì»·&#34;:5735,&#34;ĠëĤ®ê²Į&#34;:5736,&#34;Ġë¹łì§Ħ&#34;:5737,&#34;ìķĦìĿ´ê°Ģ&#34;:5738,&#34;ìķ½íķľ&#34;:5739,&#34;Ġìķŀìľ¼ë¡ľëıĦ&#34;:5740,&#34;ë³´ì§Ģë§Ī&#34;:5741,&#34;Ġêµ°ëĮĢ&#34;:5742,&#34;ĠìķĦë¦Ħëĭ¤ìĽĢ&#34;:5743,&#34;Ġì§Ŀíīģ&#34;:5744,&#34;Ġíı¬ë¥´ëħ¸&#34;:5745,&#34;40&#34;:5746,&#34;ĠãĦ±&#34;:5747,&#34;..;;&#34;:5748,&#34;Ġíĳ¸&#34;:5749,&#34;ë¡ľë§Į&#34;:5750,&#34;ìķĦìłĢìĶ¨&#34;:5751,&#34;Ġê·¸ê±´&#34;:5752,&#34;Ġëĭ¥&#34;:5753,&#34;ë³´ìŀĲ&#34;:5754,&#34;Ġë°ĳìĹĲ&#34;:5755,&#34;Ġë§Įíķľ&#34;:5756,&#34;ê·¸ë¦¼&#34;:5757,&#34;ì¹ĺê°Ģ&#34;:5758,&#34;íķĺëĬĶê±´&#34;:5759,&#34;ĠìķĪìĵ°&#34;:5760,&#34;ĠìķĮìĪĺ&#34;:5761,&#34;Ġì§Ģë£¨íĸĪëĭ¤&#34;:5762,&#34;ëħ¸ëĭµ&#34;:5763,&#34;ìĹĨëĬĶìĺģíĻĶ&#34;:5764,&#34;ĠëĤĺìĺ¤ì§Ģ&#34;:5765,&#34;Ġë¶Ħëĵ¤ìĿĢ&#34;:5766,&#34;ĠëªħìŀĳìĿ´&#34;:5767,&#34;Ġë¡ľë´ĩ&#34;:5768,&#34;Ġìľłì¹ĺíķĺëĭ¤&#34;:5769,&#34;ĠìĿĮìķħê³¼&#34;:5770,&#34;Ġíķľë²Īì¯¤&#34;:5771,&#34;Ġëĳĺì§¸&#34;:5772,&#34;Ġì©Ĳëĭ¤&#34;:5773,&#34;ĠëĭĪì½ľ&#34;:5774,&#34;ìĦ¸ìĥģìĹĲ&#34;:5775,&#34;ĠMì°½&#34;:5776,&#34;OOOê¸°&#34;:5777,&#34;Ġì£½ëĬĶì¤Ħ&#34;:5778,&#34;³¸&#34;:5779,&#34;¬ëłĪ&#34;:5780,&#34;ìĸ´ë¦¬&#34;:5781,&#34;ĠëĤ©&#34;:5782,&#34;ìĺģíĻĶìĿ¸ëį°&#34;:5783,&#34;ìĿĮìĿĦ&#34;:5784,&#34;Ġëĭ¤ëĭĪ&#34;:5785,&#34;Ġìĸ´ë¨¸ëĭĪ&#34;:5786,&#34;Ġìĺ¬ë¦¬&#34;:5787,&#34;ĠìĿ¸ëĤ´&#34;:5788,&#34;ë¬¼ìĿĦ&#34;:5789,&#34;Ġìĺģíĺ¼&#34;:5790,&#34;ĠíĥĲ&#34;:5791,&#34;Ġê°ķëł¥&#34;:5792,&#34;Ġë§Įëĵ¤ìĸ´ëıĦ&#34;:5793,&#34;Ġëĸ¨ìĸ´ì§Ħëĭ¤&#34;:5794,&#34;Ġìķ¼ëıĻ&#34;:5795,&#34;Ġìłľëª©ìĿ´&#34;:5796,&#34;Ġê¸°ëĭ¤ëł¤&#34;:5797,&#34;ĠìķĪë³¼&#34;:5798,&#34;ëłĪë©ĺ&#34;:5799,&#34;ìĺĽëĤłìĹĲ&#34;:5800,&#34;ìĭľì²Ńë¥ł&#34;:5801,&#34;Ġìĺ¤ëŀľë§Į&#34;:5802,&#34;ìĹł&#34;:5803,&#34;ĠíĦ&#34;:5804,&#34;ĠìķĦìĺĪ&#34;:5805,&#34;Ġë³´ëĭ¤ëĬĶ&#34;:5806,&#34;ë³´ëĭĪê¹Į&#34;:5807,&#34;ìłķìłģìĿ¸&#34;:5808,&#34;ìŀ¥ìĿĦ&#34;:5809,&#34;Ġì§Ģêµ¬&#34;:5810,&#34;íĨ±&#34;:5811,&#34;ë¯¸ëĦ¤&#34;:5812,&#34;Ġê¸°íļĮ&#34;:5813,&#34;ëł¤ëĭ¤ê°Ģ&#34;:5814,&#34;ë¬µ&#34;:5815,&#34;êµ¬ìĦ±&#34;:5816,&#34;Ġìŀĺìĸ´ìļ¸&#34;:5817,&#34;ĠìĥĿê°ģëĤĺìĦľ&#34;:5818,&#34;Ġìĺ¤ëŀ«&#34;:5819,&#34;Ġê¹İ&#34;:5820,&#34;ëĭ¤ëĬĶê±°&#34;:5821,&#34;ëĭ¤ëĬĶê±¸&#34;:5822,&#34;ë´¤ëĦ¤ìļĶ&#34;:5823,&#34;ĠíķĺëĬĶê²Į&#34;:5824,&#34;ìĹĲê²ĮëĬĶ&#34;:5825,&#34;ë¡Ŀë²ĦìĬ¤íĦ°&#34;:5826,&#34;ĠìľĦëĮĢíķľ&#34;:5827,&#34;ĠíķľêµŃìĺģíĻĶëĬĶ&#34;:5828,&#34;ì§Ģë£¨íķ´&#34;:5829,&#34;íĨµëł¹&#34;:5830,&#34;ĠìĹ¬ìļ´ìĿĦ&#34;:5831,&#34;ìķ½ê°Ħ&#34;:5832,&#34;ĠíĨµì¾Į&#34;:5833,&#34;ĠëĬĲëĤĦìĪĺ&#34;:5834,&#34;ĠíĿĶíķľ&#34;:5835,&#34;ĠíķĦë¦Ħ&#34;:5836,&#34;70&#34;:5837,&#34;~!!!&#34;:5838,&#34;ëĽ°&#34;:5839,&#34;Ġ/&#34;:5840,&#34;Ġl&#34;:5841,&#34;ĠìŀŃ&#34;:5842,&#34;ë¦¬íķľ&#34;:5843,&#34;Ġì§Īë&#34;:5844,&#34;ìľ¼ë¡ľëĬĶ&#34;:5845,&#34;íĺĪ&#34;:5846,&#34;ì¶¤&#34;:5847,&#34;ĠëĮĢë³¸&#34;:5848,&#34;ì¢Į&#34;:5849,&#34;Ġìµľê³łìŀħëĭĪëĭ¤&#34;:5850,&#34;ë¬¸íĻĶ&#34;:5851,&#34;íĤ¬&#34;:5852,&#34;Ġê°ĲíĿ¥&#34;:5853,&#34;ĠíĮĮìĿ´&#34;:5854,&#34;ìĿ´ëŁ°ê²Į&#34;:5855,&#34;ĠìĹ°ì¶ľê³¼&#34;:5856,&#34;ĠìĦ¸ëł¨&#34;:5857,&#34;íĺķëĭĺ&#34;:5858,&#34;ëĭĪë¥¼&#34;:5859,&#34;ê°ĲëıĻìłģìĿ¸&#34;:5860,&#34;Ġê°ĢìĬ´ìĹĲ&#34;:5861,&#34;Ġ70&#34;:5862,&#34;ë³ĳë§Ľ&#34;:5863,&#34;Ġê½ĥ&#34;:5864,&#34;Ġë¬´ìĹĩìĿ¸ì§Ģ&#34;:5865,&#34;ĠëĳĺìĿ´&#34;:5866,&#34;Ġì»¤íĶĮ&#34;:5867,&#34;Ġê°ĸê³ł&#34;:5868,&#34;ëħĦëıĦìĹĲ&#34;:5869,&#34;ĠìĺĪìģĺê³ł&#34;:5870,&#34;íĭ°ë¹ĦìĹĲìĦľ&#34;:5871,&#34;¸Ķ&#34;:5872,&#34;íĻĶë©´&#34;:5873,&#34;ì§Ģë©´&#34;:5874,&#34;ìĭ¬ë&#34;:5875,&#34;ê²ĮìļĶ&#34;:5876,&#34;íķ´ì£¼ìĦ¸ìļĶ&#34;:5877,&#34;Ġê·¸ê²ĥëıĦ&#34;:5878,&#34;ĠíķĺëĬĺ&#34;:5879,&#34;Ġë´ħëĭĪëĭ¤&#34;:5880,&#34;ì§Ħìĭ¤&#34;:5881,&#34;ìŀĪìĹĪëĭ¤&#34;:5882,&#34;ìłĦë¬¸&#34;:5883,&#34;Ġì§ĢëĤĺëıĦ&#34;:5884,&#34;ëĵľìĭľ&#34;:5885,&#34;ĠëıĦìłĦ&#34;:5886,&#34;ĠìĪĺê³ł&#34;:5887,&#34;ĠìĥĿê°ģíķĺë©´&#34;:5888,&#34;ĠìķĦëĭĻëĭĪëĭ¤&#34;:5889,&#34;Ġë§Įëĵ¤ëĭ¤ëĭĪ&#34;:5890,&#34;ìĦłìĥĿ&#34;:5891,&#34;Ġìĵ°ëłĪê¸°ê°ĻìĿĢ&#34;:5892,&#34;Ġíĥģ&#34;:5893,&#34;ì¢ħìĿ¼&#34;:5894,&#34;ĠìĹ´ë°Ľ&#34;:5895,&#34;ĠìŀĲì²´ëĬĶ&#34;:5896,&#34;Ġì¹´ë¦¬ìĬ¤ë§Ī&#34;:5897,&#34;íĻįì½©&#34;:5898,&#34;ĠìĹ¬ê¸°ìĦľ&#34;:5899,&#34;Ġì°©íķľ&#34;:5900,&#34;ĠíĹĲë¦¬ìĽĥ&#34;:5901,&#34;ìķĪëĲ¨&#34;:5902,&#34;ëłĪë©ĺíĥĢ&#34;:5903,&#34;°ľ&#34;:5904,&#34;ëĿ&#34;:5905,&#34;ë©į&#34;:5906,&#34;ìľĮ&#34;:5907,&#34;ì¤¬ëĭ¤&#34;:5908,&#34;íĺĲ&#34;:5909,&#34;ìĹ¬íĸī&#34;:5910,&#34;Ġê¸°ëıħêµĲ&#34;:5911,&#34;ëķĮëĬĶ&#34;:5912,&#34;ĠìĿ¸ê¸°&#34;:5913,&#34;ĠìłľìĻ¸&#34;:5914,&#34;ëĥĲê³ł&#34;:5915,&#34;ë²ĪìĿĢ&#34;:5916,&#34;Ġë°°ìļ°ë¥¼&#34;:5917,&#34;Ġì¡°ìŀĳ&#34;:5918,&#34;Ġë°ĺê°ľëıĦ&#34;:5919,&#34;ĠíķłìĪĺ&#34;:5920,&#34;Ġìļ¸ì»¥&#34;:5921,&#34;ĠíĬ¹ìĿ´&#34;:5922,&#34;ìĿ´ê±´ëŃĲ&#34;:5923,&#34;Ġìļ°ë¦¬ìĿĺ&#34;:5924,&#34;ĠìłĦê°ľëıĦ&#34;:5925,&#34;ìŀ¡ê³ł&#34;:5926,&#34;ê¸°ëĮĢìķĪíķĺê³ł&#34;:5927,&#34;Ġìŀłê¹Ĳ&#34;:5928,&#34;Ġê¸´ìŀ¥ê°ĲìĿ´&#34;:5929,&#34;ĠêµŃëĤ´&#34;:5930,&#34;????????&#34;:5931,&#34;Ġëĵ£ê³ł&#34;:5932,&#34;en&#34;:5933,&#34;ª¼&#34;:5934,&#34;ëĬĶëĮĢ&#34;:5935,&#34;ĠìĿ´ëıĦ&#34;:5936,&#34;ë§Įëĭ¤&#34;:5937,&#34;ìŀĲëĵ¤ìĿĺ&#34;:5938,&#34;ĠëĤ´ìĿ¸ìĥĿ&#34;:5939,&#34;Ġê¸°ì¤Ģ&#34;:5940,&#34;ë¥´ê²Į&#34;:5941,&#34;Ġë§Įëĵ¤ë©´&#34;:5942,&#34;ë´¤ì§Ģë§Į&#34;:5943,&#34;Ġë°ĶëŀįëĭĪëĭ¤&#34;:5944,&#34;ëĸ¡&#34;:5945,&#34;ĠìĤ´ëł¤&#34;:5946,&#34;Ġíı´&#34;:5947,&#34;ìĶ¨ëĬĶ&#34;:5948,&#34;ĠíĸĪìľ¼ë©´&#34;:5949,&#34;ê¸ĢìİĦ&#34;:5950,&#34;íĺķëŀĺ&#34;:5951,&#34;ĠëĬĲëĤĮìĿĺ&#34;:5952,&#34;ëĶ°ìľĦ&#34;:5953,&#34;ê·¸ëŀĺíĶ½&#34;:5954,&#34;âĻ¥âĻ¥âĻ¥&#34;:5955,&#34;Ġì¶ľìĹ°ì§Ħ&#34;:5956,&#34;ìłĲëĮĢëĬĶ&#34;:5957,&#34;ĠìŀĲìĭłìĿ´&#34;:5958,&#34;ĠëĬĲê¼Īëĭ¤&#34;:5959,&#34;ë¦¬íı¬íĦ°&#34;:5960,&#34;ĠG&#34;:5961,&#34;Ġ~~&#34;:5962,&#34;ķëĭĪëĭ¤&#34;:5963,&#34;ì§ĢìļĶ&#34;:5964,&#34;ĠëĤĻ&#34;:5965,&#34;ĠìķĦíĶĪ&#34;:5966,&#34;ìĭľìĤ¬íļĮ&#34;:5967,&#34;ĠíķĺëĦ¤&#34;:5968,&#34;Ġëĭ¤ë¥¼&#34;:5969,&#34;ê±´ë§Į&#34;:5970,&#34;ĠìĬ¤íģ¬ë¦°&#34;:5971,&#34;ë¯¸ìĬ¤&#34;:5972,&#34;ê²ĥê°ĻìĿĢ&#34;:5973,&#34;ë¶ĦìĿĢ&#34;:5974,&#34;ĠìķĪìĹĲ&#34;:5975,&#34;ĠëĤ¨ìĿĢ&#34;:5976,&#34;íĬ¸ìĿĺ&#34;:5977,&#34;ì¢ĭìķĺëĭ¤&#34;:5978,&#34;ĠëģĿìĿ´&#34;:5979,&#34;Ġë´Ĳì¤Ħ&#34;:5980,&#34;ĠëħĦ&#34;:5981,&#34;íħĮìĿ´&#34;:5982,&#34;ĠìŀĳíĴĪìĦ±&#34;:5983,&#34;Ġíĥĳ&#34;:5984,&#34;Ġëĸ¨&#34;:5985,&#34;Ġíİ¼&#34;:5986,&#34;ĠìķĦìĿ´ëıĮ&#34;:5987,&#34;ĠìŀĶëľ©&#34;:5988,&#34;Ġë°ķì§Ħ&#34;:5989,&#34;ĠìĿ¼ë³¸ìĺģíĻĶ&#34;:5990,&#34;ĠìĿ´ìłľìĦľìķ¼&#34;:5991,&#34;ë³Ħë¡ľëĭ¤&#34;:5992,&#34;ĠìŀĬì§Ģ&#34;:5993,&#34;Ġë¹¼ë©´&#34;:5994,&#34;ëłĪìĿ´ìħĺ&#34;:5995,&#34;Ġëĭ¨ìĪľíķľ&#34;:5996,&#34;ë¶Īë¥ľ&#34;:5997,&#34;ost&#34;:5998,&#34;¥ë¯¸&#34;:5999,&#34;ãģ&#34;:6000,&#34;ìĿ´ìĬ¨&#34;:6001,&#34;íķĺêµ°&#34;:6002,&#34;Ġìĭ¬ë¦¬ë&#34;:6003,&#34;ìłĲìĿ´ìĥģ&#34;:6004,&#34;Ġìŀ¬ë°©&#34;:6005,&#34;ì²©&#34;:6006,&#34;Ġëĭ¤ë£¨&#34;:6007,&#34;ìŀ¥ìķł&#34;:6008,&#34;ëĵľë¥¼&#34;:6009,&#34;ëĵľë¦½ëĭĪëĭ¤&#34;:6010,&#34;Ġìµľê°ķ&#34;:6011,&#34;ĠëĮĢíĻĶ&#34;:6012,&#34;ì¤ĳìĿ´&#34;:6013,&#34;ê°Ļê³ł&#34;:6014,&#34;íıīìłĲìĿĦ&#34;:6015,&#34;ĠíķĺëĤĺê°ĻìĿ´&#34;:6016,&#34;ìĿ´ëŁ°ê±¸&#34;:6017,&#34;ìĻĶëĭ¤&#34;:6018,&#34;ëĭĪëİģ&#34;:6019,&#34;ĠìĦ¤ëĵĿ&#34;:6020,&#34;ĠìĿ¸ê°ĦìĿ´&#34;:6021,&#34;Ġë¶ĦìľĦê¸°ê°Ģ&#34;:6022,&#34;ĠìĭľëĮĢë¥¼&#34;:6023,&#34;ĠíĥĦìĥĿ&#34;:6024,&#34;íĦ¸&#34;:6025,&#34;ëĤ®ìĿĢ&#34;:6026,&#34;íĮĮìĿ´ìĸ´&#34;:6027,&#34;ĠìķĪíĥĢê¹Įìļ´&#34;:6028,&#34;íĵ¨íĦ°&#34;:6029,&#34;ĠëŁ°&#34;:6030,&#34;Ġg&#34;:6031,&#34;ìĸ´ì§Ģ&#34;:6032,&#34;ìĸ´ìĹ¬&#34;:6033,&#34;ëĿ¼ê°Ģ&#34;:6034,&#34;ìķĦëĭĪë&#34;:6035,&#34;íķ´ì£¼ê³ł&#34;:6036,&#34;ë¶Ļ&#34;:6037,&#34;ìĥģìĿĺ&#34;:6038,&#34;ìľ¼ë¡ľë§Į&#34;:6039,&#34;Ġìĸ´ì§Ģ&#34;:6040,&#34;ê°ģë³¸&#34;:6041,&#34;ìłľìĿ¼&#34;:6042,&#34;ĠìĹ°ê¸°ìŀĺ&#34;:6043,&#34;ĠìľĦë¡ľ&#34;:6044,&#34;ê°ľëĬĶ&#34;:6045,&#34;ĠìĥĿê°ģíĸĪëĬĶëį°&#34;:6046,&#34;ĠìĿ¸ëıĦ&#34;:6047,&#34;ìĹĪëĭ¤ëĬĶ&#34;:6048,&#34;ë²Ħëł¤&#34;:6049,&#34;ĠìĤ¬ê³ł&#34;:6050,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪìĸ´ìļĶ&#34;:6051,&#34;ĠìĤ¬ëŀĳìĿĢ&#34;:6052,&#34;ê²½ì°°&#34;:6053,&#34;ìĿ´ëŁ°ê±°&#34;:6054,&#34;Ġìĸ¼ë§Ī&#34;:6055,&#34;ëĤĺìĺ¤ê³ł&#34;:6056,&#34;Ġìļķë§Ŀ&#34;:6057,&#34;Ġëĭ¬ëĭ¬&#34;:6058,&#34;ë¯¸ëĦ¤ìĿ´íĦ°&#34;:6059,&#34;´íĵ¨íĦ°&#34;:6060,&#34;Ġãħĩ&#34;:6061,&#34;ì§Ģë§Ĳê³ł&#34;:6062,&#34;ê°ĢìĿĺ&#34;:6063,&#34;ê¸°ìłĦìĹĲ&#34;:6064,&#34;ëĤĺìģľ&#34;:6065,&#34;ë¦¬ìļ°ëĵľ&#34;:6066,&#34;ìłĲìĹĲìĦľ&#34;:6067,&#34;ìłĲë§ĮìłĲ&#34;:6068,&#34;ìļ°ìĹ°íŀĪ&#34;:6069,&#34;ìķ¼íķł&#34;:6070,&#34;ê³¼ìĦľ&#34;:6071,&#34;Ġê°Ģë©´&#34;:6072,&#34;Ġê°Ģë²¼&#34;:6073,&#34;ìĨĮë£¡&#34;:6074,&#34;Ġëĵľë¦¼&#34;:6075,&#34;ëħĦìŀĳ&#34;:6076,&#34;Ġë§İìķĦìĦľ&#34;:6077,&#34;Ġìĭłë¹Ħ&#34;:6078,&#34;ĠëĦĺê²Į&#34;:6079,&#34;ĠíĶ½&#34;:6080,&#34;Ġì½ĶëĤľ&#34;:6081,&#34;Ġë°°ìĹŃ&#34;:6082,&#34;Ġë§¤ëł¥ìĿĦ&#34;:6083,&#34;Ġì§ĳì°©&#34;:6084,&#34;Ġëª¨ëĳĲê°Ģ&#34;:6085,&#34;ëıĪìĿ´&#34;:6086,&#34;Ġëĵ±ëĵ±&#34;:6087,&#34;ë§Įíģ¼ìĿĢ&#34;:6088,&#34;Ġì§Īë¦¬ì§Ģ&#34;:6089,&#34;ê·ĢìĹ¬&#34;:6090,&#34;ĠëłĪìłĦëĵľ&#34;:6091,&#34;ĠëĨĵê³ł&#34;:6092,&#34;Ġì´Īë°ĺìĹĲ&#34;:6093,&#34;ìĹīìĦ±&#34;:6094,&#34;ì§§&#34;:6095,&#34;ëį°ìĿ´&#34;:6096,&#34;ëĦĪë&#34;:6097,&#34;ĠìķĦì¤Įë§Ī&#34;:6098,&#34;ìĺĪë&#34;:6099,&#34;ìĥĪë¡&#34;:6100,&#34;ìĺģíĻĶìŀħëĭĪëĭ¤&#34;:6101,&#34;ë²Ĺ&#34;:6102,&#34;ìŀ¥íķľ&#34;:6103,&#34;Ġê²ª&#34;:6104,&#34;ëĵľë¡ľ&#34;:6105,&#34;íĨ°&#34;:6106,&#34;ë¶Ģê°Ģ&#34;:6107,&#34;ìłĢìĺĪìĤ°&#34;:6108,&#34;ë¹Ĺ&#34;:6109,&#34;ì¡°ìłĪ&#34;:6110,&#34;Ġì¹Ń&#34;:6111,&#34;Ġê±°ê¸°ìĹĲ&#34;:6112,&#34;ì¤ĦìĿ´ìķ¼&#34;:6113,&#34;ëĭĺëĵ¤&#34;:6114,&#34;ĠìĤ´ë¦°&#34;:6115,&#34;ê²°íĺ¼&#34;:6116,&#34;ìĽłì§Ģë§Į&#34;:6117,&#34;ĠìĨĲë°ľìĿ´&#34;:6118,&#34;Ġëį°ë¦¬ê³ł&#34;:6119,&#34;ëįķíĻĶ&#34;:6120,&#34;ìĿ¸ì¤ĦìķĮ&#34;:6121,&#34;íģ¬ë¡ľ&#34;:6122,&#34;ĠìŀĶìĿ¸íķľ&#34;:6123,&#34;ë¹¨ë¦¬&#34;:6124,&#34;Ġê·¸ëħĢìĿĺ&#34;:6125,&#34;ëª°ìŀħëıĦ&#34;:6126,&#34;ìĸ´ëĶĶìĦľ&#34;:6127,&#34;Ġë±Ģ&#34;:6128,&#34;Ġp&#34;:6129,&#34;Ġì¸&#34;:6130,&#34;ìĹĲëĮĢ&#34;:6131,&#34;ìĭľë©´&#34;:6132,&#34;ìľ¼ë¦¬&#34;:6133,&#34;ë²ĪëįĶ&#34;:6134,&#34;ìŀĪìĬµëĭĪëĭ¤&#34;:6135,&#34;Ġíķľëªħ&#34;:6136,&#34;ëĵľë¦¼&#34;:6137,&#34;ĠìłĦìľ¨&#34;:6138,&#34;ëłĪìĿ¸&#34;:6139,&#34;Ġì£¼ê¸°ëıĦ&#34;:6140,&#34;ë¹ĦìĬ·&#34;:6141,&#34;Ġë¹Ħë°Ģ&#34;:6142,&#34;íĭ¸&#34;:6143,&#34;ìłģìĿ´ì§Ģ&#34;:6144,&#34;ìµľê³łìµľê³ł&#34;:6145,&#34;ĠìĿĺìĻ¸ë¡ľ&#34;:6146,&#34;ìĺĪê³łíİ¸&#34;:6147,&#34;Ġëĵ¤ê³ł&#34;:6148,&#34;ë°°ìļ°ìĿĺ&#34;:6149,&#34;ì©Ŀ&#34;:6150,&#34;ĠìĹ¬ìŀĲìĿĺ&#34;:6151,&#34;ãħľãħľãħľãħľ&#34;:6152,&#34;Ġìĵ°ê³ł&#34;:6153,&#34;Ġë¶Ģë¶ĦëıĦ&#34;:6154,&#34;Ġëĭµëĭµíķĺê³ł&#34;:6155,&#34;íķĺíķĺíķĺ&#34;:6156,&#34;Ġìĸ´ìļ¸ë¦¬ëĬĶ&#34;:6157,&#34;Ġìıĺ&#34;:6158,&#34;ìĿ´ìĺĢëĭ¤&#34;:6159,&#34;ìĿ´ìĹĪìĿĮ&#34;:6160,&#34;íķĺë£¨&#34;:6161,&#34;íķĺê¸¸&#34;:6162,&#34;Ġìķī&#34;:6163,&#34;ĠìĿ´ìĹ°ê±¸&#34;:6164,&#34;ëĭĪíį¼&#34;:6165,&#34;Ġë³´ëĭ¤ëĭĪ&#34;:6166,&#34;ìĥĪë¡ľ&#34;:6167,&#34;Ġê·¸ìŀĲì²´&#34;:6168,&#34;ê±°ëĵł&#34;:6169,&#34;ìŀĲë¡ľ&#34;:6170,&#34;ì§ľë¡ľ&#34;:6171,&#34;Ġë§ĮëĤ¨&#34;:6172,&#34;ìŀ¬ë¥¼&#34;:6173,&#34;ê²łëĭ¤ëĬĶ&#34;:6174,&#34;Ġë³´ê³łëĤĺë©´&#34;:6175,&#34;ĠìĿ¼ê¹¨&#34;:6176,&#34;ĠëŃĲëĿ¼&#34;:6177,&#34;ë°ĶìĿ´&#34;:6178,&#34;ìĿ¸ëį°ëıĦ&#34;:6179,&#34;ĠìķĦëĭĪìķ¼&#34;:6180,&#34;ì½Ķë©ĶëĶĶ&#34;:6181,&#34;ì©Ĳëĭ¤&#34;:6182,&#34;ìĽłìĸ´ìļĶ&#34;:6183,&#34;..........&#34;:6184,&#34;Ġì£¼ìĿ¸ê³µìĿĢ&#34;:6185,&#34;Ġë³´ê¸°ìĹĲëĬĶ&#34;:6186,&#34;Ġë§İìĿĢëį°&#34;:6187,&#34;Ġëĸ¨ìĸ´ì§Ģê³ł&#34;:6188,&#34;ĠíĿ¥ë¯¸ë¡ľ&#34;:6189,&#34;ĠìĤ´ìķĦê°ĢëĬĶ&#34;:6190,&#34;Ġìĭľë¦¬ì¦ĪëĬĶ&#34;:6191,&#34;Ġë§Įëĵ¤ìĹĪëĬĶì§Ģ&#34;:6192,&#34;ìķĦë¦Ħëĭµ&#34;:6193,&#34;Ġìĸ´ì©ĶìĪĺ&#34;:6194,&#34;Ġê³µíı¬ë¬¼&#34;:6195,&#34;Ġíļ¨ê³¼&#34;:6196,&#34;íĻĶëł¤íķľ&#34;:6197,&#34;vd&#34;:6198,&#34;ĪìĿ´&#34;:6199,&#34;ľ©&#34;:6200,&#34;ìĿ´ìĺģíĻĶê°Ģ&#34;:6201,&#34;ĠìĺģíĻĶíĻĶ&#34;:6202,&#34;ĠìĺģíĻĶë³´ëĬĶ&#34;:6203,&#34;ëį´&#34;:6204,&#34;ëĬĶëį°ëıĦ&#34;:6205,&#34;Ġë´ī&#34;:6206,&#34;ìłģìŀĦ&#34;:6207,&#34;ê·¸ëħĢ&#34;:6208,&#34;Ġë´¤ìĿĦê¹Į&#34;:6209,&#34;Ġê°Ģì§Ģ&#34;:6210,&#34;ë¶Ģë¡ľ&#34;:6211,&#34;Ġìľ¡&#34;:6212,&#34;Ġë¬´ìĪł&#34;:6213,&#34;ĠìĥĿê°ģíķł&#34;:6214,&#34;ìĺĢìĸ´&#34;:6215,&#34;ë¹Ľ&#34;:6216,&#34;Ġíķ´ìĦĿ&#34;:6217,&#34;ĠìĽĥê²¼&#34;:6218,&#34;Ġì°¸ìľ¼ë¡ľ&#34;:6219,&#34;ëŃĲê°Ģ&#34;:6220,&#34;ĠíĺĦìĭ¤ìĦ±&#34;:6221,&#34;íıīë²Ķ&#34;:6222,&#34;Ġëª°ìŀħê°Ĳ&#34;:6223,&#34;Ġìľłì¹ĺíķ¨&#34;:6224,&#34;ë°Ľê³ł&#34;:6225,&#34;Ġê¸¸ê²Į&#34;:6226,&#34;ĠìĦ¸ìĥģìĿĦ&#34;:6227,&#34;ĠìķĦë¦Ħëĭµê²Į&#34;:6228,&#34;ìĭ¸ìĽĢ&#34;:6229,&#34;Ġì§Ħë¶Ģíķĺê³ł&#34;:6230,&#34;ĠíĮ¨ëŁ¬ëĶĶ&#34;:6231,&#34;Ġì¢ĭê²łëĦ¤ìļĶ&#34;:6232,&#34;Ġì¿ł&#34;:6233,&#34;Ġê°ĲìłķìĿ´ìŀħ&#34;:6234,&#34;as&#34;:6235,&#34;cg&#34;:6236,&#34;Ĩĵ&#34;:6237,&#34;Ġëĳ&#34;:6238,&#34;ãħĬ&#34;:6239,&#34;ì§Ģê¸Īë´ĲëıĦ&#34;:6240,&#34;ĠìŀĪëĦ¤&#34;:6241,&#34;ĠìĿ´ëŁ´&#34;:6242,&#34;ìĸ´ë²Ħë¦°&#34;:6243,&#34;ëĭĪê°Ģ&#34;:6244,&#34;ìķĦëĵ¤&#34;:6245,&#34;ìĿ¸ëĵ¤ìĿĺ&#34;:6246,&#34;êµīìŀ¥íŀĪ&#34;:6247,&#34;íķ´ì§Ģê³ł&#34;:6248,&#34;Ġê·¸ë§Įíģ¼&#34;:6249,&#34;ìĬ¤íĤ¤&#34;:6250,&#34;ìłķíĻĶ&#34;:6251,&#34;ìļ°ê°Ģ&#34;:6252,&#34;ìĥģëĭ¹íŀĪ&#34;:6253,&#34;ĠìĭľíĤ¤&#34;:6254,&#34;Ġê¸°ë°ľ&#34;:6255,&#34;êµŃìĺģ&#34;:6256,&#34;ĠìĥĿê°ģìľ¼ë¡ľ&#34;:6257,&#34;Ġê°ľë§īìŀ¥&#34;:6258,&#34;ëĲĺë©´&#34;:6259,&#34;Ġì¡°ìŀ¡&#34;:6260,&#34;Ġë´¤ëĬĶëį°ëıĦ&#34;:6261,&#34;Ġíķłë¨¸ëĭĪ&#34;:6262,&#34;Ġì£½ìĿ´ëĬĶ&#34;:6263,&#34;ìĺĪìłĦ&#34;:6264,&#34;íĮĲíĥĢì§Ģ&#34;:6265,&#34;Ġë¬µ&#34;:6266,&#34;ĠìĪł&#34;:6267,&#34;ĠëıĻìĭľìĹĲ&#34;:6268,&#34;ĠìĿ´ê±°ë³´ê³ł&#34;:6269,&#34;ĠëıĪìĿĦ&#34;:6270,&#34;ìĽłìĬµëĭĪëĭ¤&#34;:6271,&#34;ĠìĿ´ìĥģìĿĺ&#34;:6272,&#34;íĽĪíĽĪ&#34;:6273,&#34;Ġíķľë²ĪëıĦ&#34;:6274,&#34;íķľêµŃìĺģíĻĶ&#34;:6275,&#34;Ġëħ¸ëŀĺëıĦ&#34;:6276,&#34;íķ´ìķ¼ì§Ģ&#34;:6277,&#34;Ġë³Ģíĥľ&#34;:6278,&#34;ĠëŃĺê¹Į&#34;:6279,&#34;ĠìŀĲìĭłìĿĦ&#34;:6280,&#34;ĠëĵľëĿ¼ë§ĪëĿ¼&#34;:6281,&#34;ĠíĮ¬ìĿ´&#34;:6282,&#34;ëĮĦ&#34;:6283,&#34;Ġìľłëªħíķľ&#34;:6284,&#34;¤ì¼Ģ&#34;:6285,&#34;Ġ=&#34;:6286,&#34;Ġy&#34;:6287,&#34;ëĬĶëĵ¯&#34;:6288,&#34;ê°ĢëĦ¤&#34;:6289,&#34;ĠìĺģíĻĶìĿ¸ê°Ģ&#34;:6290,&#34;ìĸ´ê°ĢëĬĶ&#34;:6291,&#34;ĠìķĦëıĻ&#34;:6292,&#34;ìłĲì£¼ê³ł&#34;:6293,&#34;Ġëĭĺ&#34;:6294,&#34;ìĥģíķľ&#34;:6295,&#34;ìĺ¤ë¸Į&#34;:6296,&#34;ëħķ&#34;:6297,&#34;Ġìĸ´ëĳĲ&#34;:6298,&#34;Ġì§ĢëĤ¬&#34;:6299,&#34;ëĥĪëĭ¤&#34;:6300,&#34;Ġìĭľê±¸&#34;:6301,&#34;ĠìĬ¤íĮĮìĿ´&#34;:6302,&#34;ì¹ĺëħ¸&#34;:6303,&#34;íĸĪì§Ģ&#34;:6304,&#34;Ġëª¨íĹĺ&#34;:6305,&#34;Ġë³¸ê²Į&#34;:6306,&#34;ë¹¡&#34;:6307,&#34;Ġê°ĲëıĻìĿĺ&#34;:6308,&#34;ë¦¬ëĦ¤ìļĶ&#34;:6309,&#34;Ġì¡°ìłĪ&#34;:6310,&#34;Ġëĭ¤ìĭľëĬĶ&#34;:6311,&#34;ĠìĤ´ëĭ¤&#34;:6312,&#34;ĠíķĺëĤĺë¡ľ&#34;:6313,&#34;ĠìĺĪê³ł&#34;:6314,&#34;ê²¨ìļ¸&#34;:6315,&#34;íĶ¼ìĬ¤&#34;:6316,&#34;ĠìĿ´ê±´ëŃĲ&#34;:6317,&#34;ĠìķĦê¹ĮìĽłëĭ¤&#34;:6318,&#34;Ġê¹Ģê¸°ëįķ&#34;:6319,&#34;Ġìĭ¬ì§Ģìĸ´&#34;:6320,&#34;ìĹĩëĬĶëį°&#34;:6321,&#34;ìŀ¬ë¯¸ìŀĪê³ł&#34;:6322,&#34;Ġëªĩë²ĪìĿĦ&#34;:6323,&#34;ĠìĹ°ê¸°ëł¥ìĿĢ&#34;:6324,&#34;Ġì¹ĺë°Ģ&#34;:6325,&#34;ìĵ°ê³ł&#34;:6326,&#34;ĠìĿĺë¯¸ê°Ģ&#34;:6327,&#34;Ġëĭµëĭµíķľ&#34;:6328,&#34;ì¡¸ëĿ¼&#34;:6329,&#34;ê°ĳëĭĪëĭ¤&#34;:6330,&#34;ê±°ê°ĻìķĦìļĶ&#34;:6331,&#34;ĠìĿĢê·¼&#34;:6332,&#34;ëļĿ&#34;:6333,&#34;Ġëıĭë³´ìĿ´ëĬĶ&#34;:6334,&#34;Ġê²¨ìļ¸&#34;:6335,&#34;ë¬ĺíķľ&#34;:6336,&#34;ĠëĤĺë¨¸ì§Ģ&#34;:6337,&#34;ĠìłĦë¬¸ê°Ģ&#34;:6338,&#34;ë¢°ë§¤&#34;:6339,&#34;ë·Ķ&#34;:6340,&#34;ut&#34;:6341,&#34;Ġ;&#34;:6342,&#34;Ġë¥¼&#34;:6343,&#34;íĻĶìĿ´íĮħ&#34;:6344,&#34;Ġíľĺ&#34;:6345,&#34;ìłĲìľ¼ë¡ľ&#34;:6346,&#34;ìĭľìĺ¤&#34;:6347,&#34;ìŀĲìĻĢ&#34;:6348,&#34;ìĹĨê²Į&#34;:6349,&#34;Ġíķĺê¸¸ëŀĺ&#34;:6350,&#34;ì¹Ń&#34;:6351,&#34;ìŀĳìĿĢ&#34;:6352,&#34;íĥĳ&#34;:6353,&#34;Ġìŀ¬ë¯¸ìĻĢ&#34;:6354,&#34;Ġê°Ģê³ł&#34;:6355,&#34;Ġê°Ģë¥´&#34;:6356,&#34;íķĺëĬĶê²ĥ&#34;:6357,&#34;ëŀĺê³¤&#34;:6358,&#34;ì¤ĳìĹĲìĦľ&#34;:6359,&#34;Ġì£¼ê¸°&#34;:6360,&#34;ëķĮë¬¸&#34;:6361,&#34;ëĵłì§Ģ&#34;:6362,&#34;Ġë³¸ê±°&#34;:6363,&#34;ĠìĿ´ëŁ°ìĺģíĻĶë¥¼&#34;:6364,&#34;ìĭ¬ìľ¼ë¡ľ&#34;:6365,&#34;ĠìĬ¤íĨłë¦¬ìĿĺ&#34;:6366,&#34;Ġì¤ĳìĹĲìĦľ&#34;:6367,&#34;Ġê±°ë¶Ģ&#34;:6368,&#34;ĠìĺģêµŃ&#34;:6369,&#34;ìľĦìĹĲ&#34;:6370,&#34;ë´Ĳì£¼&#34;:6371,&#34;Ġë¶Ħëĵ¤ìĿ´&#34;:6372,&#34;ĠëıĦë§Ŀ&#34;:6373,&#34;Ġìŀ¥ë©´ëıĦ&#34;:6374,&#34;ĠëªħìŀĳìĿĦ&#34;:6375,&#34;ë³µìĿĦ&#34;:6376,&#34;Ġêµ¬ìĦ±ìĿ´&#34;:6377,&#34;ĠëĲ©ëĭĪëĭ¤&#34;:6378,&#34;ĠëĤ®ëĦ¤&#34;:6379,&#34;íıīìĿ´&#34;:6380,&#34;ëĬĶê±°ìķ¼&#34;:6381,&#34;Ġë³´ì§Ģë§ĪëĿ¼&#34;:6382,&#34;ĠìĨĲìĹĲ&#34;:6383,&#34;ĠëĶĶíħĮìĿ¼&#34;:6384,&#34;ê±´ê°ĢìļĶ&#34;:6385,&#34;Ġê´ľì°®ìĿĢëį°&#34;:6386,&#34;ĠìĿ½ìĸ´&#34;:6387,&#34;Ġì§ĢëĤĺì¹ĺê²Į&#34;:6388,&#34;łìłģìľ¼ë¡ľ&#34;:6389,&#34;ëĬ¦ê²Į&#34;:6390,&#34;ì§ĢëĦ¤&#34;:6391,&#34;ëıĦë¥¼&#34;:6392,&#34;ĠìĹł&#34;:6393,&#34;ëĤĺëĭĪ&#34;:6394,&#34;ë³´ìĺģ&#34;:6395,&#34;ìĽĶìĿ´&#34;:6396,&#34;íķĺê³łìĭ¶&#34;:6397,&#34;ê·¸ëŁ¼&#34;:6398,&#34;ê²łì£ł&#34;:6399,&#34;ìķħíķľ&#34;:6400,&#34;ĠìŀĲìĭĿ&#34;:6401,&#34;Ġìľłì§Ģ&#34;:6402,&#34;ì¢ĭëĦ¤ìļĶ&#34;:6403,&#34;Ġê°ĲìķĪ&#34;:6404,&#34;Ġìĺģìĸ´&#34;:6405,&#34;Ġìĺģíĸ¥&#34;:6406,&#34;!!!!!!&#34;:6407,&#34;íĭ°ì¦Į&#34;:6408,&#34;ĠìĿ´ìķ¼ê¸°ëĬĶ&#34;:6409,&#34;Ġë³Ħë¡ľê³ł&#34;:6410,&#34;Ġìĭ¬íķĺê²Į&#34;:6411,&#34;ëĵ±ìŀ¥&#34;:6412,&#34;ë°ĽëĬĶ&#34;:6413,&#34;ìĤ¬ë¡ľ&#34;:6414,&#34;ĠìĥĿíĻľ&#34;:6415,&#34;Ġë°ĺìłĦìĿĢ&#34;:6416,&#34;Ġë°ĽìĿĢ&#34;:6417,&#34;Ġìĵ°ëĬĶ&#34;:6418,&#34;ĠìĶ¹&#34;:6419,&#34;ëĪĦëĤĺ&#34;:6420,&#34;Ġëį°ìĿ´&#34;:6421,&#34;ĠíĽĮë¥Ńíķĺëĭ¤&#34;:6422,&#34;Ġë¬´ìĦľìĽĮ&#34;:6423,&#34;ë¶Ģë¶ĦìĿ´&#34;:6424,&#34;ĠíĻķìĭ¤&#34;:6425,&#34;ìłľëª©ìĿ´&#34;:6426,&#34;ĠêµĲíĽĪìĿĦ&#34;:6427,&#34;Ġë³¼ìĪĺìŀĪëĬĶ&#34;:6428,&#34;ìĭ«ìĸ´&#34;:6429,&#34;ëĵľëĿ¼ë§Īë¥¼&#34;:6430,&#34;ĠìĺģìĽĲíŀĪ&#34;:6431,&#34;ĠDVD&#34;:6432,&#34;ĹĦ&#34;:6433,&#34;ê¸°ê¹Įì§Ģ&#34;:6434,&#34;ìłĲìłĲ&#34;:6435,&#34;ìĭľë¦¬&#34;:6436,&#34;ë³´ìĿ´ëĬĶ&#34;:6437,&#34;Ġ13&#34;:6438,&#34;íķĺëĬĶìĺģíĻĶ&#34;:6439,&#34;ĠëĮĢì¤ĳ&#34;:6440,&#34;íĸĪìĹĪëĬĶëį°&#34;:6441,&#34;ĠìķĪìĬµ&#34;:6442,&#34;ê°ľê°Ģ&#34;:6443,&#34;Ġëª»íķĺëĭ¤&#34;:6444,&#34;Ġìķłëĵ¤ìĿĢ&#34;:6445,&#34;ê¸´íķĺì§Ģë§Į&#34;:6446,&#34;ĠìĽĥê³ł&#34;:6447,&#34;Ġìŀ¬ë¯¸ìĹĨìĹĪëĭ¤&#34;:6448,&#34;ë°ĶëŀĮ&#34;:6449,&#34;íĹĪë¬´&#34;:6450,&#34;ĠëķĮëł¤&#34;:6451,&#34;ëĬĲê»´&#34;:6452,&#34;ĠëªħìŀĳìŀħëĭĪëĭ¤&#34;:6453,&#34;ì©ľ&#34;:6454,&#34;Ġìŀ¼ëĤĺê²Į&#34;:6455,&#34;ìĥĿê°ģìĿ´&#34;:6456,&#34;ĠìŀĪëĭ¤ëĭĪ&#34;:6457,&#34;Ġì§ľì¦ĿëĤĺìĦľ&#34;:6458,&#34;íĪ¬ë¦¬&#34;:6459,&#34;Ġì§ĳìĹĲ&#34;:6460,&#34;Ġê²°ë§ĲìĿĦ&#34;:6461,&#34;Ġëĭ´ìķĦ&#34;:6462,&#34;ĠìĤ¶ìĹĲ&#34;:6463,&#34;ìĺ¬íķ´&#34;:6464,&#34;ìŀĸìķĦìļĶ&#34;:6465,&#34;Ġë²ĶìĿ¸&#34;:6466,&#34;Ġì£¼ìłľë¥¼&#34;:6467,&#34;Ġìĥīëĭ¤ë¥¸&#34;:6468,&#34;ãħħãħĤ&#34;:6469,&#34;Ġíĺ¹ìĿĢ&#34;:6470,&#34;íĢ´&#34;:6471,&#34;ĠìĨĮì¤ĳíķľ&#34;:6472,&#34;ĠëĤĺìģĺì§Ģ&#34;:6473,&#34;íĮ©íĬ¸&#34;:6474,&#34;ëī´&#34;:6475,&#34;ìĿ´ìĸ´&#34;:6476,&#34;ìĸ´íľ´&#34;:6477,&#34;ĠëĤŃ&#34;:6478,&#34;ë§ĮëıĦ&#34;:6479,&#34;ëĿ¼ìĿĺ&#34;:6480,&#34;Ġë³´ëįĺ&#34;:6481,&#34;ìĭľì¹´&#34;:6482,&#34;ĲëıĻ&#34;:6483,&#34;Ġì°°&#34;:6484,&#34;ĠëĮĢìĤ¬ëĵ¤&#34;:6485,&#34;íĸĪìĿĦëķĮ&#34;:6486,&#34;íĮł&#34;:6487,&#34;ì¤ĳíĽĪ&#34;:6488,&#34;Ġë¬´ë¦¬&#34;:6489,&#34;ë¶ĦìĿĺ&#34;:6490,&#34;ĠëĤ¨ëĦ¤ìļĶ&#34;:6491,&#34;Ġìľłë°ľ&#34;:6492,&#34;ĠìĤ¬ì§Ħ&#34;:6493,&#34;íĶĦëĭĿ&#34;:6494,&#34;Ġìĭľê°Ħê°ĢëĬĶì¤Ħ&#34;:6495,&#34;íĽĦìĿĺ&#34;:6496,&#34;Ġê°ķíķľ&#34;:6497,&#34;ìĽĥê¹Ģ&#34;:6498,&#34;Ġìĭ¬ìĺ¤&#34;:6499,&#34;ëĵľëĿ¼ë§ĪëĬĶ&#34;:6500,&#34;ĠëĤ®ì§Ģ&#34;:6501,&#34;ĠìłģìłĪ&#34;:6502,&#34;ĠìķĦìī½ì§Ģë§Į&#34;:6503,&#34;ĠíĿ¥ë¯¸ë¡ľìļ´&#34;:6504,&#34;ĠìĽĥê¸°ì§ĢëıĦ&#34;:6505,&#34;Ġìĸ´ìĥīíķĺê³ł&#34;:6506,&#34;ì»¤íĶĮ&#34;:6507,&#34;ê¹ĬìĿĢ&#34;:6508,&#34;Ġì¶ĶìĸµìĿ´&#34;:6509,&#34;ê°ĳìŀĲê¸°&#34;:6510,&#34;Ġì§ĳì¤ĳìĿ´&#34;:6511,&#34;íķĺìŀĲëĬĶ&#34;:6512,&#34;ë¿Į&#34;:6513,&#34;50&#34;:6514,&#34;Ġ007&#34;:6515,&#34;ìĿ´ìĹ°ê±¸&#34;:6516,&#34;íķĺëŁ¬&#34;:6517,&#34;ê°ĢìĹĲ&#34;:6518,&#34;ĠìĹ½&#34;:6519,&#34;Ġë§¹&#34;:6520,&#34;ìĸ´ëĤľ&#34;:6521,&#34;ìķĦì¹ĺ&#34;:6522,&#34;ĠìķĦìĹŃ&#34;:6523,&#34;ĮĢìĿĺ&#34;:6524,&#34;ìĭľê±¸&#34;:6525,&#34;ìļ°ìļ¸&#34;:6526,&#34;ìĪĺë¡ľ&#34;:6527,&#34;ìĪĺìŀĳ&#34;:6528,&#34;ìĥģìĪĺ&#34;:6529,&#34;ãħłãħľ&#34;:6530,&#34;Ġìĸ´ëĳ&#34;:6531,&#34;Ġë´¤ëĦ¤&#34;:6532,&#34;ë¶Ģë¥¼&#34;:6533,&#34;ĠìĪĺìŀħ&#34;:6534,&#34;ë¶Ħíķľ&#34;:6535,&#34;ìĦ¸íı¬&#34;:6536,&#34;ê°ĲìĤ¬&#34;:6537,&#34;Ġê°Ļê³ł&#34;:6538,&#34;Ġìµľê³łìĺĢëĭ¤&#34;:6539,&#34;íİ¸ì§ĳ&#34;:6540,&#34;ìĺĢëĦ¤&#34;:6541,&#34;ìĺĢìľ¼ë©´&#34;:6542,&#34;Ġë³¼ê±°ë¦¬&#34;:6543,&#34;ìłĢëŁ°&#34;:6544,&#34;ìĦłìĿĦ&#34;:6545,&#34;Ġìĭ¶ìĸ´ìļĶ&#34;:6546,&#34;íĺĦìŀ¬&#34;:6547,&#34;íıīìłĲìĿĢ&#34;:6548,&#34;ê°ĻìĿĢìĺģíĻĶ&#34;:6549,&#34;ĠìķĦëĭĪì§Ģ&#34;:6550,&#34;ì¦ĪìĿĺ&#34;:6551,&#34;Ġë§ĮëĵłìĺģíĻĶ&#34;:6552,&#34;ĠìĿ´ìĥģíķĺê²Į&#34;:6553,&#34;ë¹łì§Ħ&#34;:6554,&#34;ìŁģìĿ´&#34;:6555,&#34;Ġëĭ´ê²¨&#34;:6556,&#34;Ġë²łëĵľ&#34;:6557,&#34;Ġê°ĳëĭĪëĭ¤&#34;:6558,&#34;Ġìĸĳìĭ¬&#34;:6559,&#34;ëĬĲëĤĮìĿ´&#34;:6560,&#34;íĸĩëĭ¤&#34;:6561,&#34;Ġìĸ´ëĬĲìłķëıĦ&#34;:6562,&#34;Ġì²«ìĤ¬ëŀĳ&#34;:6563,&#34;Ġì°©ê°ģ&#34;:6564,&#34;Ġìĭľì²Ńë¥łìĿ´&#34;:6565,&#34;Ġë¿Į&#34;:6566,&#34;-^&#34;:6567,&#34;°Ķ&#34;:6568,&#34;ìĮ&#34;:6569,&#34;ãħĵ&#34;:6570,&#34;ìĦ¬&#34;:6571,&#34;ëıĦìķĦëĭĪê³ł&#34;:6572,&#34;ĠìĺģíĻĶìĻĢ&#34;:6573,&#34;ĠìķĦê¸°&#34;:6574,&#34;ê¹ľ&#34;:6575,&#34;ë§Īì¹ĺ&#34;:6576,&#34;Ġíķĺëĭ¤ê°Ģ&#34;:6577,&#34;ìŀĪìĿĦ&#34;:6578,&#34;ì°¡&#34;:6579,&#34;ìĦ±ê¸°&#34;:6580,&#34;Ġìŀ¬ë°Įëĭ¤ëĬĶ&#34;:6581,&#34;Ġê°ĲëıĻìĿĢ&#34;:6582,&#34;Ġê°ĲëıĻìłģìĿ´ëĭ¤&#34;:6583,&#34;ë°ķíķľ&#34;:6584,&#34;ëĤ¨ì£¼&#34;:6585,&#34;íĹĲë¦¬&#34;:6586,&#34;ĠìĺĪëĬ¥&#34;:6587,&#34;ëĬĲëģ¼&#34;:6588,&#34;ĠëĳĲë²Īì§¸&#34;:6589,&#34;ĠíĻĶëģĪ&#34;:6590,&#34;ĠìłĲìĪĺê°Ģ&#34;:6591,&#34;ĠëĤ®ìķĦ&#34;:6592,&#34;Ġë§¤ëł¥ìłģìĿ´&#34;:6593,&#34;ìĬ¤ëŁ½ì§Ģ&#34;:6594,&#34;ìŀ¬ë¯¸ìŀĪìĿĮ&#34;:6595,&#34;ĠìķĦëĭĪëĿ¼ê³ł&#34;:6596,&#34;Ġìŀ¬ëĬ¥&#34;:6597,&#34;Ġë³´ì§Ģë§Ĳ&#34;:6598,&#34;ì´ĪìĹĲ&#34;:6599,&#34;ĠìĨĲìĥī&#34;:6600,&#34;ìĹ¬ìŀĲê°Ģ&#34;:6601,&#34;ìĬ¬íĶĦ&#34;:6602,&#34;Ġìĸ´ìĦ¤íĶĦê³ł&#34;:6603,&#34;ëĿ¼ìĿ´ìĸ¸&#34;:6604,&#34;Ġê´Ģê°ĿìĿĦ&#34;:6605,&#34;Ġê¶ģê¸Īíķĺëĭ¤&#34;:6606,&#34;ë°ĺìłĦìĿ´&#34;:6607,&#34;ĠìĹīìĦ±íķĺê³ł&#34;:6608,&#34;Ġsf&#34;:6609,&#34;Ġì½ľ&#34;:6610,&#34;Ġìĭ¤ìłľë¡ľ&#34;:6611,&#34;ĠíĹ¤ìĸ´&#34;:6612,&#34;ìĻłì§Ģ&#34;:6613,&#34;ĠìĹŃëĮĢê¸ī&#34;:6614,&#34;?...&#34;:6615,&#34;es&#34;:6616,&#34;±ìłķ&#34;:6617,&#34;ĠE&#34;:6618,&#34;ì§ĢíĺĦ&#34;:6619,&#34;ìľĪ&#34;:6620,&#34;ìĬ¤ìĹĲ&#34;:6621,&#34;Ġìĺ¥&#34;:6622,&#34;ĠìĺĽ&#34;:6623,&#34;ìĹ¬ì§Ħ&#34;:6624,&#34;ìĿ¼ìĿĦ&#34;:6625,&#34;ìĿ¼ëĵ¯&#34;:6626,&#34;íķ¨ëıĦ&#34;:6627,&#34;ĠìķĬìķĦìĦľ&#34;:6628,&#34;Ġë¬´íķľ&#34;:6629,&#34;ĠëĤ´ìļ©ê³¼&#34;:6630,&#34;ì¡°ìĹ°&#34;:6631,&#34;ĠìľĦëĮĢ&#34;:6632,&#34;ë°©ê¸Ī&#34;:6633,&#34;ĠìĹĲíľ´&#34;:6634,&#34;Ġë³´ê¸°ìĹĲ&#34;:6635,&#34;ĠëĨĴìķĦìĦľ&#34;:6636,&#34;ì£¼ìĿ¸ê³µìĿĺ&#34;:6637,&#34;ìĸ¼ë§ĪëĤĺ&#34;:6638,&#34;Ġëªĩëªĩ&#34;:6639,&#34;ìĸ´ìķ¼ì§Ģ&#34;:6640,&#34;Ġì¦Ĳê²¨&#34;:6641,&#34;ĠêµĲê³¼ìĦľ&#34;:6642,&#34;Ġëª©ìĪ¨&#34;:6643,&#34;Ġíĺķëĭĺ&#34;:6644,&#34;ĠìĽĥê¸°ëĭ¤&#34;:6645,&#34;Ġë¬´ìĦŃì§ĢëıĦ&#34;:6646,&#34;ĠíĻķìĿ¸&#34;:6647,&#34;Ġì°¾ìķĦë³¼&#34;:6648,&#34;Ġì§ĳì¤ĳíķ´ìĦľ&#34;:6649,&#34;Ġë¸Ķë¡Ŀë²ĦìĬ¤íĦ°&#34;:6650,&#34;ëĨĴìĿĢ&#34;:6651,&#34;ĠìĺģíĻĺëį°&#34;:6652,&#34;Ġëĭ¤ìĸĳíķľ&#34;:6653,&#34;²Ī&#34;:6654,&#34;íĩ´&#34;:6655,&#34;ëĭ¬ë&#34;:6656,&#34;ìłĬ&#34;:6657,&#34;ëıĮìĿ´&#34;:6658,&#34;íķĺëįĶëĿ¼&#34;:6659,&#34;ĠëĤ¬&#34;:6660,&#34;Ġë³´ìķĺëĬĶëį°&#34;:6661,&#34;ë©´ìĹĲìĦľ&#34;:6662,&#34;ë°į&#34;:6663,&#34;ìłĦìĦ¤&#34;:6664,&#34;ĠìŀĪëĭ¤ê³ł&#34;:6665,&#34;Ġì§Ģê¸Īë³´&#34;:6666,&#34;Ġíķľëĭ¤ëĬĶ&#34;:6667,&#34;ì¹ĺê³¤&#34;:6668,&#34;Ġì£¼ë§Ĳ&#34;:6669,&#34;ê°ĻìķĦ&#34;:6670,&#34;Ġë§Ĳìķĺ&#34;:6671,&#34;Ġë³¼ëķĮ&#34;:6672,&#34;Ġì¡°ìļ©&#34;:6673,&#34;ëĨį&#34;:6674,&#34;íķĺì§Ģë§ĪëĿ¼&#34;:6675,&#34;Ġê²°ì½Ķ&#34;:6676,&#34;ĠëĤĺìĺ¤ëĦ¤&#34;:6677,&#34;ĠìķĦê¹Ŀê³ł&#34;:6678,&#34;ì¼°&#34;:6679,&#34;Ġì§ľì§ĳ&#34;:6680,&#34;ĠëĳĲëł¤&#34;:6681,&#34;ĠìĿ´ìĥģíķĺê³ł&#34;:6682,&#34;ĠëĤ®ëĭ¤&#34;:6683,&#34;ĠìĦłëıĻ&#34;:6684,&#34;ĠìŀĶíĺ¹&#34;:6685,&#34;Ġê°ĢìĬ´ìĿĦ&#34;:6686,&#34;ì¢ĭìĿĢëį°&#34;:6687,&#34;ìŀĩëĬĶ&#34;:6688,&#34;Ġìĭ¸ìļ°ëĬĶ&#34;:6689,&#34;ìĹ¬ëŁ¬&#34;:6690,&#34;ëĤ¸ëĭ¤&#34;:6691,&#34;Ġìĸ´ìĦ¤íĶĦ&#34;:6692,&#34;ëĺĲíķľ&#34;:6693,&#34;Ġëª°ìŀħëıĦê°Ģ&#34;:6694,&#34;ĠìķĪëĲł&#34;:6695,&#34;ìķŀìľ¼ë¡ľ&#34;:6696,&#34;ë¹¼ê³ł&#34;:6697,&#34;ĠìĬ¤íĬ¸ëłĪìĬ¤&#34;:6698,&#34;ĠìĿ¼ê¹¨ìĽĮ&#34;:6699,&#34;ìĿ´ìłł&#34;:6700,&#34;ë§Īë¬´&#34;:6701,&#34;ê³łìĥĿ&#34;:6702,&#34;ĠíĮĢ&#34;:6703,&#34;ë¡¤&#34;:6704,&#34;ìĺ¬ëĿ¼&#34;:6705,&#34;ë§Īê°Ģ&#34;:6706,&#34;ìĹĪêµ¬ëĤĺ&#34;:6707,&#34;Ġëĭ¤ìĦ¯&#34;:6708,&#34;ìĪĺíĺĦ&#34;:6709,&#34;ìŀ¥ìĿĺ&#34;:6710,&#34;ëĵľë¦¬&#34;:6711,&#34;Ġ11&#34;:6712,&#34;Ġìŀ¬ë°Įìĸ´ìĦľ&#34;:6713,&#34;ê°ĻìķĦìļĶ&#34;:6714,&#34;ìŀ¬ë¯¸ìĹĨê³ł&#34;:6715,&#34;ëıĻíĻĶ&#34;:6716,&#34;ĠëŃĲíķĺëĤĺ&#34;:6717,&#34;ìĨįìľ¼ë¡ľ&#34;:6718,&#34;ë§Īë¥¼&#34;:6719,&#34;ìĹ°ê¸°ë¥¼&#34;:6720,&#34;Ġëª°ëŀĲëĭ¤&#34;:6721,&#34;ĠìĿ´íķ´ë¥¼&#34;:6722,&#34;ĠëĺĲëĭ¤ë¥¸&#34;:6723,&#34;Ġê´Ģëł¨&#34;:6724,&#34;ĠìľĦíĹĺ&#34;:6725,&#34;Ġì§ľì¦Ŀë§Į&#34;:6726,&#34;ì£½ëĬĶ&#34;:6727,&#34;Ġìĸµì§ĢìĬ¤ëŁ¬ìļ´&#34;:6728,&#34;ĠëĤĺìĻĶëĭ¤&#34;:6729,&#34;ìŀ¬ë°ĭê²Į&#34;:6730,&#34;ìŀ¬ë°ĭìĸ´ìļĶ&#34;:6731,&#34;Ġìĸ´ì©ľ&#34;:6732,&#34;ĠíĮĲëĭ¨&#34;:6733,&#34;ĠëıĮìķĦê°Ģ&#34;:6734,&#34;ëªĩë²ĪìĿĦ&#34;:6735,&#34;Ġë§ĲëıĦìķĪëĲĺëĬĶ&#34;:6736,&#34;ëİħ&#34;:6737,&#34;Ġë°Ģëł¤&#34;:6738,&#34;ë©ĶìĿ´ëĵľ&#34;:6739,&#34;ĠìĹ°ìĺĪìĿ¸&#34;:6740,&#34;Ġìĸ´ìļ°ëŁ¬&#34;:6741,&#34;est&#34;:6742,&#34;íķĺê±°ëĤĺ&#34;:6743,&#34;íķľìĭ¬&#34;:6744,&#34;íķľê°ľëıĦ&#34;:6745,&#34;ĠìĺģíĻĶë§Į&#34;:6746,&#34;ĠìĹ¬ë¦Ħ&#34;:6747,&#34;ìĿ¸ê±´&#34;:6748,&#34;ĠìķĦëĤĺ&#34;:6749,&#34;ĠìķĦëĤ´&#34;:6750,&#34;ìĭľê°Ģ&#34;:6751,&#34;ĠìłķíĻķ&#34;:6752,&#34;ê·¸ëĭ¥&#34;:6753,&#34;ĠìĭľëģĦ&#34;:6754,&#34;ìĿ¼ìĿĺ&#34;:6755,&#34;ìĤ¬ìĻĢ&#34;:6756,&#34;ìĭłê³ł&#34;:6757,&#34;ĠìŀĺíķĺëĬĶ&#34;:6758,&#34;íĬľ&#34;:6759,&#34;Ġë¶Ģë¥´&#34;:6760,&#34;Ġì¡°ìĦł&#34;:6761,&#34;ìķłëĵ¤ìĿĢ&#34;:6762,&#34;ĠëªħíĻĶ&#34;:6763,&#34;Ġìŀ¬ë¯¸ìĹĨìĸ´ìĦľ&#34;:6764,&#34;ê²½ìĿ´&#34;:6765,&#34;íĮĲìĿ´&#34;:6766,&#34;ĠíĸĪìĿĮ&#34;:6767,&#34;ìĤ´ìķĦ&#34;:6768,&#34;Ġë³Ħë¡ľìŀĦ&#34;:6769,&#34;ì°¸ê³ł&#34;:6770,&#34;Ġëª¨ëĵłê±¸&#34;:6771,&#34;ì¢ħêµĲ&#34;:6772,&#34;ì²ĺìĿĮìľ¼ë¡ľ&#34;:6773,&#34;íĮ¨ìĬ¤&#34;:6774,&#34;Ġìŀ¬ë°ĭëĭ¤&#34;:6775,&#34;Ġëħ¸ëŀĺê°Ģ&#34;:6776,&#34;Ġìŀĳê°Ģëĭĺ&#34;:6777,&#34;Ġì©Ŀ&#34;:6778,&#34;Ġë¹µë¹µ&#34;:6779,&#34;ĠìĺĪìģĺëĭ¤&#34;:6780,&#34;íŀĺëĵ¤&#34;:6781,&#34;Ġíķľìĭ¬íķľ&#34;:6782,&#34;ª¨ë¡ľ&#34;:6783,&#34;ĠíĽĪíĽĪíķľ&#34;:6784,&#34;Ġëıħë¦½ìĺģíĻĶ&#34;:6785,&#34;ì¨Į&#34;:6786,&#34;Ġê·¸ë¦½ëĭ¤&#34;:6787,&#34;ĠìĹĲíĶ¼ìĨĮëĵľ&#34;:6788,&#34;Ġe&#34;:6789,&#34;ìĿ´ê²ĥ&#34;:6790,&#34;ë©¸&#34;:6791,&#34;Ġê·¸ëŀ&#34;:6792,&#34;ìľ¼ëł¤ê³ł&#34;:6793,&#34;ìŀĲê¾¸&#34;:6794,&#34;ĠìĹĨê²Į&#34;:6795,&#34;ĠìĹĨìĹĪìĿĮ&#34;:6796,&#34;ĠëĮĢëĨĵê³ł&#34;:6797,&#34;êµ¬ê°Ģ&#34;:6798,&#34;Ġë¬´ë£Į&#34;:6799,&#34;ĠëĬĶ&#34;:6800,&#34;ĠíĺĲ&#34;:6801,&#34;Ġìĺ¤ë¹ł&#34;:6802,&#34;ĠìĤ¬ëŀĮëıĦ&#34;:6803,&#34;ĠëĲĺëĦ¤ìļĶ&#34;:6804,&#34;Ġìĭ¶ì§Ģ&#34;:6805,&#34;ì¤ĦìĿĢ&#34;:6806,&#34;Ġê²°ìłķ&#34;:6807,&#34;ë¶ĢíĦ°ê°Ģ&#34;:6808,&#34;ìĭľê°ĦìĿĦ&#34;:6809,&#34;ĠìĹ°ì¶ľìĿĢ&#34;:6810,&#34;Ġì§ľìŀĦìĥĪ&#34;:6811,&#34;Ġìŀ¥ë©´ìĿĦ&#34;:6812,&#34;Ġê´ľì°®ìķĺëĭ¤&#34;:6813,&#34;ìĻĶëĬĶëį°&#34;:6814,&#34;Ġë§ĪìĿĮìľ¼ë¡ľ&#34;:6815,&#34;ëĤ´ê°Ģë³¸&#34;:6816,&#34;Ġì§ľì¦ĿëĤ¨&#34;:6817,&#34;íĪ°&#34;:6818,&#34;Ġë´¤ëĭ¤ë©´&#34;:6819,&#34;Ġëĸ¨ìĸ´ì§Ĳ&#34;:6820,&#34;ìµľê³łìĿĺìĺģíĻĶ&#34;:6821,&#34;ĠíĥĢê³ł&#34;:6822,&#34;ìĿĮìķħìĿ´&#34;:6823,&#34;íģ¬ëł&#34;:6824,&#34;ĠëķľìĹĲ&#34;:6825,&#34;Ġëĭ¤ë¥´ê²Į&#34;:6826,&#34;Ġë¹µíĦ°&#34;:6827,&#34;Ġë§ĪìĿ´íģ´&#34;:6828,&#34;Ġëģ¼ìĽĮ&#34;:6829,&#34;ìį¼&#34;:6830,&#34;ĠP&#34;:6831,&#34;ëĭ·&#34;:6832,&#34;íķĢ&#34;:6833,&#34;ê°ĪëķĮ&#34;:6834,&#34;ìĹĲìĿ´&#34;:6835,&#34;ĠìĺģíĻĶìĿ¸ì§Ģ&#34;:6836,&#34;ĠìĿ´ìłķ&#34;:6837,&#34;ìĸ´ìļ©&#34;:6838,&#34;ëĿ¼ëħ¸&#34;:6839,&#34;ëł´&#34;:6840,&#34;ĠìķĦëĥĲ&#34;:6841,&#34;Ġìłķìŀĳ&#34;:6842,&#34;ë¶ĢìĿĺ&#34;:6843,&#34;ĠìĥĿê°ģëıĦ&#34;:6844,&#34;Ġêµ´&#34;:6845,&#34;ëĵ¤ìĿ´ëĤĺ&#34;:6846,&#34;ĠìŀĲëŀĳ&#34;:6847,&#34;ĠìķĮê²łëĭ¤&#34;:6848,&#34;ìĺģìĽħ&#34;:6849,&#34;Ġë°°ìļ°ëıĦ&#34;:6850,&#34;ì§ģíķľ&#34;:6851,&#34;Ġê°ĲíŀĪ&#34;:6852,&#34;ìŀ¬ë°ĮëĬĶ&#34;:6853,&#34;Ġìŀ¬ë¯¸ìĹĨìĸ´ìļĶ&#34;:6854,&#34;ë¯¼ìĿ´&#34;:6855,&#34;ê·¹ìĿĦ&#34;:6856,&#34;ë³¼ëķĮë§Īëĭ¤&#34;:6857,&#34;ìĽĥê²¨&#34;:6858,&#34;Ġë§ŀì¶°&#34;:6859,&#34;íĹĪíĹĪ&#34;:6860,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:6861,&#34;ĠìķĦê¹Įìļ´ìĺģíĻĶ&#34;:6862,&#34;Ġì¶ľìĹ°íķľ&#34;:6863,&#34;ĠìĿ½ê³ł&#34;:6864,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:6865,&#34;ëĪĪë¬¼ìĿ´&#34;:6866,&#34;ìĭ«ëĭ¤&#34;:6867,&#34;ê¸°ìĸµìĹĲ&#34;:6868,&#34;ìº¬&#34;:6869,&#34;ê·¸ëŁ¬ëĤĺ&#34;:6870,&#34;ĠìĿ´ëĶ°ìľĦ&#34;:6871,&#34;ĠíĶĮë¡¯&#34;:6872,&#34;ë¡ľë§¨ìĬ¤&#34;:6873,&#34;ĠìķĦë¥ĺìŀĳ&#34;:6874,&#34;ì·&#34;:6875,&#34;ëĭĪëĿ¼&#34;:6876,&#34;ìĿ´ìĹ¬&#34;:6877,&#34;ë§Įíķĺëĭ¤&#34;:6878,&#34;ë¦¬ìĻĢ&#34;:6879,&#34;Ġìłł&#34;:6880,&#34;ë©ľ&#34;:6881,&#34;íķ´ìĦł&#34;:6882,&#34;Ġë³´íĨµ&#34;:6883,&#34;Ġëª¬&#34;:6884,&#34;ìľ¼ìĭł&#34;:6885,&#34;ĠëĦĮ&#34;:6886,&#34;ì¹ĺì§Ģ&#34;:6887,&#34;ë¶Ģëª¨&#34;:6888,&#34;Ġë³¸ê±´ëį°&#34;:6889,&#34;ĠëĤ¨ê¸°&#34;:6890,&#34;Ġê²ĥìĹĲ&#34;:6891,&#34;ìĹĪëĭ¤ê³ł&#34;:6892,&#34;Ġë¹ħ&#34;:6893,&#34;ìŀ¬ë¯¸ìŀĪëĬĶ&#34;:6894,&#34;ĠìŀĳìľĦ&#34;:6895,&#34;Ġë¶Ģíĥģ&#34;:6896,&#34;ĠìķĪë´&#34;:6897,&#34;ĠíĹĮ&#34;:6898,&#34;Ġê³łë§Ļ&#34;:6899,&#34;Ġë§¤ìĿ¼&#34;:6900,&#34;ìĭĿìĿĢ&#34;:6901,&#34;Ġìķ¡ìħĺìĿĦ&#34;:6902,&#34;Ġì¶Ķê²©&#34;:6903,&#34;ì°¨ë³Ħ&#34;:6904,&#34;Ġê´ľì°®ìĿĮ&#34;:6905,&#34;íĺķìłľ&#34;:6906,&#34;ìŀĺë´¤ìĬµëĭĪëĭ¤&#34;:6907,&#34;ìĹĩìĿĮ&#34;:6908,&#34;ìķĪëĲľ&#34;:6909,&#34;ìĥĿê°ģìĿĦ&#34;:6910,&#34;ĠìķĦìĿ´ê°Ģ&#34;:6911,&#34;Ġë°ĽëĬĶ&#34;:6912,&#34;Ġë³µìŀ¡&#34;:6913,&#34;ĠìĺģíĻĶëĿ¼ëĭĪ&#34;:6914,&#34;ìĭľëĮĢë¥¼&#34;:6915,&#34;ë³´ì§Ģë§ĪìĦ¸ìļĶ&#34;:6916,&#34;íķĺì§ĢìķĬê³ł&#34;:6917,&#34;ëĶ°ëľ»íķľ&#34;:6918,&#34;ĠìľłìĿ¼íķľ&#34;:6919,&#34;Ġê²¬ìŀĲëĭ¨&#34;:6920,&#34;ìĩĦ&#34;:6921,&#34;Ġn&#34;:6922,&#34;ħĢ&#34;:6923,&#34;ëį¸&#34;:6924,&#34;ë¡ľì§Ģ&#34;:6925,&#34;...^^&#34;:6926,&#34;ìĿ¸ì§ĢëıĦ&#34;:6927,&#34;Ġëĭ¬ëł¤&#34;:6928,&#34;ìĽ¬&#34;:6929,&#34;ë§ĪìŀĲ&#34;:6930,&#34;ĠìĹĨëĤĺìļĶ&#34;:6931,&#34;ì£¼ê¸°&#34;:6932,&#34;ìĥģìĿĢ&#34;:6933,&#34;ĠìķĪê°Ħëĭ¤&#34;:6934,&#34;ê¹Įì§ĢëıĦ&#34;:6935,&#34;Ġë¯¸ìĬ¤íĦ°&#34;:6936,&#34;ĠìĤ¬ëŀĮëĵ¤ìĹĲê²Į&#34;:6937,&#34;ì¢ĭê²Į&#34;:6938,&#34;Ġê³µê°Ħ&#34;:6939,&#34;ĠëĤ´ìļ©ìĿ¸ì§Ģ&#34;:6940,&#34;ì¤Ħê±°ë¦¬&#34;:6941,&#34;ĠìĤ´ë©´ìĦľ&#34;:6942,&#34;Ġìŀ¼ìŀĪëĭ¤&#34;:6943,&#34;ì½§&#34;:6944,&#34;ĠìĦľë¶Ģ&#34;:6945,&#34;ìŀ¡ìĿĦ&#34;:6946,&#34;ëĬĶê±°ì§Ģ&#34;:6947,&#34;Ġê·¹ìŀ¥ê°ĢìĦľ&#34;:6948,&#34;ìķ¡ìħĺìĿĢ&#34;:6949,&#34;Ġëĭ´ë°°&#34;:6950,&#34;ĠìķĦìī¬ìĽĢìĿ´&#34;:6951,&#34;ĠìĺģíĻĶìĺĢìĿĮ&#34;:6952,&#34;ĠëĤļìĭľ&#34;:6953,&#34;Ġê¿Īê¾¸&#34;:6954,&#34;ëª¨ë¥´ê³ł&#34;:6955,&#34;Ġë¹ĦëĶĶìĺ¤ë¡ľ&#34;:6956,&#34;ĠìķķëıĦ&#34;:6957,&#34;ì¤¬ìľ¼ë©´&#34;:6958,&#34;ìĻłë§Įíķĺë©´&#34;:6959,&#34;Ġì¤ĺìķ¼&#34;:6960,&#34;Ġëĭ¤ìļ´ë°ĽìķĦ&#34;:6961,&#34;ĠíķĺëĤĺíķĺëĤĺê°Ģ&#34;:6962,&#34;Ġë¥ĺìĬ¹&#34;:6963,&#34;ĠìĬ¬íİĲ&#34;:6964,&#34;ht&#34;:6965,&#34;¡íĮ¨&#34;:6966,&#34;ĠL&#34;:6967,&#34;ëĭ¼&#34;:6968,&#34;ìĸ´ìĿĺ&#34;:6969,&#34;ë¡ľìį¨&#34;:6970,&#34;ëĿ¼ëĿ¼&#34;:6971,&#34;ë¦¬ëĤĺ&#34;:6972,&#34;Ġë³´ëĤ´&#34;:6973,&#34;ìłķê³¼&#34;:6974,&#34;ìłĦìĿĦ&#34;:6975,&#34;ê·¸ëķĮ&#34;:6976,&#34;ìĬµëĭĪê¹Į&#34;:6977,&#34;ëŀĺëıĦ&#34;:6978,&#34;Ġìŀ¬ë°ĮìĿĦ&#34;:6979,&#34;Ġì¤įëĭĪëĭ¤&#34;:6980,&#34;ë¶ĦìĿĦ&#34;:6981,&#34;ìĦ¸ìĹ¬&#34;:6982,&#34;ê°Ĳìłķ&#34;:6983,&#34;Ġë³¸ë°©ìĤ¬ìĪĺ&#34;:6984,&#34;ĠëģĿëĤ¬&#34;:6985,&#34;ë§Ŀìŀĳ&#34;:6986,&#34;Ġê±°ë¶ģ&#34;:6987,&#34;ì§ĪìĿ´&#34;:6988,&#34;Ġìĭľê°ĦìĹĲ&#34;:6989,&#34;Ġëĸ¼&#34;:6990,&#34;Ġê·¹ë³µ&#34;:6991,&#34;Ġìļ¸ë¦¬ëĬĶ&#34;:6992,&#34;Ġëĭ¨íİ¸&#34;:6993,&#34;Ġì§ľì¦ĿëĤĺê³ł&#34;:6994,&#34;íĿ¬ê°Ģ&#34;:6995,&#34;ìķ¡ìħĺìĿ´&#34;:6996,&#34;ĠëłĪìķĮ&#34;:6997,&#34;Ġê·ĢìĹ¬ìĽĮ&#34;:6998,&#34;Ġìľłì¾Įíķĺê³ł&#34;:6999,&#34;Ġë¶ĪìĮįíķĺëĭ¤&#34;:7000,&#34;Ġë§ĽìĿ´&#34;:7001,&#34;Ġof&#34;:7002,&#34;Ġìĺ¬íķ´&#34;:7003,&#34;Ġë§Ĳíķĺê³łìŀĲ&#34;:7004,&#34;ĠëĨĢëŀįëĭ¤&#34;:7005,&#34;ĠìĿ´ë»ĲìĦľ&#34;:7006,&#34;Ġ&gt;&#34;:7007,&#34;ìĿ´ìļĶ&#34;:7008,&#34;ëĤĢ&#34;:7009,&#34;ë¦ĩ&#34;:7010,&#34;ìĥĺ&#34;:7011,&#34;ë³´ëĤ´&#34;:7012,&#34;ìļ°ìĿĺ&#34;:7013,&#34;ìłĦìĿ´&#34;:7014,&#34;Ġìłķì²´&#34;:7015,&#34;ĠìĬ¤íİĺ&#34;:7016,&#34;ĠìĽ°&#34;:7017,&#34;ĠìŀĲìĦ¸&#34;:7018,&#34;ĠëĤ¨íİ¸&#34;:7019,&#34;Ġìĺ¤ë¸Į&#34;:7020,&#34;ë²Ħì§Ģê°Ģ&#34;:7021,&#34;Ġë¶Ģìĭ¤&#34;:7022,&#34;Ġ21&#34;:7023,&#34;Ġê°Ĳê°ģ&#34;:7024,&#34;Ġë°ĺëĵľìĭľ&#34;:7025,&#34;ë°Ķë³´&#34;:7026,&#34;ĠìŀĳíĴĪìŀħëĭĪëĭ¤&#34;:7027,&#34;ĠëķĮë§Īëĭ¤&#34;:7028,&#34;ì²ľìŀ¬&#34;:7029,&#34;íĸĪëĭ¤ë©´&#34;:7030,&#34;ìĭľê°ĦëĤŃë¹Ħ&#34;:7031,&#34;Ġë°°ìĭł&#34;:7032,&#34;ì§ĳìĹĲ&#34;:7033,&#34;ê°ĲëıĻìłģìĿ´ê³ł&#34;:7034,&#34;ĠìłķëıĦë©´&#34;:7035,&#34;ì¡ĮìĿĮ&#34;:7036,&#34;Ġë°Ľê³ł&#34;:7037,&#34;Ġìĺģìĥģê³¼&#34;:7038,&#34;Ġë¨¸ë¦¬ê°Ģ&#34;:7039,&#34;ĠìĨĮìŀ¬ëĬĶ&#34;:7040,&#34;Ġëª¨ë¥´ê²łëĦ¤ìļĶ&#34;:7041,&#34;ì¿¡&#34;:7042,&#34;ìľ¤ë°ľ&#34;:7043,&#34;ĠíĿĲë¥´ëĬĶ&#34;:7044,&#34;ĠëģĮê³ł&#34;:7045,&#34;ĠìĿ´ë¦ĦìĿĦ&#34;:7046,&#34;Ġì±ħìŀĦ&#34;:7047,&#34;ĠíĥĪì¶ľ&#34;:7048,&#34;íķŃìĥģ&#34;:7049,&#34;ìĿ´ìĹĲ&#34;:7050,&#34;íķĺê·ł&#34;:7051,&#34;ëĤ©&#34;:7052,&#34;ëĵŃ&#34;:7053,&#34;ìķĦë§Ī&#34;:7054,&#34;ìĹĨìĿĦ&#34;:7055,&#34;ĠëĤĺëĦ¤ìļĶ&#34;:7056,&#34;Ġëĭ¤ìĿ´&#34;:7057,&#34;ìŀ¥ìĭ¤&#34;:7058,&#34;ĠëĵŃëĭĪëĭ¤&#34;:7059,&#34;Ġì§Ģê²¹&#34;:7060,&#34;íķłë§Įíķľ&#34;:7061,&#34;Ġëį¤&#34;:7062,&#34;ìĤ¬ìĹĲ&#34;:7063,&#34;Ġìŀĺë´¤ìĸ´ìļĶ&#34;:7064,&#34;íĬĢ&#34;:7065,&#34;ĠëĬ¦&#34;:7066,&#34;ĠìĿ¸íĺķ&#34;:7067,&#34;Ġê°ľëħĲ&#34;:7068,&#34;ì¤Ģíĺ¸&#34;:7069,&#34;ë³´ëĭ¤ëıĦ&#34;:7070,&#34;ëįĶëįĶ&#34;:7071,&#34;Ġìļ°ë¦¬ëĬĶ&#34;:7072,&#34;ĠìĽĥìľ¼ë©´ìĦľ&#34;:7073,&#34;Ġìĭ¤ëł¥&#34;:7074,&#34;ĠëıĻìĦ±ìķł&#34;:7075,&#34;ëª»íķ´&#34;:7076,&#34;Ġë§īíĮĲìĹĲ&#34;:7077,&#34;ĠìĦ¤ìłķìĿ´&#34;:7078,&#34;Ġë§Ŀíķľ&#34;:7079,&#34;Ġë§ŀì§Ģ&#34;:7080,&#34;ĠìĦłë¬¼&#34;:7081,&#34;Ġì§ĳìĸ´&#34;:7082,&#34;ê¸°ëıĦíķĺê³ł&#34;:7083,&#34;ĠìĹ´ìĹ°&#34;:7084,&#34;ê¸°ëĮĢíķĺê³ł&#34;:7085,&#34;Ġëĭ´ìĿĢ&#34;:7086,&#34;Ġì´Īëĵ±íķĻìĥĿ&#34;:7087,&#34;ê´ĳê³ł&#34;:7088,&#34;ĠëĤļìĺĢëĭ¤&#34;:7089,&#34;Ġìĺ¤ëŀĺëĲľ&#34;:7090,&#34;Ġëĭ¤ë¥´ëĭ¤&#34;:7091,&#34;Ġëĭ¹ìĭłìĿĢ&#34;:7092,&#34;Ġì§§ìĿĢ&#34;:7093,&#34;ĠìŀĬíĺĢì§Ģ&#34;:7094,&#34;Ġê°ĢëĵĿíķľ&#34;:7095,&#34;ĠëģĶì°į&#34;:7096,&#34;ĠëĿĦ&#34;:7097,&#34;ĠF&#34;:7098,&#34;ĠN&#34;:7099,&#34;..!&#34;:7100,&#34;¬ëĦ¤&#34;:7101,&#34;ĠìĿ´ë¯¼&#34;:7102,&#34;ìķĦì¹¨&#34;:7103,&#34;Ġê·¸ëĵ¤&#34;:7104,&#34;ìłĲë¶ĢíĦ°&#34;:7105,&#34;Ġë°ĸ&#34;:7106,&#34;ĠìĹĨìľ¼ë©´&#34;:7107,&#34;Ġì¢ĭëĭ¤ê³ł&#34;:7108,&#34;ì£¼ë©´&#34;:7109,&#34;ìŀĪì§Ģë§Į&#34;:7110,&#34;ĠëĦĪë¬´ëıĦ&#34;:7111,&#34;ĠëĮĢìĭł&#34;:7112,&#34;Ġìŀ¬ë°Įëįĺëį°&#34;:7113,&#34;ĠìķĪëĤĺ&#34;:7114,&#34;Ġìµľê³łë´ī&#34;:7115,&#34;ĠëģĪ&#34;:7116,&#34;Ġë³¸ëĵ¯&#34;:7117,&#34;Ġê°ĲëıĻë°Ľ&#34;:7118,&#34;Ġê¹į&#34;:7119,&#34;ĠíĿ¬ë&#34;:7120,&#34;Ġíĸĩ&#34;:7121,&#34;Ġì¤ĳëıħ&#34;:7122,&#34;ĠìķłêµŃ&#34;:7123,&#34;ë¬¼ë¡ł&#34;:7124,&#34;ìŀ¬ë°ĮìĹĪìĸ´ìļĶ&#34;:7125,&#34;Ġìŀ¬ë¯¸ìĹĨëĭ¤ê³ł&#34;:7126,&#34;ãĦ²&#34;:7127,&#34;ëĤ¨ê¸°&#34;:7128,&#34;ìĦ¤ë§Ī&#34;:7129,&#34;ì¦Īê°Ģ&#34;:7130,&#34;ĠìķĦê¹ĿëĦ¤ìļĶ&#34;:7131,&#34;Ġë§Īì§Ģë§īê¹Įì§Ģ&#34;:7132,&#34;ĠìĦ¤ë§Ī&#34;:7133,&#34;ĠìłĦê°ľìĻĢ&#34;:7134,&#34;Ġë°ĺìłĦìĿĦ&#34;:7135,&#34;ê¶ĮìĿ´&#34;:7136,&#34;Ġì¦Ĳê¸¸&#34;:7137,&#34;ìĵ°ëĬĶ&#34;:7138,&#34;ìŀ¬ë°ĭìĿĮ&#34;:7139,&#34;Ġìŀłìĭľ&#34;:7140,&#34;ê²¬ìŀĲëĭ¨&#34;:7141,&#34;Ġì¼Ģë¦ŃíĦ°&#34;:7142,&#34;Ġìĸ¼êµ´ìĿ´&#34;:7143,&#34;Ġê½¤ëĤĺ&#34;:7144,&#34;ìķĦëĭĮê°Ģ&#34;:7145,&#34;ìĪĻíķľ&#34;:7146,&#34;Ġì½©&#34;:7147,&#34;Ġë¬»ìĸ´&#34;:7148,&#34;âĢ¦&#34;:7149,&#34;ĠëĨĢëĿ¼ìļ´&#34;:7150,&#34;ĠíĶ¼íķ´ìŀĲ&#34;:7151,&#34;ĠìĿ¼ë¶ĢëŁ¬&#34;:7152,&#34;ìĿ´ìłķëıĦë©´&#34;:7153,&#34;Ġëĭ¤íģĲë©ĺíĦ°ë¦¬&#34;:7154,&#34;Ġyou&#34;:7155,&#34;ĳëĮĢ&#34;:7156,&#34;ĠëĹĦ&#34;:7157,&#34;ìĿ´ì£ł&#34;:7158,&#34;ìŀĪëĥĲ&#34;:7159,&#34;ì§ĢìĹĲ&#34;:7160,&#34;ëĤ©ëĭĪëĭ¤&#34;:7161,&#34;¬ëĦ¤ìļĶ&#34;:7162,&#34;ëıĦë¡ľ&#34;:7163,&#34;ëłĽ&#34;:7164,&#34;ĠìĹĨëĭ¤ê³ł&#34;:7165,&#34;ĠëĤ´ê²Į&#34;:7166,&#34;ķĮë¬¸ìĹĲ&#34;:7167,&#34;ë¯¸ì¹ĺ&#34;:7168,&#34;íŀĪê³ł&#34;:7169,&#34;íĸĪìĿĦê¹Į&#34;:7170,&#34;ĠìĪĺë§İìĿĢ&#34;:7171,&#34;ë£¨íĨł&#34;:7172,&#34;Ġë¬´ê²Į&#34;:7173,&#34;ë¶Ħëĵ¤ìĿĢ&#34;:7174,&#34;ê°ľìĿ¸&#34;:7175,&#34;ĠìĿ¸ëĶĶ&#34;:7176,&#34;Ġìµľê³łë¡ľ&#34;:7177,&#34;Ġêµ¬ë¦¬&#34;:7178,&#34;ìłĢê¸°&#34;:7179,&#34;ìķĪëı¼&#34;:7180,&#34;Ġìķłë§¤&#34;:7181,&#34;ĠìłĢê¸ī&#34;:7182,&#34;ë°ĶëĿ¼&#34;:7183,&#34;ĠìĹŃìĤ¬ë¥¼&#34;:7184,&#34;Ġë°Ķíĥķ&#34;:7185,&#34;ĠëıĻìĸĳ&#34;:7186,&#34;ë³¼ìĪĺë¡Ŀ&#34;:7187,&#34;ë³´ëĬĶê²Į&#34;:7188,&#34;Ġê´ľì°®ê³ł&#34;:7189,&#34;Ġì¤Ħê±°&#34;:7190,&#34;Ġë§¤ëł¥ìĹĲ&#34;:7191,&#34;ĠìłķëıĦê°Ģ&#34;:7192,&#34;ĠìĥĿê¸´&#34;:7193,&#34;ĠìķĦë¬´ëŁ°&#34;:7194,&#34;ĠíĮĮìĽĮ&#34;:7195,&#34;Ġê³§&#34;:7196,&#34;ìµľìķħìĿ´ëĭ¤&#34;:7197,&#34;ìĬ¬íĶĦëĭ¤&#34;:7198,&#34;ĠìķĮë°Ķëĵ¤&#34;:7199,&#34;ĠìĤ¬ìĭ¤ìĿĦ&#34;:7200,&#34;ì±ħìĿĦ&#34;:7201,&#34;ĠìķĬìķĺëįĺ&#34;:7202,&#34;ë¡Ńê³ł&#34;:7203,&#34;ê±°ê°ĻìĿĮ&#34;:7204,&#34;ìĿ´íķ´ê°Ģ&#34;:7205,&#34;Ġë¹ĽìĿĦ&#34;:7206,&#34;ĠìłĦìĦ¤ìĿĺ&#34;:7207,&#34;ìĬĪíį¼&#34;:7208,&#34;ëĳ¥ìĿ´&#34;:7209,&#34;Ġìĸ´ëł¸ìĿĦëķĮ&#34;:7210,&#34;¬ë°ķ&#34;:7211,&#34;ĠíĬ¹ìĪĺíļ¨ê³¼&#34;:7212,&#34;ĠëĮĦ&#34;:7213,&#34;ow&#34;:7214,&#34;¨¹&#34;:7215,&#34;ĠX&#34;:7216,&#34;ľ¨&#34;:7217,&#34;ìĿ´ëĵł&#34;:7218,&#34;ê°Ģë³įê²Į&#34;:7219,&#34;ëıĦìĿĺ&#34;:7220,&#34;ê²ĮíķĺëĬĶ&#34;:7221,&#34;ëĭĪì½ľ&#34;:7222,&#34;ëį°ì²´&#34;:7223,&#34;ìĺµ&#34;:7224,&#34;ìĬ¤íĨ¤&#34;:7225,&#34;ê¹ĮìļĶ&#34;:7226,&#34;Ġíķĺìĭľ&#34;:7227,&#34;ìļ°ìĬ¤&#34;:7228,&#34;Ġìĺ¹&#34;:7229,&#34;íĥĲ&#34;:7230,&#34;ĠíķľìĪ¨&#34;:7231,&#34;íķ¨ê»ĺ&#34;:7232,&#34;ë¶Ħë§ĮìĹĲ&#34;:7233,&#34;ĠìķĪëĵ¤&#34;:7234,&#34;ê°ĻìķĦìĦľ&#34;:7235,&#34;ĠìĽ¨&#34;:7236,&#34;ìŀĦìĹĲëıĦ&#34;:7237,&#34;ĠìĥĿê°ģëĤľëĭ¤&#34;:7238,&#34;ìľłë¦¬&#34;:7239,&#34;ìŀ¬ë¯¸ìŀĩ&#34;:7240,&#34;ĠëŃĲëĭĪ&#34;:7241,&#34;Ġìķłê¸°&#34;:7242,&#34;Ġê³łëĩĮ&#34;:7243,&#34;ìľĦìĽĲ&#34;:7244,&#34;ĠíĥĿ&#34;:7245,&#34;ì´¬ìĺģ&#34;:7246,&#34;Ġë©ĭì§Ģê³ł&#34;:7247,&#34;Ġê´ľì°®ëĭ¤&#34;:7248,&#34;ìĤ¬ëŀĮìĿĺ&#34;:7249,&#34;ë¨¹ìĿĢ&#34;:7250,&#34;ê·¼ëŀĺ&#34;:7251,&#34;ĠíĺĦìĭ¤ìĿ´&#34;:7252,&#34;Ġì§ľì¦ĿìĿ´&#34;:7253,&#34;ìĬ¤ëŁ¬ìĽĮ&#34;:7254,&#34;ë¶ĪíĹĪ&#34;:7255,&#34;ë³ĳíĹĮ&#34;:7256,&#34;ĠãħĤ&#34;:7257,&#34;ìĹ¬ë²Ħ&#34;:7258,&#34;Ġêµ¿êµ¿&#34;:7259,&#34;ìĿĮìķħëıĦ&#34;:7260,&#34;íĥĪì¶ľ&#34;:7261,&#34;ìŀ¬ë¯¸ìĹĨìĸ´ìļĶ&#34;:7262,&#34;Ġê¶ģê¸Īíķ´ìĦľ&#34;:7263,&#34;ìĭľëĮĢìĹĲ&#34;:7264,&#34;Ġìŀ¥ë¥´ê°Ģ&#34;:7265,&#34;Ġbê¸ī&#34;:7266,&#34;¶ĢíĦ°&#34;:7267,&#34;ê¸°ìĸµìĿ´&#34;:7268,&#34;Ġdvd&#34;:7269,&#34;ì§Ħë¶Ģíķľ&#34;:7270,&#34;ìĥĪë¡ľìļ´&#34;:7271,&#34;al&#34;:7272,&#34;ic&#34;:7273,&#34;Ġk&#34;:7274,&#34;ĠìŃī&#34;:7275,&#34;ëĬĶê±¸&#34;:7276,&#34;ìłĪë¡ľ&#34;:7277,&#34;ĠìŀĲëıĻ&#34;:7278,&#34;ĠìĺģíĻĶë³´ë©´ìĦľ&#34;:7279,&#34;ëĤĺê°Ħ&#34;:7280,&#34;ë§Įì¡±&#34;:7281,&#34;ë¦¬ìĹĦ&#34;:7282,&#34;ìķĦëħ¸&#34;:7283,&#34;ìĺģíĻĶìĹĲìĦľ&#34;:7284,&#34;ìĹĨì§Ģ&#34;:7285,&#34;ĠëĤĺìĺ¬ë&#34;:7286,&#34;ì§ĦìķĬ&#34;:7287,&#34;Ġìŀ¬ë¯¸ë¡ľ&#34;:7288,&#34;ĠëĤ´ìĥĿ&#34;:7289,&#34;íķłë¿Ĳ&#34;:7290,&#34;ìĹ°ìķł&#34;:7291,&#34;Ġê°Ģë³į&#34;:7292,&#34;ë¯¸ëĬĶ&#34;:7293,&#34;Ġ199&#34;:7294,&#34;ëĵłê°Ģ&#34;:7295,&#34;Ġì²©&#34;:7296,&#34;Ġë§ĲìŀĲ&#34;:7297,&#34;ë¬´ìĭľ&#34;:7298,&#34;ĠìĤ¬ìļ©&#34;:7299,&#34;Ġì¡°íĻĶ&#34;:7300,&#34;ĠíĮĿ&#34;:7301,&#34;íķĺì§Ģë§Ī&#34;:7302,&#34;ëĭµëĭĪëĭ¤&#34;:7303,&#34;Ġë©ĭìŀĪëĬĶ&#34;:7304,&#34;íĺ¸ê°Ĳ&#34;:7305,&#34;ìĽłê³ł&#34;:7306,&#34;ì¹´íĶĦ&#34;:7307,&#34;ì¹´ë©ĶëĿ¼&#34;:7308,&#34;Ġë§Ŀì¹ľ&#34;:7309,&#34;ì°¸ìĭł&#34;:7310,&#34;ê»ĺìļĶ&#34;:7311,&#34;Ġì°įê³ł&#34;:7312,&#34;ĠëĨĢëŀĲ&#34;:7313,&#34;Ġë´Ĳìķ¼ì§Ģ&#34;:7314,&#34;Ġìĭ¸ìĽĢ&#34;:7315,&#34;©´ìĦľ&#34;:7316,&#34;Ġê°ģìĥī&#34;:7317,&#34;ĠìĪĺìŀĳìĿ´ëĭ¤&#34;:7318,&#34;ĠëĶ°ëľ»íķĺê³ł&#34;:7319,&#34;ĠCê¸ī&#34;:7320,&#34;ĠìŀĶìŀĶíķĺê³ł&#34;:7321,&#34;íķĻêµĲìĹĲìĦľ&#34;:7322,&#34;ìŀĶìŀĶíķĺê³ł&#34;:7323,&#34;;;;;;;;;&#34;:7324,&#34;Ġë¦¬ìĸ¼ë¦¬íĭ°&#34;:7325,&#34;ĠìĨĮìĦ¤ìĿĦ&#34;:7326,&#34;ĠìĭĿìĥģíķľ&#34;:7327,&#34;ĠíĿ¬ë§ĿìĿĦ&#34;:7328,&#34;íĽĮë¥Ńíķľ&#34;:7329,&#34;199&#34;:7330,&#34;60&#34;:7331,&#34;½ķ&#34;:7332,&#34;ëĭ¤ë¥´&#34;:7333,&#34;íķĺìĿ´&#34;:7334,&#34;ĠìĺģíĻĶëĤĺ&#34;:7335,&#34;ĠìĺģíĻĶìķ¼&#34;:7336,&#34;ìĿĢëį°&#34;:7337,&#34;ëĵ¤ë¡ľ&#34;:7338,&#34;ë¥ľ&#34;:7339,&#34;ë³´ê¸´&#34;:7340,&#34;ìĬ¤ë§¨&#34;:7341,&#34;ëŀŃ&#34;:7342,&#34;ìĹĨëĤĺ&#34;:7343,&#34;ì£¼ê¸°ëıĦ&#34;:7344,&#34;ìĥģìłģìĿ¸&#34;:7345,&#34;Ġì§Ģê²¨&#34;:7346,&#34;Ġíķľë°©&#34;:7347,&#34;íķłì§Ģ&#34;:7348,&#34;ĠëĮĢíĨµëł¹&#34;:7349,&#34;ë¶ĢëĬĶ&#34;:7350,&#34;êµ¬ìĿĺ&#34;:7351,&#34;ë¶ĦëıĻìķĪ&#34;:7352,&#34;Ġì£¼ì§Ģ&#34;:7353,&#34;ê°Ĳê³¼&#34;:7354,&#34;íĬ¸ë¡ľ&#34;:7355,&#34;ĠìĿ¼ìľ¼&#34;:7356,&#34;íĤ¬ë§ģíĥĢìŀĦ&#34;:7357,&#34;Ġê°ĲëıħìĿĦ&#34;:7358,&#34;Ġìŀ¬ë¯¸ìŀĪìĬµëĭĪëĭ¤&#34;:7359,&#34;Ġë°ĺëĮĢ&#34;:7360,&#34;ë°ķì£½&#34;:7361,&#34;Ġë§¤ëĭĪìķĦ&#34;:7362,&#34;ĠëĤĺìĺ¤ëĬĶëį°&#34;:7363,&#34;ëĿ¼ëĬĶê²Į&#34;:7364,&#34;Ġë¡ľì½Ķ&#34;:7365,&#34;Ġìĸ´ëĸ¨&#34;:7366,&#34;ĠëĬĲëĤĮìĿĢ&#34;:7367,&#34;ĠíŀĺëĤ´&#34;:7368,&#34;Ġì£¼ìĿ¸ê³µëĵ¤&#34;:7369,&#34;ê°ĲëıĻëıĦ&#34;:7370,&#34;ĠëĭµìĿ´&#34;:7371,&#34;ëĤ¬ìĿĮ&#34;:7372,&#34;Ġê¹Ĭê²Į&#34;:7373,&#34;ĠìłĲìĪĺëĬĶ&#34;:7374,&#34;ĠìłĲìĪĺì¤Ģ&#34;:7375,&#34;ĪëįĶëĿ¼ë©´&#34;:7376,&#34;ìľłì¹ĺíķľ&#34;:7377,&#34;ìľłì¹ĺíķĺê³ł&#34;:7378,&#34;Ġë§Įëĵ¤ìĹĪìĿĦê¹Į&#34;:7379,&#34;Ġê·ĢìĹ½ëĭ¤&#34;:7380,&#34;Ġãħīãħīãħī&#34;:7381,&#34;ĠíĥĦíĥĦíķĺê³ł&#34;:7382,&#34;ì¶ĶìĸµìĿĺ&#34;:7383,&#34;21&#34;:7384,&#34;od&#34;:7385,&#34;ot&#34;:7386,&#34;¤ëĭ¤&#34;:7387,&#34;ìĿĺë¥¼&#34;:7388,&#34;ìķĦì§Ģ&#34;:7389,&#34;Ġê·¸ê²ĥìĿĦ&#34;:7390,&#34;ê¹ĥ&#34;:7391,&#34;Ġì¢ĭìĿĦëĵ¯&#34;:7392,&#34;Ġëĭ¤ìĭł&#34;:7393,&#34;ìŀĪì§Ģ&#34;:7394,&#34;Ġìĥĺ&#34;:7395,&#34;ìķĺìľ¼ë©´&#34;:7396,&#34;êµŃìĿĺ&#34;:7397,&#34;ĠìķĪìĸ´ìļ¸&#34;:7398,&#34;Ġìµľê³łìŀĦ&#34;:7399,&#34;ë¬´íĺĳ&#34;:7400,&#34;ë¹Ħì¶Ķ&#34;:7401,&#34;ìĺĢëĤĺ&#34;:7402,&#34;ê±¸ìŀĳ&#34;:7403,&#34;Ġë¹Ħíĸī&#34;:7404,&#34;ëıĻìĽĲ&#34;:7405,&#34;Ġìĭ¤íĹĺ&#34;:7406,&#34;ì§ĪìķĬ&#34;:7407,&#34;ìĬ¨ìĿ´&#34;:7408,&#34;ëŀĢíĭ°&#34;:7409,&#34;ĠìķĦê¹ĿëĦ¤&#34;:7410,&#34;Ġë¶Īê°ĢëĬ¥&#34;:7411,&#34;Ġì²ĺìĿĮìĹĶ&#34;:7412,&#34;Ġê·¹ëĭ¨&#34;:7413,&#34;ì©į&#34;:7414,&#34;Ġëĭ¹ìĹ°íŀĪ&#34;:7415,&#34;ĠëĬĲëĤĮìĿ´ëĭ¤&#34;:7416,&#34;ê°ĲëıĻìĿ´&#34;:7417,&#34;Īë°©&#34;:7418,&#34;ì£½ë°ķì£½&#34;:7419,&#34;Ġì»¸&#34;:7420,&#34;ĠíĮĮê´´&#34;:7421,&#34;Ġëĭ´ê¸´&#34;:7422,&#34;ĠìŀĬê³ł&#34;:7423,&#34;Ġìķ¼íķľ&#34;:7424,&#34;Ġë¬´ìĹĩìĿ¸ê°Ģ&#34;:7425,&#34;ĠíıŃë°ľ&#34;:7426,&#34;ĠíıŃíĴį&#34;:7427,&#34;ì²łíķĻ&#34;:7428,&#34;ë§ĪìĿĮìĹĲ&#34;:7429,&#34;Ġëª»ë³´ê²łëĭ¤&#34;:7430,&#34;Ġìŀ¤ëĭ¤&#34;:7431,&#34;Ġìĭľë¦¬ì¦&#34;:7432,&#34;íķĺëĶĶ&#34;:7433,&#34;íķĺëĭĪê¹Į&#34;:7434,&#34;ê°ĢëģĶ&#34;:7435,&#34;ëĤĺê°ĢëĬĶ&#34;:7436,&#34;ëĿ¼ìĿ¸&#34;:7437,&#34;ëŁ¬ëĿ¼&#34;:7438,&#34;ìŀĲë§ī&#34;:7439,&#34;ìĽŁ&#34;:7440,&#34;ĠìĹĨëĥĲ&#34;:7441,&#34;ìłģìľ¼ë¡ł&#34;:7442,&#34;Ġë§ĮëĤľ&#34;:7443,&#34;Ġìŀ¬ë¯¸ìŀĩ&#34;:7444,&#34;ìĤ¬ê·¹&#34;:7445,&#34;ê²łëĬĶëį°&#34;:7446,&#34;ê°ľëħĲ&#34;:7447,&#34;~~^^&#34;:7448,&#34;ìĹĪëĭ¤ë©´&#34;:7449,&#34;ìĶ¹&#34;:7450,&#34;ìŀ¬ë¯¸ìĹĨëĬĶ&#34;:7451,&#34;Ġë§Įëĵ¤ìĸ´ìĦľ&#34;:7452,&#34;ĠëĤ´ìļ©ìĹĲ&#34;:7453,&#34;ĠìĤ¬ëŀĳíķ´ìļĶ&#34;:7454,&#34;ìľĦê°Ģ&#34;:7455,&#34;ìľĦíķľ&#34;:7456,&#34;Ġëĭ¤ìĭľë³´ëĭĪ&#34;:7457,&#34;ìĬ¨ìĿĺ&#34;:7458,&#34;íļĮê°Ģ&#34;:7459,&#34;Ġíĥĵ&#34;:7460,&#34;ĠìĿ´íķ´ìķĪ&#34;:7461,&#34;ĠìĪĻ&#34;:7462,&#34;Ġê´ľì°®ìķĺëĬĶëį°&#34;:7463,&#34;ìŀĺëª»&#34;:7464,&#34;Ġê¹Ģë¯¼&#34;:7465,&#34;ëĤ´ìļ©ìĿ¸ì§Ģ&#34;:7466,&#34;ĠëĲ¬&#34;:7467,&#34;Ġê³¼íķĻ&#34;:7468,&#34;ì¡Įëįĺ&#34;:7469,&#34;Ġë§İìĿĢê±¸&#34;:7470,&#34;ĠíĴĪ&#34;:7471,&#34;ĠìķĬìĿĢëį°&#34;:7472,&#34;ìķ¡ìħĺëıĦ&#34;:7473,&#34;ì´Īëĵ±íķĻêµĲ&#34;:7474,&#34;ĠìĨĮìŀ¬ìĻĢ&#34;:7475,&#34;Ġìĭľìŀĳíķ´ìĦľ&#34;:7476,&#34;Ġìŀ¬ë¯¸ëıĦìĹĨê³ł&#34;:7477,&#34;ëłĪìĿ´ëĵľ&#34;:7478,&#34;ìķĹëĭ¤&#34;:7479,&#34;Ġëķ¡&#34;:7480,&#34;ĠëķĲ&#34;:7481,&#34;Ġìķ½ê°ĦìĿĺ&#34;:7482,&#34;êµ³êµ³&#34;:7483,&#34;ì´Īë°ĺìĹĲ&#34;:7484,&#34;ĠìĨĮì¤ĳíķ¨ìĿĦ&#34;:7485,&#34;Ġìĸ´ë¦´ìłģ&#34;:7486,&#34;ìĽ¨ìĿ´&#34;:7487,&#34;ĠìĹ¬ëŁ¬ê°Ģì§Ģ&#34;:7488,&#34;ê¶ģê¸Ī&#34;:7489,&#34;ĠíĬ¹ë³Ħíķľ&#34;:7490,&#34;Ġëĳĺì§¸ì¹ĺê³ł&#34;:7491,&#34;ë·°&#34;:7492,&#34;ëĭ¤ë§Į&#34;:7493,&#34;ê¸°ìĻĢ&#34;:7494,&#34;ëĤĺë¥¼&#34;:7495,&#34;ìķĦìĹŃ&#34;:7496,&#34;Ġê°Ŀ&#34;:7497,&#34;ëį°ë¯¸&#34;:7498,&#34;ĠìķĦëŀĺ&#34;:7499,&#34;ĠìķĦíĮĮ&#34;:7500,&#34;ìĭľê¸°&#34;:7501,&#34;ë³´ìĭľê¸¸&#34;:7502,&#34;ëĮĢìĹĲ&#34;:7503,&#34;ìŀĪìľ¼ë©´&#34;:7504,&#34;ìĥģìĹĲ&#34;:7505,&#34;Ġìĸ´íľ´&#34;:7506,&#34;ìĭłëıĦ&#34;:7507,&#34;Ġê±±ìłķ&#34;:7508,&#34;ëķĮë§¤&#34;:7509,&#34;ìĨĮê°Ģ&#34;:7510,&#34;ëĵ¤ìĿ´ëŀĳ&#34;:7511,&#34;ìłķë§Ĳë¡ľ&#34;:7512,&#34;Ġë¯¸ì³¤&#34;:7513,&#34;ëĲĺëĤĺ&#34;:7514,&#34;ìĦłìĿ´&#34;:7515,&#34;ìĦłíĥĿ&#34;:7516,&#34;íĭĭ&#34;:7517,&#34;ëıĻë¬¼&#34;:7518,&#34;ĠëŃĲê³ł&#34;:7519,&#34;ĠëŃĲíķĺëĬĶ&#34;:7520,&#34;ĠìĹŃëŁī&#34;:7521,&#34;Ġë°Ķëĭ¤&#34;:7522,&#34;ĠìĹĨëĬĶê²Į&#34;:7523,&#34;ì²Ńì¶ĺ&#34;:7524,&#34;ĠìĭłìĿĺ&#34;:7525,&#34;Ġì£½ìĹ¬&#34;:7526,&#34;ĠìĿ´íķ´íķłìĪĺ&#34;:7527,&#34;Ġë°°íĬ¸ë§¨&#34;:7528,&#34;ìŀĺë§Įëĵ¤&#34;:7529,&#34;...........&#34;:7530,&#34;íĮĮì¹ĺëħ¸&#34;:7531,&#34;ĠìłĲìĿ´&#34;:7532,&#34;ë¹łìł¸&#34;:7533,&#34;íıīìĥĿ&#34;:7534,&#34;ì§ĢìķĬëĬĶëĭ¤&#34;:7535,&#34;ĠìĥĿìĹĲ&#34;:7536,&#34;Ġë´Ĳìķ¼íķĺëĬĶ&#34;:7537,&#34;ëĪĦêµ°&#34;:7538,&#34;ìĽĶíķľ&#34;:7539,&#34;ĠìĭľëĮĢìĹĲ&#34;:7540,&#34;ĠìĿ´ë¦ĦìĿ´&#34;:7541,&#34;Ġê°ĸëĭ¤&#34;:7542,&#34;ìķĮë°Ķëĵ¤&#34;:7543,&#34;Ġìŀĺëª»ëĲľ&#34;:7544,&#34;Ġì«ĵ&#34;:7545,&#34;Ġì¹ľêµ¬ê°Ģ&#34;:7546,&#34;Ġë§ĮìłĲìĹĲ&#34;:7547,&#34;âĺħâĺħ&#34;:7548,&#34;ë¦¬ë©´ìĦľ&#34;:7549,&#34;ĠíĿ¡ìŀħ&#34;:7550,&#34;Ġê°Ģë²¼ìļ´&#34;:7551,&#34;ìĹĲëĮĢíķ´&#34;:7552,&#34;?..&#34;:7553,&#34;ĳ¼&#34;:7554,&#34;íķĺìĭ¤&#34;:7555,&#34;¬ë½ķ&#34;:7556,&#34;Ġíĺģ&#34;:7557,&#34;ìĭ¼&#34;:7558,&#34;ëĤĺìģĺ&#34;:7559,&#34;ĠëĤ¯&#34;:7560,&#34;ëĵ¤íķľíħĮ&#34;:7561,&#34;ìĿ¸ê²Į&#34;:7562,&#34;Ġë³´ëĿ¼ê³ł&#34;:7563,&#34;Ġê·¸ê²ĥìĿ´&#34;:7564,&#34;ĠëĤĺìĺ¬ëķĮ&#34;:7565,&#34;ĠíķĺìĹ¬&#34;:7566,&#34;ìķ¼íķľëĭ¤&#34;:7567,&#34;ì£¼ì§Ģ&#34;:7568,&#34;ì£¼ìĿ¼&#34;:7569,&#34;ìłģìĿ´ëĿ¼&#34;:7570,&#34;ĠìĻ¤ì¼Ģ&#34;:7571,&#34;ê³¼ê±°&#34;:7572,&#34;ìķĺëĦ¤&#34;:7573,&#34;íĶĶìĿ´&#34;:7574,&#34;ì¹ĺìĿĺ&#34;:7575,&#34;ĠìĨĶ&#34;:7576,&#34;Ġìŀ¬ë°ĮìĹĪëįĺ&#34;:7577,&#34;ĠìķĪê°Ģ&#34;:7578,&#34;ĠëŃ£&#34;:7579,&#34;ĠìĻľê³¡&#34;:7580,&#34;íĬ¸ë¥¼&#34;:7581,&#34;ìłĢì§Ī&#34;:7582,&#34;ëįĶë§¨&#34;:7583,&#34;ë²Ħë¦´&#34;:7584,&#34;Ġìľłíĸī&#34;:7585,&#34;Ġìŀ¥ìķł&#34;:7586,&#34;ĠìĤ¬ëŀĮìĿ´ëĿ¼ë©´&#34;:7587,&#34;ĠëĤ´ìļ©ìĿĺ&#34;:7588,&#34;ëĭĺìĿĢ&#34;:7589,&#34;Ġë§¤ë¯¸&#34;:7590,&#34;Ġë§¤ëģĦ&#34;:7591,&#34;ë§Īë¥´&#34;:7592,&#34;Ġìĭľê°Ħë§Į&#34;:7593,&#34;Ġì£½ê³ł&#34;:7594,&#34;ĠëıĦë¬´ì§Ģ&#34;:7595,&#34;ĠíķľêµŃìĹĲìĦľ&#34;:7596,&#34;Ġëĭ¹íĻ©&#34;:7597,&#34;Ġìļ¸ë©´ìĦľ&#34;:7598,&#34;Ġë§Ŀê°Ģ&#34;:7599,&#34;Ġë§ŀìķĦ&#34;:7600,&#34;ĠìłķëıĦëĬĶ&#34;:7601,&#34;ìĤ¬ëŀĳìĿĦ&#34;:7602,&#34;Ġíİ¸ìķĪ&#34;:7603,&#34;Ġê³µê°Ĳíķł&#34;:7604,&#34;Ġë²Ħë¦¬ê³ł&#34;:7605,&#34;Ġíĺ¸ê¸°&#34;:7606,&#34;ìŀĸìĿĢ&#34;:7607,&#34;ĪëįĶëĿ¼&#34;:7608,&#34;Īë³´ëĭ¤&#34;:7609,&#34;ĠìĬ¤íĨłë¦¬ëĿ¼&#34;:7610,&#34;ê·¸ëŁ°ëį°&#34;:7611,&#34;Ġë¨¼ê°Ģ&#34;:7612,&#34;ìľ¼ë©´ìĦľëıĦ&#34;:7613,&#34;ìĸ´ë¦°ìĭľìłĪ&#34;:7614,&#34;Ġë¶Īíİ¸íķľ&#34;:7615,&#34;ĠãĦ·ãĦ·ãĦ·&#34;:7616,&#34;ĠìļĶìĨĮê°Ģ&#34;:7617,&#34;Ġë£¨ì¦Ī&#34;:7618,&#34;ĠìľłìĿ¼íķĺê²Į&#34;:7619,&#34;Ġì»¨ìħī&#34;:7620,&#34;ëıħíĬ¹íķľ&#34;:7621,&#34;Ġìłłìŀ¥&#34;:7622,&#34;ì¹´íĶĦë¦¬ìĺ¤&#34;:7623,&#34;ĠK&#34;:7624,&#34;ĠìĺģíĻĶìĺĪìļĶ&#34;:7625,&#34;ĠìĹ¼&#34;:7626,&#34;ëĤĺìĿ´íĬ¸&#34;:7627,&#34;ĠìłĪë¡ľ&#34;:7628,&#34;ëĵ¤ëģ¼ë¦¬&#34;:7629,&#34;ìłķìĹĲ&#34;:7630,&#34;ìłķì©¡&#34;:7631,&#34;ë§ĪìĿĺ&#34;:7632,&#34;ê¹Įì§Ģë§Į&#34;:7633,&#34;ìļ°ë¦¬ê°Ģ&#34;:7634,&#34;ìĥģê³¼&#34;:7635,&#34;ê°ĦìĹĲ&#34;:7636,&#34;Ġìĸ´ëĶ&#34;:7637,&#34;Ġìĸ´ëķ&#34;:7638,&#34;ëĵľëŁ½ê²Į&#34;:7639,&#34;ì¹ĺë¥¼&#34;:7640,&#34;Ġë¬´ëĤľ&#34;:7641,&#34;ë¶Ħëªħ&#34;:7642,&#34;ĠìĥĿê°ģëĤĺ&#34;:7643,&#34;Ġë§ĲìķĦìķ¼&#34;:7644,&#34;Ġë§Ĳê³łëĬĶ&#34;:7645,&#34;Ġëª»ë´Ĳì£¼&#34;:7646,&#34;íı¼&#34;:7647,&#34;ĠìľłìķĦ&#34;:7648,&#34;ëĲĺëĬĶëį°&#34;:7649,&#34;ì¡°ìĦł&#34;:7650,&#34;Ġëªħìŀ¥ë©´&#34;:7651,&#34;Ġëªħë³µìĿĦ&#34;:7652,&#34;ìĹŃíķł&#34;:7653,&#34;ë°ĺê°ľëıĦ&#34;:7654,&#34;Ġíķłë§Ĳ&#34;:7655,&#34;ëĭµê²Į&#34;:7656,&#34;Ġê¸°ìĸµëĤĺëĬĶ&#34;:7657,&#34;ìķĮê³ł&#34;:7658,&#34;ìĤ¬ëŀĮìĿĦ&#34;:7659,&#34;ë¨¹ìĿĦ&#34;:7660,&#34;íħĮëŁ¬&#34;:7661,&#34;ë¦¼ìĿ´&#34;:7662,&#34;ĠíŀĺìĿĦ&#34;:7663,&#34;Ġì¤ĦìĪĺ&#34;:7664,&#34;Ġë§Įëĵ¤ìĸ´ëĿ¼&#34;:7665,&#34;ĠìĦľìļ¸&#34;:7666,&#34;ì¡Įìĸ´ìļĶ&#34;:7667,&#34;ĠìĿ¸ìĥĿìĹĲ&#34;:7668,&#34;ĠìĪĺì¤ĢìĿĦ&#34;:7669,&#34;Ġì¶Ķì²ľíķĺê³ł&#34;:7670,&#34;ëģĿëĤĺ&#34;:7671,&#34;Ġì¦Ĳê¸°&#34;:7672,&#34;Ġë³´ê²ĮëĲľ&#34;:7673,&#34;Ġëį°ë·Ķ&#34;:7674,&#34;Ġì«Ħ&#34;:7675,&#34;Ġë¹ĽëĤĺëĬĶ&#34;:7676,&#34;ëļ±ë§ŀ&#34;:7677,&#34;ĠëļĿ&#34;:7678,&#34;Ġìĵ¸ëį°ìĹĨìĿ´&#34;:7679,&#34;ãĢĤ&#34;:7680,&#34;ovie&#34;:7681,&#34;ìķĪíĥĢê¹Ŀ&#34;:7682,&#34;Ġì§ľì§ĳê¸°&#34;:7683,&#34;ĠíĺĲìĺ¤&#34;:7684,&#34;ëŀĢíĭ°ëħ¸&#34;:7685,&#34;^-^&#34;:7686,&#34;im&#34;:7687,&#34;êº&#34;:7688,&#34;ĨĴ&#34;:7689,&#34;ê°Ģìļ´&#34;:7690,&#34;ê¸°ë²ķ&#34;:7691,&#34;ìķĦíĶĦ&#34;:7692,&#34;ìĬ¤ìĻĢ&#34;:7693,&#34;ìĬ¤íİĺ&#34;:7694,&#34;ì§Ħì§Ģ&#34;:7695,&#34;ìĺ¤ëŀ«&#34;:7696,&#34;ê·¸ëŁŃìłĢëŁŃ&#34;:7697,&#34;ì°¬ëŀĢ&#34;:7698,&#34;ë¶ĢìĹĲ&#34;:7699,&#34;ê°ľë¥¼&#34;:7700,&#34;ìłĢë¦¬&#34;:7701,&#34;ĠìĨĮìĨĮíķľ&#34;:7702,&#34;ìĽĲìĿĢ&#34;:7703,&#34;ĠëĵľëĿ¼ë§ĪìŀħëĭĪëĭ¤&#34;:7704,&#34;Ġë°°ìļ°ëĬĶ&#34;:7705,&#34;ì¡°ìķĦ&#34;:7706,&#34;ĠìĤ¬ëŀĳíķ´&#34;:7707,&#34;ìļ¸ë¿Ĳ&#34;:7708,&#34;Ġì¢ĭìķĺìĿĦíħĲëį°&#34;:7709,&#34;ìĺĪìģĺ&#34;:7710,&#34;Ġìķ¡ìħĺê³¼&#34;:7711,&#34;êµĲíĽĪ&#34;:7712,&#34;Ġëª°ëĿ¼&#34;:7713,&#34;ìĭľê°ĦëıĻìķĪ&#34;:7714,&#34;ĠëĳĲê·¼&#34;:7715,&#34;ëª»íķĺëĬĶ&#34;:7716,&#34;ê²°ë¡ł&#34;:7717,&#34;ĠìĹ¬ìŀĲëĬĶ&#34;:7718,&#34;ëĤ´ìļ©ìĿĦ&#34;:7719,&#34;Ġë³¼ë§ĮíĸĪëĭ¤&#34;:7720,&#34;Ġê±´ê°Ģ&#34;:7721,&#34;Ġì°¾ìķĦìĦľ&#34;:7722,&#34;ĠìĻ¸ë¡ľ&#34;:7723,&#34;Ġë¨¹ëĬĶ&#34;:7724,&#34;Ġë§ĲìĿ´íķĦìļĶ&#34;:7725,&#34;ëģĿëĤĺê³ł&#34;:7726,&#34;Ġëĸ¨ìĸ´ì§Ģ&#34;:7727,&#34;Ġê³±&#34;:7728,&#34;ĠìĤ¶ìĿ´&#34;:7729,&#34;Ġìŀłëĵ¤&#34;:7730,&#34;ìĹ¬ëħ&#34;:7731,&#34;ìĽĲìŀĳìĿĺ&#34;:7732,&#34;Ġë¬´ìĦŃê³ł&#34;:7733,&#34;Ġëĵ¤ìĸ´ê°Ģ&#34;:7734,&#34;ì¿µ&#34;:7735,&#34;ĠìķĬìķĺì§Ģë§Į&#34;:7736,&#34;ìķĪëĲĺê³ł&#34;:7737,&#34;ĠìķĶê±¸&#34;:7738,&#34;ìŀĲì²´ëĬĶ&#34;:7739,&#34;ìħĶìķ¼&#34;:7740,&#34;Ġíģ´ëŀĺ&#34;:7741,&#34;Ġë³´ìķĺëįĺ&#34;:7742,&#34;Ġëĭ¬ì½¤&#34;:7743,&#34;ĠìĿ´ê²ĥë³´ëĭ¨&#34;:7744,&#34;ìĸ´ë¦°ìĿ´&#34;:7745,&#34;ĠìĥĿê°ģíķ´ë³´ê²Į&#34;:7746,&#34;ĠíĿĳìĿ¸&#34;:7747,&#34;Ġê°ĢëĬ¥íķľ&#34;:7748,&#34;ĠíĿĺëŁ¬ê°ĢëĬĶ&#34;:7749,&#34;Ġ:)&#34;:7750,&#34;ìĭłìĦłíķľ&#34;:7751,&#34;ë§ĺìĹĲ&#34;:7752,&#34;Ġê°Ģê¹Įìļ´&#34;:7753,&#34;Ġê±°ì§ĵë§Ĳ&#34;:7754,&#34;OST&#34;:7755,&#34;nd&#34;:7756,&#34;£Į&#34;:7757,&#34;íķĳ&#34;:7758,&#34;ê³łëıĦ&#34;:7759,&#34;íķĺìķĦ&#34;:7760,&#34;íķĺëĬĺ&#34;:7761,&#34;ìĿĢê±°&#34;:7762,&#34;ìĸ´ë¥¼&#34;:7763,&#34;ìĿ¸ìĹĲ&#34;:7764,&#34;ĠìķĦëĨĶ&#34;:7765,&#34;íķ´íĶ¼&#34;:7766,&#34;ìĺģíĻĶë³´ê³ł&#34;:7767,&#34;ë³´êµ¬&#34;:7768,&#34;ìĬ¤íı¬&#34;:7769,&#34;ìŀĲë¦¬&#34;:7770,&#34;ĠëĤĺìĿĦëĵ¯&#34;:7771,&#34;ìĥģìĥģ&#34;:7772,&#34;íĺĶ&#34;:7773,&#34;ê·¸ìĿĺ&#34;:7774,&#34;ĠìĭľìĤ¬íļĮ&#34;:7775,&#34;Ġìµľë¯¼&#34;:7776,&#34;ìĭłê¸°&#34;:7777,&#34;Ġìŀĺíķľëĭ¤&#34;:7778,&#34;Ġì£¼ìĦ±ì¹ĺ&#34;:7779,&#34;íİ¸ìĹĲìĦľ&#34;:7780,&#34;íĥĢê³ł&#34;:7781,&#34;ë²Ħë¦¬&#34;:7782,&#34;Ġë²¨&#34;:7783,&#34;ĠìĤ¬ìļ´ëĵľ&#34;:7784,&#34;ëªħíķľ&#34;:7785,&#34;Ġì¡°ì°¨&#34;:7786,&#34;Ġìĭ¶ìĹĪëĭ¤&#34;:7787,&#34;Ġìĭ¶ìĹĪëįĺ&#34;:7788,&#34;Ġì¢ĭìķĦíķĺì§Ģë§Į&#34;:7789,&#34;íķĺì§Ģë§Ĳ&#34;:7790,&#34;ĠìĹŃê²¨&#34;:7791,&#34;ê»Ģ&#34;:7792,&#34;ëħ¸ì¶ľ&#34;:7793,&#34;íıīìłĲë³´ê³ł&#34;:7794,&#34;ĠíĻĢ&#34;:7795,&#34;Ġë¶ĦëŁī&#34;:7796,&#34;ì¹ľêµ¬ëĵ¤&#34;:7797,&#34;ìĤ´ìĿ´&#34;:7798,&#34;ë³¼ìĪĺ&#34;:7799,&#34;íķĺê¸°ìĹĶ&#34;:7800,&#34;Ġë§Įëĵ¤ìĸ´ëĤ¸&#34;:7801,&#34;Ġìłģìĸ´ëıĦ&#34;:7802,&#34;ĠìĿ¸ìĥĿìĿ´&#34;:7803,&#34;ìĤ¬ëŀĳê³¼&#34;:7804,&#34;Ġëª¨ìĬµìĿĢ&#34;:7805,&#34;Ġê³µê°ĲëıĦ&#34;:7806,&#34;ĠëĨĢëŀĲëĭ¤&#34;:7807,&#34;ëŁ¬ëĥĲ&#34;:7808,&#34;ĠìŀłìĿ´&#34;:7809,&#34;ĠëĬĲëģ¼ê³ł&#34;:7810,&#34;Ġë¹¼ê³¤&#34;:7811,&#34;íķĺìĭľê³ł&#34;:7812,&#34;Ġìŀ¼ìŀĪìĸ´ìļĶ&#34;:7813,&#34;Ġë°°ê²½ìľ¼ë¡ľ&#34;:7814,&#34;ĠëĵľëĿ¼ë§Īë¡ľ&#34;:7815,&#34;íıŃëł¥&#34;:7816,&#34;Ġì£¼ìłľê°Ģ&#34;:7817,&#34;ĠìķĮìķĺìĿĮ&#34;:7818,&#34;íķĦë²Ħê·¸&#34;:7819,&#34;ĠìŀĥìĿĢ&#34;:7820,&#34;ìĹĦì²ŃëĤľ&#34;:7821,&#34;Ġê°Ķëĭ¤&#34;:7822,&#34;ĠëĥĦìĥĪ&#34;:7823,&#34;Ġëĭ¤íĸīìĿ´ëĭ¤&#34;:7824,&#34;Ġìĸ´ëłµëĭ¤&#34;:7825,&#34;ĠìĿ´ëŀĺìĦľ&#34;:7826,&#34;ĠìķĪë³¸ëĭ¤&#34;:7827,&#34;ĠíĹĽìĽĥìĿĮ&#34;:7828,&#34;ê°ľìĹ°ìĦ±&#34;:7829,&#34;ì¢ħìĿ¼ê´Ģ&#34;:7830,&#34;ë¶ĪíĹĪìłĦ&#34;:7831,&#34;ª©&#34;:7832,&#34;³¼&#34;:7833,&#34;ê³±&#34;:7834,&#34;ìļ¤&#34;:7835,&#34;ì§Ģë¶Ģ&#34;:7836,&#34;ëĤŃ&#34;:7837,&#34;ĠìĺģíĻĶìĿ´&#34;:7838,&#34;ĠìĺģíĻĶìĿ¸ì¤Ħ&#34;:7839,&#34;ìĿĦìĪĺê°Ģ&#34;:7840,&#34;Ġë§´&#34;:7841,&#34;íķ´ìł¸&#34;:7842,&#34;Ġë³´ëĭ¨&#34;:7843,&#34;ìłĲìĹĲ&#34;:7844,&#34;ìŀĲëıĦ&#34;:7845,&#34;ì§Ħíĸī&#34;:7846,&#34;ĠìĹ°ê·¹&#34;:7847,&#34;ĲëıĮ&#34;:7848,&#34;ìķĺëĦ¤ìļĶ&#34;:7849,&#34;Ġëª¨ë¥¸ëĭ¤&#34;:7850,&#34;êµŃë¯¼&#34;:7851,&#34;ë¶Ħëĵ¤ìĿ´&#34;:7852,&#34;ìħ§&#34;:7853,&#34;ĠìķĪì¢ĭìķĦ&#34;:7854,&#34;~~!!&#34;:7855,&#34;Ġì§Ħì§ľë¡ľ&#34;:7856,&#34;Ġêµīìŀ¥&#34;:7857,&#34;Ġê°ľìĦ±&#34;:7858,&#34;ĠìĨĮíĻĶ&#34;:7859,&#34;ë¹ħ&#34;:7860,&#34;^^*&#34;:7861,&#34;Ġìĭ¶ìĸ´ìĦľ&#34;:7862,&#34;ĠíĮį&#34;:7863,&#34;ì¦ĿìĿ´&#34;:7864,&#34;ìĺĪì§Ħ&#34;:7865,&#34;Ġëª¨ë¥´ì§Ģë§Į&#34;:7866,&#34;ìŀĪëĬĶìĺģíĻĶ&#34;:7867,&#34;Ġë§Īì§Ģë§īìĹĶ&#34;:7868,&#34;Ġë§Ŀì¹ĺ&#34;:7869,&#34;Ġë³´ê¸°ê°Ģ&#34;:7870,&#34;ëĶ©ëķĮ&#34;:7871,&#34;Ġìłģëĭ¹íŀĪ&#34;:7872,&#34;ëįĶëĿ¼ë©´&#34;:7873,&#34;ìĬ¹ìłĦ&#34;:7874,&#34;ëŁ¬ë¸Į&#34;:7875,&#34;ì²ĺìĿĮìĹĲëĬĶ&#34;:7876,&#34;Ġìĸ´ëĶĶë¡ľ&#34;:7877,&#34;ĠìĨĮìŀ¬ëıĦ&#34;:7878,&#34;Ġì¹´ë©Ķ&#34;:7879,&#34;ëĪĦêµ¬&#34;:7880,&#34;ĠìķŀëĴ¤&#34;:7881,&#34;ĠëĶĶìĽĮ&#34;:7882,&#34;ĠìĺģíĻĶëĿ¼ìĦľ&#34;:7883,&#34;Ġê·¸ìłĢê·¸ëŁ°&#34;:7884,&#34;Ġì±ħìĿĦ&#34;:7885,&#34;Ġë§Įëĵ¤ìĹĪëĤĺ&#34;:7886,&#34;Ġëķħ&#34;:7887,&#34;ĠíĿĶëĵ¤&#34;:7888,&#34;ĠìĻĶìĬµëĭĪëĭ¤&#34;:7889,&#34;Ġíĸ¥ìĹ°&#34;:7890,&#34;ĠìĹ¬ì£¼ê°Ģ&#34;:7891,&#34;ĠìĹ¬ëŁ¬ë¶Ħ&#34;:7892,&#34;ĠíķĻêµĲìĹĲìĦľ&#34;:7893,&#34;Ġê°ķëł¬íķľ&#34;:7894,&#34;ë¤Ħ&#34;:7895,&#34;ìķĦëĭĪë©´&#34;:7896,&#34;íģ¬ëłĪëĶ§&#34;:7897,&#34;18&#34;:7898,&#34;SF&#34;:7899,&#34;ay&#34;:7900,&#34;et&#34;:7901,&#34;ê°Ģë¦¬&#34;:7902,&#34;ê²ī&#34;:7903,&#34;ëĿ¼ìĹĲ&#34;:7904,&#34;ëĦĮ&#34;:7905,&#34;ìĭľìĿĺ&#34;:7906,&#34;ë³´ëĦ¤ìļĶ&#34;:7907,&#34;ìĬ¤íĭ±&#34;:7908,&#34;Ġë°ı&#34;:7909,&#34;ìŀĲë©´&#34;:7910,&#34;íķĺê³łëĬĶ&#34;:7911,&#34;Ġìµľì´Ī&#34;:7912,&#34;ì¹ĺëıĦ&#34;:7913,&#34;êµ¬ìĹŃ&#34;:7914,&#34;Ġëª¨ìļķ&#34;:7915,&#34;Ġìłķë§ĲìĿ´ì§Ģ&#34;:7916,&#34;Ġë¬´ëĦĪ&#34;:7917,&#34;ĠìŀĲê¸°ê°Ģ&#34;:7918,&#34;íİ¸ê³¼&#34;:7919,&#34;ĠíıīìłĲë³´ê³ł&#34;:7920,&#34;Ġë©ĶìĿ´&#34;:7921,&#34;Ġë¹Ļ&#34;:7922,&#34;ĠëģĿëĤłëķĮ&#34;:7923,&#34;Ġìļ°ë¦¬ëĵ¤&#34;:7924,&#34;Ġê°ĲëıħëıĦ&#34;:7925,&#34;ĠìĤ¬ëŀĳíķ©ëĭĪëĭ¤&#34;:7926,&#34;Ġë°ĺìĺģ&#34;:7927,&#34;ëĳĲë²Ī&#34;:7928,&#34;ĠíĽĦìĹĲ&#34;:7929,&#34;ĠëĶ°ë¦Ħ&#34;:7930,&#34;ì§ĳìĸ´&#34;:7931,&#34;ì§ĢìķĬëĭ¤&#34;:7932,&#34;Ġì°¾ëĬĶ&#34;:7933,&#34;ĠìķĦë¬´ëıĦ&#34;:7934,&#34;íĥĦíĥĦ&#34;:7935,&#34;ì¢ħìĺģ&#34;:7936,&#34;Ġì¹ĺìľł&#34;:7937,&#34;ĠëĬĲê»´ì¡Įëĭ¤&#34;:7938,&#34;ĠëıħìĿ¼&#34;:7939,&#34;ì¸Ħ&#34;:7940,&#34;ê·ĢìĹ¬ìļ´&#34;:7941,&#34;ĠëĴ¤ìĹĲ&#34;:7942,&#34;Ġìĭľë¦¬ì¦Īì¤ĳ&#34;:7943,&#34;Ġê¸¸ìĿ´&#34;:7944,&#34;Ġ60&#34;:7945,&#34;ĠíĻ©ëĭ¹íķľ&#34;:7946,&#34;Ġê¸ĢìİĦ&#34;:7947,&#34;ì±ĦëĦĲ&#34;:7948,&#34;íĸĩëĬĶëį°&#34;:7949,&#34;ìĨĮìŀ¬ê°Ģ&#34;:7950,&#34;ë³´ì§Ģë§ĪëĿ¼&#34;:7951,&#34;íĿ¥íĸī&#34;:7952,&#34;ì§ľì¦ĿëĤĺ&#34;:7953,&#34;ĠëķĢ&#34;:7954,&#34;Ġì¢ĭê²łìĬµëĭĪëĭ¤&#34;:7955,&#34;ĠìĹĦë§Īê°Ģ&#34;:7956,&#34;ĠëŁ¬ìĭľìķĦ&#34;:7957,&#34;Ġëľ¬ê¸ĪìĹĨëĬĶ&#34;:7958,&#34;ì§Ģëª»íķł&#34;:7959,&#34;Ġëª©ìĨĮë¦¬ê°Ģ&#34;:7960,&#34;ĠìĿ¸íĦ°ëĦ·&#34;:7961,&#34;ì¥¬ìĸ¼&#34;:7962,&#34;Ġì§Īì§ĪëģĮ&#34;:7963,&#34;Ġ,,&#34;:7964,&#34;ĠìŁ&#34;:7965,&#34;ëĭĲ&#34;:7966,&#34;ìĿĺìĭĿ&#34;:7967,&#34;ê¸°ìľĦíķ´&#34;:7968,&#34;ĠìĹ®&#34;:7969,&#34;ìĺĽ&#34;:7970,&#34;ìĺģíĻĶìłľ&#34;:7971,&#34;Ġê·¸ëĭ¹ìĭľ&#34;:7972,&#34;ìĬ¤íħĶ&#34;:7973,&#34;ìŀĲê·¹&#34;:7974,&#34;ë§Īë¦¬&#34;:7975,&#34;ìĹĨìĸ´ìļĶ&#34;:7976,&#34;ìĹĨìĹĪëĭ¤&#34;:7977,&#34;ĠëĤĺìĿĮ&#34;:7978,&#34;ĠëĤĺì¤ĳ&#34;:7979,&#34;ĠëĤĺëŀĳ&#34;:7980,&#34;Ġíķĺë©°&#34;:7981,&#34;Ġìłķë³´&#34;:7982,&#34;Ġì§Ģê²¹ëĭ¤&#34;:7983,&#34;ĠìĦ¬ë&#34;:7984,&#34;ë¯¸íĻĶ&#34;:7985,&#34;ìĭłë¶Ħëĵ¤&#34;:7986,&#34;ĠìĽħ&#34;:7987,&#34;~~~~~&#34;:7988,&#34;ìĭ¤ìłľ&#34;:7989,&#34;Ġë§ĲìĶĢ&#34;:7990,&#34;ĠìĹ¬íĥľ&#34;:7991,&#34;ĠìŀĲê³ł&#34;:7992,&#34;ĠìłľìŀĦìĬ¤&#34;:7993,&#34;Ġë¹ķëĭĪëĭ¤&#34;:7994,&#34;ĠìĤ¬ëŀĮëĵ¤ìĿĦ&#34;:7995,&#34;Ġë¶Ģëĭ´&#34;:7996,&#34;ĠëģĿìĹĲ&#34;:7997,&#34;ĠíĮ¬ëĵ¤&#34;:7998,&#34;ĠìłĢíıīê°Ģ&#34;:7999,&#34;ë°ĶíĥĢ&#34;:8000,&#34;ìŀ¼ìŀĪ&#34;:8001,&#34;ì¶ĶëĬĶ&#34;:8002,&#34;íĽĦìĹĲ&#34;:8003,&#34;Ġìŀ¥ë©´ëĵ¤ìĿ´&#34;:8004,&#34;Ġíıīê·ł&#34;:8005,&#34;ĠëıĻëĦ¤&#34;:8006,&#34;ëª»íķľ&#34;:8007,&#34;Ġê´ľì°®ê²Į&#34;:8008,&#34;ë³µìĪĺ&#34;:8009,&#34;ĠìĹ¬ìŀĲëĵ¤&#34;:8010,&#34;ìĽĥê¸´&#34;:8011,&#34;Ġìĭ¬ìŀ¥&#34;:8012,&#34;ĠìĦłìĤ¬&#34;:8013,&#34;ë°ĽìĿĦ&#34;:8014,&#34;Ġìłģëĭ¹íķľ&#34;:8015,&#34;Ġë°ķìĪĺ&#34;:8016,&#34;ìĤ¬ëŀĳìĿĢ&#34;:8017,&#34;ìĻ¸êµŃ&#34;:8018,&#34;Ġê·¸ëłĩì§Ģë§Į&#34;:8019,&#34;Ġê¿Ģ&#34;:8020,&#34;ĠìĽĲìŀĳìĿĺ&#34;:8021,&#34;ëħĦëĮĢìĹĲ&#34;:8022,&#34;ë¸ĶëŀĻ&#34;:8023,&#34;ê·Ģìĭł&#34;:8024,&#34;Ġë³ĢíĻĶ&#34;:8025,&#34;íĺľìĦł&#34;:8026,&#34;ëĳĶ&#34;:8027,&#34;ĠìŀĪìĹĪìľ¼ë©´&#34;:8028,&#34;Ġê·ĢìĹ¬ìĽĢ&#34;:8029,&#34;ê·¸ëŁ°ì§Ģ&#34;:8030,&#34;ĠìķĮìķĺëĦ¤&#34;:8031,&#34;íģ¬ë£¨&#34;:8032,&#34;ĠìĻłë§Įíķĺë©´&#34;:8033,&#34;................................&#34;:8034,&#34;Ġíĺ¹ìĭľ&#34;:8035,&#34;ê³³ìĹĲ&#34;:8036,&#34;ìį©&#34;:8037,&#34;Ġì§Ħì§Ģíķĺê²Į&#34;:8038,&#34;ë¡ľë§¨íĭ±&#34;:8039,&#34;Ġë³´ìĭľë©´&#34;:8040,&#34;Ġë§¡ìĿĢ&#34;:8041,&#34;ìłĦì²´ìłģìľ¼ë¡ľ&#34;:8042,&#34;íķµëħ¸ìŀ¼&#34;:8043,&#34;ĠìķĪë¬´ìĦľ&#34;:8044,&#34;ê°±ìĿ´&#34;:8045,&#34;ĠìĿ´ëıĦìłĢëıĦ&#34;:8046,&#34;ck&#34;:8047,&#34;~âĻ¥&#34;:8048,&#34;Ġ&amp;&#34;:8049,&#34;ĠW&#34;:8050,&#34;ŃĪ&#34;:8051,&#34;ìĿ´íĶĦ&#34;:8052,&#34;ìķ¡&#34;:8053,&#34;Īë¹Ħ&#34;:8054,&#34;ĠìĿ´ëĭ¤&#34;:8055,&#34;ĠìĿ´ëłĩ&#34;:8056,&#34;ĠìĿ´ë³´ëĭ¤&#34;:8057,&#34;ë¦¬ìĸ¼&#34;:8058,&#34;ìķĦì§Ħì§ľ&#34;:8059,&#34;ìķĦëĪĦ&#34;:8060,&#34;ì²¼&#34;:8061,&#34;ĠëĤĺë¬´&#34;:8062,&#34;ĠëĤĺìĿ´ê°Ģ&#34;:8063,&#34;Ġëĭ¤íģ¬&#34;:8064,&#34;Ġìĸ´ìłľ&#34;:8065,&#34;Ġê¸°íļį&#34;:8066,&#34;ë¬»&#34;:8067,&#34;ĠìĪĺë¡Ŀ&#34;:8068,&#34;ë¶ĦíŀĪ&#34;:8069,&#34;ê²łìĸ´&#34;:8070,&#34;ê²łëĭ¤ê³ł&#34;:8071,&#34;Ġë§Īìĭľê¸¸&#34;:8072,&#34;ëĵ¯íķĺëĭ¤&#34;:8073,&#34;ìľłë°ľ&#34;:8074,&#34;ì²´ìłģ&#34;:8075,&#34;ëĦĪë¬´ëĤĺëıĦ&#34;:8076,&#34;ĠëĵľëĿ¼ë§ĪìĿĺ&#34;:8077,&#34;ĠìĤ¬ìĿ´ì½Ķ&#34;:8078,&#34;ìĺģìĿĺ&#34;:8079,&#34;Ġíķ¸&#34;:8080,&#34;ì¡°ìļ©&#34;:8081,&#34;ìŀ¬ë°ĮëĦ¤&#34;:8082,&#34;Ġë°Ķëĭ¥&#34;:8083,&#34;ëĸ¨&#34;:8084,&#34;Ġìĭ¤ê°Ĳ&#34;:8085,&#34;ìŀ¼ìŀĪìĸ´ìļĶ&#34;:8086,&#34;ëĿ¼ê³łìļĶ&#34;:8087,&#34;ĠìĤ´ë¦¬ì§Ģ&#34;:8088,&#34;Ġíķłë¦¬ìļ°ëĵľ&#34;:8089,&#34;ĠìķĦëĭĪìŀĸìķĦ&#34;:8090,&#34;ĠìĿ´íķ´ë¶Īê°Ģ&#34;:8091,&#34;Ġì§ľë¦¿&#34;:8092,&#34;ĠíķľêµŃìĿĺ&#34;:8093,&#34;ĠëĤľë¬´&#34;:8094,&#34;ìĿĦê¹ĮìļĶ&#34;:8095,&#34;Ġì§Ģê¸ĪìĿĺ&#34;:8096,&#34;íĥľíĺĦ&#34;:8097,&#34;Ġì§ľì¦ĿëĤĺê²Į&#34;:8098,&#34;ĠìĦľìĸĳ&#34;:8099,&#34;ĠìĥĿê²¼&#34;:8100,&#34;ĠìĬ¤ë¦´ëıĦ&#34;:8101,&#34;Ġë°©íķ´&#34;:8102,&#34;ëĭ¤ëĭĪëĬĶ&#34;:8103,&#34;ìĦĿê·ľ&#34;:8104,&#34;ĠìĹ°ê¸°ëł¥ìĹĲ&#34;:8105,&#34;íĳľìłķ&#34;:8106,&#34;ĠìĽĲìŀĳìĿ´&#34;:8107,&#34;ì§Ģê¸ĪëıĦ&#34;:8108,&#34;êº¼ë©´&#34;:8109,&#34;ìĻķêµŃ&#34;:8110,&#34;Ġë³´ê²ĮëĲĺ&#34;:8111,&#34;Ġë¦¬ì¦Ī&#34;:8112,&#34;ìĬ¬íĶĪ&#34;:8113,&#34;Ġë¬´ìĦŃê²Į&#34;:8114,&#34;Ġìŀ¼ìŀĪìĿĮ&#34;:8115,&#34;ĠíħĲ&#34;:8116,&#34;ë¡Ńê²Į&#34;:8117,&#34;Ġë¯¼íıĲ&#34;:8118,&#34;ì§Ħìĭ¬ìľ¼ë¡ľ&#34;:8119,&#34;ĠíĤ¤ìĬ¤&#34;:8120,&#34;Ġìĸ¸ìłł&#34;:8121,&#34;ë§ĪìĿ´íģ´&#34;:8122,&#34;Ġë¯¸ìĨĮê°Ģ&#34;:8123,&#34;Ġëħ¹ìķĦ&#34;:8124,&#34;ìłĲë§ĮìłĲìĹĲ&#34;:8125,&#34;Ġë±ĢíĮĮìĿ´ìĸ´&#34;:8126,&#34;Ġìĸ´ëĳ¡&#34;:8127,&#34;dd&#34;:8128,&#34;©į&#34;:8129,&#34;Ġìª¼&#34;:8130,&#34;ìĿ´ëķĮ&#34;:8131,&#34;Ġìķµ&#34;:8132,&#34;ĠìĿĦ&#34;:8133,&#34;ĠìĺģíĻĶê°Ļ&#34;:8134,&#34;Īë¥¼&#34;:8135,&#34;ìĿĦê²ĥ&#34;:8136,&#34;ìļĶíķľ&#34;:8137,&#34;ĠëĤ¬ëĭ¤&#34;:8138,&#34;ìĺ·&#34;:8139,&#34;ìĺģíĻĶê´Ģ&#34;:8140,&#34;ìłĲìĿ´ëĤĺ&#34;:8141,&#34;ê±°ê°ĻìĿĢëį°&#34;:8142,&#34;ìłķë¯¼&#34;:8143,&#34;ì§Ħì°½&#34;:8144,&#34;Ġëĭ¤ëħĢ&#34;:8145,&#34;ìłĦìĿĺ&#34;:8146,&#34;Ġìĸ´ì§¸&#34;:8147,&#34;ëĵľë§Į&#34;:8148,&#34;ĠìĬ¤íĥ&#34;:8149,&#34;ĠìĬ¤íĭ°&#34;:8150,&#34;ĠëıĪë²&#34;:8151,&#34;Ġì°¢&#34;:8152,&#34;íŀĪìĸ´ë¡ľ&#34;:8153,&#34;ëŁ¬ì§Ģ&#34;:8154,&#34;ìħ°&#34;:8155,&#34;ìĨĮìŀ¥&#34;:8156,&#34;Ġë§ĲìĿ¸ê°Ģ&#34;:8157,&#34;ìĭ¬ìĹĲ&#34;:8158,&#34;ĠëĵľëĿ¼ë§Īëĭ¤&#34;:8159,&#34;Ġë°°ìļ°ëĵ¤ìĿĢ&#34;:8160,&#34;ë¯Ģë¡ľ&#34;:8161,&#34;Ġìļ°ìĹ°&#34;:8162,&#34;ì¡°íĺĦ&#34;:8163,&#34;ë°Ķëĭ¥&#34;:8164,&#34;ĠìĹŃê²¨ìļ´&#34;:8165,&#34;Ġìĭľê°ĦìķĦê¹Ŀëĭ¤&#34;:8166,&#34;ëĬĲëĥĲ&#34;:8167,&#34;Ġë¶Īê³¼&#34;:8168,&#34;Ġë©ĭì§Ģê²Į&#34;:8169,&#34;ì£¤&#34;:8170,&#34;ìĿ´ê±°ë³´ê³ł&#34;:8171,&#34;Ġë°°ê¸ī&#34;:8172,&#34;íĺķìĿ´&#34;:8173,&#34;Ġì°¨ìĿ´&#34;:8174,&#34;Ġë¹łë¥¸&#34;:8175,&#34;ĠíķĦìļĶê°Ģ&#34;:8176,&#34;ĠìĺģìĥģìĿ´&#34;:8177,&#34;ìĻ¸ìĿĺ&#34;:8178,&#34;ê¸°ëĮĢë¥¼&#34;:8179,&#34;Ġë²Ħë¦°&#34;:8180,&#34;ãĦ·ãĦ·ãĦ·&#34;:8181,&#34;ĠëĤ«ê²łëĭ¤&#34;:8182,&#34;Ġë¹łìł¸ìĦľ&#34;:8183,&#34;ĠíĦ°ë¯¸ëĦ¤ìĿ´íĦ°&#34;:8184,&#34;ê·¹ìŀ¥íĮĲ&#34;:8185,&#34;ĠìĥģíĻ©ìĿĦ&#34;:8186,&#34;ĠìĪ¨ê²¨ì§Ħ&#34;:8187,&#34;ĠìĻĶëĭ¤&#34;:8188,&#34;ĠìĤ¬ë¬´&#34;:8189,&#34;ĠìĿĺëıĦê°Ģ&#34;:8190,&#34;íĶĦë¡ľê·¸ëŀ¨&#34;:8191,&#34;ë²Īë´¤&#34;:8192,&#34;Ġì¿µ&#34;:8193,&#34;Ġëľ¬ê¸ĪìĹĨìĿ´&#34;:8194,&#34;Ġë¶Ģëª¨ëĭĺ&#34;:8195,&#34;Ġêµ¬ë¶Ħ&#34;:8196,&#34;ë±ħ&#34;:8197,&#34;Ġìĵ¸ëį°ìĹĨëĬĶ&#34;:8198,&#34;ì«Į&#34;:8199,&#34;ĠíĦ¸&#34;:8200,&#34;am&#34;:8201,&#34;ig&#34;:8202,&#34;le&#34;:8203,&#34;ê°ĢìľĦ&#34;:8204,&#34;ëıĦìķĪ&#34;:8205,&#34;ĠìĿ´ìĿĢ&#34;:8206,&#34;ĠìĿ´íĨłë¡Ŀ&#34;:8207,&#34;ìĸ´ìĬ¤&#34;:8208,&#34;ëĵ¤ìĹ¬&#34;:8209,&#34;ìĬ¤ì¹´&#34;:8210,&#34;ĠìĤ½&#34;:8211,&#34;ĠìĤŃìłľ&#34;:8212,&#34;ìĤŃ&#34;:8213,&#34;ĠëĤĺëłĪìĿ´ìħĺ&#34;:8214,&#34;Ġíķĺê¸°&#34;:8215,&#34;Ġìłķëĭ¹&#34;:8216,&#34;Ġë¶ķ&#34;:8217,&#34;íķłìĪĺê°Ģ&#34;:8218,&#34;Ġì§Ħë¦¬&#34;:8219,&#34;ĠìµľìĨĮ&#34;:8220,&#34;ê²ĥìĹĲ&#34;:8221,&#34;íĸĪëĭ¤ê°Ģ&#34;:8222,&#34;ìĤ¬ëıĦ&#34;:8223,&#34;ĠëŃ¥ë¯¸&#34;:8224,&#34;ĠëĵľëĦ¤ìļĶ&#34;:8225,&#34;Ġê°Ļìķĺëĭ¤&#34;:8226,&#34;ë¹Ħíķ´&#34;:8227,&#34;Ġë³¼ëł¤ê³ł&#34;:8228,&#34;ĠìľłëŁ½&#34;:8229,&#34;ĠíĸĪëįĶëĭĪ&#34;:8230,&#34;ìĺģìĽĲ&#34;:8231,&#34;Ġìķłíĭĭ&#34;:8232,&#34;Ġìĭ¶ìĬµëĭĪëĭ¤&#34;:8233,&#34;ìŀ¬ë°ĮìĬµëĭĪëĭ¤&#34;:8234,&#34;íı¬ìĿ¸íĬ¸&#34;:8235,&#34;Ġë³´ëĬĶê±°&#34;:8236,&#34;Ġê¸°ëĮĢìĿ´ìĥģ&#34;:8237,&#34;Ġê°ķê°Ħ&#34;:8238,&#34;ĠëıĻìĺģìĥģ&#34;:8239,&#34;ĠìķĦê¹ĮìĽĮìĦľ&#34;:8240,&#34;Ġê´ľì°®ìķĺìĿĮ&#34;:8241,&#34;ê¹Ģê¸°ëįķ&#34;:8242,&#34;Ġëĭ¨ì²´&#34;:8243,&#34;Ġë¹łì§ĢëĬĶ&#34;:8244,&#34;ĠìĺģìĥģëıĦ&#34;:8245,&#34;íķľíħĮëĬĶ&#34;:8246,&#34;ì´Īëĵ±íķĻìĥĿ&#34;:8247,&#34;Ġê°ľë´īíķľ&#34;:8248,&#34;ĠìĨĲë°ľ&#34;:8249,&#34;ĠíĿ¥ë¯¸ë¥¼&#34;:8250,&#34;ĠãħĦ&#34;:8251,&#34;Ġë¶Ģë¶ĦìĿĢ&#34;:8252,&#34;ĠìĭľëĤĺë¦¬ìĺ¤ê°Ģ&#34;:8253,&#34;ĠìŀĪìĹĪê³ł&#34;:8254,&#34;ĠëıĮìķĦë³´ê²Į&#34;:8255,&#34;ê²ĥê°ĻëĦ¤ìļĶ&#34;:8256,&#34;Ġëĺĳê°ĻìĿ´&#34;:8257,&#34;ìŀĳê°Ģê°Ģ&#34;:8258,&#34;ĠëĬ¥ê°Ģ&#34;:8259,&#34;ì²¨ìĹĶ&#34;:8260,&#34;Ġë§ĪìĿ´ëĦĪìĬ¤&#34;:8261,&#34;ĠìŀĲê·¹ìłģìĿ¸&#34;:8262,&#34;Ġì§Ħì§Ģíķľ&#34;:8263,&#34;Ġíģ¬ë¦¬ìĬ¤ë§ĪìĬ¤&#34;:8264,&#34;ĠëĪĦêµ°ì§Ģ&#34;:8265,&#34;ìŀĪëįĺëį°&#34;:8266,&#34;ĠìĦłìĥĿëĭĺ&#34;:8267,&#34;Ġìĸ´ëł¸ìĿĦ&#34;:8268,&#34;ë¶ĦìľĦê¸°&#34;:8269,&#34;Ġíİ¼ì³Ĳ&#34;:8270,&#34;Ġë°Ķíĥķìľ¼ë¡ľ&#34;:8271,&#34;ed&#34;:8272,&#34;Ġis&#34;:8273,&#34;ì§Ģê¸Īë³´&#34;:8274,&#34;ĠìķĦíĶĦëĭ¤&#34;:8275,&#34;Ġë³´ìĿ´ì§Ģ&#34;:8276,&#34;Ġê·¸ìķ¼ë§Ĳë¡ľ&#34;:8277,&#34;ìĬ¤íĶ¼&#34;:8278,&#34;ìŀĲìĿ¸&#34;:8279,&#34;ìĹĨëĬĶëį°&#34;:8280,&#34;ĠíķĺíĴĪ&#34;:8281,&#34;ìĪĺìĦł&#34;:8282,&#34;ë²ħ&#34;:8283,&#34;ìĥģíĥľ&#34;:8284,&#34;Ġìłķìļ°&#34;:8285,&#34;Ġê²¬ë&#34;:8286,&#34;íķĺê³łìĭ¶ìĿĢ&#34;:8287,&#34;Ġìĸ´ê±°ì§Ģ&#34;:8288,&#34;ĠìĦŃ&#34;:8289,&#34;ê³µì£¼&#34;:8290,&#34;ìĹ¬íĥľ&#34;:8291,&#34;ĠìĹ°ê¸°íķĺëĬĶ&#34;:8292,&#34;ĠìĪĺë©´&#34;:8293,&#34;êµ¬ìĻĢ&#34;:8294,&#34;ìĭ¤íĮ¨&#34;:8295,&#34;Ġë§ĲíĪ¬&#34;:8296,&#34;íĥĢì¿ł&#34;:8297,&#34;ìĺĢì§Ģ&#34;:8298,&#34;Ġíĺĳ&#34;:8299,&#34;íĬ¸ëĬĶ&#34;:8300,&#34;Ġìĺ¤ëĬĶ&#34;:8301,&#34;Ġë¹Ħíĺ¸ê°Ĳ&#34;:8302,&#34;ëģĹ&#34;:8303,&#34;ìĭ¬ìĭ¬&#34;:8304,&#34;Ġë¶ĢìŀĲìĹ°&#34;:8305,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪìĬµëĭĪëĭ¤&#34;:8306,&#34;Ġê³łìĸĳìĿ´&#34;:8307,&#34;íħľ&#34;:8308,&#34;ë§Īëŀĳ&#34;:8309,&#34;ì¶Ķìĸ´&#34;:8310,&#34;ìĶ¨ëıĦ&#34;:8311,&#34;ĠíĻĶìŀ¥ìĭ¤&#34;:8312,&#34;íķĺëĤĺíķĺëĤĺ&#34;:8313,&#34;ìŀĺìĥĿ&#34;:8314,&#34;íķĺê¸°ìĹĲ&#34;:8315,&#34;Ġì¡¸ìĹħ&#34;:8316,&#34;Ġì¶©ìĭ¤&#34;:8317,&#34;ĠìłĦê°ľëĬĶ&#34;:8318,&#34;ì¡ĮìĬµëĭĪëĭ¤&#34;:8319,&#34;ëŀľìĬ¤&#34;:8320,&#34;Ġë¨¹ì¹ł&#34;:8321,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:8322,&#34;Ġê°Ģì¡±ìĿĺ&#34;:8323,&#34;ĠêµŃê°Ģ&#34;:8324,&#34;ĠìĿ¸ìĥģìłģìĿ¸&#34;:8325,&#34;ĠìĦ¸ìĥģìĿ´&#34;:8326,&#34;ĠìĹĶëĶ©ìĿ´&#34;:8327,&#34;2014&#34;:8328,&#34;ê°ĶëĬĶëį°&#34;:8329,&#34;ĠìŀĦíĮ©íĬ¸&#34;:8330,&#34;ìĭľëĮĢìĿĺ&#34;:8331,&#34;ĠêµĲíĽĪëıĦ&#34;:8332,&#34;ĠìłĦì²´ìłģìĿ¸&#34;:8333,&#34;ìķŀìĹĲ&#34;:8334,&#34;Ġê°ľê·¸ë§¨&#34;:8335,&#34;ĠìŀĺìĥĿê²¼&#34;:8336,&#34;Ġë¹Īìķ½&#34;:8337,&#34;ĠìĺģìĽĲíķľ&#34;:8338,&#34;ëĮĢëĭ¨íķľ&#34;:8339,&#34;Ġëįĺìł¸&#34;:8340,&#34;Ġë°ĿíĺĢ&#34;:8341,&#34;ĠíĴĭíĴĭíķľ&#34;:8342,&#34;ìłĦë¬¸ê°Ģ&#34;:8343,&#34;op&#34;:8344,&#34;§íĮħ&#34;:8345,&#34;Ġãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:8346,&#34;ħķ&#34;:8347,&#34;ĵ¯&#34;:8348,&#34;ëĭ¬ë¦¬&#34;:8349,&#34;ìĿ´ê²ĥìĿ´&#34;:8350,&#34;ëıĦíķľ&#34;:8351,&#34;ë¦¬ëŀĳ&#34;:8352,&#34;ĠìĺģíĻĶì§Ģ&#34;:8353,&#34;ĠìĺģíĻĶìĿ¼&#34;:8354,&#34;ëĦĲëıĮ&#34;:8355,&#34;Ġë³´ëł¤&#34;:8356,&#34;ìĭľì§Ģ&#34;:8357,&#34;ìĭľëĦ¤&#34;:8358,&#34;ìľ¼ìħ¨&#34;:8359,&#34;ĠëĤĺìĹĲê²Ĳ&#34;:8360,&#34;Ġíķĺëĭ¤ëĭĪ&#34;:8361,&#34;Ġì¢ĭëĦ¤&#34;:8362,&#34;ê°ĦíŀĪ&#34;:8363,&#34;Ġìĺ³&#34;:8364,&#34;Ġê°ĢìĪĺ&#34;:8365,&#34;Ġê°ĢíŀĪ&#34;:8366,&#34;ì¹ĺë©´&#34;:8367,&#34;ĠëĮĢê²°&#34;:8368,&#34;ìĿ¼ìĿĢ&#34;:8369,&#34;ëŀĺëĵľ&#34;:8370,&#34;ìĤ¬ê³ł&#34;:8371,&#34;Ġë¬´ê²ģ&#34;:8372,&#34;ìħľ&#34;:8373,&#34;ĠìķĪëĤĺìĺ¨ëĭ¤&#34;:8374,&#34;Ġë§Īì°¬&#34;:8375,&#34;Ġëª»ë§Įëĵ¤&#34;:8376,&#34;ëĤľíķ´&#34;:8377,&#34;ë³¸ì§Ģ&#34;:8378,&#34;Ġê²ĥìĿ¸ê°Ģ&#34;:8379,&#34;ĠìĿ´ëŁ°ê±´&#34;:8380,&#34;ĠëģĿëıĦ&#34;:8381,&#34;ëıĻê·¼&#34;:8382,&#34;ĠìĤ¬ëŀĳíķĺê³ł&#34;:8383,&#34;Ġë³´ëĬĶê²ĥ&#34;:8384,&#34;ëĸ¼&#34;:8385,&#34;ìŀ¼ìŀĩ&#34;:8386,&#34;êµ°ëĮĢ&#34;:8387,&#34;ĠíķłëķĮ&#34;:8388,&#34;Ġì£½ìĿ´&#34;:8389,&#34;ìĺĪìĥģ&#34;:8390,&#34;ê·¹íŀĪ&#34;:8391,&#34;ë¶ĢíĦ°ëĬĶ&#34;:8392,&#34;ĠìĽĲì¡°&#34;:8393,&#34;ĠíķľêµŃìĿ¸&#34;:8394,&#34;Ġë§Įëĵłê±°&#34;:8395,&#34;Ġìŀ¼ìŀĩ&#34;:8396,&#34;ì§ĳìĹĲìĦľ&#34;:8397,&#34;ĠìĿ´ìĥģìĿĢ&#34;:8398,&#34;ĠëĤ®ëĦ¤ìļĶ&#34;:8399,&#34;ĠìłģìłĪíķľ&#34;:8400,&#34;ìŀ¡ëĬĶ&#34;:8401,&#34;íĿ¬ìĿĺ&#34;:8402,&#34;Ġìŀ¬ê°ľë´ī&#34;:8403,&#34;ëķ¡&#34;:8404,&#34;Ġëĭ´ê³ł&#34;:8405,&#34;êº¼ìķ¼&#34;:8406,&#34;Ġë§īìŀ¥ëĵľëĿ¼ë§Ī&#34;:8407,&#34;ëįķë¶ĦìĹĲ&#34;:8408,&#34;Ġ2014&#34;:8409,&#34;ì¹ĺê³łëĬĶ&#34;:8410,&#34;ĠìķłëĭĪëĬĶ&#34;:8411,&#34;Ġì²¨ìĿ´ëĭ¤&#34;:8412,&#34;ĠíĻĺê²½&#34;:8413,&#34;ëĨĢëĵľ&#34;:8414,&#34;Ġìľłì¾Įíķĺê²Į&#34;:8415,&#34;ìĸµì§Ģë¡ľ&#34;:8416,&#34;ìłľìŀĳë¹Ħ&#34;:8417,&#34;ëĨ¨ëĦ¤&#34;:8418,&#34;ë¬´ìĦľìļ´&#34;:8419,&#34;Ġê²°ë¡łìĿĢ&#34;:8420,&#34;ĠìĮ©&#34;:8421,&#34;ĠìĬ¤íı¬ì¸ł&#34;:8422,&#34;.-&#34;:8423,&#34;Dë¡ľ&#34;:8424,&#34;²ł&#34;:8425,&#34;ìµĿìĺ¤&#34;:8426,&#34;ĠR&#34;:8427,&#34;ìŀĥ&#34;:8428,&#34;ìĹĲíļ¨&#34;:8429,&#34;ìĦľë¡ľ&#34;:8430,&#34;ìĸ´ìĸ´&#34;:8431,&#34;ë§Įíķĺê³ł&#34;:8432,&#34;ìĺ¬ë¦¬ë&#34;:8433,&#34;ìĭľìĬ¤&#34;:8434,&#34;ì£¼íĸī&#34;:8435,&#34;ĠìŀĪìľ¼ëĤĺ&#34;:8436,&#34;ĠìĹ°ìĥģ&#34;:8437,&#34;ê·¸ëĭ¤ì§Ģ&#34;:8438,&#34;Ġíķľë²ĪëįĶ&#34;:8439,&#34;ìĨĮë¦¬ê°Ģ&#34;:8440,&#34;Ġëª»ë³¸&#34;:8441,&#34;ë¬´ì¡°ê±´&#34;:8442,&#34;Ġê²ĥëĵ¤ìĿ´&#34;:8443,&#34;Ġë§Įëĵ¤ìĪĺ&#34;:8444,&#34;ĠëĵľëĿ¼ë§Īì¤ĳ&#34;:8445,&#34;ë¦¬ë³´&#34;:8446,&#34;ë¦¬ëĥĲ&#34;:8447,&#34;ĠëĲĺëĬĶëį°&#34;:8448,&#34;Ġì¡°ëĭĪëİģ&#34;:8449,&#34;Ġìļ°ë¢°ë§¤&#34;:8450,&#34;íĤ¥&#34;:8451,&#34;Ġìŀ¬ë¯¸ìŀĪìĸ´&#34;:8452,&#34;ëĤ¨ëħĢ&#34;:8453,&#34;ì½ĶëĤľ&#34;:8454,&#34;Ġì¶Ķë¦¬&#34;:8455,&#34;ĠíĸĪìľ¼ëĤĺ&#34;:8456,&#34;Ġë¶ĪìķĪ&#34;:8457,&#34;Ġë©ĭìŀĪê³ł&#34;:8458,&#34;ì¹ľêµ¬ëŀĳ&#34;:8459,&#34;ĠëıĦëĳĳ&#34;:8460,&#34;ĠíıīìĿĦ&#34;:8461,&#34;ëª»íķĺê³ł&#34;:8462,&#34;ĠìĿ´ê±°ë³´ëĭ¨&#34;:8463,&#34;Ġìŀ¼ìŀĪëĬĶ&#34;:8464,&#34;Ġìļ¸ìĹĪìĸ´ìļĶ&#34;:8465,&#34;ĠìĹĲìĦľ&#34;:8466,&#34;Ġê°Ģìŀ¥íķľ&#34;:8467,&#34;ìĤ¬ë¯¸&#34;:8468,&#34;Ġê³¼ëĮĢ&#34;:8469,&#34;ëĤĺìĺ¤ëĦ¤&#34;:8470,&#34;ëŀľëĵľ&#34;:8471,&#34;Ġíı¬ìĿ¸íĬ¸&#34;:8472,&#34;ĠìķĦë¬´íĬ¼&#34;:8473,&#34;ë§ŀëĬĶ&#34;:8474,&#34;Ġëĭ´ëĭ´&#34;:8475,&#34;ì°½ìłķ&#34;:8476,&#34;ìºĲë¦¬&#34;:8477,&#34;Ġì§Īë¦¬&#34;:8478,&#34;Ġíĺķìłľ&#34;:8479,&#34;Ġíĸīë³µíķĺê²Į&#34;:8480,&#34;ìĿ¼ë³¸ìĺģíĻĶ&#34;:8481,&#34;Ġëĭµëĭµíķĺëĭ¤&#34;:8482,&#34;Ġê¸Ģê³ł&#34;:8483,&#34;ĠìĺģíĻĶëĿ¼ì§Ģë§Į&#34;:8484,&#34;ĠìĸĳìķĦì¹ĺ&#34;:8485,&#34;ĠìĭľëĮĢìĿĺ&#34;:8486,&#34;Ġë¶ĪìĮįíķľ&#34;:8487,&#34;Ġëĭ¨ìĪľíŀĪ&#34;:8488,&#34;ìºħ&#34;:8489,&#34;Ġë©ĶìĦ¸ì§Ģ&#34;:8490,&#34;Ġì°¸ìĭłíķľ&#34;:8491,&#34;ĠëĤĺë¨¸ì§ĢëĬĶ&#34;:8492,&#34;Ġê³µíı¬ë¥¼&#34;:8493,&#34;ĠíıĲì§Ģ&#34;:8494,&#34;ĠìıŁìķĦ&#34;:8495,&#34;ìĥĪë¡Ŀ&#34;:8496,&#34;Ġìĸ´ëĳĲìļ´&#34;:8497,&#34;ì¨Įëĵł&#34;:8498,&#34;ĠìĽ°ë©ĶìĿ´ëĵľ&#34;:8499,&#34;ĠìŀĲëıĻì°¨&#34;:8500,&#34;ãĤ&#34;:8501,&#34;ķħ&#34;:8502,&#34;ìĹĲëĭ¤&#34;:8503,&#34;ìĹĲëĭ¤ê°Ģ&#34;:8504,&#34;ê¸°ìĪł&#34;:8505,&#34;ĠìĿ´ë»&#34;:8506,&#34;...!&#34;:8507,&#34;ë©Ģ&#34;:8508,&#34;ìĿ¸ìĺģíĻĶ&#34;:8509,&#34;ìĿ¸ê±¸&#34;:8510,&#34;ìľ¼ëŁ¬&#34;:8511,&#34;ëĮĢê³ł&#34;:8512,&#34;Ġì¢ĭìľ¼ëĤĺ&#34;:8513,&#34;ì£¼ìĦ±ì¹ĺ&#34;:8514,&#34;Ġíķľíİ¸ìĿĺ&#34;:8515,&#34;ëĵľëĦ¤ìļĶ&#34;:8516,&#34;ëĵľìĽĮ&#34;:8517,&#34;Ġìĭľì¼ľ&#34;:8518,&#34;ĠìĬ¤íħĿ&#34;:8519,&#34;ĠìµľìĨĮíķľ&#34;:8520,&#34;Ġëª¨ìķĦ&#34;:8521,&#34;Ġë§ĲìĿĢ&#34;:8522,&#34;ëħĦê°Ħ&#34;:8523,&#34;Ġê²ĥë§Į&#34;:8524,&#34;Ġë³¼ê¹Į&#34;:8525,&#34;Ġë¶ĢíĻľ&#34;:8526,&#34;ì¢ĭìķĺìĸ´ìļĶ&#34;:8527,&#34;ĠëĵľëĿ¼ë§ĪìĹĲ&#34;:8528,&#34;ĠëģĿëĤĺìĦľ&#34;:8529,&#34;ëıĻìĿ´&#34;:8530,&#34;ĠëŃĲìŀĦ&#34;:8531,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪìĿĮ&#34;:8532,&#34;Ġìĭ¶ëĭ¤ë©´&#34;:8533,&#34;Ġê³łë¬¸&#34;:8534,&#34;ëŁ½ì§Ģ&#34;:8535,&#34;ê´ĢìĿĦ&#34;:8536,&#34;Ġìĭľê°ĦëķĮ&#34;:8537,&#34;ĠëķĮëĬĶ&#34;:8538,&#34;ĠìĺĪì¸¡&#34;:8539,&#34;ì°¨íĶ¼&#34;:8540,&#34;íĭ°ê°Ģ&#34;:8541,&#34;ĠíĸĪìĬµëĭĪëĭ¤&#34;:8542,&#34;Ġë©ĭì§Ģ&#34;:8543,&#34;Ġê´Ģìĭ¬ìĿ´&#34;:8544,&#34;ìłķëıĦìĿĺ&#34;:8545,&#34;ĠëĶ°ìľĦ&#34;:8546,&#34;ë§¤ìĿ´ìħĺ&#34;:8547,&#34;Ġë¡ľë§Ŀ&#34;:8548,&#34;ìŀĺë§Įëĵł&#34;:8549,&#34;ëĭĪë²Ħ&#34;:8550,&#34;Ġëĭ¨ìĸ´&#34;:8551,&#34;ì°¸ëĤĺ&#34;:8552,&#34;ë°°ëĬĶ&#34;:8553,&#34;ĠëĤ¨ìŀĲì£¼ìĿ¸ê³µ&#34;:8554,&#34;ĠìĿ¸ê°ĦìĿĢ&#34;:8555,&#34;Ġìĭ¤ë§ĿìĿ´&#34;:8556,&#34;ĠíĤ¹&#34;:8557,&#34;Ġëª¨ìĬµìĹĲ&#34;:8558,&#34;ĠìºĲë¦ŃíĦ°ëĵ¤&#34;:8559,&#34;ĠìºĲë¦ŃíĦ°ë¥¼&#34;:8560,&#34;ãĦ·ãĦ·ãĦ·ãĦ·&#34;:8561,&#34;ĠìĨĲê°ĢëĿ½&#34;:8562,&#34;ĠëĴ¤ì£½ë°ķì£½&#34;:8563,&#34;Ġìŀĳê°ĢìĿĺ&#34;:8564,&#34;ëħ¸ìŀ¼ëħ¸ìŀ¼&#34;:8565,&#34;ëĪĪìĿ´&#34;:8566,&#34;ĠìłĪë§Ŀ&#34;:8567,&#34;íĮ¬ìĿ´&#34;:8568,&#34;íĻķìĭ¤íŀĪ&#34;:8569,&#34;ĠíĴĢìĸ´ëĤ¸&#34;:8570,&#34;ëĤĺìĻĶìľ¼ë©´&#34;:8571,&#34;Ġíķµëħ¸ìŀ¼&#34;:8572,&#34;íķĻëħĦëķĮ&#34;:8573,&#34;ĠìĻ¸ê³ĦìĿ¸&#34;:8574,&#34;ìĬ¤íĭ°ë¸Ĳ&#34;:8575,&#34;ê´ľíŀĪ&#34;:8576,&#34;ìłĬìĿĢ&#34;:8577,&#34;&gt;&gt;&#34;:8578,&#34;´ëĵľ&#34;:8579,&#34;Ġx&#34;:8580,&#34;ìĿ´ìłķ&#34;:8581,&#34;ê³½&#34;:8582,&#34;ê³łìĭ¶ëĭ¤&#34;:8583,&#34;íĻĶëĬĶ&#34;:8584,&#34;ĠìŀĪëĥĲ&#34;:8585,&#34;ëıĦìłĢíŀĪ&#34;:8586,&#34;íķľê°Ģ&#34;:8587,&#34;ê¸°ìŀĲ&#34;:8588,&#34;ê¸°ë³´ëĭ¨&#34;:8589,&#34;ĠìĿ´ë§Įíķľ&#34;:8590,&#34;ĠìĿ´ìģ¨&#34;:8591,&#34;ìĸ´ëĤĺìĦľ&#34;:8592,&#34;ë§Įëĵ¬&#34;:8593,&#34;Ġìłĸ&#34;:8594,&#34;ëĵ¤ëŁ¬&#34;:8595,&#34;ìķĦëĵ¤ìĿ´&#34;:8596,&#34;Ġë³´ìķĦëıĦ&#34;:8597,&#34;Ġê·¸ëĵ¤ìĿ´&#34;:8598,&#34;Ġê·¸ëıĻìķĪ&#34;:8599,&#34;ìĬ¤ëłĪ&#34;:8600,&#34;ìĬ¤íĦ´&#34;:8601,&#34;ë§Ĳíķł&#34;:8602,&#34;ì¤įëĭĪëĭ¤&#34;:8603,&#34;Ġëĭ¤ë³´ê³ł&#34;:8604,&#34;ìĪĺìłķ&#34;:8605,&#34;ìŀ¥ìĿĢ&#34;:8606,&#34;ĠìĬµ&#34;:8607,&#34;Ġì§ĢíĤ¤&#34;:8608,&#34;ĠëĤ´ëıĪ&#34;:8609,&#34;Ġê¸°íĥĢ&#34;:8610,&#34;ĠìķĬìĬµëĭĪëĭ¤&#34;:8611,&#34;Ġê±·&#34;:8612,&#34;ìĨĮìĭľ&#34;:8613,&#34;Ġìµľê³łëĿ¼ê³ł&#34;:8614,&#34;Ġê°ľëĤĺ&#34;:8615,&#34;ĠìŀĲë³¸&#34;:8616,&#34;Ġë³¸ê²ĥ&#34;:8617,&#34;ìľłë¨¸&#34;:8618,&#34;ìµľê°ķ&#34;:8619,&#34;Ġê³µì§ľë¡ľ&#34;:8620,&#34;ĠëģĿìĿĦ&#34;:8621,&#34;Ġì¡°ì¹´&#34;:8622,&#34;Ġíķ´ìķ¼ì§Ģ&#34;:8623,&#34;ë¬¼ë¡ľ&#34;:8624,&#34;Ġì¹ł&#34;:8625,&#34;ëª¨ë¥¼&#34;:8626,&#34;ĠìĽĥê¸°ëĬĶ&#34;:8627,&#34;Ġìĥģì§ķ&#34;:8628,&#34;íĶĦëłĪ&#34;:8629,&#34;ĠíķłìķĦë²Ħì§Ģ&#34;:8630,&#34;ì½Ķëĵľ&#34;:8631,&#34;Ġê¸°ëĮĢìķĪíķĺê³ł&#34;:8632,&#34;ì²ľìĽĲ&#34;:8633,&#34;ĠìĹ°ì¶ľëł¥ìĿ´&#34;:8634,&#34;ĠíĸĪìĿĦê¹Į&#34;:8635,&#34;ìĤ´ëķĮ&#34;:8636,&#34;Ġë³´ìĹ¬ì¤Ħ&#34;:8637,&#34;íĽ¨&#34;:8638,&#34;íķĺê¸°ê°Ģ&#34;:8639,&#34;Ġê¼Ńë³´ìĦ¸ìļĶ&#34;:8640,&#34;ìĺĢëĭ¤ë©´&#34;:8641,&#34;Ġêµ¬ìĦ±ëıĦ&#34;:8642,&#34;ê°ĲëıĻìłģ&#34;:8643,&#34;ê°ĲëıĻê³¼&#34;:8644,&#34;Ġê±¸ê¹Į&#34;:8645,&#34;ĠìĨįìķĺëĭ¤&#34;:8646,&#34;Ġìľłì¹ĺíķ´ìĦľ&#34;:8647,&#34;Ġãħİãħİãħİãħİ&#34;:8648,&#34;Ġë°ĺìłĦìĹĲ&#34;:8649,&#34;ĠìĿ¸ê°Ħëĵ¤&#34;:8650,&#34;ìŀ¥ë©´ìĹĲìĦľ&#34;:8651,&#34;ì£½ìĿĮ&#34;:8652,&#34;ìĹŃìĭľëĤĺ&#34;:8653,&#34;ìķĦìĿ´ëıĮ&#34;:8654,&#34;ë¸Įë¦¬&#34;:8655,&#34;Ġë²Ħëł¤&#34;:8656,&#34;ë»ĶíĸĪëĭ¤&#34;:8657,&#34;Ġê°Ģì¡±ìĺģíĻĶ&#34;:8658,&#34;Ġëª¨ë¥´ê²łëĦ¤&#34;:8659,&#34;ëıĮëł¤&#34;:8660,&#34;ĠìķŀìĹĲ&#34;:8661,&#34;ê²ģëĤĺ&#34;:8662,&#34;ëĨĪìĿ´&#34;:8663,&#34;ĠíĮĲíĥĢ&#34;:8664,&#34;ìłĲëĮĢê°Ģ&#34;:8665,&#34;Ġê¸¸ìĿĦ&#34;:8666,&#34;ĠìĿ´ëĶ´ê±¸&#34;:8667,&#34;íķĦë¦Ħ&#34;:8668,&#34;íķĺìŀĲë©´&#34;:8669,&#34;ĠëĤ®ìĿĢì§Ģ&#34;:8670,&#34;Ġë§ĮëĵľëĦ¤&#34;:8671,&#34;Ġìŀĺë§Įëĵ¤ìĹĪëĭ¤&#34;:8672,&#34;ë²Īë´Ĳ&#34;:8673,&#34;ĠíĭĢë¦¼&#34;:8674,&#34;Ġì°½íĶ¼&#34;:8675,&#34;ĠìķĪë§ŀëĬĶ&#34;:8676,&#34;ĠìĽĮëĤĻ&#34;:8677,&#34;ë§¤ëł¥ìłģìĿ¸&#34;:8678,&#34;Ġìī¬ìļ´&#34;:8679,&#34;Ġíĥľìĸ´ëĤĺìĦľ&#34;:8680,&#34;ìĻĦë²½íķľ&#34;:8681,&#34;ëĤ´ìĥĿìĹĲ&#34;:8682,&#34;ìķĪëĲľëĭ¤&#34;:8683,&#34;ĠìłĲìĪĺì¤Ģê²ĥëĵ¤&#34;:8684,&#34;Ġê°Ŀê´Ģ&#34;:8685,&#34;²ķ&#34;:8686,&#34;ıëĭ¤&#34;:8687,&#34;ìĿ´ë¯¼&#34;:8688,&#34;ìĿ´ìĹĪëįĺ&#34;:8689,&#34;ìĹ£&#34;:8690,&#34;ê°Ģë©°&#34;:8691,&#34;ĠìĿ´ëĿ¼ëĬĶ&#34;:8692,&#34;ìĸ´ë¨¸ëĭĪ&#34;:8693,&#34;...(&#34;:8694,&#34;Ġê°ĵ&#34;:8695,&#34;êµ¬ë¦¬&#34;:8696,&#34;Ġë³´ëĿ¼&#34;:8697,&#34;Ġì¢ĭì§Ģ&#34;:8698,&#34;ì£¼ëĬĶëį°&#34;:8699,&#34;ì§ĦìĿĺ&#34;:8700,&#34;Ġíķľëĭ¤ê³ł&#34;:8701,&#34;ìķĺëĤĺ&#34;:8702,&#34;íķłë§ĲìĿ´&#34;:8703,&#34;Ġëį®&#34;:8704,&#34;êµ¬ëĬĶ&#34;:8705,&#34;Ġë¬´ìĭĿ&#34;:8706,&#34;ë¶ĦëıĦ&#34;:8707,&#34;íİ¸ê¹Įì§Ģ&#34;:8708,&#34;íĬ¸ë¦Ń&#34;:8709,&#34;Ġìŀ¥êµŃìĺģ&#34;:8710,&#34;Ġë²ħ&#34;:8711,&#34;Ġë¶Ģìłķ&#34;:8712,&#34;ĠìķĪë´ĲëıĦ&#34;:8713,&#34;ëªħìĿĢ&#34;:8714,&#34;ĠëģĿëĤł&#34;:8715,&#34;ĠëĤ´ìļ©ìłĦê°ľ&#34;:8716,&#34;ĠëĲĺëıĮìķĦ&#34;:8717,&#34;ëıĻê±´&#34;:8718,&#34;ì¡°íıŃ&#34;:8719,&#34;ëª¨ìĸĳ&#34;:8720,&#34;Ġëĭ¤ìĭľê¸Ī&#34;:8721,&#34;ëŃī&#34;:8722,&#34;Ġëĵ¤ê²Į&#34;:8723,&#34;Ġë§Īì§Ģë§īìĿ´&#34;:8724,&#34;Ġë³´ìĹ¬ì¤¬&#34;:8725,&#34;Ġë°°ê¼½&#34;:8726,&#34;íķĺëĤĺìļĶ&#34;:8727,&#34;ìĽłëĬĶëį°&#34;:8728,&#34;ãħľãħľãħľ&#34;:8729,&#34;Ġë§ŀëĤĺ&#34;:8730,&#34;ĠëĪĦêµ¬ë&#34;:8731,&#34;ìĤ¬ëŀĳíķĺëĬĶ&#34;:8732,&#34;ĠìķĦëĭĪëĿ¼ë©´&#34;:8733,&#34;íĿ¬ìĦł&#34;:8734,&#34;íĻ©ëĭ¹&#34;:8735,&#34;Ġë¨¸ë¦¿&#34;:8736,&#34;ìĿ´ëĿ¼ëĬĶê²Į&#34;:8737,&#34;ì²ĺìĿĮìĹĲ&#34;:8738,&#34;Ġê°Ģì¡±ìĿ´&#34;:8739,&#34;Ġê³¤&#34;:8740,&#34;ĠìŀłìĿĦ&#34;:8741,&#34;ĠëĮĢìĤ¬ëıĦ&#34;:8742,&#34;ëįķìĹĲ&#34;:8743,&#34;Ġìłľëª©ìĿĦ&#34;:8744,&#34;ĠëĦ¤íĭ°ì¦Į&#34;:8745,&#34;Ġê¸ĢìĿĦ&#34;:8746,&#34;ì°¬ìļ±&#34;:8747,&#34;Ġê·ĢìĹ¬ìĽĮìļĶ&#34;:8748,&#34;Ġì¶©ë¶Ħíķľ&#34;:8749,&#34;Ġth&#34;:8750,&#34;ĠìĬ¤íĨłë¦¬ë¡ľ&#34;:8751,&#34;Ġì»¤ë²Ħ&#34;:8752,&#34;ìłłìŀ¥&#34;:8753,&#34;ëĴ¤ë¡ľ&#34;:8754,&#34;ĠìĿ´ë¯¸ì§Ģ&#34;:8755,&#34;ĠìłĪìłľ&#34;:8756,&#34;Ġì§ģìĹħ&#34;:8757,&#34;ĠíĹĪë¬´íķľ&#34;:8758,&#34;¬ë¦°ëĭ¤&#34;:8759,&#34;Ġìĺ¤ê¸Ģê±°ë¦¬ëĬĶ&#34;:8760,&#34;ì¹ľêµ¬ê°Ģ&#34;:8761,&#34;Ġë®¤ì§ģ&#34;:8762,&#34;Ġê·¸ëł¤ëĤ¸&#34;:8763,&#34;Ġê±°ì§Ģê°ĻìĿĢ&#34;:8764,&#34;Ġëĭ¤ìļ´ë°ĽìķĦìĦľ&#34;:8765,&#34;ĠìĿ´íķĺëıĦ&#34;:8766,&#34;ìĤ´ëĭ¤ìĤ´ëĭ¤&#34;:8767,&#34;ĠíĽĦìĨįìŀĳ&#34;:8768,&#34;Ġê°Ĳëªħê¹Ĭê²Į&#34;:8769,&#34;ëĭ¨ìĪľíķľ&#34;:8770,&#34;ĠëĽ°ìĸ´ëĦĺëĬĶ&#34;:8771,&#34;ĠìłĦë°ĺìłģìľ¼ë¡ľ&#34;:8772,&#34;ëłĪë©ĺíĥĢìĿ¸&#34;:8773,&#34;ĠìĦ¤ëĵĿëł¥&#34;:8774,&#34;Ġì²©ë³´&#34;:8775,&#34;.!&#34;:8776,&#34;bc&#34;:8777,&#34;įĶ&#34;:8778,&#34;ìĹ¬ë¦Ħ&#34;:8779,&#34;ì§ĢìĦŃ&#34;:8780,&#34;ê²»&#34;:8781,&#34;ìĿĢëĵ¯&#34;:8782,&#34;ìĿĢê·¼&#34;:8783,&#34;ìĿĦì§Ģ&#34;:8784,&#34;ĠìĿ´ì¤Ģ&#34;:8785,&#34;ĠìĿ´ìĨĮë£¡&#34;:8786,&#34;ìķĦìĿĺ&#34;:8787,&#34;ìķĦëĭ´&#34;:8788,&#34;ĠìķĦëĵ¤ìĿ´&#34;:8789,&#34;ëŁ¬ë¦¬&#34;:8790,&#34;ëĮĢìŀĳ&#34;:8791,&#34;ìļ°ë¦¬ëĬĶ&#34;:8792,&#34;ìŀ¥ìĹĲ&#34;:8793,&#34;Ġìłķìĥģ&#34;:8794,&#34;ê³¼ìĹ°&#34;:8795,&#34;Ġìĺ®&#34;:8796,&#34;íĥĵ&#34;:8797,&#34;ĠíķľìĪľê°Ħ&#34;:8798,&#34;ìŀ¬ë¡ľ&#34;:8799,&#34;Ġì§ĦìĪĺ&#34;:8800,&#34;Ġë´¤ëĤĺ&#34;:8801,&#34;ìĹ¬ìĦ±&#34;:8802,&#34;ĠìĪĺëĬĶ&#34;:8803,&#34;íĦ°ë¦¬&#34;:8804,&#34;ì¤ĳíķĻêµĲ&#34;:8805,&#34;ëŁ¬ìĬ¤&#34;:8806,&#34;ëķĮëıĦ&#34;:8807,&#34;Ġë§Īëĭ¤&#34;:8808,&#34;ìĭ¤ëł¥&#34;:8809,&#34;Ġëª»ë¯¸&#34;:8810,&#34;ëĶĶì¦ĪëĭĪ&#34;:8811,&#34;íĬ¸ëĿ¼&#34;:8812,&#34;ĠìĿ´ëŁ°ìĺģíĻĶëĬĶ&#34;:8813,&#34;ĠìĨĮíĨµ&#34;:8814,&#34;ëįĶìļ±&#34;:8815,&#34;ìŀ¬ë°ĮìĹĪëĭ¤&#34;:8816,&#34;ĠìĽĥìĹĪëĭ¤&#34;:8817,&#34;Ġë°ĶëĿ¼ëĬĶ&#34;:8818,&#34;Ġìĥģíĥľ&#34;:8819,&#34;ì§Īì§Īëģ&#34;:8820,&#34;ĠëķĮê°Ģ&#34;:8821,&#34;ĠëĦĺìĸ´ìĦľ&#34;:8822,&#34;ľì°¬&#34;:8823,&#34;ëł¸ëĦ¤&#34;:8824,&#34;Ġë¡ľëĵľ&#34;:8825,&#34;ãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭãħĭ&#34;:8826,&#34;Ġìĸ´ëĸł&#34;:8827,&#34;ëª©ìĨĮë¦¬&#34;:8828,&#34;Ġê¹Ģì¹ĺ&#34;:8829,&#34;ìĽĥê¸°ê³ł&#34;:8830,&#34;Ġãħłãħłãħłãħł&#34;:8831,&#34;Ġì¤ĦìĿ´ìķ¼&#34;:8832,&#34;ìķĪë´¤&#34;:8833,&#34;ĠìķĦìĿ´ëĶĶìĸ´&#34;:8834,&#34;ìĬ¤ëŁ¬ìĽłëĭ¤&#34;:8835,&#34;ìĵ°ëłĪê¸°ìĵ°ëłĪê¸°&#34;:8836,&#34;íĿ¬ë§Ŀ&#34;:8837,&#34;ìķĦìĿ´ëĵ¤&#34;:8838,&#34;ë¿ĲìĿ´ëĭ¤&#34;:8839,&#34;Ġì¤Ģë¹Ħ&#34;:8840,&#34;ìĬ¹ìļ°&#34;:8841,&#34;ĠíĳľíĺĦìĿ´&#34;:8842,&#34;Ġëĵ±ë¡Ŀ&#34;:8843,&#34;íĹĪìĪł&#34;:8844,&#34;Ġëĸ¨ìĸ´ëľ¨&#34;:8845,&#34;ĠìķĦìī¬ìĽĮìļĶ&#34;:8846,&#34;ĠìŀĬìĿĦìĪĺ&#34;:8847,&#34;ĠìĨĮìŀ¬ìĿĺ&#34;:8848,&#34;Ġì¡°ê¸Īë§Į&#34;:8849,&#34;ĠìĤ´ìķĦìŀĪëĬĶ&#34;:8850,&#34;íļ¨ì£¼&#34;:8851,&#34;ĠìĿ¸ìĥģìłģ&#34;:8852,&#34;Ġìĭľì²ŃìŀĲ&#34;:8853,&#34;Ġê¹¨ìķĮ&#34;:8854,&#34;ì¤ĳê°Ħì¤ĳê°Ħ&#34;:8855,&#34;Ġíĺ¼ëŀĢ&#34;:8856,&#34;ìĭľíĤ¤ê³ł&#34;:8857,&#34;ìĭľíĤ¤ì§Ģ&#34;:8858,&#34;ãħĩãħĩãħĩãħĩ&#34;:8859,&#34;Ġì£ĦìĨ¡&#34;:8860,&#34;Ġìĺģìĥģë¯¸ê°Ģ&#34;:8861,&#34;ê·¸ëŀ¬&#34;:8862,&#34;Ġìĸ¸ëĭĪ&#34;:8863,&#34;ìĭľë¦¬ì¦Īì¤ĳ&#34;:8864,&#34;Ġê·¸ë¦¬ìĽĮ&#34;:8865,&#34;ĠìĺĽëĤłìĹĲ&#34;:8866,&#34;íŀĺëĵł&#34;:8867,&#34;ĠìĬ¹ë¦¬&#34;:8868,&#34;ãĦ¹ãħĩ&#34;:8869,&#34;ì°¾ìķĦ&#34;:8870,&#34;âĺħâĺĨ&#34;:8871,&#34;Ġì°¬ìĸĳ&#34;:8872,&#34;Ġíĺķíİ¸ìĹĨëĬĶ&#34;:8873,&#34;Īëł¨&#34;:8874,&#34;Ġëª½íĻĺ&#34;:8875,&#34;ìĺµëĭĪëĭ¤&#34;:8876,&#34;Ġìĸ´ëķł&#34;:8877,&#34;¤ë²Ħ&#34;:8878,&#34;Ġu&#34;:8879,&#34;Ġ;;;&#34;:8880,&#34;ģĿ&#34;:8881,&#34;..!!&#34;:8882,&#34;ì§ĢìĽĲ&#34;:8883,&#34;ê°Ģë©´ìĦľ&#34;:8884,&#34;Ġìķ¨&#34;:8885,&#34;ëĤĺëĿ¼ëĬĶ&#34;:8886,&#34;ĠìĿ´ìĦ±&#34;:8887,&#34;ĠìĿ´ë³Ħ&#34;:8888,&#34;Ġë§ģ&#34;:8889,&#34;ìĸ´ì°Į&#34;:8890,&#34;ìķĦëł¨&#34;:8891,&#34;ìĿ¸ìĥģ&#34;:8892,&#34;ìĿ¸ëĶĶ&#34;:8893,&#34;ĠìķĦìļ°&#34;:8894,&#34;Ġë³´êµ¬&#34;:8895,&#34;ë³´ëįĺ&#34;:8896,&#34;ìĬ¤ëŀĢ&#34;:8897,&#34;ëŀĮìĿ´&#34;:8898,&#34;ë§ĪëĬĶ&#34;:8899,&#34;ĠëĤĺê³ł&#34;:8900,&#34;ĠíķĺëıĦ&#34;:8901,&#34;Ġíķĺê¸°ìĹĶ&#34;:8902,&#34;ì£¼ìľ¨&#34;:8903,&#34;Ġê²ł&#34;:8904,&#34;ĠìŀĪê¸´&#34;:8905,&#34;ĠìŀĪê²łì§Ģë§Į&#34;:8906,&#34;ê°ĦëıĦ&#34;:8907,&#34;Ġê¸°ê´´&#34;:8908,&#34;íĮ¡&#34;:8909,&#34;Ġë¬´ê±°ìļ´&#34;:8910,&#34;ĠìķĪì¢ĭ&#34;:8911,&#34;ĠìķĪëĤĺìĺ´&#34;:8912,&#34;Ġëª»íķĺê²łëĭ¤&#34;:8913,&#34;Ġë³¸ì§Ī&#34;:8914,&#34;ìłĢê²ĥ&#34;:8915,&#34;ìĽĲìĹĲ&#34;:8916,&#34;ĠìĿ¼ìĿĦ&#34;:8917,&#34;ĠìĤ¬íĪ¬ë¦¬&#34;:8918,&#34;Ġë°°ìļ°ëĵ¤ìĿĦ&#34;:8919,&#34;Ġíķ´ë¦¬íı¬íĦ°&#34;:8920,&#34;Ġê°ĲìĿ´&#34;:8921,&#34;Ġê³łìĸ´&#34;:8922,&#34;íķĺì§Ģë§Ĳê³ł&#34;:8923,&#34;ĠìĹŃê²¹ëĭ¤&#34;:8924,&#34;ë¯¼ìĿĺ&#34;:8925,&#34;Ġíı°&#34;:8926,&#34;ëĬĲìĻĢë¥´&#34;:8927,&#34;ĠëĶ°ë¶Ħ&#34;:8928,&#34;ĠëĤľìŀ¡&#34;:8929,&#34;ë°©ìĭĿ&#34;:8930,&#34;ĠìºĲë¦Ń&#34;:8931,&#34;ëĪĪë&#34;:8932,&#34;Ġë³Ħê±°&#34;:8933,&#34;ê°ĲëıĻìĿĦ&#34;:8934,&#34;Ġë³¼ë§Įíķ©ëĭĪëĭ¤&#34;:8935,&#34;ìĤ¬ëĵľ&#34;:8936,&#34;ĠìłĦê°ľìĹĲ&#34;:8937,&#34;ì±Ļ&#34;:8938,&#34;ëĭ¤ìļ´ë°Ľ&#34;:8939,&#34;ìŀ¥ë©´ëıĦ&#34;:8940,&#34;ĠíĶ¼íĦ°&#34;:8941,&#34;ĠíĳľíĺĦíķł&#34;:8942,&#34;Ġëĭ´ë°±&#34;:8943,&#34;Ġìĭ¸ìļ°&#34;:8944,&#34;Ġ--&#34;:8945,&#34;íķłìĪĺìŀĪëĬĶ&#34;:8946,&#34;Ġë²łìĿ´&#34;:8947,&#34;ĠìķŀìĦľ&#34;:8948,&#34;oooo&#34;:8949,&#34;Ġìŀ¡ìķĦ&#34;:8950,&#34;ìħ¨ìľ¼ë©´&#34;:8951,&#34;íķĺìĭľê¸¸&#34;:8952,&#34;ĠìłĦìŁģìĿĺ&#34;:8953,&#34;ĠìłĦìŁģìĺģíĻĶ&#34;:8954,&#34;íı¬ë¨¸&#34;:8955,&#34;ĠíħĮìĿ´&#34;:8956,&#34;ĠìĹŃìĤ¬ìĥģ&#34;:8957,&#34;ëĴ¤ìĹĲ&#34;:8958,&#34;ëª¨ë¥´ê²Į&#34;:8959,&#34;ëª¨ë¥´ê²łëĭ¤&#34;:8960,&#34;Ġë²Ĺìĸ´ëĤĺ&#34;:8961,&#34;ĠìķĮëł¤ì£¼ëĬĶ&#34;:8962,&#34;ìĦ¼ìĬ¤&#34;:8963,&#34;ìĹ¬ê¸°ìĦľ&#34;:8964,&#34;Ġìĺ¤ê·¸ëĿ¼ëĵľëĬĶ&#34;:8965,&#34;ĠìĤ°ë§Įíķĺê³ł&#34;:8966,&#34;ĠíķĦìļĶíķľê°Ģ&#34;:8967,&#34;íİĻíĬ¸&#34;:8968,&#34;ì¼ĵëª¬&#34;:8969,&#34;Ġìļ°ëł¤ë¨¹&#34;:8970,&#34;ë©įì²Ń&#34;:8971,&#34;Ġì¹Ńì°¬&#34;:8972,&#34;ĠìĦ¬ëľ©&#34;:8973,&#34;Ġë§Īì°¬ê°Ģì§Ģ&#34;:8974,&#34;Good&#34;:8975,&#34;bad&#34;:8976,&#34;Ļíķ©&#34;:8977,&#34;ê³łíķľ&#34;:8978,&#34;ãħĲ&#34;:8979,&#34;Ġìķ¡&#34;:8980,&#34;ĠìĺģíĻĶëŀĳ&#34;:8981,&#34;ë¦¬ìĸ¸&#34;:8982,&#34;ëĭĪìĬ¤&#34;:8983,&#34;Ġì§¬ë½ķ&#34;:8984,&#34;ë©´ìĿĦ&#34;:8985,&#34;ìĭľëıĦ&#34;:8986,&#34;Ġëĭ¿&#34;:8987,&#34;ìŀĲìĭĿ&#34;:8988,&#34;ìĽį&#34;:8989,&#34;ĠëĦĲ&#34;:8990,&#34;ë§ĪìłĢëıĦ&#34;:8991,&#34;ĠìĹĨëįĺ&#34;:8992,&#34;ĠëĤĺìĹ´&#34;:8993,&#34;ìĺ¤ê³ł&#34;:8994,&#34;Ġìĸ´ìłķì©¡&#34;:8995,&#34;ì°Ĳ&#34;:8996,&#34;Ġíķľê±°&#34;:8997,&#34;Ġì§Ħíķľ&#34;:8998,&#34;íķĺëĬĶê²ĥëıĦ&#34;:8999,&#34;ê°ľìĿĺ&#34;:9000,&#34;ê°Ĳìĥģ&#34;:9001,&#34;Ġë§Īìķ½&#34;:9002,&#34;Ġìµľê³łëĦ¤ìļĶ&#34;:9003,&#34;Ġê°ľëĺ¥&#34;:9004,&#34;ë³¸ìĥī&#34;:9005,&#34;Ġë³´ê³łìŀĪëĬĶëį°&#34;:9006,&#34;ìłĢëĤĺ&#34;:9007,&#34;ë²ĪìĿ´ëĤĺ&#34;:9008,&#34;ĠìĤ¬ìĥģ&#34;:9009,&#34;ëĭ¤ëĬĶê±´&#34;:9010,&#34;ëıĻìļ±&#34;:9011,&#34;Ġìļ°ëĬĶ&#34;:9012,&#34;ĠëŃĲëĿ¼ê³ł&#34;:9013,&#34;Ġê°Ĳëıħê³¼&#34;:9014,&#34;Ġê³łëıħ&#34;:9015,&#34;Ġê±°ê¸°ëĭ¤&#34;:9016,&#34;Ġê±°ëĵŃ&#34;:9017,&#34;ĠìĥģëĮĢ&#34;:9018,&#34;Ġìĵ°ëłĪê¸°ë¥¼&#34;:9019,&#34;íıīìłĲì¡°ìłĪ&#34;:9020,&#34;Ġìĭłê³ł&#34;:9021,&#34;íļĮë¶ĢíĦ°&#34;:9022,&#34;ë´Ĳìķ¼ì§Ģ&#34;:9023,&#34;ìĿ´ëŁ°ìĺģíĻĶê°Ģ&#34;:9024,&#34;ĠìĿĺíķ´&#34;:9025,&#34;ìĺĪê³ł&#34;:9026,&#34;ìł¸ìķ¼&#34;:9027,&#34;Ġê¸°ëĮĢìĹĨìĿ´&#34;:9028,&#34;íĮĲìĿĦ&#34;:9029,&#34;ĠíĸĪìĸ´ìļĶ&#34;:9030,&#34;ĠíĸĪëĬĶì§Ģ&#34;:9031,&#34;Ġëĵ¤ìĸ´ìĦľ&#34;:9032,&#34;Ġë©ĭì§Ĳ&#34;:9033,&#34;ĠìĽĲíķĺëĬĶ&#34;:9034,&#34;íĺ¸ëŁ¬&#34;:9035,&#34;ĠëıĦìĻĢ&#34;:9036,&#34;ĠëĤľë¦¬&#34;:9037,&#34;Ġëĭ¹ìŀ¥&#34;:9038,&#34;ĠìĹ¬ìŀĲë¥¼&#34;:9039,&#34;Ġë§īíĮĲ&#34;:9040,&#34;ìĹĩìĸ´ìļĶ&#34;:9041,&#34;ìĪľìĿ´&#34;:9042,&#34;ĠëĨĴì§Ģ&#34;:9043,&#34;ì¼ĢìĿ´&#34;:9044,&#34;ì¡Įìľ¼ë©´&#34;:9045,&#34;ë¶Īíĺ¸ê°Ģ&#34;:9046,&#34;Ġì½Ķë¯¸ëĶĶìĺģíĻĶ&#34;:9047,&#34;ĠëĿ¼ìĬ¤íĬ¸&#34;:9048,&#34;ĠëĬĲê»´ì§Ģ&#34;:9049,&#34;ë§Įíģ¼ìĿ´ëĤĺ&#34;:9050,&#34;ĠíĮĮê²©&#34;:9051,&#34;ĠìĽĲìŀĳìĹĲ&#34;:9052,&#34;ê¿Ģ&#34;:9053,&#34;ĠìłĪëĮĢë¡ľ&#34;:9054,&#34;ĠêµŃìĸ´&#34;:9055,&#34;íĶĮë¦°&#34;:9056,&#34;ì¿¨&#34;:9057,&#34;Ġíļį&#34;:9058,&#34;ê²ĥê°ĻìĿĮ&#34;:9059,&#34;íķĺíķĺíķĺíķĺ&#34;:9060,&#34;Ġë¸Įë£¨ìĬ¤&#34;:9061,&#34;Ġë§Īëĥ¥&#34;:9062,&#34;Ġíı¬ìĬ¤íĦ°ê°Ģ&#34;:9063,&#34;ë¶ģíķľ&#34;:9064,&#34;ĠíĿĳë°±&#34;:9065,&#34;ĠíĤ¬ë§ģíĥĢìŀĦìļ©ìľ¼ë¡ľ&#34;:9066,&#34;ë²Įìį¨&#34;:9067,&#34;ìĨĮìĦ¤ìĿĦ&#34;:9068,&#34;ìĺ¤ê¸Ģê±°&#34;:9069,&#34;Ġë©Ķìĭľì§Ģ&#34;:9070,&#34;****&#34;:9071,&#34;ìķĦê¹Įìļ´ìĺģíĻĶ&#34;:9072,&#34;Ġë°¤ìĹĲ&#34;:9073,&#34;ìŀ¥ë¥´ê°Ģ&#34;:9074,&#34;Ġê²©íĪ¬&#34;:9075,&#34;Ġê¹ĶëģĶíķľ&#34;:9076,&#34;very&#34;:9077,&#34;Ġê±°ê¸°ìĦľ&#34;:9078,&#34;ĠíĿīëĤ´&#34;:9079,&#34;ĠìĬ¤ë¦´ëŁ¬ë¬¼&#34;:9080,&#34;ë®¤ì§Ģì»¬&#34;:9081,&#34;ìľłì¾Įíķĺê³ł&#34;:9082,&#34;ĠìĿ´ëģĮìĸ´&#34;:9083,&#34;ĠëĤ©ëĵĿ&#34;:9084,&#34;ĠëĳĶ&#34;:9085,&#34;ëĵ±ìŀ¥ìĿ¸ë¬¼&#34;:9086,&#34;ê°ĲìĤ¬íķ©ëĭĪëĭ¤&#34;:9087,&#34;BC&#34;:9088,&#34;íĳ¼&#34;:9089,&#34;Ġh&#34;:9090,&#34;Ġ??&#34;:9091,&#34;ĠâĢ&#34;:9092,&#34;ê³łëĵ±íķĻêµĲ&#34;:9093,&#34;Ġêº&#34;:9094,&#34;íķĺê¸¸ëŀĺ&#34;:9095,&#34;ê°Ģìķ¼&#34;:9096,&#34;ë§Įíķ´ìĦł&#34;:9097,&#34;Ġê°±&#34;:9098,&#34;ìĿ¸ì§ĢëĬĶ&#34;:9099,&#34;ìĬ¤ìĹĲìĦľ&#34;:9100,&#34;ìľ¼ëł¤&#34;:9101,&#34;!!âĻ¥&#34;:9102,&#34;ëĮĢê¸°&#34;:9103,&#34;ê¹Įë´Ĳ&#34;:9104,&#34;ìĹĨì§Ģë§Į&#34;:9105,&#34;Ġì¢ĭëįĶëĿ¼&#34;:9106,&#34;ì§Ħìłķ&#34;:9107,&#34;ĠìĥĪë²½ìĹĲ&#34;:9108,&#34;ìĨĶì§ģ&#34;:9109,&#34;ëĤ´ì§Ģ&#34;:9110,&#34;íĥ±&#34;:9111,&#34;íŀĲë§ģ&#34;:9112,&#34;ìĹ°ìĺĪ&#34;:9113,&#34;ê³µë¶Ģ&#34;:9114,&#34;ì¹ĺëĭ¤&#34;:9115,&#34;Ġëª¨ìĸĳ&#34;:9116,&#34;ĠìķĬìķĦìļĶ&#34;:9117,&#34;ëłĪìķĮ&#34;:9118,&#34;ĠìĥĿê°ģìĿĢ&#34;:9119,&#34;ĠìĥĿê°ģë§Į&#34;:9120,&#34;ìŀħëĭĪê¹Į&#34;:9121,&#34;Ġëª»íķ´ìĦľ&#34;:9122,&#34;ĠìĹ¬ì¹ľ&#34;:9123,&#34;ë¹ĦìĹĲ&#34;:9124,&#34;ĠëĤ¨ìĿĺ&#34;:9125,&#34;ĠëĤ¨ëĬĶê²Į&#34;:9126,&#34;ìłĢëĥ¥&#34;:9127,&#34;ĠìĨĮê°ľ&#34;:9128,&#34;ĠìĨĮëħĦ&#34;:9129,&#34;ëįĶêµ°&#34;:9130,&#34;Ġë¯¸íķĻ&#34;:9131,&#34;Ġë¯¸ëħĢ&#34;:9132,&#34;ê³Ħë¥¼&#34;:9133,&#34;íĭĪ&#34;:9134,&#34;Ġê³µë£¡&#34;:9135,&#34;ãħľãħł&#34;:9136,&#34;ĠëĲĺë©´&#34;:9137,&#34;Ġë°ĺìĿĳ&#34;:9138,&#34;ìĹŃìĿĦ&#34;:9139,&#34;ê²½ìĿĦ&#34;:9140,&#34;ìĹ°ê¸°ìĹĲ&#34;:9141,&#34;ĠìķĦëĭĪìĹĪëĭ¤&#34;:9142,&#34;ê·¹ìĿĺ&#34;:9143,&#34;Ġë§Īì§Ģë§īíļĮ&#34;:9144,&#34;ëł¸ëįĺ&#34;:9145,&#34;ĠëıĪëĤ´ê³ł&#34;:9146,&#34;ê¹Ģë¯¼&#34;:9147,&#34;íķĺê¸°ëĬĶ&#34;:9148,&#34;ĠìĿĮìĭĿ&#34;:9149,&#34;Ġë§Ŀíķł&#34;:9150,&#34;ĠìļĶìĥĪ&#34;:9151,&#34;íıīìĿĦ&#34;:9152,&#34;ëĬĶê±°ëĥĲ&#34;:9153,&#34;íĨµìĪĺ&#34;:9154,&#34;Ġì°įëĬĶ&#34;:9155,&#34;Ġìĭ¤ë§ĿìĿ´ëĭ¤&#34;:9156,&#34;ĠìºĲë¦ŃíĦ°ëıĦ&#34;:9157,&#34;ĠìºĲë¦ŃíĦ°ëĵ¤ìĿ´&#34;:9158,&#34;ĠëĨĢëŀĢ&#34;:9159,&#34;ëŁ¬ë©´&#34;:9160,&#34;ë¸ĮëĿ¼&#34;:9161,&#34;íĸ¥ìĿ´&#34;:9162,&#34;ĠëĤĺìĻĶëįĺ&#34;:9163,&#34;ĠëĤĺìĻĢìķ¼&#34;:9164,&#34;Ġê¸´ìŀ¥ê°ĲìĿĦ&#34;:9165,&#34;ĠìłķìĭłìĿ´&#34;:9166,&#34;ĠëĮĢìĤ¬ìĻĢ&#34;:9167,&#34;ìĭ¶ìĿĢëį°&#34;:9168,&#34;ëĤ¨ìŀĲìĿĺ&#34;:9169,&#34;ëĿ¼ìĿ´ì¦Ī&#34;:9170,&#34;ĠìļĶì¦ĺìĿĢ&#34;:9171,&#34;Ġê°ķì¶Ķíķ©ëĭĪëĭ¤&#34;:9172,&#34;Ġì¼Ģë¹Ī&#34;:9173,&#34;íĥĪë¦¬ìķĦ&#34;:9174,&#34;Ġì¶©ê²©ìłģìĿ¸&#34;:9175,&#34;Ġë¯¼ì¡±&#34;:9176,&#34;êµ³ìĿ´&#34;:9177,&#34;ĠìĹ¬ì£¼ìĿ¸ê³µìĿ´&#34;:9178,&#34;Ġëĵ£ê¸°&#34;:9179,&#34;Ġìĸ´ë¥¸ìĿ´&#34;:9180,&#34;ĠíħĮëŁ¬&#34;:9181,&#34;Ġíķľìĭ¬íķĺëĭ¤&#34;:9182,&#34;ë¹¼ê³¤&#34;:9183,&#34;ĠìĽĥê²¨ìĦľ&#34;:9184,&#34;ĠëĪĦêµ°ê°Ģ&#34;:9185,&#34;ë²Īë³´ê³ł&#34;:9186,&#34;ìĿ´ëŀĺìĦľ&#34;:9187,&#34;ìĿ´íĽĦë¡ľ&#34;:9188,&#34;!~&#34;:9189,&#34;ìª&#34;:9190,&#34;ŀĢ&#34;:9191,&#34;ìĿ´íķľ&#34;:9192,&#34;ìĿ´ìĻĢ&#34;:9193,&#34;ãħĮ&#34;:9194,&#34;íķľíİ¸&#34;:9195,&#34;ĠìĺģíĻĶê°Ļëĭ¤&#34;:9196,&#34;ìĸ´ì©Ķ&#34;:9197,&#34;Ġë³´ìĿ´ê³ł&#34;:9198,&#34;Ġê·¸ê°Ģ&#34;:9199,&#34;ìĭľë¥¼&#34;:9200,&#34;ë³´ëŁ¬&#34;:9201,&#34;ĠìĹĨìĹĪ&#34;:9202,&#34;ĠëĤĺë©´&#34;:9203,&#34;ĠíķĺìĦ¸ìļĶ&#34;:9204,&#34;ìĪĺëĭĺ&#34;:9205,&#34;ìĥģìļ°&#34;:9206,&#34;ĠìŀĪìĹĪìĿĮ&#34;:9207,&#34;íķĺê³łëıĦ&#34;:9208,&#34;¬ëŀĲ&#34;:9209,&#34;ëĤ´ìĦľ&#34;:9210,&#34;ìĹ°ìĿĺ&#34;:9211,&#34;Ġë´¤ê³ł&#34;:9212,&#34;ìĹ¬ëıĦ&#34;:9213,&#34;ìĿ¼ì§Ģ&#34;:9214,&#34;ìĿ¼ëŁ¬&#34;:9215,&#34;Ġëª¨ë¥¸&#34;:9216,&#34;ĠìłĦíĻĶ&#34;:9217,&#34;ë¶Ħì§ľë¦¬&#34;:9218,&#34;ê°Ĳíķľ&#34;:9219,&#34;ê·Ħ&#34;:9220,&#34;ĠìłľìĿ´ìĬ¨&#34;:9221,&#34;ëįĶìĿ´ìĥģ&#34;:9222,&#34;Ġë²¤&#34;:9223,&#34;Ġë¶Ģë¶Ģ&#34;:9224,&#34;ë²ĪìĹĲ&#34;:9225,&#34;Ġíķĳ&#34;:9226,&#34;ĠíķŃ&#34;:9227,&#34;Ġìŀ¬ë¯¸ìŀĪìĹĪëĬĶëį°&#34;:9228,&#34;ìŀ¬ë°ĮìĹĪëĬĶëį°&#34;:9229,&#34;ĠìĽĥìĿĦ&#34;:9230,&#34;ëĤ¨ìĿĦ&#34;:9231,&#34;Ġëĭ¤ìĭľë³´ê³ł&#34;:9232,&#34;ì²ŃìĨĮëħĦ&#34;:9233,&#34;ĠíķĺëĤĺëĬĶ&#34;:9234,&#34;ĠëĤĺìĺ¤ëĦ¤ìļĶ&#34;:9235,&#34;Ġìķ¡ìħĺìĶ¬&#34;:9236,&#34;Ġì½ĶìĬ¤&#34;:9237,&#34;êµĲìľ¡&#34;:9238,&#34;ĠìĿ´íķ´ëıĦ&#34;:9239,&#34;Ġë¶Īê°Ģ&#34;:9240,&#34;Ġì²ĺìĿĮìĿ´ëĦ¤&#34;:9241,&#34;Ġì²ĺìĿĮë³¸ëĭ¤&#34;:9242,&#34;Ġê·¹íĺĲ&#34;:9243,&#34;ìłķëıĦëĬĶ&#34;:9244,&#34;íĤ¤ëĵľ&#34;:9245,&#34;ë¥ĺìĺģíĻĶ&#34;:9246,&#34;ĠíıīíĻĶ&#34;:9247,&#34;ìĿ´ê±°ëĤĺ&#34;:9248,&#34;Ġê·¸ëŁ°ê±°&#34;:9249,&#34;ìķĮìķĦ&#34;:9250,&#34;Ġê´ľì°®ëĦ¤ìļĶ&#34;:9251,&#34;............&#34;:9252,&#34;íķĺê¸°ê¹Įì§Ģ&#34;:9253,&#34;Ġìĭ¬íĺķëŀĺ&#34;:9254,&#34;ĠíĹĪëĤĺ&#34;:9255,&#34;ĠíĺĦìĭ¤ê³¼&#34;:9256,&#34;ĠíĺĦìĭ¤ìłģìĿ´&#34;:9257,&#34;ê»ĺìĦľ&#34;:9258,&#34;ĠíĶ¼ìķĦëħ¸&#34;:9259,&#34;Ġìĸµì§ĢìĬ¤ëŁ½ê³ł&#34;:9260,&#34;ìĬ¹íĹĮ&#34;:9261,&#34;Ġìŀħìŀ¥ìĹĲìĦľ&#34;:9262,&#34;Ġíĺ¸íĿ¡&#34;:9263,&#34;ĠìĻĢìĦľ&#34;:9264,&#34;Ġì¹ľêµ¬ëŀĳ&#34;:9265,&#34;ì§Ģê¸ĪìĿĢ&#34;:9266,&#34;Ġìĸ´ëĶĶìĹĲ&#34;:9267,&#34;Ġì¡°ê¸ĪìĿĢ&#34;:9268,&#34;Ġë³´ê²ĮëĲĺëĬĶ&#34;:9269,&#34;ìĽĲìŀĳìĿ´&#34;:9270,&#34;Ġìĸ´ìĦ¤íĶĦëĭ¤&#34;:9271,&#34;Ġë¬¸ìłľë¥¼&#34;:9272,&#34;ĠìŀĶìŀĶíķĺê²Į&#34;:9273,&#34;Ġìĸĺê¸°ë¥¼&#34;:9274,&#34;Ġê°ĲëıĻìłģìĿ´ëĦ¤ìļĶ&#34;:9275,&#34;ìĨĲê°ĢëĿ½&#34;:9276,&#34;ĠíĴįìŀĲ&#34;:9277,&#34;Ġìĸ´ìļ¸ë¦¬ì§Ģ&#34;:9278,&#34;ìķĶíĬ¼&#34;:9279,&#34;Ġì½ĺ&#34;:9280,&#34;Ġíĺ¹ìĭľëĤĺ&#34;:9281,&#34;Ġìį°&#34;:9282,&#34;Ġë¬ĺíķľ&#34;:9283,&#34;Ġë²Ĺìĸ´&#34;:9284,&#34;Ġmovie&#34;:9285,&#34;ĠìĥĪìĤ¼&#34;:9286,&#34;ì§ĪëķĮ&#34;:9287,&#34;ì¤ĦìķĮìķĺëĭ¤&#34;:9288,&#34;ë½Ģ&#34;:9289,&#34;ĠìĹ¬ëŁ¬ë²Ī&#34;:9290,&#34;íĿ¥ë¯¸ì§Ħì§Ħ&#34;:9291,&#34;Ġìµľê·¼ìĹĲ&#34;:9292,&#34;ëĤ«ëĭ¤&#34;:9293,&#34;ĠëĿ¼ìĿ´ìĸ¸&#34;:9294,&#34;Ġì¸¡&#34;:9295,&#34;ìĺ¤ëŀ«ë§ĮìĹĲ&#34;:9296,&#34;ëĿ¼ìĹĲëª½&#34;:9297,&#34;25&#34;:9298,&#34;il&#34;:9299,&#34;vs&#34;:9300,&#34;ľìĭľ&#34;:9301,&#34;íķĺìĿĺ&#34;:9302,&#34;ê¸°ë°ľ&#34;:9303,&#34;ĠìĿ´ê²ĥìĿ´&#34;:9304,&#34;ìĸ´ëłµ&#34;:9305,&#34;ë¦¬ëĬĶëį°&#34;:9306,&#34;ĠìłĪë&#34;:9307,&#34;ìķĦëŀĺ&#34;:9308,&#34;ĠìķĦíĮł&#34;:9309,&#34;íķ´ê°ĢëĬĶ&#34;:9310,&#34;Ġë³´ê²łëĭ¤&#34;:9311,&#34;Ġë³´ìĦĿ&#34;:9312,&#34;ìĥĮ&#34;:9313,&#34;Ġê·¸ìĿ´ìĥģ&#34;:9314,&#34;ë°ĳ&#34;:9315,&#34;ëŀ´&#34;:9316,&#34;ĠìĤĲ&#34;:9317,&#34;ëĮĢì¤ĳ&#34;:9318,&#34;Ġì§Ģê²½&#34;:9319,&#34;ê·¸ëĭ¹ìĭľ&#34;:9320,&#34;Ġíķľê°ľëıĦ&#34;:9321,&#34;ìķĺìĸ´&#34;:9322,&#34;ë¯¸ë¥¼&#34;:9323,&#34;ë¯¸ëŀĺ&#34;:9324,&#34;ê²ĥê°ĻìĿĢëį°&#34;:9325,&#34;ìĿ¼ìĹĲ&#34;:9326,&#34;Ġìŀĺë³´ê³ł&#34;:9327,&#34;ĠëĬ¦ê²Į&#34;:9328,&#34;ê°ĲíŀĪ&#34;:9329,&#34;Ġê°Ļì§Ģë§Į&#34;:9330,&#34;ĠëĤ¨ê¸°ëĬĶ&#34;:9331,&#34;íĬ¸ìĻĢ&#34;:9332,&#34;Ġìĺ¤íķ´&#34;:9333,&#34;ĠìĬ¤íĨłë¦¬ìłĦê°ľ&#34;:9334,&#34;ìķĪìĹĲìĦľ&#34;:9335,&#34;ìĽĲìĹĲìĦľ&#34;:9336,&#34;Ġì§Ģë£¨íķ¨ìĿĦ&#34;:9337,&#34;ìĽĮì¦Ī&#34;:9338,&#34;ĠëĲĺìĦľ&#34;:9339,&#34;Ġíķ´ì¤Ģëĭ¤&#34;:9340,&#34;íĤ´&#34;:9341,&#34;Ġìĭ¶ìĹĪëĬĶëį°&#34;:9342,&#34;Ġê°ĲìĦ±ìĿĦ&#34;:9343,&#34;ĠìĤ¬ëŀĳìĬ¤ëŁ½ëĭ¤&#34;:9344,&#34;Ġìŀ¬ë¯¸ìĹĨëĦ¤ìļĶ&#34;:9345,&#34;Ġìĭľê°ĦëıĦ&#34;:9346,&#34;ĠíķĺëĤĺê°Ģ&#34;:9347,&#34;ìµľê³łìŀħëĭĪëĭ¤&#34;:9348,&#34;Ġì£½ìĸ´&#34;:9349,&#34;ì¦ĿìĿĦ&#34;:9350,&#34;ìĹĲê²ĮëıĦ&#34;:9351,&#34;ĠíĽĦíķĺê²Į&#34;:9352,&#34;ĠìĦ±ìĿ¸ìĿ´&#34;:9353,&#34;ĠíĸĪëĤĺ&#34;:9354,&#34;Ġì§ľë¦¬&#34;:9355,&#34;Ġíģ¬ë&#34;:9356,&#34;ì½ķ&#34;:9357,&#34;ĠìĿ´ìĥģíķ´&#34;:9358,&#34;ìĸ¸ë§¨&#34;:9359,&#34;ĠìłģìĿĢ&#34;:9360,&#34;ĠìłģëĤĺëĿ¼&#34;:9361,&#34;Ġê±´ì§Ī&#34;:9362,&#34;ìŀ¬ë¯¸ìŀĪìĹĪìĸ´ìļĶ&#34;:9363,&#34;ĠìķĦë¬´ëĤĺ&#34;:9364,&#34;Ġìĸµìļ¸&#34;:9365,&#34;ĠìĹ´ê´ĳ&#34;:9366,&#34;ĠìºĲë¦ŃíĦ°ëĬĶ&#34;:9367,&#34;ë§ŀê³ł&#34;:9368,&#34;ĠìĺģíĻĶìĺĢëĬĶëį°&#34;:9369,&#34;ĪëĶ°&#34;:9370,&#34;ìĬ¬íį¼&#34;:9371,&#34;ìķĦê¹Ŀê³ł&#34;:9372,&#34;Ġëĵ¤ìĸ´ê°Ħ&#34;:9373,&#34;ĠìķĪë³´ëĬĶê²Į&#34;:9374,&#34;ĠëĤļìĿ´ì§Ģ&#34;:9375,&#34;Ġë°°ê²½ìĿĮìķħ&#34;:9376,&#34;ìĻĦìĦ±ëıĦ&#34;:9377,&#34;ĠãħİãĦ·ãĦ·&#34;:9378,&#34;ĠìĽĥìĿĮìĿĦ&#34;:9379,&#34;Ġê²°êµŃìĿĢ&#34;:9380,&#34;íŀĪë´¤&#34;:9381,&#34;ë§ĪìĿĮìĿĦ&#34;:9382,&#34;Ġëĭ¹ìĭľìĹĲ&#34;:9383,&#34;ĠìłĪìłķ&#34;:9384,&#34;Ġê°Ģì¹ĺëıĦ&#34;:9385,&#34;,,,,,,,,&#34;:9386,&#34;ĠìĤ¼ë¥ĺìĺģíĻĶ&#34;:9387,&#34;Ġìĸ´ì©Įë©´&#34;:9388,&#34;ìŀ¬ë¯¸ëıĦìĹĨê³ł&#34;:9389,&#34;ìĿ´íķĺëĵľ&#34;:9390,&#34;Ġì¿¨&#34;:9391,&#34;ê¼¬ë§Ī&#34;:9392,&#34;Ġë©ĶìĦ¸ì§Ģë¥¼&#34;:9393,&#34;ì¤ĦìķĮìķĺëĬĶëį°&#34;:9394,&#34;íĹĪìłĳíķľ&#34;:9395,&#34;ĠìĥĪë¡Ŀ&#34;:9396,&#34;¬ëł¸&#34;:9397,&#34;ìµľê·¼ìĹĲ&#34;:9398,&#34;Ġê²īë©ĭ&#34;:9399,&#34;ĠìĿµìĪĻ&#34;:9400,&#34;ëıħë¦½ìĺģíĻĶ&#34;:9401,&#34;ëŀľìĬ¤íı¬ë¨¸&#34;:9402,&#34;_^&#34;:9403,&#34;id&#34;:9404,&#34;´£&#34;:9405,&#34;ëĩ&#34;:9406,&#34;Ġ!!!!&#34;:9407,&#34;Ġì¾Į&#34;:9408,&#34;ĠìĮĵ&#34;:9409,&#34;ê°Īë&#34;:9410,&#34;ëĭ¤ì½Ķ&#34;:9411,&#34;ê³°&#34;:9412,&#34;ëĬĶê²ĥëıĦ&#34;:9413,&#34;ê³łìłĦ&#34;:9414,&#34;Ġíī&#34;:9415,&#34;ĠìĺģíĻĶëĵ¤ìĿ´&#34;:9416,&#34;Ġë§Īë²ķ&#34;:9417,&#34;ìķĦë¦¬&#34;:9418,&#34;ìĿ¸íĦ°&#34;:9419,&#34;ë³´ëł¤ê³ł&#34;:9420,&#34;ìĬ¤íĭ´&#34;:9421,&#34;ëĮĢì¶©&#34;:9422,&#34;ĠìĹĨìĸ´ëıĦ&#34;:9423,&#34;ì§Ħíķľ&#34;:9424,&#34;ìŀĪìĿĦê¹Į&#34;:9425,&#34;ìłĦëĭ¬&#34;:9426,&#34;ĠìŀĪìĹĪìĬµëĭĪëĭ¤&#34;:9427,&#34;ĠìĹ°ê´Ģ&#34;:9428,&#34;Ġíķľê°Ģ&#34;:9429,&#34;ì¹ĺë§Į&#34;:9430,&#34;Ġìŀ¬ë°ĮìĹĪ&#34;:9431,&#34;ĠëĬĳëĮĢ&#34;:9432,&#34;Ġì£¼ìľ¤ë°ľ&#34;:9433,&#34;Ġë§ĪëĭĪ&#34;:9434,&#34;Ġëª»íķĺëĦ¤&#34;:9435,&#34;ĠìĹ¬ìĭł&#34;:9436,&#34;ĠìŀĲìķĦ&#34;:9437,&#34;Ġê²ĥëĵ¤&#34;:9438,&#34;Ġìĺ¤ì§Ģ&#34;:9439,&#34;Ġê¹¡íĮ¨&#34;:9440,&#34;ìŀ¬ë¯¸ìŀĪëĦ¤ìļĶ&#34;:9441,&#34;ë²ĦíĬ¼&#34;:9442,&#34;ëªħíĻĶ&#34;:9443,&#34;ĠìķłìŀĶ&#34;:9444,&#34;ĠìłĢì§Ģ&#34;:9445,&#34;ĠíĹ¬&#34;:9446,&#34;Ġê±°íĴĪ&#34;:9447,&#34;ĠìĹŃìŀĳ&#34;:9448,&#34;Ġëĭ¤ìĭľë³´ê¸°&#34;:9449,&#34;ëŃĩ&#34;:9450,&#34;Ġëª°ìķĦ&#34;:9451,&#34;Ġê¸°ëĮĢê°Ģ&#34;:9452,&#34;Ġê°ķìļĶ&#34;:9453,&#34;ìĻľìĿ´ëłĩê²Į&#34;:9454,&#34;ëĲĲëĭ¤&#34;:9455,&#34;ĠíĺĦìĭ¤ìĿĢ&#34;:9456,&#34;ìĥĿê°ģìľ¼ë¡ľ&#34;:9457,&#34;Ġë¦°&#34;:9458,&#34;ĠìĦłìłķ&#34;:9459,&#34;Ġê³¼ìłķìĿ´&#34;:9460,&#34;Ġë°©ê¸Ī&#34;:9461,&#34;êµ¬ë¨¼&#34;:9462,&#34;ĠìĿ¼ë³¸ìĿĺ&#34;:9463,&#34;ëĬĶê±°ëĭ¤&#34;:9464,&#34;Ġê¸´ë°ķ&#34;:9465,&#34;Īë°ľ&#34;:9466,&#34;ì£½ìĿ´ê³ł&#34;:9467,&#34;ĠëĨĢëŀĺ&#34;:9468,&#34;Ġì»¬&#34;:9469,&#34;Ġì»·&#34;:9470,&#34;Ġì»´íĵ¨íĦ°&#34;:9471,&#34;Ġìŀħìŀ¥&#34;:9472,&#34;Ġê³°&#34;:9473,&#34;íķłìĪĺëıĦ&#34;:9474,&#34;ĠëĤłìķĦ&#34;:9475,&#34;ĠíĺķìĤ¬&#34;:9476,&#34;ì¤ĺëıĦ&#34;:9477,&#34;Ġì´Īëĵ±íķĻêµĲ&#34;:9478,&#34;Ġìĭľë¦¬ì¦ĪìĿĺ&#34;:9479,&#34;ë²łìĬ¤íĬ¸&#34;:9480,&#34;ìĿ´ìķ¼ê¸°ê°Ģ&#34;:9481,&#34;Ġë°°ê²½ìĿ´&#34;:9482,&#34;Ġì£¼ê³łìĭ¶ëĭ¤&#34;:9483,&#34;Ġì¶©ë¶Ħíķĺëĭ¤&#34;:9484,&#34;Ġì»¤ìĦľ&#34;:9485,&#34;ìĹ°ì¶ľìĿ´&#34;:9486,&#34;ë»ĳ&#34;:9487,&#34;ê½Ŀ&#34;:9488,&#34;Ġìĵ¸ìĵ¸&#34;:9489,&#34;ĠìŀĲìĹ°ìĬ¤ëŁ½ê²Į&#34;:9490,&#34;íĭ°ë¹Ħë¡ľ&#34;:9491,&#34;ëłĪëĶĶ&#34;:9492,&#34;ĠìĺģíĻĶìĿ¸ê²ĥ&#34;:9493,&#34;ìµľìķħìĿĺìĺģíĻĶ&#34;:9494,&#34;Ġìĭ«ìĸ´íķĺëĬĶ&#34;:9495,&#34;ìłĲì¤Ģê²ĥ&#34;:9496,&#34;ì¶ľìĹ°ì§Ħ&#34;:9497,&#34;Ġì©Ķìĸ´&#34;:9498,&#34;Ġë¸ĶëŀĻì½Ķë¯¸ëĶĶ&#34;:9499,&#34;Ġìĭľë¦¬ì¦Īë¥¼&#34;:9500,&#34;ĠìĨĮìŀ¥íķĺê³ł&#34;:9501,&#34;Ġë»¥&#34;:9502,&#34;Ġì¯§&#34;:9503,&#34;ĠíķĺìĿ´íĭ´&#34;:9504,&#34;ëŀĺê³¤ë³¼&#34;:9505,&#34;!?&#34;:9506,&#34;ëĭ¤ìĨĮ&#34;:9507,&#34;ëĤĦ&#34;:9508,&#34;ĠìĺģíĻĶìĨį&#34;:9509,&#34;ĠìĺģíĻĶëŀĢ&#34;:9510,&#34;ìĭ¬ë¦¬&#34;:9511,&#34;ìĿĺëıĦ&#34;:9512,&#34;ìĿĢìĺģíĻĶ&#34;:9513,&#34;ëĤĺìĺ¬ëķĮ&#34;:9514,&#34;Ġë§Ļ&#34;:9515,&#34;ìĸ´ìĿ´ê°Ģ&#34;:9516,&#34;ë¡ľë´ĩ&#34;:9517,&#34;ë¡ľë²ĦíĬ¸&#34;:9518,&#34;ëłĢ&#34;:9519,&#34;ëį°ëłĲ&#34;:9520,&#34;íķ´ìĦĿ&#34;:9521,&#34;íķ´ì¤Ģëĭ¤&#34;:9522,&#34;ìĥĪë²½ìĹĲ&#34;:9523,&#34;ìĺģíĻĶë³´ë©´ìĦľ&#34;:9524,&#34;ìĬ¤íĮĮìĿ´&#34;:9525,&#34;ëŀ¬ëĭ¤&#34;:9526,&#34;ì¤į&#34;:9527,&#34;ìłķì¹ĺ&#34;:9528,&#34;Ġì¢ĭìķĹ&#34;:9529,&#34;ìŀĪìĹĪëĬĶëį°&#34;:9530,&#34;ìłĦìĿĢ&#34;:9531,&#34;ìĥģëıĦ&#34;:9532,&#34;ìĺ¤ìļ°&#34;:9533,&#34;Ġìĺ¬ëĵľ&#34;:9534,&#34;Ġë§Įìķ½&#34;:9535,&#34;ìĦ±íķľ&#34;:9536,&#34;ĠìĹ°ê¸°ë§Į&#34;:9537,&#34;Ġëª¨ë°©&#34;:9538,&#34;ĠìķĪê°ĢëĬĶ&#34;:9539,&#34;Ġì£¼ìĦ¸ìļĶ&#34;:9540,&#34;ĠëĵľëĶĶìĸ´&#34;:9541,&#34;ĠìĿ¸ì¢ħ&#34;:9542,&#34;Ġìµľê³łìĿ¸ëĵ¯&#34;:9543,&#34;ìľłì¾Įíķľ&#34;:9544,&#34;Ġë¹Ħíķ´ìĦľ&#34;:9545,&#34;Ġë¹Ħì¥¬ìĸ¼&#34;:9546,&#34;Ġë¯¸ì³Ĳ&#34;:9547,&#34;ë²Ħì¸ł&#34;:9548,&#34;Ġë§Įëĵ¤ìĹĪëĥĲ&#34;:9549,&#34;ëĲĺìļĶ&#34;:9550,&#34;ĠëģĿëĤ¨&#34;:9551,&#34;ìĺģêµŃ&#34;:9552,&#34;ë¦¬ëĵľ&#34;:9553,&#34;ë¦¬ë·°&#34;:9554,&#34;Ġìķłì´ĪìĹĲ&#34;:9555,&#34;OOOO&#34;:9556,&#34;ĠëĬĲëĤĢëĭ¤&#34;:9557,&#34;ĠìŀĪëĬĶê°Ģ&#34;:9558,&#34;ĠëĦĺì³Ĳ&#34;:9559,&#34;Ġë©ĭìł¸ìļĶ&#34;:9560,&#34;Ġê¸°ìĸµëĤľëĭ¤&#34;:9561,&#34;Ġìŀ¥ë©´ë§Į&#34;:9562,&#34;ĠíķľêµŃìĺģíĻĶìĿĺ&#34;:9563,&#34;Ġê´ľì°®ìķĺëįĺ&#34;:9564,&#34;Ġìĭ¬íķĺëĭ¤&#34;:9565,&#34;ê¼Ī&#34;:9566,&#34;ì§ĳì¤ĳ&#34;:9567,&#34;ĠìļĶë¦¬&#34;:9568,&#34;ĠìķĦìĿ´ëĶĶ&#34;:9569,&#34;Ġë§Įëĵ¤ìĸ´ì£¼&#34;:9570,&#34;Ġê³µíı¬ê°Ģ&#34;:9571,&#34;íĿ¬ëĬĶ&#34;:9572,&#34;Ġíİ¸ìĿ¸ëį°&#34;:9573,&#34;ìķĦìĿ´ëĵ¤ê³¼&#34;:9574,&#34;Ġê²ĮìĬ¤íĬ¸&#34;:9575,&#34;ë´ĲëıĦë´ĲëıĦ&#34;:9576,&#34;íĬ¹ìľłìĿĺ&#34;:9577,&#34;ê¸°ëĮĢìĿ´ìĥģ&#34;:9578,&#34;ëŁ¬ë¬¼&#34;:9579,&#34;ĠëĬĲê»´ì§Ģì§Ģ&#34;:9580,&#34;ì°½ìĭľìłĪ&#34;:9581,&#34;ĠìłľëĮĢë¡ľëĲľ&#34;:9582,&#34;ĠíĨµíķ´ìĦľ&#34;:9583,&#34;ĠìĿĺë¯¸ëıĦ&#34;:9584,&#34;ì¤ĺìķ¼ì§Ģ&#34;:9585,&#34;ë²ķíķľ&#34;:9586,&#34;ìĬ¬íĶĦê³ł&#34;:9587,&#34;ì§Ŀíīģ&#34;:9588,&#34;ì¶©ë¶ĦíŀĪ&#34;:9589,&#34;Ġíŀĺëĵ¤ìĹĪëĭ¤&#34;:9590,&#34;Ġì²¨ë¶ĢíĦ°&#34;:9591,&#34;ëªħìŀĳìĿ´ëĭ¤&#34;:9592,&#34;Ġìĸ´ë¦°ìĭľìłĪ&#34;:9593,&#34;Ġì£ł&#34;:9594,&#34;ìĺ¬ëł¤&#34;:9595,&#34;ĠìĦ¤ìłķìĿĢ&#34;:9596,&#34;Ġê°ĲëıĻìłģìĿ´ìĹĲìļĶ&#34;:9597,&#34;ĠìĿ¸ë¬¼ëĵ¤ìĿĺ&#34;:9598,&#34;ĠìĻłë§Įíķľ&#34;:9599,&#34;ìķĦëĭĮëį°&#34;:9600,&#34;ĠëĨĴìĿĢì§Ģ&#34;:9601,&#34;ĠëįĶìļ±ëįĶ&#34;:9602,&#34;ë¦¬ìĿĺë¦¬ìĿĺ&#34;:9603,&#34;ĠìĹĲë¡ľìĺģíĻĶ&#34;:9604,&#34;íĢĦ&#34;:9605,&#34;ë²Īë´ĲëıĦ&#34;:9606,&#34;Ġê¼¬ë§Ī&#34;:9607,&#34;Ġê»Ĳëĭ¤&#34;:9608,&#34;Ġë©´ìĿ´&#34;:9609,&#34;Ġê¹ľëĨĢ&#34;:9610,&#34;Ġëļľ&#34;:9611,&#34;Ġì°¬ìĤ¬ë¥¼&#34;:9612,&#34;Ġëºı&#34;:9613,&#34;ĠìķĪë¬´ìĦŃ&#34;:9614,&#34;ĠìĺĨìĹĲ&#34;:9615,&#34;ì¼ĢìĿ´ë¸ĶìĹĲìĦľ&#34;:9616,&#34;°ľê²¬&#34;:9617,&#34;íıīë²Ķíķľ&#34;:9618,&#34;ĠìĤ´ëĭ¤ìĤ´ëĭ¤&#34;:9619,&#34;ìĦ¸íı¬ìĨĮëħĢ&#34;:9620,&#34;ĠíĿ¬ëĮĢìĿĺ&#34;:9621,&#34;ì§Ģë¶Ģì§Ģ&#34;:9622,&#34;Ġíķ¸ëĵľ&#34;:9623,&#34;âĺħâĺĨâĺħâĺĨ&#34;:9624,&#34;el&#34;:9625,&#34;ff&#34;:9626,&#34;ĠH&#34;:9627,&#34;ģëĭĪëĭ¤&#34;:9628,&#34;ì§ĪëĿ¼&#34;:9629,&#34;ëĵ¦&#34;:9630,&#34;ë¦ħ&#34;:9631,&#34;ĠìĺģíĻĶìłģ&#34;:9632,&#34;ìĭ¹&#34;:9633,&#34;ëĤĺë³´ëĭ¤&#34;:9634,&#34;ĠìĿ´ìĸ´ì§ĢëĬĶ&#34;:9635,&#34;...;;&#34;:9636,&#34;Ġì§¤&#34;:9637,&#34;ìĺģíĻĶë³´ëĬĶ&#34;:9638,&#34;ìĺģíĻĶìĺĢëĭ¤&#34;:9639,&#34;ë³´ìĨĮ&#34;:9640,&#34;ìĬ¤íħĿ&#34;:9641,&#34;ëŀ¨&#34;:9642,&#34;ê±°ìĽĮ&#34;:9643,&#34;ìĤĲ&#34;:9644,&#34;ìŀĲëĵ¤ìĿĢ&#34;:9645,&#34;ìŀĲìĭłìĿĺ&#34;:9646,&#34;ìķ¼ìŀĲ&#34;:9647,&#34;ì£¼ë§Ĳ&#34;:9648,&#34;ìłģìĿĦ&#34;:9649,&#34;Ġì§Ģìĺ¥&#34;:9650,&#34;Ġíķľìĭľê°Ħ&#34;:9651,&#34;ëĵľë¦½&#34;:9652,&#34;ĠìĭľíĹĺ&#34;:9653,&#34;Ġì°Ķ&#34;:9654,&#34;ê²ĥìĿ¸ê°Ģ&#34;:9655,&#34;ì¹ĺê¸°&#34;:9656,&#34;Ġê¸°ì¡´&#34;:9657,&#34;Ġëª¨ëıħ&#34;:9658,&#34;ìĤ¬ê¸°&#34;:9659,&#34;ìĤ¬ì§Ħ&#34;:9660,&#34;Ġë¬´ìĿĺë¯¸&#34;:9661,&#34;ëŁ¬ìĽĢ&#34;:9662,&#34;ĠìķĪê²¨&#34;:9663,&#34;ĠìķĪì¢ĭìĿĢ&#34;:9664,&#34;Ġì¡±&#34;:9665,&#34;Ġê°ľì½ĺ&#34;:9666,&#34;Ġë³¸ì§Ģ&#34;:9667,&#34;ìĺĢëĦ¤ìļĶ&#34;:9668,&#34;Ġìĺ¤ë¡ľì§Ģ&#34;:9669,&#34;ĠìĤ¬ê³¼&#34;:9670,&#34;Ġì¤ĳìĿĺ&#34;:9671,&#34;ìĺģìĿĢ&#34;:9672,&#34;ëıĻìĿĦ&#34;:9673,&#34;Ġíķ´ìļĶ&#34;:9674,&#34;ì¡°íķľ&#34;:9675,&#34;íĤµ&#34;:9676,&#34;Ġìĭ¶ìĿĮ&#34;:9677,&#34;ìķłê¸°&#34;:9678,&#34;Ġì¢ĭìķĦíķĺìĭľëĬĶ&#34;:9679,&#34;ìľĦë¥¼&#34;:9680,&#34;¬ë¦½&#34;:9681,&#34;ĠìĭłìĿ¸&#34;:9682,&#34;ìĿ´ëŁ°ê±´&#34;:9683,&#34;ìķĬê³ł&#34;:9684,&#34;Ġëª¨ë¥´ê²Ł&#34;:9685,&#34;ĠìķĦê¹ĿìĬµëĭĪëĭ¤&#34;:9686,&#34;ĠíĬĢ&#34;:9687,&#34;Ġë§Īì§Ģë§īìĿĢ&#34;:9688,&#34;Ġì²ĺìĿĮìĹĲ&#34;:9689,&#34;ĠìĿ´ìķ¼ê¸°ìĿĺ&#34;:9690,&#34;ë¡łìĿĺ&#34;:9691,&#34;íĺĢìĦľ&#34;:9692,&#34;ĠìĦ¸ìĽĶ&#34;:9693,&#34;ĠíıīìĨĮ&#34;:9694,&#34;Ġë§Įëĵłê±´ì§Ģ&#34;:9695,&#34;Ġë³Ħë¡ľìĺĢëĭ¤&#34;:9696,&#34;ĠëĬĲëĤĮëıĦ&#34;:9697,&#34;ì¹´ëį°ë¯¸&#34;:9698,&#34;ìķĪë³¸&#34;:9699,&#34;ì§Ģë£¨íķ´ìĦľ&#34;:9700,&#34;ë©Ķë¦¬&#34;:9701,&#34;ĠëĤ¨ìŀĲëĬĶ&#34;:9702,&#34;Ġë»Ķíķĺëĭ¤&#34;:9703,&#34;Ġëª¨ëĵłê²ĥìĿ´&#34;:9704,&#34;ĠìĻ¸ë©´&#34;:9705,&#34;ĠìĹ´ìłķ&#34;:9706,&#34;ëıĪìľ¼ë¡ľ&#34;:9707,&#34;ë¸Įë£¨ìĬ¤&#34;:9708,&#34;Ġìŀ¬ë°ĭìĸ´ìļĶ&#34;:9709,&#34;ĠìĤ´ìķĦìķ¼&#34;:9710,&#34;ëĨĪìĿĺ&#34;:9711,&#34;ë¶Ģë¶ĦìĿĢ&#34;:9712,&#34;íĺľêµĲ&#34;:9713,&#34;Ġìĸ´ìĿ´ìĹĨìĿĮ&#34;:9714,&#34;Ġë¯¸êµŃìĿĺ&#34;:9715,&#34;ê²ĥê°ĻìķĦ&#34;:9716,&#34;ëĭĿë§¨&#34;:9717,&#34;Ġì»¤ëħķ&#34;:9718,&#34;Ġê°ĸê²Į&#34;:9719,&#34;Ġê°ĸì¶ĺ&#34;:9720,&#34;ĠìĹŃìĤ¬ìĹĲ&#34;:9721,&#34;Ġìĺ¤ê¸Ģìĺ¤ê¸Ģ&#34;:9722,&#34;Ġì¤ĺëıĦ&#34;:9723,&#34;Ġ2000&#34;:9724,&#34;Ġê·¸ëŁ´ëĵ¯&#34;:9725,&#34;ì´ĪëĶ©ëķĮ&#34;:9726,&#34;ĠìĨĮë¦Ħëģ¼ì¹ĺëĬĶ&#34;:9727,&#34;ë§ĲìĿ´íķĦìļĶìĹĨëĭ¤&#34;:9728,&#34;ìķĦìī½ëĭ¤&#34;:9729,&#34;ìĦ¹ìĬ¤&#34;:9730,&#34;Ġì¼ĢìĿ´ë¸ĶìĹĲìĦľ&#34;:9731,&#34;Ġë¬´ìĸ¸ê°Ģ&#34;:9732,&#34;ĠìķĪë´ĲìĦľ&#34;:9733,&#34;ê¹ģëĭĪëĭ¤&#34;:9734,&#34;cn&#34;:9735,&#34;¥¸&#34;:9736,&#34;ª»&#34;:9737,&#34;Ħĺ&#34;:9738,&#34;ìĿ´ë»Ĳ&#34;:9739,&#34;ìłĪë&#34;:9740,&#34;ì§ĢíĻĺ&#34;:9741,&#34;íķľíħĲ&#34;:9742,&#34;ëĤĺë¬´&#34;:9743,&#34;ëĤĺìłĢëĤĺ&#34;:9744,&#34;ê²ĮìĿ´&#34;:9745,&#34;ê²Įëĭ¤ê°Ģ&#34;:9746,&#34;ìĬ¬ë¦¬&#34;:9747,&#34;ë¦¬ìī¬&#34;:9748,&#34;ëłĺ&#34;:9749,&#34;ìĿ¸ìľ¼ë¡ľ&#34;:9750,&#34;ìĿ¸íĺľ&#34;:9751,&#34;ìĺģíĻĶëĦ¤ìļĶ&#34;:9752,&#34;ìĭľì¼°&#34;:9753,&#34;ìĹĨìĬµëĭĪëĭ¤&#34;:9754,&#34;ĠíķĺëĭĪ&#34;:9755,&#34;ì§Ħíĺ¸&#34;:9756,&#34;íĨ¡&#34;:9757,&#34;íķłìĪľ&#34;:9758,&#34;Ġë´¤ëĬĶì§Ģ&#34;:9759,&#34;ê²ĥëĵ¤ìĿ´&#34;:9760,&#34;ê²ĥì²ĺëŁ¼&#34;:9761,&#34;ìłľë¥¼&#34;:9762,&#34;Ġê¸°ìŀĲ&#34;:9763,&#34;ĠìĹ°ê¸°ìŀĲëĵ¤&#34;:9764,&#34;ëŀĺìļĶ&#34;:9765,&#34;ìĭłíĺľ&#34;:9766,&#34;ëŁ¬ëĭĪ&#34;:9767,&#34;ëŁ¬íĭ°ë¸Į&#34;:9768,&#34;ĠìķĪëĭ¤&#34;:9769,&#34;ĠìķĪê°Ĳ&#34;:9770,&#34;ĠìķĪëĤĺìĺ¤ëĬĶ&#34;:9771,&#34;~~!&#34;:9772,&#34;Ġë§Ĳìķĺëĭ¤&#34;:9773,&#34;ëħĦìĿĦ&#34;:9774,&#34;ĠìłľìĻķ&#34;:9775,&#34;íĥĢìĿ´&#34;:9776,&#34;Ġê²ĥê³¼&#34;:9777,&#34;Ġìĺ¤íĶĦëĭĿ&#34;:9778,&#34;ĠìĨĮíĴĪ&#34;:9779,&#34;ìĭ¬ìĿĢ&#34;:9780,&#34;Ġë§İëĦ¤&#34;:9781,&#34;ìĽĲëıĦ&#34;:9782,&#34;Ġê³µì¤ĳ&#34;:9783,&#34;ìŀ¬ë°ĮìĹĪìĿĮ&#34;:9784,&#34;ĠìĽĥê²¼ëĭ¤&#34;:9785,&#34;ëĳĲê·¼&#34;:9786,&#34;Ġê±°ìķ¼&#34;:9787,&#34;ë°Ķëĭ¤&#34;:9788,&#34;Ġë°ĶëŀĢëĭ¤&#34;:9789,&#34;êµ°ìĿ´&#34;:9790,&#34;!!!!!!!&#34;:9791,&#34;Ġíı¼&#34;:9792,&#34;íĮĲìľ¼ë¡ľ&#34;:9793,&#34;Ġìµľê³łìĿĺìĺģíĻĶ&#34;:9794,&#34;Ġì²ĺìĿĮìĿ´&#34;:9795,&#34;ĠëªħìŀĳìĿĢ&#34;:9796,&#34;Ġë¡ľë¹Ī&#34;:9797,&#34;Ġë¡ľë²ĦíĬ¸&#34;:9798,&#34;ìĻľìĿ´ë¦¬&#34;:9799,&#34;íħĮë¦¬&#34;:9800,&#34;ê²©ìĿ´&#34;:9801,&#34;Ġë§īìĥģ&#34;:9802,&#34;¬ëĿ¼ê¸°&#34;:9803,&#34;ĠìĿĮëª¨&#34;:9804,&#34;Ġêµ¬ì¡°&#34;:9805,&#34;Ġë§ŀì¶Ķ&#34;:9806,&#34;ì¼ĢíĮħ&#34;:9807,&#34;ĠìķĦìĿ´ëĵ¤ìĿĺ&#34;:9808,&#34;ĠìķĦìĿ´ëĵ¤ìĹĲê²Į&#34;:9809,&#34;Ġëª°ìŀħíķĺê²Į&#34;:9810,&#34;ì§ĢìķĬê²Į&#34;:9811,&#34;ì§ĢìķĬìķĦ&#34;:9812,&#34;ìĵ°ëłĪê¸°ëĭ¤&#34;:9813,&#34;ĠëĪĦëĤĺ&#34;:9814,&#34;ĠìĿ¸ìĥĿìĿĢ&#34;:9815,&#34;Ġë°ĽìķĦìķ¼&#34;:9816,&#34;Ġê·¹ìŀ¥ìĹĲ&#34;:9817,&#34;ëĦĺì¹ĺëĬĶ&#34;:9818,&#34;íĬ¹ë³Ħ&#34;:9819,&#34;Ġë²Ħë¬´&#34;:9820,&#34;ĠìĹĦì²ŃëĤĺê²Į&#34;:9821,&#34;ĠìĹīëļ±&#34;:9822,&#34;ĠìĨĲê¼½&#34;:9823,&#34;ĠíĮĮê³ł&#34;:9824,&#34;Ġë¬¼ìĸ´&#34;:9825,&#34;Ġê°Ģì¡±ìĿĦ&#34;:9826,&#34;Ġê°Ģì¡±ìķł&#34;:9827,&#34;Ġë¶Ģì¡±íķĺê³ł&#34;:9828,&#34;ĠìĤ¶ê³¼&#34;:9829,&#34;Ġìŀ¬ë°ĭìĿĮ&#34;:9830,&#34;ĠëĤĺë¦ĦëĮĢë¡ľ&#34;:9831,&#34;Ġê´Ģê°ĿìĿ´&#34;:9832,&#34;Ġì¶©ê²©ìłģìĿ´&#34;:9833,&#34;ë¡Ńëĭ¤&#34;:9834,&#34;Ġíģ´ë¦¬&#34;:9835,&#34;ĠìºĲìĬ¤íĮħìĿ´&#34;:9836,&#34;Ġë¹ĦêµĲíķĺë©´&#34;:9837,&#34;Ġì±ĦìĽĮ&#34;:9838,&#34;ĠìĪľìĪĺíķĺê³ł&#34;:9839,&#34;ìĵ¸ëį°&#34;:9840,&#34;ìŀĳê°Ģëĭĺ&#34;:9841,&#34;Ġìıł&#34;:9842,&#34;Ġë¹µìłĲ&#34;:9843,&#34;Ġì´ĪëĶ©ëķĮ&#34;:9844,&#34;Ġìī½ì§Ģ&#34;:9845,&#34;Ġë°ĶëĿ¼ë³´ëĬĶ&#34;:9846,&#34;ĠìĨĮë¦¬ë§Į&#34;:9847,&#34;Ġê·¸ëŁ´ìĭ¸&#34;:9848,&#34;íķĺê²łìĬµëĭĪëĭ¤&#34;:9849,&#34;ĠíĻįì½©ìĺģíĻĶ&#34;:9850,&#34;ë±Ģ&#34;:9851,&#34;ëĭ¬ëĿ¼ê³ł&#34;:9852,&#34;ìĹ¬ëŁ¬ë¶Ħ&#34;:9853,&#34;ĠìĦ¸ëł¨ëĲľ&#34;:9854,&#34;Ġìĺģíĸ¥ìĿĦ&#34;:9855,&#34;ĠíħĲëį°&#34;:9856,&#34;ëº&#34;:9857,&#34;ìĿ´ëŁ´&#34;:9858,&#34;ìķ¨&#34;:9859,&#34;ìĿĺìĻ¸ë¡ľ&#34;:9860,&#34;ë¡ľëĿ¼&#34;:9861,&#34;ë§Įìķ½&#34;:9862,&#34;ë¦¬ìĬ¨&#34;:9863,&#34;ĠìłĪë°ĺ&#34;:9864,&#34;ìķĦëĥĲ&#34;:9865,&#34;ìĿ¸íķ´&#34;:9866,&#34;ëį°ìļĶ&#34;:9867,&#34;Ġì§Ļ&#34;:9868,&#34;ĠëĭĪëĦ¤&#34;:9869,&#34;ĠëĤĺìķĦ&#34;:9870,&#34;Ġëĭ¤ë¦Ħ&#34;:9871,&#34;ìłĦíŀĪ&#34;:9872,&#34;Ġìłķì£¼íĸī&#34;:9873,&#34;ĠìŀĪìĸ´ëıĦ&#34;:9874,&#34;ìĺ¤ë²Ħ&#34;:9875,&#34;Ġëĵ¬&#34;:9876,&#34;Ġìĸ´ëĶĺ&#34;:9877,&#34;Ġìĺ¬ë¦¬ë&#34;:9878,&#34;Ġì§Ģê°Ģ&#34;:9879,&#34;ê·¸ëŁ´&#34;:9880,&#34;ì¡ĭ&#34;:9881,&#34;Ġê°Ģê¹Ŀ&#34;:9882,&#34;ìĦ±ê³µ&#34;:9883,&#34;íŀĪíŀĪ&#34;:9884,&#34;ìĸĢ&#34;:9885,&#34;Ġê¸°ë²ķ&#34;:9886,&#34;ìĿ¼ëķĮ&#34;:9887,&#34;ĠìĹ°ê¸°íķľ&#34;:9888,&#34;ĠìĹ°ê¸°ë¡ľ&#34;:9889,&#34;êµ¬êµ¬&#34;:9890,&#34;ĠìłĦëıĦ&#34;:9891,&#34;êµŃëĤ´&#34;:9892,&#34;ì¤ĳíķľ&#34;:9893,&#34;ë¶ĦìłķëıĦ&#34;:9894,&#34;ê²łê³ł&#34;:9895,&#34;ëł¥ìĹĲ&#34;:9896,&#34;ëħĦëıĻìķĪ&#34;:9897,&#34;ìĺĢêµ¬ëĤĺ&#34;:9898,&#34;ĠìĸĦ&#34;:9899,&#34;ĠëĤ¨ê¸´&#34;:9900,&#34;ĠìĿ´ëŁ°ìĭĿìľ¼ë¡ľ&#34;:9901,&#34;ìĽĲìĿĦ&#34;:9902,&#34;ìĪĢ&#34;:9903,&#34;Ġë²ĦìłĦ&#34;:9904,&#34;ĠëĵľëĿ¼ë§ĪëıĦ&#34;:9905,&#34;ë¦¬ëł¤&#34;:9906,&#34;ĠìĤ¬ëŀĳìĬ¤ëŁ½ê³ł&#34;:9907,&#34;íĹī&#34;:9908,&#34;ë°ĺëĭ´&#34;:9909,&#34;¬ë¦´&#34;:9910,&#34;ë°ľë¡ľ&#34;:9911,&#34;ìĿ´ëŁ°ìĺģíĻĶëĬĶ&#34;:9912,&#34;ĠìĿĺë¦¬&#34;:9913,&#34;Ġì¶Ķê°Ģ&#34;:9914,&#34;Ġê¸°ëĮĢíĸĪ&#34;:9915,&#34;ĠìĦ±íĺķ&#34;:9916,&#34;ë°°ìļ°ëĵ¤ëıĦ&#34;:9917,&#34;ìĤ¬ëŀĮëıĦ&#34;:9918,&#34;ĠìĹĲìĿ´&#34;:9919,&#34;ĠëĲĲëĭ¤&#34;:9920,&#34;íģ¬ê°Ģ&#34;:9921,&#34;Ġë§¤ëł¥ìłģ&#34;:9922,&#34;ì§ĢìķĬìĿĮ&#34;:9923,&#34;ĠìķĪëĲĺëĦ¤&#34;:9924,&#34;Ġìłģê·¹&#34;:9925,&#34;ĠìĥĿì¡´&#34;:9926,&#34;íĪŃ&#34;:9927,&#34;ĠëĪĪìľ¼ë¡ľ&#34;:9928,&#34;ĠìļķìĿĦ&#34;:9929,&#34;Ġìŀ¬ìķĻ&#34;:9930,&#34;ìłĲëıĦìķĦê¹Ŀëĭ¤&#34;:9931,&#34;Ġì¹ĺë£Į&#34;:9932,&#34;Ġê°Ħì§Ģ&#34;:9933,&#34;ëģĿëĤ´&#34;:9934,&#34;ĠìŀĲì²´ëıĦ&#34;:9935,&#34;Ġìķħë§Ī&#34;:9936,&#34;ĠëĤĺìĻĢëıĦ&#34;:9937,&#34;ê²ģê²Į&#34;:9938,&#34;ë¸Ķë£¨&#34;:9939,&#34;ìĽĲìŀĳìĹĲ&#34;:9940,&#34;ëŃĶì§Ģ&#34;:9941,&#34;Ġìĸ´ìĿ´ìĹĨëĭ¤&#34;:9942,&#34;ĠìĦ¸ìĥģìĹĲìĦľ&#34;:9943,&#34;ì±ħìŀĦ&#34;:9944,&#34;Ġì¶ĶìĸµìĿĦ&#34;:9945,&#34;ìĭ¸ìĿ´ì½Ķ&#34;:9946,&#34;ĠìĬ¤íĥĢëİĢ&#34;:9947,&#34;ĠìķĶìļ¸&#34;:9948,&#34;Ġëª°ìŀħëıĦëıĦ&#34;:9949,&#34;ìĹĶëĶ©ìĿ´&#34;:9950,&#34;íģ¬ë¥¼&#34;:9951,&#34;ĠìĥģíĻ©ìĿ´&#34;:9952,&#34;ìĭŃëĭĪëĭ¤&#34;:9953,&#34;Ġëĵ£ëĬĶ&#34;:9954,&#34;ëĤĺë¦ĦëĮĢë¡ľ&#34;:9955,&#34;ë³¼ë§Įíķ¨&#34;:9956,&#34;ĠìĹīë§Ŀì§Ħì°½&#34;:9957,&#34;ĠëĨĪëĵ¤&#34;:9958,&#34;ĠìĿ´ëĶ°ìľĦë¡ľ&#34;:9959,&#34;ĠíĹĪìĪłíķľ&#34;:9960,&#34;Ġìĥģì²ĺë¥¼&#34;:9961,&#34;ìłĲì£¼ê¸°ëıĦ&#34;:9962,&#34;íĿĶíķľ&#34;:9963,&#34;BSìĹĲìĦľ&#34;:9964,&#34;ĠíĿīëĤ´ëĤ´&#34;:9965,&#34;ĠìĦ¬ìĦ¸íķľ&#34;:9966,&#34;ĪĦëĿ¼&#34;:9967,&#34;ĠëĵľëĿ¼ë§ĪëĿ¼ê³ł&#34;:9968,&#34;ĠìĿĢê·¼íŀĪ&#34;:9969,&#34;ĠìĹ½ê¸°&#34;:9970,&#34;ê¹ľì§Ŀ&#34;:9971,&#34;Ġìĸ´ìĦ¤íĶĦê²Į&#34;:9972,&#34;ĠìķĪë´Ħ&#34;:9973,&#34;.(&#34;:9974,&#34;.?&#34;:9975,&#34;na&#34;:9976,&#34;ģìĹĲ&#34;:9977,&#34;..;&#34;:9978,&#34;ëĬĻ&#34;:9979,&#34;ìķµ&#34;:9980,&#34;ìķľ&#34;:9981,&#34;ìļ¬&#34;:9982,&#34;íķĺëįĶëĭĪ&#34;:9983,&#34;ê¸°ì¢ħìĺģ&#34;:9984,&#34;ìĿĦêº¼&#34;:9985,&#34;ĠìĿ´ì§Ģ&#34;:9986,&#34;ìĸ´ëł¤&#34;:9987,&#34;ë¡ľìĿĺ&#34;:9988,&#34;ë¦¬ì¦ĺ&#34;:9989,&#34;ëĵ¤ë¦°&#34;:9990,&#34;ìķĦì¤Įë§Ī&#34;:9991,&#34;ìĿ¸ëĵ¤ìĿĢ&#34;:9992,&#34;ìĺģíĻĶì¤ĳìĹĲ&#34;:9993,&#34;ë³´ìĿ¸ëĭ¤&#34;:9994,&#34;ìŀĲìľł&#34;:9995,&#34;ìĹĪìĿĦíħĲëį°&#34;:9996,&#34;Ġíķĺìłķìļ°&#34;:9997,&#34;ìķ¼ê²łëĭ¤&#34;:9998,&#34;ìĪĺê³ł&#34;:9999} . !head /gdrive/My Drive/nlpbook/bbpe/merges.txt . #version: 0.2 - Trained by `huggingface/tokenizers` Ġ ì Ġ ë ì Ŀ ë ĭ í ķ ê ° . . ìĿ ´ ëĭ ¤ . BPE 어휘집합과 바이그램 쌍/병합우선순위 일부를 출력했습니다. . BERT &#53664;&#53356;&#45208;&#51060;&#51200; &#44396;&#52629; . import os os.makedirs(&#39;/gdrive/My Drive/nlpbook/wordpiece&#39;, exist_ok= True) . from tokenizers import BertWordPieceTokenizer wordpiece_tokenizer = BertWordPieceTokenizer(lowercase = False) wordpiece_tokenizer.train( files=[&#39;/root/train.txt&#39;, &#39;/root/test.txt&#39;], vocab_size = 10000, ) wordpiece_tokenizer.save_model(&#39;/gdrive/My Drive/nlpbook/wordpiece&#39;) . [&#39;/gdrive/My Drive/nlpbook/wordpiece/vocab.txt&#39;] . 워드피스 방식입니다. BPE와 차이점은 단순히 빈도를 기준으로 병합하는 것이 아니라 병합했을 때 말뭉치의 우도를 높이는 쌍을 병합합니다. . !head /gdrive/My Drive/nlpbook/wordpiece/vocab.txt . [PAD] [UNK] [CLS] [SEP] [MASK] ! &#34; % &amp; &#39; . 워드피스 수행 결과의 일부 입니다. . &#53664;&#53360;&#54868;&#54616;&#44592; - GPT . from transformers import GPT2Tokenizer tokenizer_gpt = GPT2Tokenizer.from_pretrained(&#39;/gdrive/My Drive/nlpbook/bbpe&#39;) tokenizer_gpt.pad_token = &#39;[PAD]&#39; . file /gdrive/My Drive/nlpbook/bbpe/config.json not found . sentences = [ &quot;아 더빙.. 진짜 짜증나네요 목소리&quot;, &quot;흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나&quot;, &quot;별루 였다..&quot;, ] tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences] tokenized_sentences . [[&#39;ìķĦ&#39;, &#39;ĠëįĶë¹Ļ&#39;, &#39;..&#39;, &#39;Ġì§Ħì§ľ&#39;, &#39;Ġì§ľì¦ĿëĤĺ&#39;, &#39;ëĦ¤ìļĶ&#39;, &#39;Ġëª©ìĨĮë¦¬&#39;], [&#39;íĿł&#39;, &#39;...&#39;, &#39;íı¬ìĬ¤íĦ°&#39;, &#39;ë³´ê³ł&#39;, &#39;Ġì´ĪëĶ©&#39;, &#39;ìĺģíĻĶ&#39;, &#39;ì¤Ħ&#39;, &#39;....&#39;, &#39;ìĺ¤ë²Ħ&#39;, &#39;ìĹ°ê¸°&#39;, &#39;ì¡°ì°¨&#39;, &#39;Ġê°Ģë³į&#39;, &#39;ì§Ģ&#39;, &#39;ĠìķĬ&#39;, &#39;êµ¬ëĤĺ&#39;], [&#39;ë³Ħë£¨&#39;, &#39;Ġìĺ&#39;, &#39;Ģëĭ¤&#39;, &#39;..&#39;]] . GPT 토크나이저로 토큰화 해봤는데요. 알수 없는 언어들이 출력됩니다. . 이는 GPT 모델은 바이트 기준 BPE를 적용하기 때문입니다. . batch_inputs = tokenizer_gpt( sentences, padding = &#39;max_length&#39;, # 문장 최대 길이에 맞춰 패딩 max_length = 12, # 문장 토큰 기준 최대 길이 truncation = True, # 문장 잘림 허용 옵션 ) batch_inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;]) . 실제 모델 입력값 입니다. 결과는 input_ids, attention_mask 두개로 나옵니다. . batch_inputs[&#39;input_ids&#39;] . [[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0], [3693, 336, 2876, 758, 2883, 356, 806, 422, 9875, 875, 2960, 7292], [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]] . input_ids는 토큰화 결과를 가지고 각 토큰을 인덱스로 바꾼 것 입니다. . 여기서 모든 문장의 길이가 12로 맞춰줬는데 앞선 max_length 인자에 12를 넣었기 때문입니다. . [PAD] 토큰은 인덱스 0이며 더미 토큰입니다. 즉 위 값 중 0이 있으면 문장이 짧아 토큰 길이를 맞춰준 것으로 생각할 수 있습니다. . 문장 잘림을 허용하는 옵션 때문에 문장2는 토큰길이가 원래 15였는데 12안에 들어왔습니다. . batch_inputs[&#39;attention_mask&#39;] . [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]] . attention_mask는 일반 토큰이 자리한 곳(1)과 패딩 토큰이 자리한 곳(0)을 구분해주는 장치입니다. . &#53664;&#53360;&#54868;&#54616;&#44592; - BERT . from transformers import BertTokenizer tokenizer_bert = BertTokenizer.from_pretrained( &#39;/gdrive/My Drive/nlpbook/wordpiece&#39;, do_lower_case = False, ) . file /gdrive/My Drive/nlpbook/wordpiece/config.json not found . sentences = [ &quot;아 더빙.. 진짜 짜증나네요 목소리&quot;, &quot;흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나&quot;, &quot;별루 였다..&quot;, ] tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences] tokenized_sentences . [[&#39;아&#39;, &#39;더빙&#39;, &#39;.&#39;, &#39;.&#39;, &#39;진짜&#39;, &#39;짜증나&#39;, &#39;##네요&#39;, &#39;목소리&#39;], [&#39;흠&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;포스터&#39;, &#39;##보고&#39;, &#39;초딩&#39;, &#39;##영화&#39;, &#39;##줄&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;오버&#39;, &#39;##연기&#39;, &#39;##조차&#39;, &#39;가볍&#39;, &#39;##지&#39;, &#39;않&#39;, &#39;##구나&#39;], [&#39;별루&#39;, &#39;였다&#39;, &#39;.&#39;, &#39;.&#39;]] . 토큰 일부에 있는 ##은 해당 토큰이 어절의 시작이 아님을 나타냅니다. . batch_inputs = tokenizer_bert( sentences, padding = &#39;max_length&#39;, max_length = 12, truncation = True, ) batch_inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]) . 코드 적합시 input_ids, token_type_ids, attention_mask 총 3개의 출력물이 나옵니다. . batch_inputs[&#39;input_ids&#39;] . [[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0], [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1040, 16, 3], [2, 3274, 9507, 16, 16, 3, 0, 0, 0, 0, 0, 0]] . 모든 문장 시작시 2, 끝날 때 3이 붙은 것을 알 수 있습니다. . 여기서 2는 [CLS], 3은 [SEP]라는 토큰에 대응하는 인덱스입니다. . batch_inputs[&#39;attention_mask&#39;] . [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]] . attention_mask은 GPT와 마찬가지로 일반 토큰이 차지한 부분을 구분하는 역할을 합니다. . batch_inputs[&#39;token_type_ids&#39;] . [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] . token_type_ids는 세그먼트에 해당합니다. BERT 모델은 기본적으로 문서 2개를 입력받는데요. . 첫 번째 세그먼트(문서 혹은 문장)은 0, 두 번째 세그먼트는 1을 주어 둘을 구분합니다. . 이번 실습에서 문장을 하나씩 넣었으므로 모든 값이 0입니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/do%20it/natural%20language/gpt/bert/token/2021/12/20/Do_natural_language1.html",
            "relUrl": "/book/jupyter/do%20it/natural%20language/gpt/bert/token/2021/12/20/Do_natural_language1.html",
            "date": " • Dec 20, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "[SSUDA] 캐글 이용자 2021 설문조사 결과 분석",
            "content": ". &#52880;&#44544;&#44284; &#50672;&#46041;&#54616;&#44592; . !pip install kaggle !pip install --upgrade --force-reinstall --no-deps kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Collecting kaggle Downloading kaggle-1.5.12.tar.gz (58 kB) |████████████████████████████████| 58 kB 2.5 MB/s Building wheels for collected packages: kaggle Building wheel for kaggle (setup.py) ... done Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=540a3a7d36ae6106f20d1f7c29b1afe562ca44be8bd818181fa99f5c13aeecb8 Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5 Successfully built kaggle Installing collected packages: kaggle Attempting uninstall: kaggle Found existing installation: kaggle 1.5.12 Uninstalling kaggle-1.5.12: Successfully uninstalled kaggle-1.5.12 Successfully installed kaggle-1.5.12 . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;ff1e945a67cd54bc7068e3afe4a03ad6&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c kaggle-survey-2021 . Downloading kaggle-survey-2021.zip to /content 0% 0.00/3.01M [00:00&lt;?, ?B/s] 100% 3.01M/3.01M [00:00&lt;00:00, 103MB/s] . !unzip kaggle-survey-2021.zip . Archive: kaggle-survey-2021.zip inflating: kaggle_survey_2021_responses.csv inflating: supplementary_data/kaggle_survey_2021_answer_choices.pdf inflating: supplementary_data/kaggle_survey_2021_methodology.pdf . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import gc # For Memory Optimization import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Not sure if I used this from wordcloud import WordCloud from scipy.stats import norm # Some more necessary libraries (These are for drawing the image on the bar charts) import matplotlib.font_manager as fm from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox import matplotlib.image as mpimg # To Avoid unnecessary warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) # Since there are many columns, I would like to view them all pd.set_option(&#39;display.max_rows&#39;, 100) pd.set_option(&#39;display.max_columns&#39;, 400) . df = pd.read_csv(&#39;kaggle_survey_2021_responses.csv&#39;) df = df.iloc[1:,:] # The first row was describing the columns. Better to look at the description from the Metadata file provided df.head(3).style.set_properties(**{&quot;background-color&quot;: &quot;#76c5d6&quot;,&quot;color&quot;: &quot;black&quot;, &quot;border-color&quot;: &quot;black&quot;}) . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 Q7_Part_4 Q7_Part_5 Q7_Part_6 Q7_Part_7 Q7_Part_8 Q7_Part_9 Q7_Part_10 Q7_Part_11 Q7_Part_12 Q7_OTHER Q8 Q9_Part_1 Q9_Part_2 Q9_Part_3 Q9_Part_4 Q9_Part_5 Q9_Part_6 Q9_Part_7 Q9_Part_8 Q9_Part_9 Q9_Part_10 Q9_Part_11 Q9_Part_12 Q9_OTHER Q10_Part_1 Q10_Part_2 Q10_Part_3 Q10_Part_4 Q10_Part_5 Q10_Part_6 Q10_Part_7 Q10_Part_8 Q10_Part_9 Q10_Part_10 Q10_Part_11 Q10_Part_12 Q10_Part_13 Q10_Part_14 Q10_Part_15 Q10_Part_16 Q10_OTHER Q11 Q12_Part_1 Q12_Part_2 Q12_Part_3 Q12_Part_4 Q12_Part_5 Q12_OTHER Q13 Q14_Part_1 Q14_Part_2 Q14_Part_3 Q14_Part_4 Q14_Part_5 Q14_Part_6 Q14_Part_7 Q14_Part_8 Q14_Part_9 Q14_Part_10 Q14_Part_11 Q14_OTHER Q15 Q16_Part_1 Q16_Part_2 Q16_Part_3 Q16_Part_4 Q16_Part_5 Q16_Part_6 Q16_Part_7 Q16_Part_8 Q16_Part_9 Q16_Part_10 Q16_Part_11 Q16_Part_12 Q16_Part_13 Q16_Part_14 Q16_Part_15 Q16_Part_16 Q16_Part_17 Q16_OTHER Q17_Part_1 Q17_Part_2 Q17_Part_3 Q17_Part_4 Q17_Part_5 Q17_Part_6 Q17_Part_7 Q17_Part_8 Q17_Part_9 Q17_Part_10 Q17_Part_11 Q17_OTHER Q18_Part_1 Q18_Part_2 Q18_Part_3 Q18_Part_4 Q18_Part_5 Q18_Part_6 Q18_OTHER Q19_Part_1 Q19_Part_2 Q19_Part_3 Q19_Part_4 Q19_Part_5 Q19_OTHER Q20 Q21 Q22 Q23 Q24_Part_1 Q24_Part_2 Q24_Part_3 Q24_Part_4 Q24_Part_5 Q24_Part_6 Q24_Part_7 Q24_OTHER Q25 Q26 Q27_A_Part_1 Q27_A_Part_2 Q27_A_Part_3 Q27_A_Part_4 Q27_A_Part_5 Q27_A_Part_6 Q27_A_Part_7 Q27_A_Part_8 Q27_A_Part_9 Q27_A_Part_10 Q27_A_Part_11 Q27_A_OTHER Q28 Q29_A_Part_1 Q29_A_Part_2 Q29_A_Part_3 Q29_A_Part_4 Q29_A_OTHER Q30_A_Part_1 Q30_A_Part_2 Q30_A_Part_3 Q30_A_Part_4 Q30_A_Part_5 Q30_A_Part_6 Q30_A_Part_7 Q30_A_OTHER Q31_A_Part_1 Q31_A_Part_2 Q31_A_Part_3 Q31_A_Part_4 Q31_A_Part_5 Q31_A_Part_6 Q31_A_Part_7 Q31_A_Part_8 Q31_A_Part_9 Q31_A_OTHER Q32_A_Part_1 Q32_A_Part_2 Q32_A_Part_3 Q32_A_Part_4 Q32_A_Part_5 Q32_A_Part_6 Q32_A_Part_7 Q32_A_Part_8 Q32_A_Part_9 Q32_A_Part_10 Q32_A_Part_11 Q32_A_Part_12 Q32_A_Part_13 Q32_A_Part_14 Q32_A_Part_15 Q32_A_Part_16 Q32_A_Part_17 Q32_A_Part_18 Q32_A_Part_19 Q32_A_Part_20 Q32_A_OTHER Q33 Q34_A_Part_1 Q34_A_Part_2 Q34_A_Part_3 Q34_A_Part_4 Q34_A_Part_5 Q34_A_Part_6 Q34_A_Part_7 Q34_A_Part_8 Q34_A_Part_9 Q34_A_Part_10 Q34_A_Part_11 Q34_A_Part_12 Q34_A_Part_13 Q34_A_Part_14 Q34_A_Part_15 Q34_A_Part_16 Q34_A_OTHER Q35 Q36_A_Part_1 Q36_A_Part_2 Q36_A_Part_3 Q36_A_Part_4 Q36_A_Part_5 Q36_A_Part_6 Q36_A_Part_7 Q36_A_OTHER Q37_A_Part_1 Q37_A_Part_2 Q37_A_Part_3 Q37_A_Part_4 Q37_A_Part_5 Q37_A_Part_6 Q37_A_Part_7 Q37_A_OTHER Q38_A_Part_1 Q38_A_Part_2 Q38_A_Part_3 Q38_A_Part_4 Q38_A_Part_5 Q38_A_Part_6 Q38_A_Part_7 Q38_A_Part_8 Q38_A_Part_9 Q38_A_Part_10 Q38_A_Part_11 Q38_A_OTHER Q39_Part_1 Q39_Part_2 Q39_Part_3 Q39_Part_4 Q39_Part_5 Q39_Part_6 Q39_Part_7 Q39_Part_8 Q39_Part_9 Q39_OTHER Q40_Part_1 Q40_Part_2 Q40_Part_3 Q40_Part_4 Q40_Part_5 Q40_Part_6 Q40_Part_7 Q40_Part_8 Q40_Part_9 Q40_Part_10 Q40_Part_11 Q40_OTHER Q41 Q42_Part_1 Q42_Part_2 Q42_Part_3 Q42_Part_4 Q42_Part_5 Q42_Part_6 Q42_Part_7 Q42_Part_8 Q42_Part_9 Q42_Part_10 Q42_Part_11 Q42_OTHER Q27_B_Part_1 Q27_B_Part_2 Q27_B_Part_3 Q27_B_Part_4 Q27_B_Part_5 Q27_B_Part_6 Q27_B_Part_7 Q27_B_Part_8 Q27_B_Part_9 Q27_B_Part_10 Q27_B_Part_11 Q27_B_OTHER Q29_B_Part_1 Q29_B_Part_2 Q29_B_Part_3 Q29_B_Part_4 Q29_B_OTHER Q30_B_Part_1 Q30_B_Part_2 Q30_B_Part_3 Q30_B_Part_4 Q30_B_Part_5 Q30_B_Part_6 Q30_B_Part_7 Q30_B_OTHER Q31_B_Part_1 Q31_B_Part_2 Q31_B_Part_3 Q31_B_Part_4 Q31_B_Part_5 Q31_B_Part_6 Q31_B_Part_7 Q31_B_Part_8 Q31_B_Part_9 Q31_B_OTHER Q32_B_Part_1 Q32_B_Part_2 Q32_B_Part_3 Q32_B_Part_4 Q32_B_Part_5 Q32_B_Part_6 Q32_B_Part_7 Q32_B_Part_8 Q32_B_Part_9 Q32_B_Part_10 Q32_B_Part_11 Q32_B_Part_12 Q32_B_Part_13 Q32_B_Part_14 Q32_B_Part_15 Q32_B_Part_16 Q32_B_Part_17 Q32_B_Part_18 Q32_B_Part_19 Q32_B_Part_20 Q32_B_OTHER Q34_B_Part_1 Q34_B_Part_2 Q34_B_Part_3 Q34_B_Part_4 Q34_B_Part_5 Q34_B_Part_6 Q34_B_Part_7 Q34_B_Part_8 Q34_B_Part_9 Q34_B_Part_10 Q34_B_Part_11 Q34_B_Part_12 Q34_B_Part_13 Q34_B_Part_14 Q34_B_Part_15 Q34_B_Part_16 Q34_B_OTHER Q36_B_Part_1 Q36_B_Part_2 Q36_B_Part_3 Q36_B_Part_4 Q36_B_Part_5 Q36_B_Part_6 Q36_B_Part_7 Q36_B_OTHER Q37_B_Part_1 Q37_B_Part_2 Q37_B_Part_3 Q37_B_Part_4 Q37_B_Part_5 Q37_B_Part_6 Q37_B_Part_7 Q37_B_OTHER Q38_B_Part_1 Q38_B_Part_2 Q38_B_Part_3 Q38_B_Part_4 Q38_B_Part_5 Q38_B_Part_6 Q38_B_Part_7 Q38_B_Part_8 Q38_B_Part_9 Q38_B_Part_10 Q38_B_Part_11 Q38_B_OTHER . 1 910 | 50-54 | Man | India | Bachelor’s degree | Other | 5-10 years | Python | R | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Python | nan | nan | nan | nan | nan | nan | nan | nan | Vim / Emacs | nan | nan | nan | nan | nan | Colab Notebooks | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | A laptop | nan | Google Cloud TPUs | nan | nan | nan | nan | 2-5 times | Matplotlib | Seaborn | nan | Ggplot / ggplot2 | Shiny | nan | nan | nan | nan | Leaflet / Folium | nan | nan | 5-10 years | Scikit-learn | TensorFlow | nan | nan | nan | nan | nan | nan | nan | nan | nan | Caret | nan | nan | nan | nan | nan | nan | Linear or Logistic Regression | Decision Trees or Random Forests | Gradient Boosting Machines (xgboost, lightgbm, etc) | Bayesian Approaches | nan | Dense Neural Networks (MLPs, etc) | Convolutional Neural Networks | nan | Recurrent Neural Networks | nan | nan | nan | General purpose image/video tools (PIL, cv2, skimage, etc) | nan | nan | nan | nan | nan | nan | Word embeddings/vectors (GLoVe, fastText, word2vec) | nan | nan | nan | nan | nan | Manufacturing/Fabrication | 50-249 employees | 3-4 | No (we do not use ML methods) | nan | nan | nan | nan | nan | nan | None of these activities are an important part of my role at work | nan | 25,000-29,999 | $100-$999 | nan | nan | Google Cloud Platform (GCP) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Google Cloud Compute Engine | nan | nan | nan | nan | nan | nan | Google Cloud Storage (GCS) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | PostgreSQL | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | GitHub | nan | Kaggle | nan | nan | nan | nan | Coursera | edX | Kaggle Learn Courses | DataCamp | nan | Udacity | Udemy | nan | nan | nan | nan | nan | Local development environments (RStudio, JupyterLab, etc.) | nan | Email newsletters (Data Elixir, O&#39;Reilly Data &amp; AI, etc) | nan | Kaggle (notebooks, forums, etc) | nan | YouTube (Kaggle YouTube, Cloud AI Adventures, etc) | Podcasts (Chai Time Data Science, O’Reilly Data Show, etc) | Blogs (Towards Data Science, Analytics Vidhya, etc) | Journal Publications (peer-reviewed journals, conference proceedings, etc) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2 784 | 50-54 | Man | Indonesia | Master’s degree | Program/Project Manager | 20+ years | nan | nan | SQL | C | C++ | Java | nan | nan | nan | nan | nan | nan | nan | Python | nan | nan | nan | nan | nan | nan | Notepad++ | nan | nan | nan | Jupyter Notebook | nan | nan | Kaggle Notebooks | Colab Notebooks | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc) | nan | nan | nan | nan | None | nan | Never | Matplotlib | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Under 1 year | Scikit-learn | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Linear or Logistic Regression | Decision Trees or Random Forests | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Manufacturing/Fabrication | 1000-9,999 employees | 1-2 | We are exploring ML methods (and may one day put a model into production) | nan | Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data | nan | nan | nan | nan | nan | nan | 60,000-69,999 | $0 ($USD) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Kaggle Learn Courses | nan | nan | nan | nan | nan | Cloud-certification programs (direct from AWS, Azure, GCP, or similar) | University Courses (resulting in a university degree) | nan | nan | Advanced statistical software (SPSS, SAS, etc.) | nan | nan | nan | nan | nan | nan | nan | nan | Journal Publications (peer-reviewed journals, conference proceedings, etc) | nan | nan | nan | nan | nan | Google Cloud Platform (GCP) | nan | Oracle Cloud | nan | nan | nan | nan | nan | nan | nan | nan | nan | Google Cloud Compute Engine | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | MySQL | nan | SQLite | Oracle Database | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Google Cloud SQL | nan | nan | nan | nan | nan | nan | nan | Google Data Studio | nan | nan | nan | nan | Qlik | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Automated model selection (e.g. auto-sklearn, xcessiv) | nan | nan | nan | nan | nan | Google Cloud AutoML | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | . 3 924 | 22-24 | Man | Pakistan | Master’s degree | Software Engineer | 1-3 years | Python | nan | nan | nan | C++ | Java | nan | nan | nan | nan | nan | nan | nan | Python | nan | nan | nan | nan | PyCharm | nan | nan | nan | nan | nan | Jupyter Notebook | nan | Other | Kaggle Notebooks | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | A laptop | nan | nan | nan | nan | nan | Other | Never | Matplotlib | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | I do not use machine learning methods | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Academics/Education | 1000-9,999 employees | 0 | I do not know | nan | nan | nan | nan | nan | nan | None of these activities are an important part of my role at work | nan | $0-999 | $0 ($USD) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | DataRobot | nan | nan | nan | nan | nan | nan | MySQL | nan | nan | nan | MongoDB | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | MySQL | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | None | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | No / None | nan | nan | nan | nan | nan | nan | nan | nan | nan | I do not share my work publicly | nan | nan | nan | nan | DataCamp | nan | nan | nan | nan | nan | nan | nan | nan | Basic statistical software (Microsoft Excel, Google Sheets, etc.) | nan | nan | nan | Kaggle (notebooks, forums, etc) | nan | YouTube (Kaggle YouTube, Cloud AI Adventures, etc) | nan | nan | nan | nan | nan | nan | Amazon Web Services (AWS) | nan | Google Cloud Platform (GCP) | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Microsoft Azure Virtual Machines | Google Cloud Compute Engine | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Azure Machine Learning Studio | Google Cloud Vertex AI | DataRobot | nan | nan | nan | nan | nan | nan | MySQL | PostgreSQL | nan | nan | MongoDB | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Microsoft Power BI | nan | nan | nan | Tableau | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | Automated model selection (e.g. auto-sklearn, xcessiv) | nan | nan | nan | nan | nan | nan | nan | nan | DataRobot AutoML | nan | nan | nan | nan | nan | nan | nan | nan | TensorBoard | nan | nan | nan | nan | nan | nan | nan | . print(&#39;Number of rows:&#39;, df.shape[0]) print(&#39;Number of columns:&#39;, df.shape[1]) . Number of rows: 25973 Number of columns: 369 . &#52395;&#48264;&#51704; &#51656;&#47928; : &#45208;&#51060; . df[&#39;Q1&#39;].value_counts() . 25-29 4931 18-21 4901 22-24 4694 30-34 3441 35-39 2504 40-44 1890 45-49 1375 50-54 964 55-59 592 60-69 553 70+ 128 Name: Q1, dtype: int64 . fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) # Method for image def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 # Creating a DataFrame to get the values and their counts (this was for my purpose) # new_df = pd.DataFrame(df[&#39;Q1&#39;].value_counts()) # I wanted to have the highest value in the middle, so i wrote the following two code lines age_bucket = [&#39;70+&#39;,&#39;55-59&#39;,&#39;45-49&#39;,&#39;35-39&#39;,&#39;22-24&#39;,&#39;25-29&#39;,&#39;18-21&#39;,&#39;30-34&#39;,&#39;40-44&#39;,&#39;50-54&#39;,&#39;60-69&#39;] #new_df.index age_bucket_cnt = [128,592,1375,2504,4694,4931,4901,3441,1890,964,553] #list(new_df.Q1.values) color = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;] # Deciding the color width = [0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.8, 0.8, 0.8, 0.8] # The Width alpha = [0.3, 0.45, 0.5, 0.6, 0.75, 1.0, 0.75, 0.6, 0.5, 0.45, 0.3] # The Opacity fontsize= [20, 20, 20, 20, 25, 35, 30, 20, 20, 20, 20] x_num = [0,1,2,3,4,5,6,7,8,9,10] for i in range(11): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Age Bucket of all Kagglers&quot;,x=5,y=5500, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.2, 5, 4700) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 확실히 대학생이나 취업 준비생이 많이 이용하는 느낌이다. . 다만 18-21세 연령대 이용률이 생각보다 높은 것이 신기했다. . &#46160;&#48264;&#51704; &#51656;&#47928;: &#49457;&#48324; . df[&#39;Q2&#39;].value_counts() . Man 20598 Woman 4890 Prefer not to say 355 Nonbinary 88 Prefer to self-describe 42 Name: Q2, dtype: int64 . Gender = [&#39;Man&#39;, &#39;Woman&#39;, &#39;Others&#39;] # Setting size in Chart based on # given values Gender_cnt = [20598, 4890, 485] # colors colors = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#FFFF00&#39;, &#39;#ADFF2F&#39;, &#39;#FFA500&#39;] # explosion explode = (0.05, 0.05, 0.2) plt.figure(figsize=[20,10]) # Pie Chart plt.pie(Gender_cnt, colors=colors, autopct=&#39;%1.1f%%&#39;, pctdistance=1.2, explode=explode,) # draw circle centre_circle = plt.Circle((0, 0), 0.70, fc=&#39;white&#39;) fig = plt.gcf() plt.legend(Gender, loc = &quot;upper right&quot;,title=&quot;Genders&quot;, prop={&#39;size&#39;: 15}) # Adding Circle in Pie chart fig.gca().add_artist(centre_circle) plt.rcParams[&#39;font.size&#39;] = 25 # Adding Title of chart plt.text(s=&quot;Gender Diversity in Kaggle&quot;,x=0,y=1.3, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) gc.collect() # Displaing Chart plt.show() . 남자가 약 80%, 여자가 약 18%이고 기타 이유(공개 희망 안함, 미 기제 등) 2% 입니다. . 확실히 남성이 주류인 분야인 것 같습니다. . &#49464;&#48264;&#51704; &#51656;&#47928;: &#44397;&#51201; . df[&#39;Q3&#39;].value_counts() . India 7434 United States of America 2650 Other 1270 Japan 921 China 814 Brazil 751 Russia 742 Nigeria 702 United Kingdom of Great Britain and Northern Ireland 550 Pakistan 530 Egypt 482 Germany 470 Spain 454 Indonesia 444 Turkey 416 France 401 South Korea 359 Taiwan 334 Canada 331 Bangladesh 317 Italy 311 Mexico 279 Viet Nam 277 Australia 264 Kenya 248 Colombia 225 Poland 219 Iran, Islamic Republic of... 195 Ukraine 186 Singapore 182 Argentina 182 Malaysia 156 Netherlands 153 South Africa 146 Morocco 140 Israel 138 Thailand 123 Portugal 119 Peru 117 United Arab Emirates 111 Tunisia 109 Philippines 108 Sri Lanka 106 Chile 102 Greece 102 Ghana 99 Saudi Arabia 89 Ireland 84 Sweden 81 Hong Kong (S.A.R.) 79 Nepal 75 Switzerland 71 I do not wish to disclose my location 69 Belgium 65 Czech Republic 63 Romania 61 Austria 51 Belarus 51 Ecuador 50 Denmark 48 Uganda 47 Norway 45 Kazakhstan 45 Algeria 44 Ethiopia 43 Iraq 43 Name: Q3, dtype: int64 . !pip install geopandas import geopandas as gpd # List of countries we are interested in lis_countries = [&quot;Algeria&quot;,&quot;Argentina&quot;,&quot;Australia&quot;,&quot;Austria&quot;,&quot;Bangladesh&quot;,&quot;Belarus&quot;,&quot;Belgium&quot;,&quot;Brazil&quot;,&quot;Canada&quot;,&quot;Chile&quot;,&quot;China&quot;,&quot;Colombia&quot;, &quot;Czechia&quot;,&quot;Denmark&quot;,&quot;Ecuador&quot;,&quot;Egypt&quot;,&quot;Ethiopia&quot;,&quot;France&quot;,&quot;Germany&quot;,&quot;Ghana&quot;,&quot;Greece&quot;,&quot;India&quot;,&quot;Indonesia&quot;,&quot;Iraq&quot;,&quot;Ireland&quot;, &quot;Israel&quot;,&quot;Italy&quot;,&quot;Japan&quot;,&quot;Kazakhstan&quot;,&quot;Kenya&quot;,&quot;Malaysia&quot;,&quot;Mexico&quot;,&quot;Morocco&quot;,&quot;Nepal&quot;,&quot;Netherlands&quot;,&quot;Nigeria&quot;,&quot;Norway&quot;,&quot;Pakistan&quot;, &quot;Peru&quot;,&quot;Philippines&quot;,&quot;Poland&quot;,&quot;Portugal&quot;,&quot;Romania&quot;,&quot;Russia&quot;,&quot;Saudi Arabia&quot;,&quot;South Africa&quot;,&quot;South Korea&quot;,&quot;Spain&quot;,&quot;Sri Lanka&quot;, &quot;Sweden&quot;,&quot;Switzerland&quot;,&quot;Taiwan&quot;,&quot;Thailand&quot;,&quot;Tunisia&quot;,&quot;Turkey&quot;,&quot;Uganda&quot;,&quot;Ukraine&quot;,&quot;United Arab Emirates&quot;,&quot;United Kingdom&quot;, &quot;United States of America&quot;,&quot;Vietnam&quot;] # Reading the geopandas data world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) country_data = lis_countries # Passing the list of countries here country_geo = list(world[&#39;name&#39;]) # The country list from the geopandas dataset # List of all the values of population of Kagglers from each country lis_pop = [44,182,264,51,317,51,65,751,331,102,814,225,63,48,50,482,43,401,470,99,102,7434,444,43,84,138,311,921,45,248,156,279,140,75,153, 702,45,530,117,108,219,119,61,742,89,146,359,454,106,81,71,334,123,109,416,47,186,111,550,2650,277] # Next we need to create a dataframe with lis_countries and lis_pop our_country_analysis = pd.DataFrame(lis_countries, columns=[&#39;Country&#39;]) our_country_analysis[&#39;KagglePopulation&#39;] = lis_pop # Next, we are going to visualize this... mapped = world.set_index(&#39;name&#39;).join(our_country_analysis.set_index(&#39;Country&#39;)).reset_index() to_be_mapped = &#39;KagglePopulation&#39; vmin, vmax = 0,10000 fig, ax = plt.subplots(1, figsize=(25,30)) mapped.dropna().plot(column=to_be_mapped, cmap=&#39;cividis&#39;, linewidth=0.8, ax=ax, edgecolors=&#39;1&#39;, alpha=0.7) ax.text(s=&quot;Kagglers All Around the Globe&quot;,x=0,y=100, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) ax.set_axis_off() sm = plt.cm.ScalarMappable(cmap=&#39;cividis&#39;, norm=plt.Normalize(vmin=vmin, vmax=vmax)) sm._A = [] gc.collect() cbar = fig.colorbar(sm, orientation=&#39;vertical&#39;, shrink= .25) . Requirement already satisfied: geopandas in /usr/local/lib/python3.7/dist-packages (0.10.2) Requirement already satisfied: fiona&gt;=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.20) Requirement already satisfied: shapely&gt;=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.0) Requirement already satisfied: pyproj&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (3.2.1) Requirement already satisfied: pandas&gt;=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (2021.10.8) Requirement already satisfied: six&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (1.15.0) Requirement already satisfied: attrs&gt;=17 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (21.2.0) Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (2.5.0) Requirement already satisfied: cligj&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (0.7.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (57.4.0) Requirement already satisfied: click-plugins&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (1.1.1) Requirement already satisfied: click&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (7.1.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2018.9) Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25.0-&gt;geopandas) (1.19.5) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2.8.2) . 포화도가 높을 수록 노란색에 가까워 지는 것을 알 수 있습니다. . 인도 사람들이 확실히 많이 이용하는 모습이군요. . 중간중간 하얗게 빈 나라들도 있습니다. . &#45348;&#48264;&#51704; &#51656;&#47928;: &#54617;&#47141; . df[&#39;Q4&#39;].value_counts() . Master’s degree 10132 Bachelor’s degree 9907 Doctoral degree 2795 Some college/university study without earning a bachelor’s degree 1735 I prefer not to answer 627 No formal education past high school 417 Professional doctorate 360 Name: Q4, dtype: int64 . fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) # Method for image def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 # I wanted to have the highest value in the middle, so i wrote the following two code lines age_bucket = [&#39;Professional Doctorate&#39;,&#39;High School&#39;,&#39;Bachelor’s degree&#39;,&#39;Master’s degree&#39;,&#39;Doctoral degree&#39;,&#39;Others&#39;,&#39;No Answer&#39;] age_bucket_cnt = [360,417,9907,10132,2795,1735,627] color = [&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;] # Deciding the color width = [0.8, 0.8, 0.9, 0.9, 0.9, 0.8, 0.8,] # The Width alpha = [0.5, 0.6, 0.75, 1.0, 0.75, 0.6, 0.5] # The Opacity fontsize= [12, 16, 18, 21, 16, 16, 16] x_num = [0,1,2,3,4,5,6] for i in range(7): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Educational Qualifications of all Kagglers&quot;,x=3,y=11000, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.25, 3, 9500) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 대부분의 캐글 이용자들은 학사 이상의 학위를 가지고 있습니다. . (Master&#39;s degree : 석사, Bachelor&#39;s degree : 학사, Doctoral degree : 박사 학위) . &#45796;&#49455;&#48264;&#51704; &#51656;&#47928;: &#51649;&#50629; . df[&#39;Q5&#39;].value_counts() . Student 6804 Data Scientist 3616 Software Engineer 2449 Other 2393 Data Analyst 2301 Currently not employed 1986 Research Scientist 1538 Machine Learning Engineer 1499 Business Analyst 968 Program/Project Manager 849 Data Engineer 668 Product Manager 319 Statistician 313 DBA/Database Engineer 171 Developer Relations/Advocacy 99 Name: Q5, dtype: int64 . # Method for image def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) # Creating a DataFrame to get the values and their counts (this was for my purpose) # new_df = pd.DataFrame(df[&#39;Q1&#39;].value_counts()) # I wanted to have the highest value in the middle, so i wrote the following two code lines age_bucket = [&#39;Developer n Relations n/Advocacy&#39;,&#39;Statistician&#39;,&#39;Data n Engineer&#39;,&#39;Business n Analyst&#39;,&#39;Research n Scientist&#39;,&#39;Data n Analyst&#39;,&#39;Software n Engineer&#39;,&#39;Student&#39;, &#39;Data n Scientist&#39;,&#39;Other&#39;,&#39;Unemployed&#39;,&#39;ML n Engineer&#39;,&#39;Project n Manager&#39;,&#39;Product n Manager&#39;,&#39;DB n Engineer&#39;] #new_df.index age_bucket_cnt = [99,313,668,968,1538,2301,2449,6804,3414,2393,1986,1499,849,319,171] #list(new_df.Q1.values) color = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#E6E6E6&#39;, &#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#189AB4&#39;, &#39;#E6E6E6&#39;] # Deciding the color width = [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8] # The Width alpha = [0.3, 0.45, 0.3, 0.45, 0.5, 0.6, 0.75, 1.0, 0.75, 0.6, 0.5, 0.45, 0.3, 0.3, 0.45] # The Opacity fontsize= [12, 12, 14, 14, 14, 14, 18, 20, 16, 14, 12, 14, 14, 12, 12] x_num = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14] for i in range(15): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Current Role of all Kagglers&quot;,x=7.5,y=7500, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.15, 7, 6500) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 작성자의 예측과 다르게 학생이 압도적으로 높은 수치가 나왔습니다. . (대부분이 ML 전문가나 데이터 분석가가 나올것이라고 생각한 것 같아요.) . 여기서 주목할 점이 Others 입니다. 꽤 상위권에 위치하는데요. . 타 분야 사람이 캐글 이용에 적극적인 것으로 생각할 수 있는데요. 데이터 분석이 많은 분야에서 응용될 수 있다는 것을 보여주는 것 같아요. . &#50668;&#49455;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#44221;&#47141; . df[&#39;Q6&#39;].value_counts() . 1-3 years 7874 &lt; 1 years 5881 3-5 years 4061 5-10 years 3099 10-20 years 2166 20+ years 1860 I have never written code 1032 Name: Q6, dtype: int64 . years_bin = [&#39;1-3years&#39;,&#39;&lt;1years&#39;,&#39;3-5years&#39;,&#39;5-10years&#39;,&#39;10-20years&#39;,&#39;20+years&#39;,&#39;Never Coded&#39;] years_cnt = [7874, 5881, 4061, 3099, 2166, 1860, 1032] fig = plt.figure(figsize=(20,10)) plt.barh(width=years_cnt, y=years_bin, height=0.7, color = [&#39;#189AB4&#39;, &#39;#189AB4&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;], alpha=0.8) ##################### For the Years of Experience ################################### s1 = [&#39;1-3years&#39;,&#39;&lt;1years&#39;,&#39;3-5years&#39;,&#39;5-10years&#39;,&#39;10-20years&#39;,&#39;20+years&#39;,&#39;Never Coded&#39;] x1 = [8874, 6881, 5061, 4099, 3366, 2860, 2432] y1 = [0,1,2,3,4,5,6] for i in range(7): plt.text(s = s1[i], x=x1[i], y=y1[i] ,fontsize=25,va=&#39;center&#39;,ha=&#39;right&#39;,alpha=0.8) plt.title(&quot;Average Years of Programming Experience of Kagglers&quot;, fontsize=42, pad=20, color=&#39;#189AB4&#39;) plt.axis(&#39;off&#39;) plt.gca().invert_yaxis() plt.show() . 캐글 내에 생각보다 코딩 경력이 오래된 사람이 많지 않습니다. . 젏은 플렛폼이라고도 생각할 수 있고, 초보자가 접근하기 어렵지 않다고도 생각할 수 있겠네요. . &#51068;&#44273;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#50616;&#50612; . df[&#39;Q7_Part_1&#39;].value_counts() . Python 21860 Name: Q7_Part_1, dtype: int64 . df[&#39;Q7_Part_2&#39;].value_counts() . R 5334 Name: Q7_Part_2, dtype: int64 . Tool = [&#39;Python&#39;, &#39;R&#39;] # Setting size in Chart based on # given values Tool_cnt = [21860, 5334] # colors colors = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;] # explosion explode = (0.05, 0.05) plt.figure(figsize=[20,10]) # Pie Chart plt.pie(Tool_cnt, colors=colors, autopct=&#39;%1.1f%%&#39;, pctdistance=1.2, explode=explode,) # draw circle centre_circle = plt.Circle((0, 0), 0.70, fc=&#39;white&#39;) fig = plt.gcf() plt.legend(Tool, loc = &quot;upper right&quot;,title=&quot;Programming Languages&quot;, prop={&#39;size&#39;: 15}) # Adding Circle in Pie chart fig.gca().add_artist(centre_circle) plt.rcParams[&#39;font.size&#39;] = 25 # Adding Title of chart plt.text(s=&quot;Which Programming Tool do they Prefer?&quot;,x=0,y=1.3, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) gc.collect() # Displaing Chart plt.show() . 파이썬과 R 이외에 다른 선택지도 있었고, 중복 선택이 허용된 문항이지만 작성자는 파이썬과 R만을 비교했습니다. . 파이썬이 80% 이상으로 압도적인 사용률을 보였는데요. . 앞서 조사한 결과에서 학생인 사람이 많고, 타 분야 전문가도 많기 때문에 쉬운 언어인 파이썬의 사용률이 높지 않을까 생각했어요. . &#50668;&#45919;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#50616;&#50612;2 . df[&#39;Q8&#39;] = df[&#39;Q8&#39;].apply(lambda x: &#39;Others&#39; if x not in [&#39;Python&#39;,&#39;R&#39;,&#39;SQL&#39;] else x) df[&#39;Q8&#39;].value_counts() . Python 20213 Others 2977 R 1445 SQL 1338 Name: Q8, dtype: int64 . Tool = [&#39;Python&#39;, &#39;R&#39;, &#39;SQL&#39;, &#39;Others&#39;] # Setting size in Chart based on # given values Tool_cnt = [20213, 1445, 1338, 2977] # colors colors = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#FFFF00&#39;, &#39;#ADFF2F&#39;] # explosion explode = (0.05, 0.05, 0.05, 0.05) plt.figure(figsize=[20,10]) # Pie Chart plt.pie(Tool_cnt, colors=colors, autopct=&#39;%1.1f%%&#39;, pctdistance=1.2, explode=explode,) # draw circle centre_circle = plt.Circle((0, 0), 0.70, fc=&#39;white&#39;) fig = plt.gcf() plt.legend(Tool, loc = &quot;upper right&quot;,title=&quot;Programming Languages&quot;, prop={&#39;size&#39;: 15}) # Adding Circle in Pie chart fig.gca().add_artist(centre_circle) plt.rcParams[&#39;font.size&#39;] = 25 # Adding Title of chart plt.text(s=&quot;What do they Recommend for Data Science?&quot;,x=0,y=1.3, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) gc.collect() # Displaing Chart plt.show() . 앞선 조사와 비슷한데, 차이점은 중복선택이 안된다는 점입니다. . 선택지가 꽤 많았는데도 파이썬이 압도적인 선택률을 보이네요. . &#50500;&#54857;&#48264;&#51704; &#51656;&#47928;: &#54532;&#47196;&#44536;&#47000;&#48141; &#54872;&#44221;(IDE) . df[&#39;Q9_Part_1&#39;].value_counts() . Jupyter (JupyterLab, Jupyter Notebooks, etc) 5488 Name: Q9_Part_1, dtype: int64 . df[&#39;Q9_Part_2&#39;].value_counts() . RStudio 4771 Name: Q9_Part_2, dtype: int64 . 이런식으로 값을 추출해서 적용한 것 같아요. . name = [&#39;JupyterLab&#39;,&#39;RStudio&#39;,&#39;Visual Studio&#39;,&#39;VS Code&#39;,&#39;PyCharm&#39;,&#39;Spyder&#39;,&#39;Notepad++&#39;,&#39;Sublime Text&#39;,&#39;Vim/Emacs&#39;,&#39;MATLAB&#39;,&#39;Jupyter Notebook&#39;,&#39;None&#39;,&#39;Other&#39;] value = [5488,4771,4110,10040,7468,3794,3937,2839,1646,2203,16233,526,1491] # Creating a dataframe to store this information df_nine_ = pd.DataFrame(name, columns=[&#39;IDE&#39;]) df_nine_[&#39;Values&#39;] = value df_nine_ = df_nine_.sort_values(by=&quot;Values&quot;, ascending=False) df_nine_ fig = plt.figure(figsize=(20,10)) plt.barh(width=list(df_nine_[&#39;Values&#39;].unique()), y=list(df_nine_[&#39;IDE&#39;].unique()), height=0.7, color = [&#39;#189AB4&#39;, &#39;#189AB4&#39;, &#39;#189AB4&#39;, &#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;, &#39;#E6E6E6&#39;], alpha=0.8) ##################### For the Years of Experience ################################### s1 = list(df_nine_[&#39;IDE&#39;].unique()) x1 = [19833,12040,9468,7788,6471,6810,6437,5294,5539,4003,3946,2691,1726] y1 = [0,1,2,3,4,5,6,7,8,9,10,11,12] for i in range(13): plt.text(s = s1[i], x=x1[i], y=y1[i] , fontsize=25,va=&#39;center&#39;,ha=&#39;right&#39;,alpha=0.8) plt.title(&quot;Preferred IDE of Kagglers&quot;, fontsize=42, pad=20, color=&#39;#189AB4&#39;) plt.axis(&#39;off&#39;) plt.gca().invert_yaxis() gc.collect() plt.show() . 주피터 노트북이 사용자 친화적이라고 코멘트를 합니다. 시프트+엔터시 결과물이 바로 나와 편리하다는 근거와 함께. . VS CODE는 다른 언어(C) 할때 저도 사용했는데, 깃허브와 연동이 좋아서 사용이 편리합니다. 역시 많은 사용자가 이용하는 것 같아요. . 파이참도 저는 써보진 않았지만 높은 순위를 기록합니다. . R을 사용하는 사람 비율 대비 R스튜디오도 많이 쓰는 모습을 보이는데, 대부분에 R 사용자가 R스튜디오를 사용한다고 생각됩니다. . &#50676;&#48264;&#51704; &#51656;&#47928;: &#51452; &#49324;&#50857; &#45432;&#53944;&#48513; . df[&#39;Q10_Part_1&#39;].value_counts() . Kaggle Notebooks 9507 Name: Q10_Part_1, dtype: int64 . df[&#39;Q10_Part_2&#39;].value_counts() . Colab Notebooks 9792 Name: Q10_Part_2, dtype: int64 . 코랩 노트북, 캐글 노트북 이용자 이외는 Other로 생각한 것 같습니다. . def make_img(img,zoom, x, y): img = mpimg.imread(img) imagebox = OffsetImage(img, zoom=zoom) ab = AnnotationBbox(imagebox, (x,y),frameon=False) ax.add_artist(ab) img_file = &quot;https://www.freeiconspng.com/thumbs/crown-icon/queen-crown-icon-4.png&quot; zoom = 1 img_y= 4.8 # Visualizing the Hosted Notebooks. (Hidden Input) fig, ax = plt.subplots(figsize=(25,10), facecolor=&quot;w&quot;) age_bucket = [&#39;None&#39;,&#39;Colab Notebook&#39;,&#39;Kaggle Notebook&#39;] age_bucket_cnt = [7174,9792,9507] color = [&#39;#E6E6E6&#39;,&#39;#189AB4&#39;,&#39;#E6E6E6&#39;] # Deciding the color width = [0.9, 0.9, 0.9] # The Width alpha = [0.55, 1.0, 0.75] # The Opacity fontsize= [25, 45, 30] x_num = [0,1,2] for i in range(3): plt.bar(x=age_bucket[i],height=age_bucket_cnt[i], width=width[i], color=color[i], alpha=alpha[i]) plt.text(s=age_bucket[i],x=x_num[i],y=age_bucket_cnt[i],va=&#39;bottom&#39;,ha=&#39;center&#39;,fontsize=fontsize[i], alpha=alpha[i]) plt.text(s=&quot;Preferred Hosted Notebooks&quot;,x=1,y=11000, fontsize=50,va=&#39;bottom&#39;,ha=&#39;center&#39;,color=&#39;#189AB4&#39;) # Placing the image make_img(img_file,0.3, 1, 9000) gc.collect() # For Memory Optimization plt.axis(&#39;off&#39;) plt.show() . 코랩 노트북과 캐글 노트북의 사용자 수가 비슷합니다. . 코랩 노트북은 점유율 1위로, GPU 사용이 일부 가능하고 구글 드라이브와 연동이 잘된다는 점을 큰 장점으로 소개합니다. . 물론 캐글 이용자 조사이기 때문에 캐글 데이터와 캐글 노트북 간 호완성, 접근성이 좋아서 캐글 노트북 사용자가 다소 많이 집계됬습니다. . 다만 캐글 노트북 만에 분명한 장점이 있겠죠? 한번 어느 환경인지 기회될때 탐색하는 것도 좋을 것 같아요. . 또 특이한 점은 두 노트북 이외 각자의 PC환경을 사용하는 사람도 꽤 많다는 것입니다. . &#50676;&#54620;&#48264;&#51704; &#51656;&#47928;: &#44032;&#49549;&#44592; &#50976;&#47924; . df[&#39;Q12_Part_1&#39;].value_counts() . NVIDIA GPUs 8036 Name: Q12_Part_1, dtype: int64 . df[&#39;Q12_Part_2&#39;].value_counts() . Google Cloud TPUs 3451 Name: Q12_Part_2, dtype: int64 . df[&#39;Q12_Part_3&#39;].value_counts() . AWS Trainium Chips 414 Name: Q12_Part_3, dtype: int64 . df[&#39;Q12_Part_4&#39;].value_counts() . AWS Inferentia Chips 416 Name: Q12_Part_4, dtype: int64 . df[&#39;Q12_Part_5&#39;].value_counts() . None 13234 Name: Q12_Part_5, dtype: int64 . df[&#39;Q12_OTHER&#39;].value_counts() . Other 867 Name: Q12_OTHER, dtype: int64 . name = [&quot;None&quot;,&quot;NVIDIA GPUs&quot;,&quot;Google Cloud TPUs&quot;,&quot;Other&quot;,&quot;AWS Inferentia Chips&quot;,&quot;AWS Trainium Chips&quot;] count = [13234,8036,3451,867,416,414] # Visualizing using a barh: fig = plt.figure(figsize=(20,10)) plt.barh(width=count, y=name, height=0.7, color = [&#39;#E6E6E6&#39;, &#39;#189AB4&#39;, &#39;#189AB4&#39;, &#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;,&#39;#E6E6E6&#39;], alpha=0.8) ##################### For the Years of Experience ################################### s1 = name x1 = [14234,10236,6651,2067,3916,3714] y1 = [0,1,2,3,4,5] for i in range(6): plt.text(s = s1[i], x=x1[i], y=y1[i] , fontsize=25,va=&#39;center&#39;,ha=&#39;right&#39;,alpha=0.8) plt.title(&quot;Specialized Hardware&quot;, fontsize=42, pad=20, color=&#39;#189AB4&#39;) plt.axis(&#39;off&#39;) plt.gca().invert_yaxis() gc.collect() plt.show() . GPU나 TPU를 사용하지 않는 캐글 사용자가 상당히 많이 있네요. . &#45712;&#45184;&#51216; . 대회참가를 위한 데이터 공부가 아니라 설문조사를 시각화 하는 공부였습니다. . 이쁘게 시각화 하기 위해서 작성자가 다양하게 노력한 모습을 확인했습니다. . 또한 설문조사가 캐글 이용자 관련 설문조사라서 결과에 대해 더 흥미롭게 확인 한 것 같아요. . 가볍게 공부하기 좋은 데이터 셋인것 같습니다. . 대회 출처 : https://www.kaggle.com/c/kaggle-survey-2021 . 코드 출처 : https://www.kaggle.com/vivek468/what-s-up-kaggle-kaggle-survey-2021 .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/visualization/survey/2021/12/19/kagglessu8.html",
            "relUrl": "/ssuda/jupyter/kaggle/visualization/survey/2021/12/19/kagglessu8.html",
            "date": " • Dec 19, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "[SSUDA] 캐글 제품 분류",
            "content": ". from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, 100) pd.set_option(&#39;display.max_rows&#39;, 100) from sklearn.preprocessing import LabelEncoder, OneHotEncoder from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV from sklearn.feature_selection import SelectFromModel from sklearn.metrics import accuracy_score, confusion_matrix, classification_report import xgboost as xg from collections import Counter !pip install kneed # kneed is not installed in kaggle. uncomment the above line. from kneed import KneeLocator import warnings warnings.filterwarnings(&quot;ignore&quot;) . Collecting kneed Downloading kneed-0.7.0-py2.py3-none-any.whl (9.4 kB) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from kneed) (1.4.1) Requirement already satisfied: numpy&gt;=1.14.2 in /usr/local/lib/python3.7/dist-packages (from kneed) (1.19.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from kneed) (3.2.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (3.0.6) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;kneed) (0.11.0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;kneed) (1.15.0) Installing collected packages: kneed Successfully installed kneed-0.7.0 . path = &#39;/content/drive/MyDrive/otto_group/&#39; train = pd.read_csv(path + &#39;train.csv&#39;) test = pd.read_csv(path + &#39;test.csv&#39;) sample_submission = pd.read_csv(path + &#39;sampleSubmission.csv&#39;) train.head() . id feat_1 feat_2 feat_3 feat_4 feat_5 feat_6 feat_7 feat_8 feat_9 feat_10 feat_11 feat_12 feat_13 feat_14 feat_15 feat_16 feat_17 feat_18 feat_19 feat_20 feat_21 feat_22 feat_23 feat_24 feat_25 feat_26 feat_27 feat_28 feat_29 feat_30 feat_31 feat_32 feat_33 feat_34 feat_35 feat_36 feat_37 feat_38 feat_39 feat_40 feat_41 feat_42 feat_43 feat_44 feat_45 feat_46 feat_47 feat_48 feat_49 feat_50 feat_51 feat_52 feat_53 feat_54 feat_55 feat_56 feat_57 feat_58 feat_59 feat_60 feat_61 feat_62 feat_63 feat_64 feat_65 feat_66 feat_67 feat_68 feat_69 feat_70 feat_71 feat_72 feat_73 feat_74 feat_75 feat_76 feat_77 feat_78 feat_79 feat_80 feat_81 feat_82 feat_83 feat_84 feat_85 feat_86 feat_87 feat_88 feat_89 feat_90 feat_91 feat_92 feat_93 target . 0 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 4 | 1 | 1 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | 0 | 11 | 0 | 1 | 1 | 0 | 1 | 0 | 7 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 1 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 2 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 3 4 | 1 | 0 | 0 | 1 | 6 | 1 | 5 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 2 | 2 | 0 | 0 | 0 | 58 | 0 | 10 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 2 | 0 | 1 | 2 | 1 | 3 | 0 | 0 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 5 | 0 | 0 | 4 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 2 | 0 | 22 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 4 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | Class_1 | . &#45936;&#51060;&#53552; &#53456;&#49353; . train.columns . Index([&#39;id&#39;, &#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, &#39;feat_6&#39;, &#39;feat_7&#39;, &#39;feat_8&#39;, &#39;feat_9&#39;, &#39;feat_10&#39;, &#39;feat_11&#39;, &#39;feat_12&#39;, &#39;feat_13&#39;, &#39;feat_14&#39;, &#39;feat_15&#39;, &#39;feat_16&#39;, &#39;feat_17&#39;, &#39;feat_18&#39;, &#39;feat_19&#39;, &#39;feat_20&#39;, &#39;feat_21&#39;, &#39;feat_22&#39;, &#39;feat_23&#39;, &#39;feat_24&#39;, &#39;feat_25&#39;, &#39;feat_26&#39;, &#39;feat_27&#39;, &#39;feat_28&#39;, &#39;feat_29&#39;, &#39;feat_30&#39;, &#39;feat_31&#39;, &#39;feat_32&#39;, &#39;feat_33&#39;, &#39;feat_34&#39;, &#39;feat_35&#39;, &#39;feat_36&#39;, &#39;feat_37&#39;, &#39;feat_38&#39;, &#39;feat_39&#39;, &#39;feat_40&#39;, &#39;feat_41&#39;, &#39;feat_42&#39;, &#39;feat_43&#39;, &#39;feat_44&#39;, &#39;feat_45&#39;, &#39;feat_46&#39;, &#39;feat_47&#39;, &#39;feat_48&#39;, &#39;feat_49&#39;, &#39;feat_50&#39;, &#39;feat_51&#39;, &#39;feat_52&#39;, &#39;feat_53&#39;, &#39;feat_54&#39;, &#39;feat_55&#39;, &#39;feat_56&#39;, &#39;feat_57&#39;, &#39;feat_58&#39;, &#39;feat_59&#39;, &#39;feat_60&#39;, &#39;feat_61&#39;, &#39;feat_62&#39;, &#39;feat_63&#39;, &#39;feat_64&#39;, &#39;feat_65&#39;, &#39;feat_66&#39;, &#39;feat_67&#39;, &#39;feat_68&#39;, &#39;feat_69&#39;, &#39;feat_70&#39;, &#39;feat_71&#39;, &#39;feat_72&#39;, &#39;feat_73&#39;, &#39;feat_74&#39;, &#39;feat_75&#39;, &#39;feat_76&#39;, &#39;feat_77&#39;, &#39;feat_78&#39;, &#39;feat_79&#39;, &#39;feat_80&#39;, &#39;feat_81&#39;, &#39;feat_82&#39;, &#39;feat_83&#39;, &#39;feat_84&#39;, &#39;feat_85&#39;, &#39;feat_86&#39;, &#39;feat_87&#39;, &#39;feat_88&#39;, &#39;feat_89&#39;, &#39;feat_90&#39;, &#39;feat_91&#39;, &#39;feat_92&#39;, &#39;feat_93&#39;, &#39;target&#39;], dtype=&#39;object&#39;) . 컬럼수는 93개 입니다. . train[&#39;target&#39;].unique() . array([&#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;, &#39;Class_6&#39;, &#39;Class_7&#39;, &#39;Class_8&#39;, &#39;Class_9&#39;], dtype=object) . Y 변수의 클레스 종류가 9개 입니다. . sample_submission.head() . id Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 0 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 5 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 저번이랑 비슷하게 제출 파일 형식은 각 클레스 별로 분류 될 확률을 기제하면 되겠네요. . sum((train.isnull()).sum()) . 0 . 결측값이 있는지 확인했습니다. info 함수로 확인하기에는 피처가 너무 커서 직관적으로 확인하기 힘듭니다. . from sklearn.preprocessing import LabelEncoder le=LabelEncoder() train[&#39;target&#39;]=le.fit_transform(train[&#39;target&#39;]) plt.figure(figsize=(12,5)) sns.countplot(train[&#39;target&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0cca07b90&gt; . 라벨 인코더를 통해 클레스 이름을 간단하게( Class_1 =&gt; 0) 바꿨습니다. . 클레스 개수가 각각 몇개있는지 파악했는데요. 균등하진 않아보입니다. . &#47784;&#45944; 1 . from sklearn.model_selection import train_test_split list_models=[] list_scores=[] y = train[&#39;target&#39;] x = train.drop([&#39;target&#39;, &#39;id&#39;],axis=1) x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2) . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score lr=LogisticRegression(max_iter=100000) lr.fit(x_train,y_train) pred_1=lr.predict(x_test) score_1=accuracy_score(y_test,pred_1) list_models.append(&#39;logistic regression&#39;) list_scores.append(score_1) . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_1,ax=axes[0]) sns.countplot(y_test,ax=axes[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0cbd85e10&gt; . 간단한 로지스틱 회귀로, 왼쪽이 예측값, 오른쪽이 실제값입니다. 실제 비율이 제일 높은 1번은 더 많이 예측하는 모습을 보입니다. . 비율이 높은 편인 5번, 7번, 8번은 실제 값과 예측 값이 비슷합니다. . 하지만 나머지 값들은 실제 값에 있는 비율 만큼 예측 값에서 비슷한 개수로 추정해주지 못했습니다. . 물론 이 현상만으로 비율을 일관적으로 예측할 수는 없습니다.(8번은 예측/실제 값 개수 비슷, 2번은 실제 값에 비해 예측값이 너무 적음) . 다만 불균형한 테스터 셋을 분류하는 문제에서 다음과 같은 문제가 있다는걸 인지해야겠습니다. . 개수가 많은 클레스를 예측하는 확률은 높아지고, 개수가 적은 클레스를 예측하는 확률은 낮아진다는 점 입니다. . from sklearn.ensemble import RandomForestClassifier rfc=RandomForestClassifier() rfc.fit(x_train,y_train) pred_2=rfc.predict(x_test) score_2=accuracy_score(y_test,pred_2) list_scores.append(score_2) list_models.append(&#39;random forest classifier&#39;) . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_2,ax=axes[0]) sns.countplot(y_test,ax=axes[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc0c14ea1d0&gt; . 렌덤 포레스트 분류기법입니다. 앞서 말한것과 비슷한 일이 벌어집니다. . fig,axes=plt.subplots(1,2) fig.set_size_inches(11.7, 8.27) sns.countplot(pred_1,ax=axes[0]) axes[0].legend(title=&#39;predictions by logistic regression&#39;) sns.countplot(pred_2,ax=axes[1]) axes[1].legend(title=&#39;predictions by random forest&#39;) . No handles with labels found to put in legend. No handles with labels found to put in legend. . &lt;matplotlib.legend.Legend at 0x7fc0c1429ad0&gt; . 두 모델이 비슷한 현상을 보인다는 걸 다시한번 보여준 것 같습니다. . from sklearn.svm import SVC svm=SVC() svm.fit(x_train,y_train) pred_3=svm.predict(x_test) score_3=accuracy_score(y_test,pred_3) list_scores.append(score_3) list_models.append(&#39;support vector machines&#39;) . from xgboost import XGBClassifier xgb=XGBClassifier() xgb.fit(x_train,y_train) pred_4=xgb.predict(x_test) score_4=accuracy_score(y_test,pred_4) list_models.append(&#39;xgboost classifier&#39;) list_scores.append(score_4) . plt.figure(figsize=(12,5)) plt.bar(list_models,list_scores,width=0.3) plt.xlabel(&#39;classifictions models&#39;) plt.ylabel(&#39;accuracy scores&#39;) plt.show() . SVM, XGB 모델도 적용시켜보았습니다. . 랜덤 포레스트 분류 모델이 성능이 가장 괜찮아 보입니다. . &#47784;&#45944; 2 . !pip install &quot;autogluon.tabular[all]==0.1.1b20210312&quot; . Collecting autogluon.tabular[all]==0.1.1b20210312 Downloading autogluon.tabular-0.1.1b20210312-py3-none-any.whl (234 kB) |████████████████████████████████| 234 kB 4.2 MB/s Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.19.5) Collecting scikit-learn&lt;0.25,&gt;=0.22.0 Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB) |████████████████████████████████| 22.3 MB 1.6 MB/s Collecting autogluon.features==0.1.1b20210312 Downloading autogluon.features-0.1.1b20210312-py3-none-any.whl (48 kB) |████████████████████████████████| 48 kB 4.4 MB/s Requirement already satisfied: pandas&lt;2.0,&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.1.5) Collecting scipy==1.5.4 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) |████████████████████████████████| 25.9 MB 1.8 MB/s Requirement already satisfied: networkx&lt;3.0,&gt;=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (2.6.3) Collecting autogluon.core==0.1.1b20210312 Downloading autogluon.core-0.1.1b20210312-py3-none-any.whl (312 kB) |████████████████████████████████| 312 kB 50.1 MB/s Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (3.6.4) Requirement already satisfied: psutil&lt;=5.7.0,&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (5.4.8) Requirement already satisfied: torch&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.10.0+cu111) Requirement already satisfied: fastai&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.1.1b20210312) (1.0.61) Collecting lightgbm&lt;4.0,&gt;=3.0 Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB) |████████████████████████████████| 2.0 MB 49.3 MB/s Collecting catboost&lt;0.25,&gt;=0.23.0 Downloading catboost-0.24.4-cp37-none-manylinux1_x86_64.whl (65.7 MB) |████████████████████████████████| 65.7 MB 46 kB/s Collecting xgboost&lt;1.4,&gt;=1.3.2 Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB) |████████████████████████████████| 157.5 MB 63 kB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.23.0) Collecting dill==0.3.3 Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB) |████████████████████████████████| 81 kB 9.6 MB/s Requirement already satisfied: autograd&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3) Collecting paramiko&gt;=2.4 Downloading paramiko-2.8.0-py2.py3-none-any.whl (206 kB) |████████████████████████████████| 206 kB 50.7 MB/s Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.29.24) Requirement already satisfied: tqdm&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.62.3) Requirement already satisfied: tornado&gt;=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (5.1.1) Collecting boto3 Downloading boto3-1.20.14-py3-none-any.whl (131 kB) |████████████████████████████████| 131 kB 49.8 MB/s Collecting ConfigSpace==0.4.18 Downloading ConfigSpace-0.4.18.tar.gz (950 kB) |████████████████████████████████| 950 kB 49.4 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.2.2) Collecting graphviz&lt;0.9.0,&gt;=0.8.1 Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB) Collecting distributed&gt;=2.6.0 Downloading distributed-2021.11.2-py3-none-any.whl (802 kB) |████████████████████████████████| 802 kB 50.6 MB/s Requirement already satisfied: dask&gt;=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.12.0) Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace==0.4.18-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.6) Requirement already satisfied: future&gt;=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd&gt;=1.3-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.16.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.15.0) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.4.1) Collecting dask&gt;=2.6.0 Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB) |████████████████████████████████| 1.0 MB 40.4 MB/s Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.4.0) Requirement already satisfied: toolz&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.2) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.11.3) Requirement already satisfied: click&gt;=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.1.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (57.4.0) Requirement already satisfied: msgpack&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.2) Requirement already satisfied: tblib&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.7.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.13) Requirement already satisfied: zict&gt;=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.0) Collecting cloudpickle&gt;=1.5.0 Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from dask&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (21.3) Collecting partd&gt;=0.3.10 Downloading partd-1.2.0-py3-none-any.whl (19 kB) Collecting fsspec&gt;=0.6.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 54.6 MB/s Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.1+cu111) Requirement already satisfied: spacy&gt;=2.0.18 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.2.4) Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.352.0) Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.7.3) Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.6.3) Requirement already satisfied: fastprogress&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.0) Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.1.2) Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.2) Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&lt;4.0,&gt;=3.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.37.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&lt;2.0,&gt;=1.0.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&lt;2.0,&gt;=1.0.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.8.2) Collecting cryptography&gt;=2.5 Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB) |████████████████████████████████| 3.6 MB 43.1 MB/s Collecting pynacl&gt;=1.0.1 Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB) |████████████████████████████████| 961 kB 45.2 MB/s Collecting bcrypt&gt;=3.1.3 Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB) |████████████████████████████████| 63 kB 2.3 MB/s Requirement already satisfied: cffi&gt;=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.15.0) Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.1-&gt;bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.21) Collecting locket Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&lt;0.25,&gt;=0.22.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&lt;0.25,&gt;=0.22.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.1.0) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (7.4.0) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.5) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.8.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.6) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.6) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.1.3) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.4.1) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (4.8.2) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.10.0.2) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai&lt;2.0,&gt;=1.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.6.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (3.0.4) Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict&gt;=0.1.3-&gt;distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.0.1) Collecting jmespath&lt;1.0.0,&gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting botocore&lt;1.24.0,&gt;=1.23.14 Downloading botocore-1.23.14-py3-none-any.whl (8.2 MB) |████████████████████████████████| 8.2 MB 37.3 MB/s Collecting s3transfer&lt;0.6.0,&gt;=0.5.0 Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 7.5 MB/s Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 49.6 MB/s Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;distributed&gt;=2.6.0-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (2.0.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;autogluon.core==0.1.1b20210312-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.11.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost&lt;0.25,&gt;=0.23.0-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.3.3) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (21.2.0) Requirement already satisfied: more-itertools&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (8.11.0) Requirement already satisfied: atomicwrites&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.4.0) Requirement already satisfied: pluggy&lt;0.8,&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (0.7.1) Requirement already satisfied: py&gt;=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest-&gt;autogluon.tabular[all]==0.1.1b20210312) (1.11.0) Building wheels for collected packages: ConfigSpace Building wheel for ConfigSpace (PEP 517) ... done Created wheel for ConfigSpace: filename=ConfigSpace-0.4.18-cp37-cp37m-linux_x86_64.whl size=2880650 sha256=7b9c24d3da86378fe64cff390f09a143606ba3ac7a45f5b37fa2827e1aed4124 Stored in directory: /root/.cache/pip/wheels/36/f7/0f/36f368c419ea1a8024fc3d6c078c3111dfef43fa1d14cfebe0 Successfully built ConfigSpace Installing collected packages: urllib3, locket, jmespath, partd, fsspec, cloudpickle, botocore, scipy, s3transfer, pynacl, dask, cryptography, bcrypt, scikit-learn, paramiko, graphviz, distributed, dill, ConfigSpace, boto3, autogluon.core, autogluon.features, xgboost, lightgbm, catboost, autogluon.tabular Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 Attempting uninstall: cloudpickle Found existing installation: cloudpickle 1.3.0 Uninstalling cloudpickle-1.3.0: Successfully uninstalled cloudpickle-1.3.0 Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: dask Found existing installation: dask 2.12.0 Uninstalling dask-2.12.0: Successfully uninstalled dask-2.12.0 Attempting uninstall: scikit-learn Found existing installation: scikit-learn 1.0.1 Uninstalling scikit-learn-1.0.1: Successfully uninstalled scikit-learn-1.0.1 Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 Attempting uninstall: distributed Found existing installation: distributed 1.25.3 Uninstalling distributed-1.25.3: Successfully uninstalled distributed-1.25.3 Attempting uninstall: dill Found existing installation: dill 0.3.4 Uninstalling dill-0.3.4: Successfully uninstalled dill-0.3.4 Attempting uninstall: xgboost Found existing installation: xgboost 0.90 Uninstalling xgboost-0.90: Successfully uninstalled xgboost-0.90 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. multiprocess 0.70.12.2 requires dill&gt;=0.3.4, but you have dill 0.3.3 which is incompatible. gym 0.17.3 requires cloudpickle&lt;1.7.0,&gt;=1.2.0, but you have cloudpickle 2.0.0 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible. Successfully installed ConfigSpace-0.4.18 autogluon.core-0.1.1b20210312 autogluon.features-0.1.1b20210312 autogluon.tabular-0.1.1b20210312 bcrypt-3.2.0 boto3-1.20.14 botocore-1.23.14 catboost-0.24.4 cloudpickle-2.0.0 cryptography-36.0.0 dask-2021.11.2 dill-0.3.3 distributed-2021.11.2 fsspec-2021.11.1 graphviz-0.8.4 jmespath-0.10.0 lightgbm-3.3.1 locket-0.2.1 paramiko-2.8.0 partd-1.2.0 pynacl-1.4.0 s3transfer-0.5.0 scikit-learn-0.24.2 scipy-1.5.4 urllib3-1.25.11 xgboost-1.3.3 . from autogluon.tabular import TabularDataset, TabularPredictor from autogluon.tabular.models.knn.knn_rapids_model import KNNRapidsModel from autogluon.tabular.models.lr.lr_rapids_model import LinearRapidsModel path = &#39;/content/drive/MyDrive/otto_group/&#39; train = TabularDataset(path + &#39;train.csv&#39;) test = TabularDataset(path + &#39;test.csv&#39;) label = &#39;target&#39; . Loaded data from: /content/drive/MyDrive/otto_group/train.csv | Columns = 95 / 95 | Rows = 61878 -&gt; 61878 Loaded data from: /content/drive/MyDrive/otto_group/test.csv | Columns = 94 / 94 | Rows = 144368 -&gt; 144368 . !pip install cuml . Collecting cuml Downloading cuml-0.6.1.post1.tar.gz (1.1 kB) Building wheels for collected packages: cuml Building wheel for cuml (setup.py) ... error ERROR: Failed building wheel for cuml Running setup.py clean for cuml Failed to build cuml Installing collected packages: cuml Running setup.py install for cuml ... error ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c &#39;import io, os, sys, setuptools, tokenize; sys.argv[0] = &#39;&#34;&#39;&#34;&#39;/tmp/pip-install-d9q8bg1e/cuml_e5625faa4a144d1cb1dbda39971d1a35/setup.py&#39;&#34;&#39;&#34;&#39;; __file__=&#39;&#34;&#39;&#34;&#39;/tmp/pip-install-d9q8bg1e/cuml_e5625faa4a144d1cb1dbda39971d1a35/setup.py&#39;&#34;&#39;&#34;&#39;;f = getattr(tokenize, &#39;&#34;&#39;&#34;&#39;open&#39;&#34;&#39;&#34;&#39;, open)(__file__) if os.path.exists(__file__) else io.StringIO(&#39;&#34;&#39;&#34;&#39;from setuptools import setup; setup()&#39;&#34;&#39;&#34;&#39;);code = f.read().replace(&#39;&#34;&#39;&#34;&#39; r n&#39;&#34;&#39;&#34;&#39;, &#39;&#34;&#39;&#34;&#39; n&#39;&#34;&#39;&#34;&#39;);f.close();exec(compile(code, __file__, &#39;&#34;&#39;&#34;&#39;exec&#39;&#34;&#39;&#34;&#39;))&#39; install --record /tmp/pip-record-yfs4_fyl/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/cuml Check the logs for full command output. . predictor = TabularPredictor( label=label, eval_metric=&#39;log_loss&#39;, learner_kwargs={&#39;ignored_columns&#39;: [&#39;id&#39;]} ).fit( train, presets=&#39;best_quality&#39;, hyperparameters={ KNNRapidsModel: {}, LinearRapidsModel: {}, &#39;RF&#39;: {}, &#39;XGB&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;CAT&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;GBM&#39;: [{}, {&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, &#39;GBMLarge&#39;], &#39;NN&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &#39;FASTAI&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, }, ) . No path specified. Models will be saved in: &#34;AutogluonModels/ag-20211127_122603/&#34; Presets specified: [&#39;best_quality&#39;] Beginning AutoGluon training ... AutoGluon will save models to &#34;AutogluonModels/ag-20211127_122603/&#34; AutoGluon Version: 0.1.1b20210312 Train Data Rows: 61878 Train Data Columns: 94 Preprocessing data ... AutoGluon infers your prediction problem is: &#39;multiclass&#39; (because dtype of label-column == object). 9 unique label values: [&#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;, &#39;Class_6&#39;, &#39;Class_7&#39;, &#39;Class_8&#39;, &#39;Class_9&#39;] If &#39;multiclass&#39; is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: [&#39;binary&#39;, &#39;multiclass&#39;, &#39;regression&#39;]) Train Data Class Count: 9 Using Feature Generators to preprocess the data ... Dropping user-specified ignored columns: [&#39;id&#39;] Fitting AutoMLPipelineFeatureGenerator... Available Memory: 12407.99 MB Train Data (Original) Memory Usage: 46.04 MB (0.4% of available memory) Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features. Stage 1 Generators: Fitting AsTypeFeatureGenerator... Stage 2 Generators: Fitting FillNaFeatureGenerator... Stage 3 Generators: Fitting IdentityFeatureGenerator... Stage 4 Generators: Fitting DropUniqueFeatureGenerator... Types of features in original data (raw dtype, special dtypes): (&#39;int&#39;, []) : 93 | [&#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, ...] Types of features in processed data (raw dtype, special dtypes): (&#39;int&#39;, []) : 93 | [&#39;feat_1&#39;, &#39;feat_2&#39;, &#39;feat_3&#39;, &#39;feat_4&#39;, &#39;feat_5&#39;, ...] 0.5s = Fit runtime 93 features in original data used to generate 93 features in processed data. Train Data (Processed) Memory Usage: 46.04 MB (0.4% of available memory) Data preprocessing and feature engineering runtime = 0.7s ... AutoGluon will gauge predictive performance using evaluation metric: &#39;log_loss&#39; This metric expects predicted probabilities rather than predicted class labels, so you&#39;ll need to use predict_proba() instead of predict() To change this, specify the eval_metric argument of fit() Custom Model Type Detected: &lt;class &#39;autogluon.tabular.models.knn.knn_rapids_model.KNNRapidsModel&#39;&gt; Custom Model Type Detected: &lt;class &#39;autogluon.tabular.models.lr.lr_rapids_model.LinearRapidsModel&#39;&gt; . ModuleNotFoundError Traceback (most recent call last) /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/try_import.py in try_import_rapids_cuml() 162 try: --&gt; 163 import cuml 164 except ImportError: ModuleNotFoundError: No module named &#39;cuml&#39; During handling of the above exception, another exception occurred: ImportError Traceback (most recent call last) &lt;ipython-input-12-488de9012a7d&gt; in &lt;module&gt;() 14 &#39;GBM&#39;: [{}, {&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, &#39;GBMLarge&#39;], 15 &#39;NN&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, &gt; 16 &#39;FASTAI&#39;: {&#39;ag_args_fit&#39;: {&#39;num_gpus&#39;: 1}}, 17 }, 18 ) /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/decorators.py in _call(*args, **kwargs) 27 def _call(*args, **kwargs): 28 gargs, gkwargs = g(*other_args, *args, **kwargs) &gt; 29 return f(*gargs, **gkwargs) 30 return _call 31 return _unpack_inner /usr/local/lib/python3.7/dist-packages/autogluon/tabular/predictor/predictor.py in fit(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, **kwargs) 689 self._learner.fit(X=train_data, X_val=tuning_data, X_unlabeled=unlabeled_data, 690 holdout_frac=holdout_frac, num_bag_folds=num_bag_folds, num_bag_sets=num_bag_sets, num_stack_levels=num_stack_levels, --&gt; 691 hyperparameters=hyperparameters, core_kwargs=core_kwargs, time_limit=time_limit, verbosity=verbosity) 692 self._set_post_fit_vars() 693 /usr/local/lib/python3.7/dist-packages/autogluon/tabular/learner/abstract_learner.py in fit(self, X, X_val, **kwargs) 124 raise AssertionError(&#39;Learner is already fit.&#39;) 125 self._validate_fit_input(X=X, X_val=X_val, **kwargs) --&gt; 126 return self._fit(X=X, X_val=X_val, **kwargs) 127 128 def _fit(self, X: DataFrame, X_val: DataFrame = None, scheduler_options=None, hyperparameter_tune=False, /usr/local/lib/python3.7/dist-packages/autogluon/tabular/learner/default_learner.py in _fit(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, verbosity, **trainer_fit_kwargs) 93 94 self.save() &gt; 95 trainer.fit(X, y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, holdout_frac=holdout_frac, time_limit=time_limit_trainer, **trainer_fit_kwargs) 96 self.save_trainer(trainer=trainer) 97 time_end = time.time() /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/auto_trainer.py in fit(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, feature_prune, holdout_frac, num_stack_levels, core_kwargs, time_limit, **kwargs) 50 self._train_multi_and_ensemble(X, y, X_val, y_val, X_unlabeled=X_unlabeled, hyperparameters=hyperparameters, 51 feature_prune=feature_prune, &gt; 52 num_stack_levels=num_stack_levels, time_limit=time_limit, core_kwargs=core_kwargs) 53 54 def get_models_distillation(self, hyperparameters, **kwargs): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in _train_multi_and_ensemble(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, **kwargs) 1290 self._num_cols_train = len(list(X.columns)) 1291 model_names_fit = self.train_multi_levels(X, y, hyperparameters=hyperparameters, X_val=X_val, y_val=y_val, -&gt; 1292 X_unlabeled=X_unlabeled, level_start=1, level_end=num_stack_levels+1, time_limit=time_limit, **kwargs) 1293 if len(self.get_model_names()) == 0: 1294 raise ValueError(&#39;AutoGluon did not successfully train any models&#39;) /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in train_multi_levels(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, feature_prune, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack) 259 models=hyperparameters, level=level, base_model_names=base_model_names, 260 feature_prune=feature_prune, --&gt; 261 core_kwargs=core_kwargs_level, aux_kwargs=aux_kwargs_level, name_suffix=name_suffix, 262 ) 263 model_names_fit += base_model_names + aux_models /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in stack_new_level(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, feature_prune, core_kwargs, aux_kwargs, name_suffix) 285 aux_kwargs[&#39;name_suffix&#39;] = aux_kwargs.get(&#39;name_suffix&#39;, &#39;&#39;) + name_suffix 286 core_models = self.stack_new_level_core(X=X, y=y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, models=models, --&gt; 287 level=level, base_model_names=base_model_names, feature_prune=feature_prune, **core_kwargs) 288 289 if self.bagged_mode: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/abstract_trainer.py in stack_new_level_core(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, **kwargs) 342 )) 343 --&gt; 344 models, model_args_fit = get_models_func(hyperparameters=models, **get_models_kwargs) 345 if model_args_fit: 346 hyperparameter_tune_kwargs = { /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/auto_trainer.py in get_models(self, hyperparameters, **kwargs) 26 return get_preset_models(path=path, problem_type=problem_type, eval_metric=eval_metric, 27 num_classes=num_classes, hyperparameters=hyperparameters, invalid_model_names=invalid_model_names, &gt; 28 feature_metadata=feature_metadata, silent=silent, **kwargs) 29 30 def fit(self, X, y, hyperparameters, X_val=None, y_val=None, X_unlabeled=None, feature_prune=False, holdout_frac=0.1, num_stack_levels=0, core_kwargs: dict = None, time_limit=None, **kwargs): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/model_presets/presets.py in get_preset_models(path, problem_type, eval_metric, hyperparameters, feature_metadata, num_classes, level, ensemble_type, ensemble_kwargs, ag_args_fit, ag_args, ag_args_ensemble, name_suffix, default_priorities, invalid_model_names, excluded_model_types, hyperparameter_preprocess_func, hyperparameter_preprocess_kwargs, silent) 189 model = model_factory(model_cfg, path=path, problem_type=problem_type, eval_metric=eval_metric, 190 num_classes=num_classes, name_suffix=name_suffix, ensemble_type=ensemble_type, ensemble_kwargs=ensemble_kwargs, --&gt; 191 invalid_name_set=invalid_name_set, level=level, feature_metadata=feature_metadata) 192 invalid_name_set.add(model.name) 193 if &#39;hyperparameter_tune_kwargs&#39; in model_cfg[AG_ARGS]: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/trainer/model_presets/presets.py in model_factory(model, path, problem_type, eval_metric, num_classes, name_suffix, ensemble_type, ensemble_kwargs, invalid_name_set, level, feature_metadata) 296 model_params.pop(AG_ARGS, None) 297 model_params.pop(AG_ARGS_ENSEMBLE, None) --&gt; 298 model_init = model_type(path=path, name=name, problem_type=problem_type, eval_metric=eval_metric, num_classes=num_classes, hyperparameters=model_params, feature_metadata=feature_metadata) 299 300 if ensemble_kwargs is not None: /usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/knn/knn_model.py in __init__(self, **kwargs) 25 def __init__(self, **kwargs): 26 super().__init__(**kwargs) &gt; 27 self._model_type = self._get_model_type() 28 29 def _get_model_type(self): /usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/knn/knn_rapids_model.py in _get_model_type(self) 26 &#34;&#34;&#34; 27 def _get_model_type(self): &gt; 28 try_import_rapids_cuml() 29 from cuml.neighbors import KNeighborsClassifier, KNeighborsRegressor 30 if self.problem_type == REGRESSION: /usr/local/lib/python3.7/dist-packages/autogluon/core/utils/try_import.py in try_import_rapids_cuml() 163 import cuml 164 except ImportError: --&gt; 165 raise ImportError(&#34;`import cuml` failed. n&#34; 166 &#34;Ensure that you have a GPU and CUDA installation, and then install RAPIDS. n&#34; 167 &#34;You will likely need to create a fresh conda environment based off of a RAPIDS install, and then install AutoGluon on it. n&#34; ImportError: `import cuml` failed. Ensure that you have a GPU and CUDA installation, and then install RAPIDS. You will likely need to create a fresh conda environment based off of a RAPIDS install, and then install AutoGluon on it. RAPIDS is highly experimental within AutoGluon, and we recommend to only use RAPIDS if you are an advanced user / developer. Please refer to RAPIDS install instructions for more information: https://rapids.ai/start.html#get-rapids NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &#34;Open Examples&#34; button below. . submission = test[[&#39;id&#39;]] test_pred_proba = predictor.predict_proba(test) submission = pd.concat([submission, test_pred_proba], axis=1) submission.to_csv(&#39;submission.csv&#39;, index=False) submission.head() . 오류가 지속적으로 나서 실행을 못했습니다. (autogluon 모델) . 자동으로 분석해주는 모델인것 같고 실제로 이 모델 점수 상위 1%를 기록했다고 합니다. . &#47784;&#45944; 3 . from patsy import dmatrices from sklearn.neural_network import MLPClassifier columns = train.columns[1:-1] X = train[columns] y = np.ravel(train[&#39;target&#39;]) model = MLPClassifier(solver=&#39;lbfgs&#39;, alpha=1e-5, hidden_layer_sizes = (30, 10), random_state = 0, verbose = True) model.fit(X, y) . MLPClassifier(alpha=1e-05, hidden_layer_sizes=(30, 10), random_state=0, solver=&#39;lbfgs&#39;, verbose=True) . pred = model.predict(X) print(model.score(X, y)) print(sum(pred == y) / len(y)) . 0.8057629529073338 0.8057629529073338 . Xtest = test[test.columns[1:]] test_prob = model.predict_proba(Xtest) solution = pd.DataFrame(test_prob, columns=[&#39;Class_1&#39;,&#39;Class_2&#39;,&#39;Class_3&#39;,&#39;Class_4&#39;,&#39;Class_5&#39;,&#39;Class_6&#39;,&#39;Class_7&#39;,&#39;Class_8&#39;,&#39;Class_9&#39;]) solution[&#39;id&#39;] = test[&#39;id&#39;] cols = solution.columns.tolist() cols = cols[-1:] + cols[:-1] solution = solution[cols] solution.to_csv(&#39;otto_prediction.csv&#39;, index = False) .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/labelencoder/randomforest/xgboost/svm/mlp/classifier/2021/11/27/kagglessu7.html",
            "relUrl": "/ssuda/jupyter/kaggle/labelencoder/randomforest/xgboost/svm/mlp/classifier/2021/11/27/kagglessu7.html",
            "date": " • Nov 27, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "[SSUDA] 신용카드 사용자 연체 예측",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, 100) import warnings warnings.filterwarnings(&quot;ignore&quot;) from lightgbm import LGBMClassifier from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import OneHotEncoder import random train = pd.read_csv(&quot;/content/drive/MyDrive/carddata/train.csv&quot;) test = pd.read_csv(&#39;/content/drive/MyDrive/carddata/test.csv&#39;) sample_submission = pd.read_csv(&#39;/content/drive/MyDrive/carddata/sample_submission.csv&#39;) train.head() . index gender car reality child_num income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email occyp_type family_size begin_month credit . 0 0 | F | N | N | 0 | 202500.0 | Commercial associate | Higher education | Married | Municipal apartment | -13899 | -4709 | 1 | 0 | 0 | 0 | NaN | 2.0 | -6.0 | 1.0 | . 1 1 | F | N | Y | 1 | 247500.0 | Commercial associate | Secondary / secondary special | Civil marriage | House / apartment | -11380 | -1540 | 1 | 0 | 0 | 1 | Laborers | 3.0 | -5.0 | 1.0 | . 2 2 | M | Y | Y | 0 | 450000.0 | Working | Higher education | Married | House / apartment | -19087 | -4434 | 1 | 0 | 1 | 0 | Managers | 2.0 | -22.0 | 2.0 | . 3 3 | F | N | Y | 0 | 202500.0 | Commercial associate | Secondary / secondary special | Married | House / apartment | -15088 | -2092 | 1 | 0 | 1 | 0 | Sales staff | 2.0 | -37.0 | 0.0 | . 4 4 | F | Y | Y | 0 | 157500.0 | State servant | Higher education | Married | House / apartment | -15037 | -2105 | 1 | 0 | 0 | 0 | Managers | 2.0 | -26.0 | 2.0 | . &#44592;&#48376; &#48320;&#49688; &#49444;&#47749; . gender : 성별(F/M), car : 차량 소유 유무(Y/N), reality : 부동산 소유 유무(Y/N), child_num : 자녀 수 . income_total : 연간 소득, income_type : 소득 분류(5개로 분리), edu_type : 교육 수준(5개로 분리) . family_type : 결혼 여부(5개로 분리), house_type : 생활 방식(6개로 분리), DAYS_BIRTH : 출생일(수집일부터 음수로 계산) . DAYS_EMPLOYED : 업무 시작일(수집일부터 음수로 계산, 업무 안하는 사람은 365243 값 부여), FLAG_MOBIL : 핸드폰 소유 여부 . work_phone : 업무용 전화 소유 여부, phone : 가정용 전화 소유 여부, email : 이메일 소유 여부 . occyp_type : 직업 유형, family_size: 가족 규모, begin_month : 신용카드 발급 월(수집일로부터 음수 계산) . 반응변수 =&gt; credit : 사용자의 신용카드 대금 연체를 기준으로 한 신용도. 낮을수록 높은 신용임. . train.describe() . index child_num income_total DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email family_size begin_month credit . count 26457.000000 | 26457.000000 | 2.645700e+04 | 26457.000000 | 26457.000000 | 26457.0 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | . mean 13228.000000 | 0.428658 | 1.873065e+05 | -15958.053899 | 59068.750728 | 1.0 | 0.224742 | 0.294251 | 0.091280 | 2.196848 | -26.123294 | 1.519560 | . std 7637.622372 | 0.747326 | 1.018784e+05 | 4201.589022 | 137475.427503 | 0.0 | 0.417420 | 0.455714 | 0.288013 | 0.916717 | 16.559550 | 0.702283 | . min 0.000000 | 0.000000 | 2.700000e+04 | -25152.000000 | -15713.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | -60.000000 | 0.000000 | . 25% 6614.000000 | 0.000000 | 1.215000e+05 | -19431.000000 | -3153.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -39.000000 | 1.000000 | . 50% 13228.000000 | 0.000000 | 1.575000e+05 | -15547.000000 | -1539.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -24.000000 | 2.000000 | . 75% 19842.000000 | 1.000000 | 2.250000e+05 | -12446.000000 | -407.000000 | 1.0 | 0.000000 | 1.000000 | 0.000000 | 3.000000 | -12.000000 | 2.000000 | . max 26456.000000 | 19.000000 | 1.575000e+06 | -7705.000000 | 365243.000000 | 1.0 | 1.000000 | 1.000000 | 1.000000 | 20.000000 | 0.000000 | 2.000000 | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 26457 entries, 0 to 26456 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 index 26457 non-null int64 1 gender 26457 non-null object 2 car 26457 non-null object 3 reality 26457 non-null object 4 child_num 26457 non-null int64 5 income_total 26457 non-null float64 6 income_type 26457 non-null object 7 edu_type 26457 non-null object 8 family_type 26457 non-null object 9 house_type 26457 non-null object 10 DAYS_BIRTH 26457 non-null int64 11 DAYS_EMPLOYED 26457 non-null int64 12 FLAG_MOBIL 26457 non-null int64 13 work_phone 26457 non-null int64 14 phone 26457 non-null int64 15 email 26457 non-null int64 16 occyp_type 18286 non-null object 17 family_size 26457 non-null float64 18 begin_month 26457 non-null float64 19 credit 26457 non-null float64 dtypes: float64(4), int64(8), object(8) memory usage: 4.0+ MB . 유일하게 occyp_type(직업유형) 변수가 null 값이 존재합니다. . NAN으로 채워넣겠습니다. . train.fillna(&#39;NAN&#39;, inplace=True) test.fillna(&#39;NAN&#39;, inplace=True) . plt.subplots(figsize = (8,8)) plt.pie(train[&#39;credit&#39;].value_counts(), labels = train[&#39;credit&#39;].value_counts().index, autopct=&quot;%.2f%%&quot;, shadow = True, startangle = 90) plt.title(&#39;credit ratio&#39;, size=20) plt.show() . matplotlib 패키지 내 pie 차트를 이용해 반응변수의 비율을 확인했습니다. . 신용등급이 떨어지는 2번의 비율이 상당히 크군요. . &#48276;&#51452;&#54805; &#48320;&#49688;&#47484; &#49888;&#50857;&#46321;&#44553;&#48324;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . train_0 = train[train[&#39;credit&#39;]==0.0] train_1 = train[train[&#39;credit&#39;]==1.0] train_2 = train[train[&#39;credit&#39;]==2.0] def cat_plot(column): f, ax = plt.subplots(1, 3, figsize=(16, 6)) sns.countplot(x = column, data = train_0, ax = ax[0], order = train_0[column].value_counts().index) ax[0].tick_params(labelsize=12) ax[0].set_title(&#39;credit = 0&#39;) ax[0].set_ylabel(&#39;count&#39;) ax[0].tick_params(rotation=50) sns.countplot(x = column, data = train_1, ax = ax[1], order = train_1[column].value_counts().index) ax[1].tick_params(labelsize=12) ax[1].set_title(&#39;credit = 1&#39;) ax[1].set_ylabel(&#39;count&#39;) ax[1].tick_params(rotation=50) sns.countplot(x = column, data = train_2, ax = ax[2], order = train_2[column].value_counts().index) ax[2].tick_params(labelsize=12) ax[2].set_title(&#39;credit = 2&#39;) ax[2].set_ylabel(&#39;count&#39;) ax[2].tick_params(rotation=50) plt.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() cat_plot(&quot;gender&quot;) . train 데이터를 신용등급에 따라 분류한 뒤 설명변수와에 관계를 그래프로 보는 함수를 만들었습니다. . 성별에 대해서 살펴봤는데, 절대적으로 여성이 그냥 많은 것 같습니다. . 더불어 성별에 따른 신용등급 차이는 모두 비슷한 비율에 그래프인 것으로 보아 확인하기 힘듭니다. . cat_plot(&#39;car&#39;) . 우선 차량보유를 하지 않은 사람이 모든 비율에서 많습니다. . 다만 신용 등급과에 연관성은 그래프로 봤을땐 크게 없는 것 같네요. . cat_plot(&#39;reality&#39;) . 모든 신용 등급에서 부동산을 소유한 사람들이 많았습니다. . 딱히 신용 등급에 따른 차이가 존재하지 않는 것 같네요. . cat_plot(&#39;income_type&#39;) . 소득 종류 변수도 신용 등급 별로 차이가 두드러지진 않습니다. . 다만 학생은 신용등급 0에 없는 점이 눈에 띄네요. . cat_plot(&#39;edu_type&#39;) . 교육 수준 변수 또한 신용 등급별로 차이가 있어보이진 않네요. . cat_plot(&#39;family_type&#39;) . 가족 구성 변수에 따른 신용등급 변수도 차이가 없는 것 같아요. . 전반적으로 결혼한 사람이 많은 것이 눈에 띄네요. . cat_plot(&#39;house_type&#39;) . house_type 변수 또한 큰 의미가 없는 변수인 것 같습니다. 대부분 House / apartment 타입이기 때문에 의미가 더더욱 없습니다. . cat_plot(&#39;FLAG_MOBIL&#39;) . 여기에 나온 모든 사람은 스마트폰을 보유하고 있습니다. . cat_plot(&#39;work_phone&#39;) . 신용 등급 그룹 별 가정 전화 비율이 차이가 없습니다. 가정용 전화기 보유률이 떨어지는게 눈에 띄네요. . cat_plot(&#39;email&#39;) . 이메일 변수 또한 유의미하지 않아 보입니다. . f, ax = plt.subplots(1, 3, figsize=(16, 6)) sns.countplot(y = &#39;occyp_type&#39;, data = train_0, order = train_0[&#39;occyp_type&#39;].value_counts().index, ax=ax[0]) sns.countplot(y = &#39;occyp_type&#39;, data = train_1, order = train_1[&#39;occyp_type&#39;].value_counts().index, ax=ax[1]) sns.countplot(y = &#39;occyp_type&#39;, data = train_2, order = train_2[&#39;occyp_type&#39;].value_counts().index, ax=ax[2]) plt.subplots_adjust(wspace=0.5, hspace=0.3) plt.show() . 직업 유형 변수를 신용 등급별로 비교했습니다. . 전반적인 경향은 비슷하지만, 세세한 차이가 조금 있어보입니다. . &#50672;&#49549;&#54805; &#48320;&#49688;&#47484; &#49888;&#50857;&#46321;&#44553;&#48324;&#47196; &#51900;&#44060;&#49436; &#44288;&#52272;&#54644;&#48372;&#44592; . def num_plot(column): fig, axes = plt.subplots(1, 3, figsize=(16, 6)) sns.distplot(train_0[column], ax = axes[0]) axes[0].tick_params(labelsize=12) axes[0].set_title(&#39;credit = 0&#39;) axes[0].set_ylabel(&#39;count&#39;) sns.distplot(train_1[column], ax = axes[1]) axes[1].tick_params(labelsize=12) axes[1].set_title(&#39;credit = 1&#39;) axes[1].set_ylabel(&#39;count&#39;) sns.distplot(train_2[column], ax = axes[2]) axes[2].tick_params(labelsize=12) axes[2].set_title(&#39;credit = 2&#39;) axes[2].set_ylabel(&#39;count&#39;) plt.subplots_adjust(wspace=0.3, hspace=0.3) num_plot(&quot;child_num&quot;) . 자녀 수 변수입니다. 신용 등급별로 큰 차이는 없어보입니다. . 다만 신용등급 2에 자녀가 아주 많은 소수의 변수가 존재하는 걸 알 수 있습니다. . num_plot(&quot;family_size&quot;) . 가족 수 변수도 자식 수 변수와 마찬가지 결과를 보이는 것 같아요. . num_plot(&quot;income_total&quot;) . 신용등급에 따른 월간 소득 차이는 크게 없어 보입니다. (??) . sns.distplot(train_0[&#39;income_total&#39;],label=&#39;0.0&#39;, hist=False) sns.distplot(train_1[&#39;income_total&#39;],label=&#39;0.1&#39;, hist=False) sns.distplot(train_2[&#39;income_total&#39;],label=&#39;0.2&#39;, hist=False) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f8be19fa9d0&gt; . 정확히 확인하기 위해 그래프를 겹첬는데요. 조금 차이는 있으나 많이 비슷한 것을 볼 수 있습니다. . num_plot(&quot;DAYS_BIRTH&quot;) . 숫자의 절대값이 작을 수록 젊은 사람 변수 입니다. 그래프가 전반적으로 비슷해 보입니다. . train_0[&#39;Month&#39;] = abs(train_0[&#39;begin_month&#39;]) train_1[&#39;Month&#39;] = abs(train_1[&#39;begin_month&#39;]) train_2[&#39;Month&#39;] = abs(train_2[&#39;begin_month&#39;]) train_0 = train_0.astype({&#39;Month&#39;: &#39;int&#39;}) train_1 = train_1.astype({&#39;Month&#39;: &#39;int&#39;}) train_2 = train_2.astype({&#39;Month&#39;: &#39;int&#39;}) train_0[&#39;Month&#39;].head() num_plot(&quot;Month&quot;) . 카드 생성일 변수를 양수로 바꿔서 분석했습니다. . 전반적으로 흐름은 비슷해보이는데, 카드 발급 초기에서 약 70프로 정도는 신용등급 1을, 약 30프로는 0을 부여하는 것 같습니다. . &#44036;&#45800;&#54620; &#47784;&#45944; &#51201;&#54633; . object_col = [] for col in train.columns: if train[col].dtype == &#39;object&#39;: object_col.append(col) enc = OneHotEncoder() enc.fit(train.loc[:,object_col]) train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), columns=enc.get_feature_names(object_col)) train.drop(object_col, axis=1, inplace=True) train = pd.concat([train, train_onehot_df], axis=1) test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), columns=enc.get_feature_names(object_col)) test.drop(object_col, axis=1, inplace=True) test = pd.concat([test, test_onehot_df], axis=1) . 범주형 변수는 모두 원-핫 인코딩을 해줍니다. . sample_submission . index 0 1 2 . 0 26457 | 0 | 0 | 0 | . 1 26458 | 0 | 0 | 0 | . 2 26459 | 0 | 0 | 0 | . 3 26460 | 0 | 0 | 0 | . 4 26461 | 0 | 0 | 0 | . ... ... | ... | ... | ... | . 9995 36452 | 0 | 0 | 0 | . 9996 36453 | 0 | 0 | 0 | . 9997 36454 | 0 | 0 | 0 | . 9998 36455 | 0 | 0 | 0 | . 9999 36456 | 0 | 0 | 0 | . 10000 rows × 4 columns . 이 대회는 0, 1, 2의 확률이 어떻게 되는지 예측하는 모델입니다. . skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) folds=[] for train_idx, valid_idx in skf.split(train, train[&#39;credit&#39;]): folds.append((train_idx, valid_idx)) random.seed(42) lgb_models={} for fold in range(5): print(f&#39;===================================={fold+1}============================================&#39;) train_idx, valid_idx = folds[fold] X_train, X_valid, y_train, y_valid = train.drop([&#39;credit&#39;],axis=1).iloc[train_idx].values, train.drop([&#39;credit&#39;],axis=1).iloc[valid_idx].values, train[&#39;credit&#39;][train_idx].values, train[&#39;credit&#39;][valid_idx].values lgb = LGBMClassifier(n_estimators=1000) lgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=30, verbose=100) lgb_models[fold]=lgb print(f&#39;================================================================================ n n&#39;) . ====================================1============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676692 valid_1&#39;s multi_logloss: 0.766702 [200] training&#39;s multi_logloss: 0.596634 valid_1&#39;s multi_logloss: 0.755074 [300] training&#39;s multi_logloss: 0.53456 valid_1&#39;s multi_logloss: 0.751863 [400] training&#39;s multi_logloss: 0.482683 valid_1&#39;s multi_logloss: 0.750901 Early stopping, best iteration is: [385] training&#39;s multi_logloss: 0.489523 valid_1&#39;s multi_logloss: 0.750597 ================================================================================ ====================================2============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.673988 valid_1&#39;s multi_logloss: 0.778812 [200] training&#39;s multi_logloss: 0.593911 valid_1&#39;s multi_logloss: 0.766056 [300] training&#39;s multi_logloss: 0.532019 valid_1&#39;s multi_logloss: 0.762532 Early stopping, best iteration is: [358] training&#39;s multi_logloss: 0.500235 valid_1&#39;s multi_logloss: 0.761024 ================================================================================ ====================================3============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676709 valid_1&#39;s multi_logloss: 0.771762 [200] training&#39;s multi_logloss: 0.593522 valid_1&#39;s multi_logloss: 0.758924 Early stopping, best iteration is: [236] training&#39;s multi_logloss: 0.57026 valid_1&#39;s multi_logloss: 0.758105 ================================================================================ ====================================4============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.675515 valid_1&#39;s multi_logloss: 0.7694 [200] training&#39;s multi_logloss: 0.597206 valid_1&#39;s multi_logloss: 0.758117 [300] training&#39;s multi_logloss: 0.533343 valid_1&#39;s multi_logloss: 0.753141 Early stopping, best iteration is: [308] training&#39;s multi_logloss: 0.528916 valid_1&#39;s multi_logloss: 0.752857 ================================================================================ ====================================5============================================ Training until validation scores don&#39;t improve for 30 rounds. [100] training&#39;s multi_logloss: 0.676696 valid_1&#39;s multi_logloss: 0.767947 [200] training&#39;s multi_logloss: 0.595696 valid_1&#39;s multi_logloss: 0.757343 [300] training&#39;s multi_logloss: 0.531936 valid_1&#39;s multi_logloss: 0.753206 Early stopping, best iteration is: [346] training&#39;s multi_logloss: 0.50629 valid_1&#39;s multi_logloss: 0.752064 ================================================================================ . sample_submission.iloc[:,1:]=0 for fold in range(5): sample_submission.iloc[:,1:] += lgb_models[fold].predict_proba(test)/5 sample_submission.to_csv(&#39;ssu6_submission.csv&#39;, index=False) sample_submission.head() . index 0 1 2 . 0 26457 | 0.018329 | 0.187203 | 0.794468 | . 1 26458 | 0.061934 | 0.121026 | 0.817041 | . 2 26459 | 0.027629 | 0.203945 | 0.768427 | . 3 26460 | 0.067723 | 0.199497 | 0.732780 | . 4 26461 | 0.079370 | 0.229451 | 0.691179 | .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/visualization/onehotencoder/lgm/classifier/2021/11/14/kagglessu6.html",
            "relUrl": "/ssuda/jupyter/kaggle/visualization/onehotencoder/lgm/classifier/2021/11/14/kagglessu6.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "[SSUDA] 따릉이 데이터 예측 코드",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) train = pd.read_csv(&quot;/content/drive/MyDrive/bicycle/train.csv&quot;) test = pd.read_csv(&#39;/content/drive/MyDrive/bicycle/test.csv&#39;) sample_submission = pd.read_csv(&#39;/content/drive/MyDrive/bicycle/sample_submission.csv&#39;) train.head() . date_time wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability number_of_rentals . 0 2018-04-01 | 207.500 | 4.000 | 0.000 | 3.050 | 75.000 | 12.600 | 21.000 | 30.000 | 22994 | . 1 2018-04-02 | 208.317 | 2.950 | 0.000 | 3.278 | 69.833 | 12.812 | 19.000 | 19.500 | 28139 | . 2 2018-04-03 | 213.516 | 2.911 | 0.000 | 2.690 | 74.879 | 10.312 | 15.316 | 19.113 | 26817 | . 3 2018-04-04 | 143.836 | 3.692 | 0.425 | 3.138 | 71.849 | 8.312 | 12.368 | 43.493 | 26034 | . 4 2018-04-05 | 95.905 | 4.000 | 0.723 | 3.186 | 73.784 | 5.875 | 10.421 | 63.378 | 2833 | . &#48320;&#49688; &#53456;&#49353; . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 273 entries, 0 to 272 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 date_time 273 non-null object 1 wind_direction 273 non-null float64 2 sky_condition 273 non-null float64 3 precipitation_form 273 non-null float64 4 wind_speed 273 non-null float64 5 humidity 273 non-null float64 6 low_temp 273 non-null float64 7 high_temp 273 non-null float64 8 Precipitation_Probability 273 non-null float64 9 number_of_rentals 273 non-null int64 dtypes: float64(8), int64(1), object(1) memory usage: 21.5+ KB . number_of_rentals : 따릉이 대여량(Y값), date_time : 날짜, wind_direction : 풍향 . sky_condition : 하늘 상태(1 : 맑음, 3 : 구름 많음, 4 : 흐림, 하루에 8번 측정한 값 평균) . precipitation_form : 강수 형태(0 : 맑음, 1 : 비, 마찬가지로 하루에 8번 측정한 값 평균) . wind_speed : 풍속, humidity : 습도, low_temp : 최저기온, high_temp : 최고기온, precipitation_Probability : 강수확률 . 결측값은 없습니다. . import matplotlib.pyplot as plt plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train[&#39;number_of_rentals&#39;].values)) plt.show() . 반응변수의 이상치은 관찰되지 않는 것으로 보입니다. . train[&#39;date_time&#39;] = pd.to_datetime(train[&#39;date_time&#39;]) test[&#39;date_time&#39;] = pd.to_datetime(test[&#39;date_time&#39;]) train[&#39;day&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).day test[&#39;day&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).day train[&#39;month&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).month test[&#39;month&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).month train[&#39;year&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).year test[&#39;year&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).year train[&#39;weekday&#39;]=pd.DatetimeIndex(train[&#39;date_time&#39;]).weekday test[&#39;weekday&#39;]=pd.DatetimeIndex(test[&#39;date_time&#39;]).weekday . date_time이 날짜 변수이기 때문에 데이터 형식을 datetime으로 바꾸어줍니다. . 그 후 datetime 데이터 형식으로 얻을 수 있는 이점, 날/달/연/주말 변수를 추출합니다. . train[&#39;wind_direction&#39;].hist() train[&#39;wind_direction&#39;].max() . 321.622 . wind_direction은 바람 방향 변수입니다. 아마 방향을 360도로 생각해서 만든 변수인 것 같습니다. . 다만 바람 방향과 따릉이 대여량은 상관 없을 것 같습니다. . 물론, 서울 자전거 도로가 한강 기준으로 많이 구성되어 있어 도로가 동-서 기준으로 많이 있긴 합니다. . 하지만 바람 방향이 오늘은 이쪽이니 자전거를 타자라는 생각을 하진 않을 것 같습니다. 바람 세기가 더 중요하죠. . 또 바람 방향 변수는 하루에도 계속 바뀌기 때문에 평균적인 방향인 것 같은데, 만약 바람이 주로 0에서 20, 340에서 360 각도로 불었을때 평균치는 약 180입니다. . (방향이 동쪽에서 위 아래로만 움직인다면 10에서 350으로 쉽게 바뀔 수 있습니다.) . 이 수치가 과연 유의미할지 개인적으로 의문이 들어서, 이 변수는 빼는 것이 좋아보입니다. . train[&#39;precipitation_form&#39;].corr(train[&#39;Precipitation_Probability&#39;]) . 0.9106089542607185 . train[&#39;precipitation_form&#39;].corr(train[&#39;sky_condition&#39;]) . 0.6738137525457335 . 비가 오는 상황을 예측하는 두 변수 precipitation_form와 Precipitation_Probability간 상관관계는 당연히 높습니다. . 다만 Precipitation_Probability는 강우 확률 예측 변수 입니다. . 때문에 일일 강우 단기예측 기록인 precipitation_form 변수가 하루 비가 오는 날을 더 잘 표현할 것으로 생각됩니다. . 비슷한 부분을 설명하는 두 변수이기 때문에 precipitation_form 변수만 사용하겠습니다. . precipitation_form 변수는 하늘 상태를 나타내는 sky_condition 변수와도 상관관계가 높지만 극단적이진 않습니다. . 날씨가 흐린것 자체가 따릉이 대여량에 부정적인 영향을 준다고 생각하기 때문에 sky_condition 변수는 사용하겠습니다. . import matplotlib.pyplot as plt plt.figure(figsize=(20, 10)) plt.bar(train[&#39;date_time&#39;][train[&#39;year&#39;] == 2018], train[&#39;number_of_rentals&#39;][train[&#39;year&#39;] == 2018], width=0.6, color=&#39;grey&#39;) . &lt;BarContainer object of 91 artists&gt; . train[&#39;day&#39;][train[&#39;month&#39;] == 5] += 30 train[&#39;day&#39;][train[&#39;month&#39;] == 6] += 61 test[&#39;day&#39;][test[&#39;month&#39;] == 5] += 30 test[&#39;day&#39;][test[&#39;month&#39;] == 6] += 61 . 따릉이 대여량을 2018년 기준으로 날짜순으로 확인했습니다. . 4~6월 데이터인 만큼, 날이 점점 따뜻해지는 영향으로 변동이 심하긴 하지만 증가하는 추세가 보이는 것 같습니다. . (중간중간 값이 급격히 작아지는 것은 아마 비가 오는날인거 같습니다.) . 그래서 날짜 변수를 쓰는것 보다, 누적된 날짜가 몇일인지를 기록하는 변수를 쓰는게 좋을 것 같습니다. . (4월 15일 =&gt; 15일, 5월 2일 =&gt; 30일 + 2일 = 32일, 6월 10일 =&gt; 30일 + 31일 + 10일 = 71일) . 이렇게 되면 달 변수 또한 쓰지 않는게 좋을 것 같습니다. 만든 변수가 달 변수가 설명할 부분까지 설명하기 때문이죠. . import seaborn as sns def barplots(variable): plot = train.groupby(variable)[&#39;number_of_rentals&#39;].mean() sns.barplot(plot.index,plot.values) barplots(&#39;year&#39;) . 연도별 따릉이 이용자수를 나타내는 그래프 입니다. . 시간이 지날수록 따릉이 이용자수가 늘어나는 것을 확인할 수 있습니다. 그러므로 연도 변수는 매우 중요한 변수임을 알 수 있겠죠. . barplots(&#39;weekday&#39;) . 요일별 따릉이 이용자수를 나타내는 그래프 입니다. weekday 변수는 0은 월요일, 6은 일요일을 나타내는 요일 변수입니다. . 직관적으로 확인했을때 일요일에 따릉이 이용자수가 유의미하게 적은 것이 눈에 띕니다. . train_label = train[&#39;number_of_rentals&#39;] train.drop([&#39;date_time&#39;,&#39;wind_direction&#39;, &#39;Precipitation_Probability&#39;, &#39;month&#39;, &#39;number_of_rentals&#39;], axis = 1, inplace= True) test.drop([&#39;date_time&#39;,&#39;wind_direction&#39;, &#39;Precipitation_Probability&#39;, &#39;month&#39;], axis = 1, inplace= True) . 앞서 설명한 변수들을 제거합니다. . &#47784;&#45944; &#51201;&#54633; . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(random_state = 0, n_estimators = 100) rf.fit(train,train_label) sample_submission[&#39;number_of_rentals&#39;] = rf.predict(test) sample_submission.to_csv(&#39;bicycle_final_4.csv&#39;,encoding=&#39;UTF-8&#39;,index=False) . from xgboost import XGBRegressor xgb = XGBRegressor() xgb.fit(train,train_label) sample_submission[&#39;number_of_rentals&#39;] = xgb.predict(test) sample_submission.to_csv(&#39;bicycle_final_7.csv&#39;,encoding=&#39;UTF-8&#39;,index=False) . [11:30:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . 간단한 랜덤 포레스트 모델을 사용했습니다. 다른 모델을 사용하거나 하이퍼 파라미터를 조정하면 점수가 더 오를수도 있겠죠? .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/dacon/datetime/xgboost/regression/2021/11/04/kagglessu5_plus.html",
            "relUrl": "/ssuda/jupyter/dacon/datetime/xgboost/regression/2021/11/04/kagglessu5_plus.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "[SSUDA] 판매량 예측 데이터 분석",
            "content": ". &#52880;&#44544;&#44284; &#50672;&#46041;&#54616;&#44592; . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;23e68db36970b65937516103c630ba75&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c competitive-data-science-predict-future-sales . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sample_submission.csv.zip to /content 0% 0.00/468k [00:00&lt;?, ?B/s] 100% 468k/468k [00:00&lt;00:00, 69.0MB/s] Downloading sales_train.csv.zip to /content 38% 5.00M/13.3M [00:00&lt;00:01, 5.79MB/s] 100% 13.3M/13.3M [00:00&lt;00:00, 14.4MB/s] Downloading item_categories.csv to /content 0% 0.00/3.49k [00:00&lt;?, ?B/s] 100% 3.49k/3.49k [00:00&lt;00:00, 2.51MB/s] Downloading shops.csv to /content 0% 0.00/2.91k [00:00&lt;?, ?B/s] 100% 2.91k/2.91k [00:00&lt;00:00, 10.6MB/s] Downloading test.csv.zip to /content 0% 0.00/1.02M [00:00&lt;?, ?B/s] 100% 1.02M/1.02M [00:00&lt;00:00, 156MB/s] Downloading items.csv.zip to /content 0% 0.00/368k [00:00&lt;?, ?B/s] 100% 368k/368k [00:00&lt;00:00, 117MB/s] . !unzip items.csv.zip !unzip sales_train.csv.zip !unzip sample_submission.csv.zip !unzip test.csv.zip . Archive: items.csv.zip inflating: items.csv Archive: sales_train.csv.zip inflating: sales_train.csv Archive: sample_submission.csv.zip inflating: sample_submission.csv Archive: test.csv.zip inflating: test.csv . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from matplotlib import pylab as plt import matplotlib.dates as mdates plt.rcParams[&#39;figure.figsize&#39;] = (15.0, 8.0) import seaborn as sns . train = pd.read_csv(&#39;./sales_train.csv&#39;) print (&#39;number of shops: &#39;, train[&#39;shop_id&#39;].max()) print (&#39;number of items: &#39;, train[&#39;item_id&#39;].max()) num_month = train[&#39;date_block_num&#39;].max() print (&#39;number of month: &#39;, num_month) print (&#39;size of train: &#39;, train.shape) train.head() . number of shops: 59 number of items: 22169 number of month: 33 size of train: (2935849, 6) . date date_block_num shop_id item_id item_price item_cnt_day . 0 02.01.2013 | 0 | 59 | 22154 | 999.00 | 1.0 | . 1 03.01.2013 | 0 | 25 | 2552 | 899.00 | 1.0 | . 2 05.01.2013 | 0 | 25 | 2552 | 899.00 | -1.0 | . 3 06.01.2013 | 0 | 25 | 2554 | 1709.05 | 1.0 | . 4 15.01.2013 | 0 | 25 | 2555 | 1099.00 | 1.0 | . 변수 설명 . date : 날짜 변수, date_block_num : 달 변수(2013년 1월 =&gt; 0, 2015년 10월 =&gt; 33) . shop_id, item_id : 상점/제품의 고유번호 변수 . item_price : 제품의 가격 변수, item_cnt_dat : 그 날 제품이 팔린 개수 . (여기서 item_cnt_dat 변수가 음수인 것은 물건이 반품된 것을 의미하는 것 같습니다.) . test = pd.read_csv(&#39;./test.csv&#39;) test.head() . ID shop_id item_id . 0 0 | 5 | 5037 | . 1 1 | 5 | 5320 | . 2 2 | 5 | 5233 | . 3 3 | 5 | 5232 | . 4 4 | 5 | 5268 | . sub = pd.read_csv(&#39;./sample_submission.csv&#39;) sub.head() . ID item_cnt_month . 0 0 | 0.5 | . 1 1 | 0.5 | . 2 2 | 0.5 | . 3 3 | 0.5 | . 4 4 | 0.5 | . 2015년 11월 데이터를 예측하는 캐글 대회입니다. . date_block_num 변수는 34가 되겠죠. . items = pd.read_csv(&#39;./items.csv&#39;) print (&#39;number of categories: &#39;, items[&#39;item_category_id&#39;].max()) # the maximun number of category id items.head() . number of categories: 83 . item_name item_id item_category_id . 0 ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.) D | 0 | 40 | . 1 !ABBYY FineReader 12 Professional Edition Full... | 1 | 76 | . 2 ***В ЛУЧАХ СЛАВЫ (UNV) D | 2 | 40 | . 3 ***ГОЛУБАЯ ВОЛНА (Univ) D | 3 | 40 | . 4 ***КОРОБКА (СТЕКЛО) D | 4 | 40 | . train_clean = train.drop(labels = [&#39;date&#39;, &#39;item_price&#39;], axis = 1) train_clean.head() . date_block_num shop_id item_id item_cnt_day . 0 0 | 59 | 22154 | 1.0 | . 1 0 | 25 | 2552 | 1.0 | . 2 0 | 25 | 2552 | -1.0 | . 3 0 | 25 | 2554 | 1.0 | . 4 0 | 25 | 2555 | 1.0 | . 날짜는 대체하는 date_block_num 변수가 있기 때문에 빼줍니다. . 또 제품 가격 변수 또한 빼줍니다. . train_clean = train_clean.groupby([&quot;item_id&quot;,&quot;shop_id&quot;,&quot;date_block_num&quot;]).sum().reset_index() train_clean = train_clean.rename(index=str, columns = {&quot;item_cnt_day&quot;:&quot;item_cnt_month&quot;}) train_clean = train_clean[[&quot;item_id&quot;,&quot;shop_id&quot;,&quot;date_block_num&quot;,&quot;item_cnt_month&quot;]] train_clean . item_id shop_id date_block_num item_cnt_month . 0 0 | 54 | 20 | 1.0 | . 1 1 | 55 | 15 | 2.0 | . 2 1 | 55 | 18 | 1.0 | . 3 1 | 55 | 19 | 1.0 | . 4 1 | 55 | 20 | 1.0 | . ... ... | ... | ... | ... | . 1609119 22168 | 12 | 8 | 1.0 | . 1609120 22168 | 16 | 1 | 1.0 | . 1609121 22168 | 42 | 1 | 1.0 | . 1609122 22168 | 43 | 2 | 1.0 | . 1609123 22169 | 25 | 14 | 1.0 | . 1609124 rows × 4 columns . 같은 달별로(= date_block_num 변수가 같은 값으로) 묶어줍니다. . 테스트 데이터에서 예측하고자 하는 값의 범위가 달 단위이기 때문입니다. . 변수 이름 또한 그에 맞게 item_cnt_month로 바꿨습니다. . &#49884;&#44228;&#50676; &#45936;&#51060;&#53552; &#50672;&#49845;&#54616;&#44592; . check = train_clean[[&quot;shop_id&quot;,&quot;item_id&quot;,&quot;date_block_num&quot;,&quot;item_cnt_month&quot;]] check = check.loc[check[&#39;shop_id&#39;] == 5] check = check.loc[check[&#39;item_id&#39;] == 5037] check . shop_id item_id date_block_num item_cnt_month . 400439 5 | 5037 | 20 | 1.0 | . 400440 5 | 5037 | 22 | 1.0 | . 400441 5 | 5037 | 23 | 2.0 | . 400442 5 | 5037 | 24 | 2.0 | . 400443 5 | 5037 | 28 | 1.0 | . 400444 5 | 5037 | 29 | 1.0 | . 400445 5 | 5037 | 30 | 1.0 | . 400446 5 | 5037 | 31 | 3.0 | . 400447 5 | 5037 | 32 | 1.0 | . 특정 shop_id와 item_id 값을 가지는 값만 모았습니다. . 시계열 분석을 처음하기 때문에 1차로 소량의 데이터를 다루었습니다. . 이렇게 데이터 분석을 공부하면 보다 직관적으로 LSTM 모델을 학습할 수 있을 것 같습니다. . plt.figure(figsize=(10,4)) plt.title(&#39;Check - Sales of Item 5037 at Shop 5&#39;) plt.xlabel(&#39;Month&#39;) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) plt.plot(check[&quot;date_block_num&quot;],check[&quot;item_cnt_month&quot;]); . 단순히 Y값에 대해 그림을 그려보았습니다. . month_list=[i for i in range(num_month+1)] # num_month = train[&#39;date_block_num&#39;].max(), 최고값 shop = [] for i in range(num_month+1): shop.append(5) item = [] for i in range(num_month+1): item.append(5037) months_full = pd.DataFrame({&#39;shop_id&#39;:shop, &#39;item_id&#39;:item,&#39;date_block_num&#39;:month_list}) months_full.head(10) . shop_id item_id date_block_num . 0 5 | 5037 | 0 | . 1 5 | 5037 | 1 | . 2 5 | 5037 | 2 | . 3 5 | 5037 | 3 | . 4 5 | 5037 | 4 | . 5 5 | 5037 | 5 | . 6 5 | 5037 | 6 | . 7 5 | 5037 | 7 | . 8 5 | 5037 | 8 | . 9 5 | 5037 | 9 | . 빈 데이터를 없애기 위해 처음부터 데이터프레임을 세팅하는 모습입니다. . shop = [] for i in range(num_month+1): shop.append(5) . 다만 이 코드 보다는 [5]*(num_month+1) 식으로 리스트를 구성하는게 더 깔끔한 것 같습니다. . sales_33month = pd.merge(check, months_full, how=&#39;right&#39;, on=[&#39;shop_id&#39;,&#39;item_id&#39;,&#39;date_block_num&#39;]) sales_33month = sales_33month.sort_values(by=[&#39;date_block_num&#39;]) sales_33month.fillna(0.00,inplace=True) plt.figure(figsize=(10,4)) plt.title(&#39;Check - Sales of Item 5037 at Shop 5 for whole period&#39;) plt.xlabel(&#39;Month&#39;) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) plt.plot(sales_33month[&quot;date_block_num&quot;],sales_33month[&quot;item_cnt_month&quot;]); . 물품 구매가 없는 데이터까지 0 값을 넣어서 그림을 그렸습니다. . for i in range(1,6): sales_33month[&quot;T_&quot; + str(i)] = sales_33month.item_cnt_month.shift(i) sales_33month.fillna(0.0, inplace=True) df = sales_33month[[&#39;shop_id&#39;,&#39;item_id&#39;,&#39;date_block_num&#39;,&#39;T_1&#39;,&#39;T_2&#39;,&#39;T_3&#39;,&#39;T_4&#39;,&#39;T_5&#39;, &#39;item_cnt_month&#39;]].reset_index() df = df.drop(labels = [&#39;index&#39;], axis = 1) df . shop_id item_id date_block_num T_1 T_2 T_3 T_4 T_5 item_cnt_month . 0 5 | 5037 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 5 | 5037 | 1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 5 | 5037 | 2 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 5 | 5037 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 5 | 5037 | 4 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 5 | 5037 | 5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 6 5 | 5037 | 6 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 7 5 | 5037 | 7 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 8 5 | 5037 | 8 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 9 5 | 5037 | 9 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 10 5 | 5037 | 10 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 11 5 | 5037 | 11 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 12 5 | 5037 | 12 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 13 5 | 5037 | 13 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 14 5 | 5037 | 14 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 15 5 | 5037 | 15 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 16 5 | 5037 | 16 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 17 5 | 5037 | 17 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 18 5 | 5037 | 18 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 19 5 | 5037 | 19 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 20 5 | 5037 | 20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 21 5 | 5037 | 21 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 22 5 | 5037 | 22 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 23 5 | 5037 | 23 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 2.0 | . 24 5 | 5037 | 24 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | 2.0 | . 25 5 | 5037 | 25 | 2.0 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 26 5 | 5037 | 26 | 0.0 | 2.0 | 2.0 | 1.0 | 0.0 | 0.0 | . 27 5 | 5037 | 27 | 0.0 | 0.0 | 2.0 | 2.0 | 1.0 | 0.0 | . 28 5 | 5037 | 28 | 0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 1.0 | . 29 5 | 5037 | 29 | 1.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | . 30 5 | 5037 | 30 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 31 5 | 5037 | 31 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 3.0 | . 32 5 | 5037 | 32 | 3.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | . 33 5 | 5037 | 33 | 1.0 | 3.0 | 1.0 | 1.0 | 1.0 | 0.0 | . 시계열 분석을 기초부터 뜯어본 것 같습니다. . T1 ~ T5에 의미는 최근 5달간 이전 Y값의 기록입니다. 예를 들면 T1은 한달 전 Y값을 나타냅니다. . 시간의 흐름에 따라 예측값이 영향을 받기 때문에 이러한 방식이 지금 이 데이터에서 적절합니다. . LSTM &#47784;&#45944; &#49324;&#50857; . train_df = df[:-3] val_df = df[-3:] x_train,y_train = train_df.drop([&quot;item_cnt_month&quot;],axis=1),train_df.item_cnt_month x_val,y_val = val_df.drop([&quot;item_cnt_month&quot;],axis=1),val_df.item_cnt_month . 맨 마지막 3개 데이터를 test 데이터로 사용합니다. . from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM model_lstm = Sequential() model_lstm.add(LSTM(15, input_shape=(1,8))) model_lstm.add(Dense(1)) model_lstm.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . from sklearn.preprocessing import StandardScaler,MinMaxScaler scaler = StandardScaler() scaler = MinMaxScaler(feature_range=(-1, 1)) x_train_scaled = scaler.fit_transform(x_train) x_valid_scaled = scaler.fit_transform(x_val) . x_train_reshaped = x_train_scaled.reshape((x_train_scaled.shape[0], 1, x_train_scaled.shape[1])) x_val_resaped = x_valid_scaled.reshape((x_valid_scaled.shape[0], 1, x_valid_scaled.shape[1])) history = model_lstm.fit(x_train_reshaped, y_train, validation_data=(x_val_resaped, y_val),epochs=70, batch_size=12, verbose=2, shuffle=False) y_pre = model_lstm.predict(x_val_resaped) . Epoch 1/70 3/3 - 2s - loss: 0.4119 - accuracy: 0.7742 - val_loss: 3.6385 - val_accuracy: 0.3333 Epoch 2/70 3/3 - 0s - loss: 0.3959 - accuracy: 0.7742 - val_loss: 3.5825 - val_accuracy: 0.3333 Epoch 3/70 3/3 - 0s - loss: 0.3818 - accuracy: 0.7742 - val_loss: 3.5290 - val_accuracy: 0.3333 Epoch 4/70 3/3 - 0s - loss: 0.3689 - accuracy: 0.7742 - val_loss: 3.4781 - val_accuracy: 0.3333 Epoch 5/70 3/3 - 0s - loss: 0.3571 - accuracy: 0.7742 - val_loss: 3.4296 - val_accuracy: 0.3333 Epoch 6/70 3/3 - 0s - loss: 0.3464 - accuracy: 0.7742 - val_loss: 3.3839 - val_accuracy: 0.3333 Epoch 7/70 3/3 - 0s - loss: 0.3368 - accuracy: 0.7742 - val_loss: 3.3409 - val_accuracy: 0.3333 Epoch 8/70 3/3 - 0s - loss: 0.3281 - accuracy: 0.7742 - val_loss: 3.3008 - val_accuracy: 0.3333 Epoch 9/70 3/3 - 0s - loss: 0.3203 - accuracy: 0.7742 - val_loss: 3.2637 - val_accuracy: 0.3333 Epoch 10/70 3/3 - 0s - loss: 0.3132 - accuracy: 0.7742 - val_loss: 3.2296 - val_accuracy: 0.3333 Epoch 11/70 3/3 - 0s - loss: 0.3069 - accuracy: 0.7742 - val_loss: 3.1984 - val_accuracy: 0.3333 Epoch 12/70 3/3 - 0s - loss: 0.3012 - accuracy: 0.7742 - val_loss: 3.1702 - val_accuracy: 0.3333 Epoch 13/70 3/3 - 0s - loss: 0.2960 - accuracy: 0.7742 - val_loss: 3.1451 - val_accuracy: 0.3333 Epoch 14/70 3/3 - 0s - loss: 0.2913 - accuracy: 0.7742 - val_loss: 3.1228 - val_accuracy: 0.3333 Epoch 15/70 3/3 - 0s - loss: 0.2869 - accuracy: 0.7742 - val_loss: 3.1035 - val_accuracy: 0.3333 Epoch 16/70 3/3 - 0s - loss: 0.2829 - accuracy: 0.7742 - val_loss: 3.0871 - val_accuracy: 0.3333 Epoch 17/70 3/3 - 0s - loss: 0.2791 - accuracy: 0.7742 - val_loss: 3.0733 - val_accuracy: 0.3333 Epoch 18/70 3/3 - 0s - loss: 0.2755 - accuracy: 0.7742 - val_loss: 3.0623 - val_accuracy: 0.3333 Epoch 19/70 3/3 - 0s - loss: 0.2720 - accuracy: 0.7742 - val_loss: 3.0537 - val_accuracy: 0.3333 Epoch 20/70 3/3 - 0s - loss: 0.2687 - accuracy: 0.7742 - val_loss: 3.0476 - val_accuracy: 0.3333 Epoch 21/70 3/3 - 0s - loss: 0.2654 - accuracy: 0.7742 - val_loss: 3.0437 - val_accuracy: 0.3333 Epoch 22/70 3/3 - 0s - loss: 0.2622 - accuracy: 0.7742 - val_loss: 3.0419 - val_accuracy: 0.3333 Epoch 23/70 3/3 - 0s - loss: 0.2590 - accuracy: 0.7742 - val_loss: 3.0421 - val_accuracy: 0.3333 Epoch 24/70 3/3 - 0s - loss: 0.2558 - accuracy: 0.8065 - val_loss: 3.0440 - val_accuracy: 0.3333 Epoch 25/70 3/3 - 0s - loss: 0.2527 - accuracy: 0.8065 - val_loss: 3.0477 - val_accuracy: 0.3333 Epoch 26/70 3/3 - 0s - loss: 0.2495 - accuracy: 0.8387 - val_loss: 3.0528 - val_accuracy: 0.3333 Epoch 27/70 3/3 - 0s - loss: 0.2463 - accuracy: 0.8387 - val_loss: 3.0592 - val_accuracy: 0.3333 Epoch 28/70 3/3 - 0s - loss: 0.2432 - accuracy: 0.8387 - val_loss: 3.0669 - val_accuracy: 0.3333 Epoch 29/70 3/3 - 0s - loss: 0.2401 - accuracy: 0.8387 - val_loss: 3.0756 - val_accuracy: 0.3333 Epoch 30/70 3/3 - 0s - loss: 0.2370 - accuracy: 0.8387 - val_loss: 3.0853 - val_accuracy: 0.3333 Epoch 31/70 3/3 - 0s - loss: 0.2339 - accuracy: 0.8387 - val_loss: 3.0958 - val_accuracy: 0.3333 Epoch 32/70 3/3 - 0s - loss: 0.2308 - accuracy: 0.8387 - val_loss: 3.1070 - val_accuracy: 0.3333 Epoch 33/70 3/3 - 0s - loss: 0.2278 - accuracy: 0.8065 - val_loss: 3.1187 - val_accuracy: 0.6667 Epoch 34/70 3/3 - 0s - loss: 0.2248 - accuracy: 0.8065 - val_loss: 3.1310 - val_accuracy: 0.6667 Epoch 35/70 3/3 - 0s - loss: 0.2219 - accuracy: 0.8065 - val_loss: 3.1436 - val_accuracy: 0.6667 Epoch 36/70 3/3 - 0s - loss: 0.2190 - accuracy: 0.7742 - val_loss: 3.1565 - val_accuracy: 0.6667 Epoch 37/70 3/3 - 0s - loss: 0.2162 - accuracy: 0.7742 - val_loss: 3.1696 - val_accuracy: 0.3333 Epoch 38/70 3/3 - 0s - loss: 0.2134 - accuracy: 0.7742 - val_loss: 3.1829 - val_accuracy: 0.3333 Epoch 39/70 3/3 - 0s - loss: 0.2107 - accuracy: 0.8065 - val_loss: 3.1963 - val_accuracy: 0.3333 Epoch 40/70 3/3 - 0s - loss: 0.2081 - accuracy: 0.8065 - val_loss: 3.2096 - val_accuracy: 0.3333 Epoch 41/70 3/3 - 0s - loss: 0.2056 - accuracy: 0.8065 - val_loss: 3.2229 - val_accuracy: 0.3333 Epoch 42/70 3/3 - 0s - loss: 0.2031 - accuracy: 0.8065 - val_loss: 3.2361 - val_accuracy: 0.3333 Epoch 43/70 3/3 - 0s - loss: 0.2008 - accuracy: 0.8065 - val_loss: 3.2492 - val_accuracy: 0.3333 Epoch 44/70 3/3 - 0s - loss: 0.1985 - accuracy: 0.8065 - val_loss: 3.2621 - val_accuracy: 0.3333 Epoch 45/70 3/3 - 0s - loss: 0.1963 - accuracy: 0.8065 - val_loss: 3.2748 - val_accuracy: 0.3333 Epoch 46/70 3/3 - 0s - loss: 0.1941 - accuracy: 0.8065 - val_loss: 3.2872 - val_accuracy: 0.3333 Epoch 47/70 3/3 - 0s - loss: 0.1921 - accuracy: 0.8065 - val_loss: 3.2994 - val_accuracy: 0.3333 Epoch 48/70 3/3 - 0s - loss: 0.1901 - accuracy: 0.8065 - val_loss: 3.3113 - val_accuracy: 0.3333 Epoch 49/70 3/3 - 0s - loss: 0.1882 - accuracy: 0.8065 - val_loss: 3.3229 - val_accuracy: 0.3333 Epoch 50/70 3/3 - 0s - loss: 0.1864 - accuracy: 0.8065 - val_loss: 3.3342 - val_accuracy: 0.3333 Epoch 51/70 3/3 - 0s - loss: 0.1847 - accuracy: 0.8065 - val_loss: 3.3451 - val_accuracy: 0.3333 Epoch 52/70 3/3 - 0s - loss: 0.1830 - accuracy: 0.8065 - val_loss: 3.3558 - val_accuracy: 0.3333 Epoch 53/70 3/3 - 0s - loss: 0.1814 - accuracy: 0.8065 - val_loss: 3.3661 - val_accuracy: 0.3333 Epoch 54/70 3/3 - 0s - loss: 0.1799 - accuracy: 0.8065 - val_loss: 3.3760 - val_accuracy: 0.3333 Epoch 55/70 3/3 - 0s - loss: 0.1785 - accuracy: 0.8065 - val_loss: 3.3855 - val_accuracy: 0.3333 Epoch 56/70 3/3 - 0s - loss: 0.1771 - accuracy: 0.8065 - val_loss: 3.3947 - val_accuracy: 0.3333 Epoch 57/70 3/3 - 0s - loss: 0.1757 - accuracy: 0.8065 - val_loss: 3.4036 - val_accuracy: 0.3333 Epoch 58/70 3/3 - 0s - loss: 0.1745 - accuracy: 0.8065 - val_loss: 3.4120 - val_accuracy: 0.3333 Epoch 59/70 3/3 - 0s - loss: 0.1732 - accuracy: 0.8065 - val_loss: 3.4201 - val_accuracy: 0.3333 Epoch 60/70 3/3 - 0s - loss: 0.1720 - accuracy: 0.8065 - val_loss: 3.4278 - val_accuracy: 0.3333 Epoch 61/70 3/3 - 0s - loss: 0.1709 - accuracy: 0.8065 - val_loss: 3.4351 - val_accuracy: 0.3333 Epoch 62/70 3/3 - 0s - loss: 0.1698 - accuracy: 0.8065 - val_loss: 3.4420 - val_accuracy: 0.3333 Epoch 63/70 3/3 - 0s - loss: 0.1687 - accuracy: 0.8065 - val_loss: 3.4485 - val_accuracy: 0.3333 Epoch 64/70 3/3 - 0s - loss: 0.1677 - accuracy: 0.8065 - val_loss: 3.4547 - val_accuracy: 0.3333 Epoch 65/70 3/3 - 0s - loss: 0.1667 - accuracy: 0.8065 - val_loss: 3.4605 - val_accuracy: 0.3333 Epoch 66/70 3/3 - 0s - loss: 0.1658 - accuracy: 0.8065 - val_loss: 3.4659 - val_accuracy: 0.3333 Epoch 67/70 3/3 - 0s - loss: 0.1648 - accuracy: 0.8065 - val_loss: 3.4710 - val_accuracy: 0.3333 Epoch 68/70 3/3 - 0s - loss: 0.1639 - accuracy: 0.8065 - val_loss: 3.4758 - val_accuracy: 0.3333 Epoch 69/70 3/3 - 0s - loss: 0.1631 - accuracy: 0.8065 - val_loss: 3.4802 - val_accuracy: 0.3333 Epoch 70/70 3/3 - 0s - loss: 0.1622 - accuracy: 0.8065 - val_loss: 3.4844 - val_accuracy: 0.3333 . fig, ax = plt.subplots() ax.plot(x_val[&#39;date_block_num&#39;], y_val, label=&#39;Actual&#39;) ax.plot(x_val[&#39;date_block_num&#39;], y_pre, label=&#39;Predicted&#39;) plt.title(&#39;LSTM Prediction vs Actual Sales for last 3 months&#39;) plt.xlabel(&#39;Month&#39;) plt.xticks(x_val[&#39;date_block_num&#39;]) plt.ylabel(&#39;Sales of Item 5037 at Shop 5&#39;) ax.legend() plt.show() . LSTM 모델을 적용시킨 모습입니다. . 잘 맞췄다면 잘 맞췄다고도 말 할수 있고 아쉽다면 아쉽다고 할 수 있는 결과인 것 같습니다. . &#45936;&#51060;&#53552; &#53456;&#49353; . sales_data = pd.read_csv(&#39;./sales_train.csv&#39;) item_cat = pd.read_csv(&#39;./item_categories.csv&#39;) items = pd.read_csv(&#39;./items.csv&#39;) shops = pd.read_csv(&#39;./shops.csv&#39;) sample_submission = pd.read_csv(&#39;./sample_submission.csv&#39;) test_data = pd.read_csv(&#39;./test.csv&#39;) . def basic_eda(df): print(&quot;-TOP 5 RECORDS--&quot;) print(df.head(5)) print(&quot;-INFO--&quot;) print(df.info()) print(&quot;-Describe-&quot;) print(df.describe()) print(&quot;-Columns--&quot;) print(df.columns) print(&quot;-Data Types--&quot;) print(df.dtypes) print(&quot;-Missing Values-&quot;) print(df.isnull().sum()) print(&quot;-NULL values-&quot;) print(df.isna().sum()) print(&quot;--Shape Of Data-&quot;) print(df.shape) print(&quot;=============================Sales Data=============================&quot;) basic_eda(sales_data) print(&quot;=============================Test data=============================&quot;) basic_eda(test_data) print(&quot;=============================Item Categories=============================&quot;) basic_eda(item_cat) print(&quot;=============================Items=============================&quot;) basic_eda(items) print(&quot;=============================Shops=============================&quot;) basic_eda(shops) print(&quot;=============================Sample Submission=============================&quot;) basic_eda(sample_submission) . =============================Sales Data============================= -TOP 5 RECORDS-- date date_block_num shop_id item_id item_price item_cnt_day 0 02.01.2013 0 59 22154 999.00 1.0 1 03.01.2013 0 25 2552 899.00 1.0 2 05.01.2013 0 25 2552 899.00 -1.0 3 06.01.2013 0 25 2554 1709.05 1.0 4 15.01.2013 0 25 2555 1099.00 1.0 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2935849 entries, 0 to 2935848 Data columns (total 6 columns): # Column Dtype -- 0 date object 1 date_block_num int64 2 shop_id int64 3 item_id int64 4 item_price float64 5 item_cnt_day float64 dtypes: float64(2), int64(3), object(1) memory usage: 134.4+ MB None -Describe- date_block_num shop_id item_id item_price item_cnt_day count 2.935849e+06 2.935849e+06 2.935849e+06 2.935849e+06 2.935849e+06 mean 1.456991e+01 3.300173e+01 1.019723e+04 8.908532e+02 1.242641e+00 std 9.422988e+00 1.622697e+01 6.324297e+03 1.729800e+03 2.618834e+00 min 0.000000e+00 0.000000e+00 0.000000e+00 -1.000000e+00 -2.200000e+01 25% 7.000000e+00 2.200000e+01 4.476000e+03 2.490000e+02 1.000000e+00 50% 1.400000e+01 3.100000e+01 9.343000e+03 3.990000e+02 1.000000e+00 75% 2.300000e+01 4.700000e+01 1.568400e+04 9.990000e+02 1.000000e+00 max 3.300000e+01 5.900000e+01 2.216900e+04 3.079800e+05 2.169000e+03 -Columns-- Index([&#39;date&#39;, &#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;, &#39;item_price&#39;, &#39;item_cnt_day&#39;], dtype=&#39;object&#39;) -Data Types-- date object date_block_num int64 shop_id int64 item_id int64 item_price float64 item_cnt_day float64 dtype: object -Missing Values- date 0 date_block_num 0 shop_id 0 item_id 0 item_price 0 item_cnt_day 0 dtype: int64 -NULL values- date 0 date_block_num 0 shop_id 0 item_id 0 item_price 0 item_cnt_day 0 dtype: int64 --Shape Of Data- (2935849, 6) =============================Test data============================= -TOP 5 RECORDS-- ID shop_id item_id 0 0 5 5037 1 1 5 5320 2 2 5 5233 3 3 5 5232 4 4 5 5268 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214200 entries, 0 to 214199 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 ID 214200 non-null int64 1 shop_id 214200 non-null int64 2 item_id 214200 non-null int64 dtypes: int64(3) memory usage: 4.9 MB None -Describe- ID shop_id item_id count 214200.000000 214200.000000 214200.000000 mean 107099.500000 31.642857 11019.398627 std 61834.358168 17.561933 6252.644590 min 0.000000 2.000000 30.000000 25% 53549.750000 16.000000 5381.500000 50% 107099.500000 34.500000 11203.000000 75% 160649.250000 47.000000 16071.500000 max 214199.000000 59.000000 22167.000000 -Columns-- Index([&#39;ID&#39;, &#39;shop_id&#39;, &#39;item_id&#39;], dtype=&#39;object&#39;) -Data Types-- ID int64 shop_id int64 item_id int64 dtype: object -Missing Values- ID 0 shop_id 0 item_id 0 dtype: int64 -NULL values- ID 0 shop_id 0 item_id 0 dtype: int64 --Shape Of Data- (214200, 3) =============================Item Categories============================= -TOP 5 RECORDS-- item_category_name item_category_id 0 PC - Гарнитуры/Наушники 0 1 Аксессуары - PS2 1 2 Аксессуары - PS3 2 3 Аксессуары - PS4 3 4 Аксессуары - PSP 4 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 84 entries, 0 to 83 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 item_category_name 84 non-null object 1 item_category_id 84 non-null int64 dtypes: int64(1), object(1) memory usage: 1.4+ KB None -Describe- item_category_id count 84.000000 mean 41.500000 std 24.392622 min 0.000000 25% 20.750000 50% 41.500000 75% 62.250000 max 83.000000 -Columns-- Index([&#39;item_category_name&#39;, &#39;item_category_id&#39;], dtype=&#39;object&#39;) -Data Types-- item_category_name object item_category_id int64 dtype: object -Missing Values- item_category_name 0 item_category_id 0 dtype: int64 -NULL values- item_category_name 0 item_category_id 0 dtype: int64 --Shape Of Data- (84, 2) =============================Items============================= -TOP 5 RECORDS-- item_name item_id item_category_id 0 ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.) D 0 40 1 !ABBYY FineReader 12 Professional Edition Full... 1 76 2 ***В ЛУЧАХ СЛАВЫ (UNV) D 2 40 3 ***ГОЛУБАЯ ВОЛНА (Univ) D 3 40 4 ***КОРОБКА (СТЕКЛО) D 4 40 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22170 entries, 0 to 22169 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 item_name 22170 non-null object 1 item_id 22170 non-null int64 2 item_category_id 22170 non-null int64 dtypes: int64(2), object(1) memory usage: 519.7+ KB None -Describe- item_id item_category_id count 22170.00000 22170.000000 mean 11084.50000 46.290753 std 6400.07207 15.941486 min 0.00000 0.000000 25% 5542.25000 37.000000 50% 11084.50000 40.000000 75% 16626.75000 58.000000 max 22169.00000 83.000000 -Columns-- Index([&#39;item_name&#39;, &#39;item_id&#39;, &#39;item_category_id&#39;], dtype=&#39;object&#39;) -Data Types-- item_name object item_id int64 item_category_id int64 dtype: object -Missing Values- item_name 0 item_id 0 item_category_id 0 dtype: int64 -NULL values- item_name 0 item_id 0 item_category_id 0 dtype: int64 --Shape Of Data- (22170, 3) =============================Shops============================= -TOP 5 RECORDS-- shop_name shop_id 0 !Якутск Орджоникидзе, 56 фран 0 1 !Якутск ТЦ &#34;Центральный&#34; фран 1 2 Адыгея ТЦ &#34;Мега&#34; 2 3 Балашиха ТРК &#34;Октябрь-Киномир&#34; 3 4 Волжский ТЦ &#34;Волга Молл&#34; 4 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 60 entries, 0 to 59 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 shop_name 60 non-null object 1 shop_id 60 non-null int64 dtypes: int64(1), object(1) memory usage: 1.1+ KB None -Describe- shop_id count 60.000000 mean 29.500000 std 17.464249 min 0.000000 25% 14.750000 50% 29.500000 75% 44.250000 max 59.000000 -Columns-- Index([&#39;shop_name&#39;, &#39;shop_id&#39;], dtype=&#39;object&#39;) -Data Types-- shop_name object shop_id int64 dtype: object -Missing Values- shop_name 0 shop_id 0 dtype: int64 -NULL values- shop_name 0 shop_id 0 dtype: int64 --Shape Of Data- (60, 2) =============================Sample Submission============================= -TOP 5 RECORDS-- ID item_cnt_month 0 0 0.5 1 1 0.5 2 2 0.5 3 3 0.5 4 4 0.5 -INFO-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214200 entries, 0 to 214199 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 ID 214200 non-null int64 1 item_cnt_month 214200 non-null float64 dtypes: float64(1), int64(1) memory usage: 3.3 MB None -Describe- ID item_cnt_month count 214200.000000 214200.0 mean 107099.500000 0.5 std 61834.358168 0.0 min 0.000000 0.5 25% 53549.750000 0.5 50% 107099.500000 0.5 75% 160649.250000 0.5 max 214199.000000 0.5 -Columns-- Index([&#39;ID&#39;, &#39;item_cnt_month&#39;], dtype=&#39;object&#39;) -Data Types-- ID int64 item_cnt_month float64 dtype: object -Missing Values- ID 0 item_cnt_month 0 dtype: int64 -NULL values- ID 0 item_cnt_month 0 dtype: int64 --Shape Of Data- (214200, 2) . 앞 코드와 다른 사람 코드입니다. . 여기서 train 데이터 프레임을 이 사람은 sales_data 이름으로 했네요. . 사실 데이터 탐색하는 함수를 잘 만들어 놓은것 같아서 향후 다른 데이터 분석시 복사를 위해 가져왔습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . sales_data[&#39;date&#39;] = pd.to_datetime(sales_data[&#39;date&#39;],format = &#39;%d.%m.%Y&#39;) dataset = sales_data.pivot_table(index = [&#39;shop_id&#39;,&#39;item_id&#39;], values = [&#39;item_cnt_day&#39;],columns = [&#39;date_block_num&#39;],fill_value = 0,aggfunc=&#39;sum&#39;) dataset.reset_index(inplace = True) dataset.head() . shop_id item_id item_cnt_day . date_block_num 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 . 0 0 | 30 | 0 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 31 | 0 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 32 | 6 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 33 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 35 | 1 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 판다스 내 피벗 테이블을 사용하는 모습입니다. group_by 함수를 확장한 것으로 생각할 수 있습니다. . 피벗 테이블은 우선 index로 데이터를 구분 짓습니다. 여기서 shop_id, item_id가 모두 같은 값을 가진 행끼리 그룹을 짓습니다. . 다음으로 columns로 한번 더 데이터를 구분 짓습니다. 같은 상점, 같은 제품을 달별로 나누었습니다. . values는 실제 적용되는 값을 의미합니다. 여기서는 item_cnt_day 변수를 사용했습니다. . 상점, 제품, 달이 같은 데이터 별로 구분했을때 여러개의 item_cnt_day 값을 더해주는 함수(aggfunc=&#39;sum&#39;)를 사용합니다. . 빈 값도 충분히 존재할 가능성이 있는데, 그 경우 거래 기록이 존재하지 않았다는 의미이므로 0값을 채웁니다.(fill_value = 0) . dataset = pd.merge(test_data,dataset,on = [&#39;item_id&#39;,&#39;shop_id&#39;],how = &#39;left&#39;) dataset.fillna(0,inplace = True) dataset.head() . /usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right) warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) . ID shop_id item_id (item_cnt_day, 0) (item_cnt_day, 1) (item_cnt_day, 2) (item_cnt_day, 3) (item_cnt_day, 4) (item_cnt_day, 5) (item_cnt_day, 6) (item_cnt_day, 7) (item_cnt_day, 8) (item_cnt_day, 9) (item_cnt_day, 10) (item_cnt_day, 11) (item_cnt_day, 12) (item_cnt_day, 13) (item_cnt_day, 14) (item_cnt_day, 15) (item_cnt_day, 16) (item_cnt_day, 17) (item_cnt_day, 18) (item_cnt_day, 19) (item_cnt_day, 20) (item_cnt_day, 21) (item_cnt_day, 22) (item_cnt_day, 23) (item_cnt_day, 24) (item_cnt_day, 25) (item_cnt_day, 26) (item_cnt_day, 27) (item_cnt_day, 28) (item_cnt_day, 29) (item_cnt_day, 30) (item_cnt_day, 31) (item_cnt_day, 32) (item_cnt_day, 33) . 0 0 | 5 | 5037 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 2.0 | 2.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 1.0 | 3.0 | 1.0 | 0.0 | . 1 1 | 5 | 5320 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 2 | 5 | 5233 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 2.0 | 0.0 | 1.0 | 3.0 | 1.0 | . 3 3 | 5 | 5232 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 4 4 | 5 | 5268 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 피벗 테이블을 사용해 같은 상점, 제품을 달 별로 거래기록이 몇건 있었는가를 나타내는 데이터 프레임입니다. . 이를 활용해 test 데이터 프레임과 병합한다면 테스트 데이터에 있는 상점, 제품의 이전 달별 거래기록을 전부 알 수 있습니다. . 이때 만약 병합이 안된 데이터가 있다면(이전 거래기록이 없는 데이터이겠죠?) 0으로 값을 넣어줍니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . dataset.drop([&#39;shop_id&#39;,&#39;item_id&#39;,&#39;ID&#39;],inplace = True, axis = 1) dataset.head() X_train = np.expand_dims(dataset.values[:,:-1],axis = 2) y_train = dataset.values[:,-1:] X_test = np.expand_dims(dataset.values[:,1:],axis = 2) print(X_train.shape,y_train.shape,X_test.shape) . (214200, 33, 1) (214200, 1) (214200, 33, 1) . 데이터를 모델링 하기 위해 상점, 제품 데이터를 지우고, train과 test 데이터 셋을 만들었습니다. . X_train : 0번째 달부터 32번째 달까지 거래 기록 데이터 . y_train : 33번째 달 거래 기록 데이터 . X_test : 1번째 달부터 33번째 달까지 거래 기록 데이터(train과 test간 데이터 형식을 맞추기 위해) . 우리가 예측해야할 y_test는 34번째 달 거래 기록 데이터, 즉 2015년 10월 거래 기록 데이터 입니다. . from keras.models import Sequential from keras.layers import LSTM,Dense,Dropout my_model = Sequential() my_model.add(LSTM(units = 64,input_shape = (33,1))) my_model.add(Dropout(0.4)) my_model.add(Dense(1)) my_model.compile(loss = &#39;mse&#39;,optimizer = &#39;adam&#39;, metrics = [&#39;mean_squared_error&#39;]) my_model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_1 (LSTM) (None, 64) 16896 _________________________________________________________________ dropout (Dropout) (None, 64) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ . my_model.fit(X_train,y_train,batch_size = 4096,epochs = 10) . Epoch 1/10 53/53 [==============================] - 27s 471ms/step - loss: 30.6011 - mean_squared_error: 30.6011 Epoch 2/10 53/53 [==============================] - 25s 466ms/step - loss: 30.2430 - mean_squared_error: 30.2430 Epoch 3/10 53/53 [==============================] - 24s 462ms/step - loss: 30.0014 - mean_squared_error: 30.0014 Epoch 4/10 53/53 [==============================] - 25s 481ms/step - loss: 29.8476 - mean_squared_error: 29.8476 Epoch 5/10 53/53 [==============================] - 26s 482ms/step - loss: 29.7404 - mean_squared_error: 29.7404 Epoch 6/10 53/53 [==============================] - 26s 487ms/step - loss: 29.7396 - mean_squared_error: 29.7396 Epoch 7/10 53/53 [==============================] - 25s 480ms/step - loss: 29.7369 - mean_squared_error: 29.7369 Epoch 8/10 53/53 [==============================] - 25s 473ms/step - loss: 29.6503 - mean_squared_error: 29.6503 Epoch 9/10 53/53 [==============================] - 25s 472ms/step - loss: 29.6353 - mean_squared_error: 29.6353 Epoch 10/10 53/53 [==============================] - 25s 468ms/step - loss: 29.5096 - mean_squared_error: 29.5096 . &lt;keras.callbacks.History at 0x7f2b3e51ff90&gt; . 모델을 LSTM(시계열 분석) 방법을 사용해서 분석합니다. 사실 LSTM 모델을 처음 사용했는데요. . 이번주에 다소 시간이 부족해 LSTM 모델의 사용방법이나 원리 등은 아직 파악하지 못했네요. (다른 사람 발표를 경청하겠습니다.) . submission_pfs = my_model.predict(X_test) submission_pfs = submission_pfs.clip(0,20) submission = pd.DataFrame({&#39;ID&#39;:test_data[&#39;ID&#39;],&#39;item_cnt_month&#39;:submission_pfs.ravel()}) submission.to_csv(&#39;./submission.csv&#39;,index = False) submission . ID item_cnt_month . 0 0 | 0.396485 | . 1 1 | 0.103207 | . 2 2 | 0.743674 | . 3 3 | 0.135947 | . 4 4 | 0.103207 | . ... ... | ... | . 214195 214195 | 0.331131 | . 214196 214196 | 0.103207 | . 214197 214197 | 0.097571 | . 214198 214198 | 0.103207 | . 214199 214199 | 0.069235 | . 214200 rows × 2 columns . 데이터를 모델에 적용시켜 예측값을 찾은 뒤, 제출 형식에 맞게 데이터 프레임 형식을 조정했습니다. . 이때 clip 함수는 이상치 조정 함수입니다. . clip(최솟값, 최댓값) 구조로 범위를 벗어나면 범위 내로 값을 조정시켜줍니다. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; data = {&#39;col_0&#39;: [9, -3, 0, -1, 5], &#39;col_1&#39;: [-2, -7, 6, 8, -5]} df = pd.DataFrame(data) df . col_0 col_1 . 0 9 | -2 | . 1 -3 | -7 | . 2 0 | 6 | . 3 -1 | 8 | . 4 5 | -5 | . df.clip(-4, 6) . col_0 col_1 . 0 6 | -2 | . 1 -3 | -4 | . 2 0 | 6 | . 3 -1 | 6 | . 4 5 | -4 | . 예시를 보면 보다 직관적으로 이해가 가능할 것 같습니다. . 이 함수는 범용성이 넓으니 다른 데이터 분석에 자주 쓰일 수 있어 따로 정리했네요. . !kaggle competitions submit -c competitive-data-science-predict-future-sales -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 3.55M/3.55M [00:04&lt;00:00, 769kB/s] Successfully submitted to Predict Future Sales . 캐글에 파일을 자동 제출하는 코드입니다. . 스코어는 약 1.02로 만 2천명 중 6천등 정도를 기록합니다. . &#45712;&#45184;&#51216; . 우선 공부하기 좋은 데이터를 찾아 줘서 고맙습니다. . 시계열 자료가 현실에서 상당히 많아 꼭 공부해보고 싶은 분야였는데, 이번 기회에 분석하게 되서 너무 좋습니다. . 개인적으로 공부하고 싶은 분야가 이미지 분류같은 것 보다는 자연어 처리, 시계열 분석 등 현실 세계를 설명할 수 있는 것 입니다. . 이번엔 시간이 다소 부족해서 자주쓰는 시계열 모델인 LSTM 모델의 탐구가 부족했습니다. . 다른 사람 발표 경청하고, 시간이 있을때 LSTM 모델을 열심히 공부해보고 싶네요. . 감사합니다. . &lt;/div&gt;",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/time%20series/lstm/datetime/clip/2021/10/28/kagglessu4.html",
            "relUrl": "/ssuda/jupyter/kaggle/time%20series/lstm/datetime/clip/2021/10/28/kagglessu4.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "[시뮬레이션] 중간고사 범위 복습",
            "content": ". &#53076;&#47017;&#50640;&#49436; R &#49324;&#50857;&#48277; by &#54805;&#46973; . https://colab.research.google.com/notebook#create=true&amp;language=r . 뒷부분에 language=r 만 붙여주면 정상적으로 코랩 R버전이 실행됩니다. . for (i in 1:10){ print(i) } . [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 . 6.6&#51208; &#48372;&#54744;&#44552; &#52397;&#44396; &#47928;&#51228; . 기존 보험 가입자 n0(사용값 1명), 기존 자본금 a0 (사용값 25000), 기간 365. 보함 가입자는 기간 1당 보험금 C(사용값 11000)을 각각 지불합니다. . 이때 자본금이 음이 되지 않을 확률을 모의실험으로 구하는 문제 입니다. . 일어날 사건은 보험금청구, 신규고객 가입, 기존계약해지 인데요. . 보험금 청구는 도착률 알파(사용값 10)인 포아송 과정, 이때 청구 금액은 지수분포 (사용 람다값 1/1000)을 따릅니다. . 포아송 과정이란 사건 발생 시간 분포가 평균 1/알파인 지수분포 입니다. . 이 모의실험을 300번 실시해봅니다. . (신규고객 가입, 기존계약 해지는 무시합니다.) . n.sim &lt;- 300 # 모의실험 실행 횟수 n0 &lt;- 1; a0 &lt;- 25000; T &lt;- 365; c &lt;- 11000 # 가입자, 자본금, 기간, 단위기간당 보험금 초기값 부여 alpha &lt;- 10; nu &lt;- 0; mu &lt;- 0 # 알파값, 신규계약과 기존계약 해지는 무시합니다. generate.Y &lt;- function() rexp(1, rate = 1/1000) # 청구금액 만드는 함수를 생성합니다. I &lt;- numeric(length = n.sim) # 자본금이 음이되는지 여부를 실험마다 기록하는 변수 입니다. for (i in 1:n.sim){ # 실험 n.sim(300)번 실행 t &lt;- 0; a &lt;- a0; n &lt;- n0 # 시점, 자본금, 고객 수 초기값 부여 total.rate &lt;- nu + n * mu + n * alpha # 사건 발생 람다값 부여. 여기서 유효한 값은 n * alpha(보험금 청구) 입니다. tE &lt;- rexp(1, rate = total.rate) # 첫 사건 발생 시간 repeat{ if (tE &gt; T) { # 주어진 기간을 초과했을 경우 I[i] &lt;- 1 # 중간에 중단되지 않고 주어진 기간(365)를 무사히 초과했기 때문에 이번 실험은 성공임을 기록해줍니다. break # 반복분 끝내기. 다음 모의 실험이 실행되겠죠. } if (tE &lt;= T){ # 주어진 기간 내. a &lt;- a + n * c * (tE - t) # 보험금 수금. 여기서 tE는 새 사건 발생 시간, t는 과거 사건 발생시간. t &lt;- tE # 시점을 새 사건 발생시간에 맞춰줍니다. J &lt;- sample(1:3, 1, prob = c(nu, n*mu, n*alpha)) # 이번 사건은 어떤사건인지 정해줍니다. 하지만 여기선 무조건 J는 3이됩니다. if (J == 1) n &lt;- n + 1 # 신규고객 가입 if (J == 2) n &lt;- n - 1 # 기존고객 해지 if (J == 3){ Y &lt;- generate.Y(); # 보험금 청구 금액 찾기 if (Y &gt; a){ # 현재 자본금보다 보험 청구 금액이 많으면 = 자본금이 음수가 됨. I[i] &lt;- 0 # 이번 실험은 실패임을 기록 break # 반복문 끝내기 } else a &lt;- a - Y # 자본금이 음수가 되는 일이 벌어지지 않으면 자본금에서 돈을 쓰면 되겠죠. } tE &lt;- t + rexp(1,rate=total.rate) # 다음 사건이 일어날 시점을 탐색합니다. } } } mean(I) cat(&#39;자본금이 남아있을 확률 95%신뢰구간 [&#39;,mean(I)- 1.96*sd(I) /sqrt(n.sim),&#39;,&#39;,mean(I) + 1.96*sd(I)/sqrt(n.sim), &#39;] n&#39;) . 0.916666666666667 자본금이 남아있을 확률 95%신뢰구간 [ 0.8853385 , 0.9479949 ] . 자본금이 음수가 되지 않을 확률이 90%정도 됩니다. 신뢰구간도 구할 수 있군요. . 강의는 여기까지 가르쳤는데요. 시험문제는 이를 응용하는 문제가 나올 수 있습니다. . 제일 쉬운 예시로 고객의 가입과 탈퇴가 포함된 함수를 만드는 문제가 나올수도 있겠습니다. . 신규고객 가입을 람다가 1인 포아송 과정으로, 기존 고객 탈퇴를 람다가 0.1인 포아송 과정으로 하겠습니다. . 그리고 기존 고객의 초기 수를 10으로 하겠습니다. . n.sim &lt;- 100 n0 &lt;- 10; a0 &lt;- 25000; T &lt;- 365; c &lt;- 11000 alpha &lt;- 10; nu &lt;- 1; mu &lt;- 0.1 # 이부분만 바꿔주면 됨 generate.Y &lt;- function() rexp(1, rate = 1/1000) I &lt;- numeric(length = n.sim) for (i in 1:n.sim){ t &lt;- 0; a &lt;- a0; n &lt;- n0 total.rate &lt;- nu + n * mu + n * alpha tE &lt;- rexp(1, rate = total.rate) repeat{ if (tE &gt; T) { I[i] &lt;- 1 break } if (tE &lt;= T){ a &lt;- a + n * c * (tE - t) t &lt;- tE J &lt;- sample(1:3, 1, prob = c(nu, n*mu, n*alpha)) if (J == 1) n &lt;- n + 1 if (J == 2){ n &lt;- n - 1 if (n == 0){ #보험금이 0이된 경우. I[i] &lt;- 0 break } } if (J == 3){ Y &lt;- generate.Y(); if (Y &gt; a){ I[i] &lt;- 0 break } else a &lt;- a - Y } tE &lt;- t + rexp(1,rate=total.rate) } } } mean(I) cat(&#39;자본금이 남아있을 확률 95%신뢰구간 [&#39;,mean(I)- 1.96*sd(I) /sqrt(n.sim),&#39;,&#39;,mean(I) + 1.96*sd(I)/sqrt(n.sim), &#39;] n&#39;) . 0.36 자본금이 남아있을 확률 95%신뢰구간 [ 0.265446 , 0.454554 ] . 이미 세 사건이 일어날걸 가정하고 함수를 다 만들어나서 단순히 nu와 mu값만 넣어주면 됩니다. . 다만 보험 가입자가 0명이 될 경우도 있는데 그 경우 또한 실패로 하겠습니다. . 확실히 가입자가 늘어나고 탈퇴를 할 수 있는 등 불확실성이 커지니 자본금이 음이 될 확률이 줄어들었습니다. . 6.8&#51208; &#51452;&#49885; &#50741;&#49496; &#54665;&#49324; &#51204;&#47029; . t시점에 주식 가격 S(t) = S(0) * exp(x1 + x2 .. + xt) 이라고 가정합니다. 이때 xi는 iid인 정규분포 변수입니다. . 알파 &lt; 평균 + 분산 / 2 (정규분포 평균, 분산) 일때 좋은 전략이 다음과 같이 알려져있습니다. . P(m) = S(N-m) 이라고 할때(만기를 m 앞둔 시점에서의 주가) 다음 조건이 만족하면 옵션을 행사합니다. . P(m) &gt; K(옵션권한가격 = 초기가격) 쉽게 얘기해 주식 가격이 초기가격보다 높아야합니다. 당연하죠. | P(m) &gt; K + f(i) 을 i = 1,2, .. , m 구간에서 모두 만족해야합니다. f(i)는 식이 복잡해 생략합니다. | 또 다른 전략은 끝나는 시점까지 기다렸다가 최종 시점 주식 가격이 K보다 클때만 사는 전략입니다. . 두 전략중 어떤 전략이 좋을지 모의실험 1000회를 통해 알아봅시다. . n.sim &lt;- 1000 # 실험횟수 N &lt;- 20; K &lt;- 100; S.zero &lt;- 100; mu &lt;- -0.05 # N : 기간, K, S.zero : 초기 금액(사실 어느값을 써도 비슷함) sg &lt;- 0.3; alp &lt;- mu + 0.5*sg^2 # mu = -0.05, 시그마 = 0.3 E &lt;- numeric(length = n.sim) E2 &lt;- numeric(length = n.sim) for (i in 1:n.sim){ S &lt;- S.zero * exp(cumsum(rnorm(N,mu,sg))) # 주식 가격을 구해놓음. E2[i] &lt;- max(S[N] - K, 0) # 최종시점 주식 가격이 K보다 크면 그만큼 이득, 아니면 이득 0. P &lt;- numeric(length=N+1) # P[0]은 R에서 쓸수 없음. 그래서 길이 자체를 N+1로 해줌. P[N+1] &lt;- S.zero #초기값 부여 m &lt;- N - 1 # m 초기값 부여. 미리 값 1을 뺀 모양새.(위에서 P[N+1] 초기값을 부여했기 때문에) flag &lt;- FALSE repeat{ m.plus &lt;- m + 1 # P에서는 m값을 1을 올려서 해줌. P[m.plus] &lt;- S[N-m] # 값 넣어줌. if(P[m.plus] &gt; K) flag &lt;- TRUE # 1번조건 만족 표현 if(flag &amp; m &gt; 0){ # 조건1만족 + m이 0아닐때(m이 0일때는 1번조건만 따짐.) b &lt;- ((1:m)*mu - log(K/P[m.plus])) / (sg*sqrt(1:m)) op &lt;- P[m.plus] * exp((1:m)*alp)* pnorm(sg*sqrt(1:m) + b) - K * pnorm(b) # 복잡한 식 f(i), 벡터 형태로 되어있음. flag &lt;- all(P[m.plus] &gt; K + op) # all은 모든 조건이 true일때만 true를 보내줌. } if(flag) break else m &lt;- m - 1 # 조건을 모두 만족하면 즉시 옵션 행사. if(m &lt; 0) break # 시점이 모두 끝났으면 종료. } if (flag) E[i] &lt;- P[m.plus] - K else E[i] &lt;- 0 # 조건 만족시 이득본 만큼 기록. } cat(&#39;전략1 95%신뢰구간 [&#39;,mean(E) - 1.96*sd(E)/sqrt(n.sim), &#39;,&#39;,mean(E) + 1.96*sd(E)/sqrt(n.sim), &#39;] n&#39;) cat(&#39;전략2 95%신뢰구간 [&#39;,mean(E2) - 1.96*sd(E2)/sqrt(n.sim), &#39;,&#39;,mean(E2) + 1.96*sd(E2)/sqrt(n.sim), &#39;] n&#39;) . 전략1 95%신뢰구간 [ 35.36105 , 45.49005 ] 전략2 95%신뢰구간 [ 28.57401 , 44.68403 ] . 여러번 실행을 해보면 전략1과 전략2의 이득 평균이 비슷합니다. . 다만 전략2의 신뢰구간이 큰 것은 그만큼 전략2가 불안정한 전략임을 알 수 있습니다. . &#48512;&#53944;&#49828;&#53944;&#47129; &#51060;&#47200; . 뽑힌 표본들을 새로운 분포로 가정하여 반복추출(복원추출)을 통해 모수를 추정하는 방법 입니다. . 표본들이 실제 분포와 비슷할 수록 모수 추정이 더 정확해집니다. . 이 방법의 장점은 중심극한정리 등 분포가정을 하지 않아도 된다는 점입니다. . 다음은 부트스트렙 예시로, 표본들이 있을때 Var(s^2)의 추정량을 구하는 문제입니다. . x &lt;- c(5,4,9,6,21,17,11,20,7,10,21,15,13,16,8) n &lt;- 15 B &lt;- 400 # 400번 실시 f.var &lt;- function(x) var(sample(x, n, rep = T)) b.var &lt;- replicate(B, f.var(x)) var(b.var) # estimate of var(s^2) hist(b.var) . 57.6508361857024",
            "url": "https://ksy1526.github.io/myblog/school/jupyter/simulation/r/bootstrap/2021/10/21/%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98_%EA%B3%BC%EB%AA%A9_%EB%B3%B5%EC%8A%B5.html",
            "relUrl": "/school/jupyter/simulation/r/bootstrap/2021/10/21/%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98_%EA%B3%BC%EB%AA%A9_%EB%B3%B5%EC%8A%B5.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "[SSUDA] 택시 데이터 분석",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle (3).json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;23e68db36970b65937516103c630ba75&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c nyc-taxi-trip-duration . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) train.zip: Skipping, found more recently modified local copy (use --force to force download) test.zip: Skipping, found more recently modified local copy (use --force to force download) sample_submission.zip: Skipping, found more recently modified local copy (use --force to force download) . !unzip train.zip !unzip test.zip !unzip sample_submission.zip . Archive: train.zip replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: train.csv Archive: test.zip replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: test.csv Archive: sample_submission.zip replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y inflating: sample_submission.csv . 압축되어 있는 데이터라서 압축 풀어줍니다. . %matplotlib inline import pandas as pd from datetime import datetime import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge,BayesianRidge from sklearn.cluster import MiniBatchKMeans from sklearn.metrics import mean_squared_error from math import radians, cos, sin, asin, sqrt import seaborn as sns import matplotlib import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [16, 10] . train = pd.read_csv(&#39;./train.csv&#39;) test = pd.read_csv(&#39;./test.csv&#39;) . &#45936;&#51060;&#53552; &#53456;&#49353; . train.head() . id vendor_id pickup_datetime dropoff_datetime passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude store_and_fwd_flag trip_duration . 0 id2875421 | 2 | 2016-03-14 17:24:55 | 2016-03-14 17:32:30 | 1 | -73.982155 | 40.767937 | -73.964630 | 40.765602 | N | 455 | . 1 id2377394 | 1 | 2016-06-12 00:43:35 | 2016-06-12 00:54:38 | 1 | -73.980415 | 40.738564 | -73.999481 | 40.731152 | N | 663 | . 2 id3858529 | 2 | 2016-01-19 11:35:24 | 2016-01-19 12:10:48 | 1 | -73.979027 | 40.763939 | -74.005333 | 40.710087 | N | 2124 | . 3 id3504673 | 2 | 2016-04-06 19:32:31 | 2016-04-06 19:39:40 | 1 | -74.010040 | 40.719971 | -74.012268 | 40.706718 | N | 429 | . 4 id2181028 | 2 | 2016-03-26 13:30:55 | 2016-03-26 13:38:10 | 1 | -73.973053 | 40.793209 | -73.972923 | 40.782520 | N | 435 | . train.describe() . vendor_id passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude trip_duration . count 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | . mean 1.534950e+00 | 1.664530e+00 | -7.397349e+01 | 4.075092e+01 | -7.397342e+01 | 4.075180e+01 | 9.594923e+02 | . std 4.987772e-01 | 1.314242e+00 | 7.090186e-02 | 3.288119e-02 | 7.064327e-02 | 3.589056e-02 | 5.237432e+03 | . min 1.000000e+00 | 0.000000e+00 | -1.219333e+02 | 3.435970e+01 | -1.219333e+02 | 3.218114e+01 | 1.000000e+00 | . 25% 1.000000e+00 | 1.000000e+00 | -7.399187e+01 | 4.073735e+01 | -7.399133e+01 | 4.073588e+01 | 3.970000e+02 | . 50% 2.000000e+00 | 1.000000e+00 | -7.398174e+01 | 4.075410e+01 | -7.397975e+01 | 4.075452e+01 | 6.620000e+02 | . 75% 2.000000e+00 | 2.000000e+00 | -7.396733e+01 | 4.076836e+01 | -7.396301e+01 | 4.076981e+01 | 1.075000e+03 | . max 2.000000e+00 | 9.000000e+00 | -6.133553e+01 | 5.188108e+01 | -6.133553e+01 | 4.392103e+01 | 3.526282e+06 | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1458644 entries, 0 to 1458643 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 id 1458644 non-null object 1 vendor_id 1458644 non-null int64 2 pickup_datetime 1458644 non-null object 3 dropoff_datetime 1458644 non-null object 4 passenger_count 1458644 non-null int64 5 pickup_longitude 1458644 non-null float64 6 pickup_latitude 1458644 non-null float64 7 dropoff_longitude 1458644 non-null float64 8 dropoff_latitude 1458644 non-null float64 9 store_and_fwd_flag 1458644 non-null object 10 trip_duration 1458644 non-null int64 dtypes: float64(4), int64(3), object(4) memory usage: 122.4+ MB . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 625134 entries, 0 to 625133 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 id 625134 non-null object 1 vendor_id 625134 non-null int64 2 pickup_datetime 625134 non-null object 3 passenger_count 625134 non-null int64 4 pickup_longitude 625134 non-null float64 5 pickup_latitude 625134 non-null float64 6 dropoff_longitude 625134 non-null float64 7 dropoff_latitude 625134 non-null float64 8 store_and_fwd_flag 625134 non-null object dtypes: float64(4), int64(2), object(3) memory usage: 42.9+ MB . dropoff_datetime 변수가 test에는 없습니다. 도착 시간을 맞추는 예제이기 때문에 그렇습니다. . &#48152;&#51025;&#48320;&#49688; &#44288;&#52272; . plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train.trip_duration.values)) plt.xlabel(&#39;index&#39;, fontsize=12) plt.ylabel(&#39;trip duration&#39;, fontsize=12) plt.show() . 반응변수의 이상치가 많아보입니다. 제거하겠습니다. . m = np.mean(train[&#39;trip_duration&#39;]) s = np.std(train[&#39;trip_duration&#39;]) train = train[train[&#39;trip_duration&#39;] &lt;= m + 2*s] train = train[train[&#39;trip_duration&#39;] &gt;= m - 2*s] plt.figure(figsize=(8,6)) plt.scatter(range(train.shape[0]), np.sort(train.trip_duration.values)) plt.xlabel(&#39;index&#39;, fontsize=12) plt.ylabel(&#39;trip duration&#39;, fontsize=12) plt.show() . 이상치는 대부분 제거된 것 같습니다. 다만 일부 데이터가 큰 값을 갖는거 같아요. . plt.hist(train[&#39;trip_duration&#39;].values, bins=100) plt.xlabel(&#39;trip_duration&#39;) plt.ylabel(&#39;number of train records&#39;) plt.show() . 히스토그램으로 확인하니 그렇습니다. 우측 꼬리가 긴 모양으로 로그변환이 필요해보입니다. . train[&#39;log_trip_duration&#39;] = np.log(train[&#39;trip_duration&#39;].values + 1) plt.hist(train[&#39;log_trip_duration&#39;].values, bins=100) plt.xlabel(&#39;log(trip_duration)&#39;) plt.ylabel(&#39;number of train records&#39;) plt.show() sns.distplot(train[&quot;log_trip_duration&quot;], bins =100) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f99495dd450&gt; . 확실히 그래프 모양이 괜찮아졌습니다. distplot 함수를 통해 그리기도 하였네요 . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . train = train[train[&#39;pickup_longitude&#39;] &lt;= -73.75] train = train[train[&#39;pickup_longitude&#39;] &gt;= -74.03] train = train[train[&#39;pickup_latitude&#39;] &lt;= 40.85] train = train[train[&#39;pickup_latitude&#39;] &gt;= 40.63] train = train[train[&#39;dropoff_longitude&#39;] &lt;= -73.75] train = train[train[&#39;dropoff_longitude&#39;] &gt;= -74.03] train = train[train[&#39;dropoff_latitude&#39;] &lt;= 40.85] train = train[train[&#39;dropoff_latitude&#39;] &gt;= 40.63] . 뉴욕의 위도는 (-74.03, -73.75) 경도는 (40.63, 40.85) 사이 입니다. . 이 값을 벗어나는 위도/경도 데이터를 제거하겠습니다. . train[&#39;pickup_datetime&#39;] = pd.to_datetime(train.pickup_datetime) test[&#39;pickup_datetime&#39;] = pd.to_datetime(test.pickup_datetime) train.loc[:, &#39;pickup_date&#39;] = train[&#39;pickup_datetime&#39;].dt.date test.loc[:, &#39;pickup_date&#39;] = test[&#39;pickup_datetime&#39;].dt.date train[&#39;dropoff_datetime&#39;] = pd.to_datetime(train.dropoff_datetime) #Not in Test . to_datetime 함수로 datetime 변수로 바궈주었습니다. . plt.plot(train.groupby(&#39;pickup_date&#39;).count()[[&#39;id&#39;]], &#39;o-&#39;, label=&#39;train&#39;) plt.plot(test.groupby(&#39;pickup_date&#39;).count()[[&#39;id&#39;]], &#39;o-&#39;, label=&#39;test&#39;) plt.title(&#39;Trips over Time.&#39;) plt.legend(loc=0) plt.ylabel(&#39;Trips&#39;) plt.show() . 트레인과 테스트 데이터를 같이 그리니 유사한 측면을 발견하기가 쉬운것 같아요. . 1월 하순경 이동횟수가 급격하게 감소한것이 관찰됩니다. 또 5월 하순경 감소세가 또 관찰됩니다. . 계절적으로 추운것도 있겠지만 작성자는 다른 요인이 있지 않을까 생각하네요. . import warnings warnings.filterwarnings(&quot;ignore&quot;) plot_vendor = train.groupby(&#39;vendor_id&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=800) plt.ylim(ymax=840) sns.barplot(plot_vendor.index,plot_vendor.values) plt.title(&#39;Time per Vendor&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) . No handles with labels found to put in legend. . Text(0, 0.5, &#39;Time in Seconds&#39;) . 범위를 800~840으로 두어서 그렇지 두 vendor 간 큰 차이를 보이진 않습니다. . snwflag = train.groupby(&#39;store_and_fwd_flag&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=0) plt.ylim(ymax=1100) plt.title(&#39;Time per store_and_fwd_flag&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) sns.barplot(snwflag.index,snwflag.values) . No handles with labels found to put in legend. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f993c2c18d0&gt; . 공급업체에 보내기 전 기록이 잘 저장되었는지 나타내는 변수로 꽤 많이 차이가 납니다. . 작성자는 일부 직원이 이동시간을 정확히 기록하지 못해 발생하는 왜곡이라고 말합니다. . pc = train.groupby(&#39;passenger_count&#39;)[&#39;trip_duration&#39;].mean() plt.subplots(1,1,figsize=(17,10)) plt.ylim(ymin=0) plt.ylim(ymax=1100) plt.title(&#39;Time per store_and_fwd_flag&#39;) plt.legend(loc=0) plt.ylabel(&#39;Time in Seconds&#39;) sns.barplot(pc.index,pc.values) . No handles with labels found to put in legend. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f993c2b0550&gt; . 승객 수는 뚜렷한 여행을 주지 못합니다. . 승객을 아무도 태우지 않았는데 4분정도 이동한 것은 직원의 실수로 보입니다. . train.groupby(&#39;passenger_count&#39;).size() . passenger_count 0 52 1 1018715 2 206864 3 58989 4 27957 5 76912 6 47639 dtype: int64 . &#50948;&#52824; &#45936;&#51060;&#53552; . city_long_border = (-74.03, -73.75) city_lat_border = (40.63, 40.85) fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True) ax[0].scatter(train[&#39;pickup_longitude&#39;].values[:100000], train[&#39;pickup_latitude&#39;].values[:100000], color=&#39;blue&#39;, s=1, label=&#39;train&#39;, alpha=0.1) ax[1].scatter(test[&#39;pickup_longitude&#39;].values[:100000], test[&#39;pickup_latitude&#39;].values[:100000], color=&#39;green&#39;, s=1, label=&#39;test&#39;, alpha=0.1) fig.suptitle(&#39;Train and test area complete overlap.&#39;) ax[0].legend(loc=0) ax[0].set_ylabel(&#39;latitude&#39;) ax[0].set_xlabel(&#39;longitude&#39;) ax[1].set_xlabel(&#39;longitude&#39;) ax[1].legend(loc=0) plt.ylim(city_lat_border) plt.xlim(city_long_border) plt.show() . 자세한 코드 관찰은 위치 데이터 분석을 할때 다시 확인하겠습니다. . train, test 간 위치 데이터가 매우 유사함을 알 수 있습니다. . def haversine_array(lat1, lng1, lat2, lng2): lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) AVG_EARTH_RADIUS = 6371 # in km lat = lat2 - lat1 lng = lng2 - lng1 d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) return h def dummy_manhattan_distance(lat1, lng1, lat2, lng2): a = haversine_array(lat1, lng1, lat1, lng2) b = haversine_array(lat1, lng1, lat2, lng1) return a + b def bearing_array(lat1, lng1, lat2, lng2): AVG_EARTH_RADIUS = 6371 # in km lng_delta_rad = np.radians(lng2 - lng1) lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) y = np.sin(lng_delta_rad) * np.cos(lat2) x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) return np.degrees(np.arctan2(y, x)) . train.loc[:, &#39;distance_haversine&#39;] = haversine_array(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;distance_haversine&#39;] = haversine_array(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) train.loc[:, &#39;distance_dummy_manhattan&#39;] = dummy_manhattan_distance(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;distance_dummy_manhattan&#39;] = dummy_manhattan_distance(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) train.loc[:, &#39;direction&#39;] = bearing_array(train[&#39;pickup_latitude&#39;].values, train[&#39;pickup_longitude&#39;].values, train[&#39;dropoff_latitude&#39;].values, train[&#39;dropoff_longitude&#39;].values) test.loc[:, &#39;direction&#39;] = bearing_array(test[&#39;pickup_latitude&#39;].values, test[&#39;pickup_longitude&#39;].values, test[&#39;dropoff_latitude&#39;].values, test[&#39;dropoff_longitude&#39;].values) . 위도/경도를 활용하여 다양한 관측값을 나타내는 함수입니다. . 이해하기에 조금 벅차서 일단 다양한 변수를 추가해줄수 있구나 하고 넘어갔네요. . coords = np.vstack((train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]].values, train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]].values)) sample_ind = np.random.permutation(len(coords))[:500000] kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind]) train.loc[:, &#39;pickup_cluster&#39;] = kmeans.predict(train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]]) train.loc[:, &#39;dropoff_cluster&#39;] = kmeans.predict(train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]]) test.loc[:, &#39;pickup_cluster&#39;] = kmeans.predict(test[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]]) test.loc[:, &#39;dropoff_cluster&#39;] = kmeans.predict(test[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]]) . np.vstack는 데이터를 묶어주는 함수입니다. . 위도, 경도 데이터를 클러스트로 묶어주었습니다. . fig, ax = plt.subplots(ncols=1, nrows=1) ax.scatter(train.pickup_longitude.values[:500000], train.pickup_latitude.values[:500000], s=10, lw=0, c=train.pickup_cluster[:500000].values, cmap=&#39;autumn&#39;, alpha=0.2) ax.set_xlim(city_long_border) ax.set_ylim(city_lat_border) ax.set_xlabel(&#39;Longitude&#39;) ax.set_ylabel(&#39;Latitude&#39;) plt.show() . 군집화가 잘된 것을 시각적으로 확인하였습니다. . &#45216;&#51676; &#45936;&#51060;&#53552; . train[&#39;Month&#39;] = train[&#39;pickup_datetime&#39;].dt.month test[&#39;Month&#39;] = test[&#39;pickup_datetime&#39;].dt.month train[&#39;DayofMonth&#39;] = train[&#39;pickup_datetime&#39;].dt.day test[&#39;DayofMonth&#39;] = test[&#39;pickup_datetime&#39;].dt.day train[&#39;Hour&#39;] = train[&#39;pickup_datetime&#39;].dt.hour test[&#39;Hour&#39;] = test[&#39;pickup_datetime&#39;].dt.hour train[&#39;dayofweek&#39;] = train[&#39;pickup_datetime&#39;].dt.dayofweek test[&#39;dayofweek&#39;] = test[&#39;pickup_datetime&#39;].dt.dayofweek . 픽업된 시간으로 다양한 파생 날짜/시간 데이터를 생성한 모습입니다. datetime 변수이기에 가능한 모습입니다. . 여기서 dayofweek 변수는 요일변수로 0을 일요일로 생각하여 6을 토요일까지 쓰는 변수입니다. . train.loc[:, &#39;avg_speed_h&#39;] = 1000 * train[&#39;distance_haversine&#39;] / train[&#39;trip_duration&#39;] train.loc[:, &#39;avg_speed_m&#39;] = 1000 * train[&#39;distance_dummy_manhattan&#39;] / train[&#39;trip_duration&#39;] fig, ax = plt.subplots(ncols=3, sharey=True) ax[0].plot(train.groupby(&#39;Hour&#39;).mean()[&#39;avg_speed_h&#39;], &#39;bo-&#39;, lw=2, alpha=0.7) ax[1].plot(train.groupby(&#39;dayofweek&#39;).mean()[&#39;avg_speed_h&#39;], &#39;go-&#39;, lw=2, alpha=0.7) ax[2].plot(train.groupby(&#39;Month&#39;).mean()[&#39;avg_speed_h&#39;], &#39;ro-&#39;, lw=2, alpha=0.7) ax[0].set_xlabel(&#39;Hour of Day&#39;) ax[1].set_xlabel(&#39;Day of Week&#39;) ax[2].set_xlabel(&#39;Month of Year&#39;) ax[0].set_ylabel(&#39;Average Speed&#39;) fig.suptitle(&#39;Average Traffic Speed by Date-part&#39;) plt.show() . 정확히 이해하진 못했지만 distance_haversine가 위치 변수를 보고 만든 거리 변수입니다. . 그렇기 때문에 거리 / 시간 = 평균속도 변수를 만들었습니다. 이 평균속도를 시각/요일/달 별로 얼마나 다른지 시각화했습니다. . 물론 분모인 시간이 반응변수 이기 때문에 분석에 사용할수는 없습니다. . 보통 오전 5시~9시, 오후 5시(17시) ~ 7시(19시) 사이가 가장 도로가 혼잡해 속도가 떨어집니다. . 예상과 어느정도 일치하면서도 출/퇴근 이외 근무시간도 속도가 출/퇴근 시간과 비슷하게 떨어집니다. . 또 금토일의 평균속도가 상대적으로 빠르며 달별로는 겨울의 평균속도가 빠릅니다. . &#50896;&#54635;&#51064;&#53076;&#46377; . vendor_train = pd.get_dummies(train[&#39;vendor_id&#39;], prefix=&#39;vi&#39;, prefix_sep=&#39;_&#39;) vendor_test = pd.get_dummies(test[&#39;vendor_id&#39;], prefix=&#39;vi&#39;, prefix_sep=&#39;_&#39;) passenger_count_train = pd.get_dummies(train[&#39;passenger_count&#39;], prefix=&#39;pc&#39;, prefix_sep=&#39;_&#39;) passenger_count_test = pd.get_dummies(test[&#39;passenger_count&#39;], prefix=&#39;pc&#39;, prefix_sep=&#39;_&#39;) store_and_fwd_flag_train = pd.get_dummies(train[&#39;store_and_fwd_flag&#39;], prefix=&#39;sf&#39;, prefix_sep=&#39;_&#39;) store_and_fwd_flag_test = pd.get_dummies(test[&#39;store_and_fwd_flag&#39;], prefix=&#39;sf&#39;, prefix_sep=&#39;_&#39;) cluster_pickup_train = pd.get_dummies(train[&#39;pickup_cluster&#39;], prefix=&#39;p&#39;, prefix_sep=&#39;_&#39;) cluster_pickup_test = pd.get_dummies(test[&#39;pickup_cluster&#39;], prefix=&#39;p&#39;, prefix_sep=&#39;_&#39;) cluster_dropoff_train = pd.get_dummies(train[&#39;dropoff_cluster&#39;], prefix=&#39;d&#39;, prefix_sep=&#39;_&#39;) cluster_dropoff_test = pd.get_dummies(test[&#39;dropoff_cluster&#39;], prefix=&#39;d&#39;, prefix_sep=&#39;_&#39;) month_train = pd.get_dummies(train[&#39;Month&#39;], prefix=&#39;m&#39;, prefix_sep=&#39;_&#39;) month_test = pd.get_dummies(test[&#39;Month&#39;], prefix=&#39;m&#39;, prefix_sep=&#39;_&#39;) dom_train = pd.get_dummies(train[&#39;DayofMonth&#39;], prefix=&#39;dom&#39;, prefix_sep=&#39;_&#39;) dom_test = pd.get_dummies(test[&#39;DayofMonth&#39;], prefix=&#39;dom&#39;, prefix_sep=&#39;_&#39;) hour_train = pd.get_dummies(train[&#39;Hour&#39;], prefix=&#39;h&#39;, prefix_sep=&#39;_&#39;) hour_test = pd.get_dummies(test[&#39;Hour&#39;], prefix=&#39;h&#39;, prefix_sep=&#39;_&#39;) dow_train = pd.get_dummies(train[&#39;dayofweek&#39;], prefix=&#39;dow&#39;, prefix_sep=&#39;_&#39;) dow_test = pd.get_dummies(test[&#39;dayofweek&#39;], prefix=&#39;dow&#39;, prefix_sep=&#39;_&#39;) . 범주형 변수들을 전부 원핫인코딩을 했습니다. . prefix 와 prefix_sep 으로 원핫인코딩 변수 이름도 설정할수 있네요. . passenger_count_test = passenger_count_test.drop(&#39;pc_9&#39;, axis = 1) . 다만 9명이 탑승한 2건은 표본이 너무 적어 과적합될수도 있고 직관적으로도 말이 안되서 열을 삭제합니다. . train = train.drop([&#39;id&#39;,&#39;vendor_id&#39;,&#39;passenger_count&#39;,&#39;store_and_fwd_flag&#39;,&#39;Month&#39;,&#39;DayofMonth&#39;,&#39;Hour&#39;,&#39;dayofweek&#39;,&#39;pickup_datetime&#39;, &#39;pickup_date&#39;,&#39;pickup_longitude&#39;,&#39;pickup_latitude&#39;,&#39;dropoff_longitude&#39;,&#39;dropoff_latitude&#39;],axis = 1) Test_id = test[&#39;id&#39;] test = test.drop([&#39;id&#39;,&#39;vendor_id&#39;,&#39;passenger_count&#39;,&#39;store_and_fwd_flag&#39;,&#39;Month&#39;,&#39;DayofMonth&#39;,&#39;Hour&#39;,&#39;dayofweek&#39;, &#39;pickup_datetime&#39;, &#39;pickup_date&#39;, &#39;pickup_longitude&#39;,&#39;pickup_latitude&#39;,&#39;dropoff_longitude&#39;,&#39;dropoff_latitude&#39;], axis = 1) train = train.drop([&#39;dropoff_datetime&#39;,&#39;avg_speed_h&#39;,&#39;avg_speed_m&#39;,&#39;trip_duration&#39;], axis = 1) . 원핫인코딩 된 변수들, 시각화를 위해 만들었던 변수들, 변환한 변수들, id 등 필요없는 변수를 제거합니다. . Train_Master = pd.concat([train, vendor_train, passenger_count_train, store_and_fwd_flag_train, cluster_pickup_train, cluster_dropoff_train, month_train, dom_train, hour_test, dow_train ], axis=1) Test_master = pd.concat([test, vendor_test, passenger_count_test, store_and_fwd_flag_test, cluster_pickup_test, cluster_dropoff_test, month_test, dom_test, hour_test, dow_test], axis=1) Train_Master.shape,Test_master.shape . ((1446345, 285), (625134, 284)) . 원핫인코딩했던 변수들을 합쳐줍니다. . &#47784;&#45944; &#51201;&#54633; . X_train = Train_Master.drop([&#39;log_trip_duration&#39;], axis=1) Y_train = Train_Master[&quot;log_trip_duration&quot;] Y_train = Y_train.reset_index().drop(&#39;index&#39;,axis = 1) . 이 코드 이후로 모델적합을 해야하는데 코랩에서 계속 램이 부족하다고 하네요. . 데이터도 크고, 열 개수도 원핫인코딩으로 늘려서 그런거 같습니다. . XGB였다가 LGB로 바꾸고, 노말모델로 하고 어떻게 해도 계속 램이 부족해서 실행이 안되네요. . 코드를 리뷰하는 목적이고 요즘 시간이 넉넉하지 못해서 여기까지 하겠습니다. . from lightgbm import LGBMRegressor model = LGBMRegressor() model.fit(X_train, Y_train) . pred = model.predict(Test_master) pred = np.exp(pred) submission = pd.concat([Test_id, pd.DataFrame(pred)], axis=1) submission.columns = [&#39;id&#39;,&#39;trip_duration&#39;] submission[&#39;trip_duration&#39;] = submission.apply(lambda x : 1 if (x[&#39;trip_duration&#39;] &lt;= 0) else x[&#39;trip_duration&#39;], axis = 1) submission.to_csv(&quot;./submission.csv&quot;, index=False) . !kaggle competitions submit -c nyc-taxi-trip-duration -f submission.csv -m &quot;Message&quot; . &#45712;&#45184;&#51216; . 우선 스스로 고른 데이터인데 위도, 경도를 이용한 데이터여서 조금 어려웠습니다. . 주에 하나씩 코드 리뷰를 하는데 열심히 하면 위치 데이터를 이해하는데 큰 도움이 되겠지만 당장 필요한 기술이 아니라 넘어갔네요. . 한것만 보면 크게 고생한거 같진 않지만, 너무 복잡한 코드들이 많아서 어느정도 했다가 어려워서 처음부터 다시 세번정도 한거 같습니다. . 그래도 남의 코드를 보면서 참 많은걸 배우네요. 시간이 생각보다 많이 들긴 했는데, 그 만큼 배워가는게 있는거 같아요. . 여기에 못담고 지운 코드들 중에도 배운 코드가 많아요. 예를 들어 판다스 옵션을 건드는 코드? . 데이콘 대회에도 도움이 될 거 같습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/datetime/scale/location/regression/2021/10/07/kagglestudy3.html",
            "relUrl": "/ssuda/jupyter/kaggle/datetime/scale/location/regression/2021/10/07/kagglestudy3.html",
            "date": " • Oct 7, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "[머신러닝 가이드] 6-1 주성분 분석(PCA)",
            "content": ". &#48531;&#44867; &#45936;&#51060;&#53552; . from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;] irisDF = pd.DataFrame(iris.data, columns = columns) irisDF[&#39;target&#39;] = iris.target irisDF.head(3) . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . markers = [&#39;^&#39;,&#39;s&#39;,&#39;o&#39;] for i, marker in enumerate(markers): x_axis_data = irisDF[irisDF[&#39;target&#39;] == i][&#39;sepal_length&#39;] y_axis_data = irisDF[irisDF[&#39;target&#39;] == i][&#39;sepal_width&#39;] plt.scatter(x_axis_data, y_axis_data, marker = marker, label = iris.target_names[i]) plt.legend() plt.xlabel(&#39;sepal length&#39;) plt.ylabel(&#39;sepal width&#39;) plt.show() . 길이를 x축 너비를 y축으로, 도형으로 붓꽃 데이터를 구분했습니다. . 파란색 데이터는 y축값 3이상, x축값 6이하인 곳에 일정하게 분포돼 있습니다. . 노란색과 초록색 데이터는 이 두 특성으로 구분하기 힘듭니다. . from sklearn.preprocessing import StandardScaler iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:,:-1]) . 타겟 값을 제외한 모든 특성을 표준 정규 분포를 따르게 변환했습니다. . PCA방법은 특성의 스케일에 영향을 받기 때문에 동일한 스케일로 변환하는 것이 필수입니다. . from sklearn.decomposition import PCA pca = PCA(n_components = 2) pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) print(iris_pca.shape) . (150, 2) . 4차원 데이터를 2차원 PCA 데이터로 변환하였습니다. . pca_columns = [&#39;pca_component_1&#39;, &#39;pca_component_2&#39;] irisDF_pca = pd.DataFrame(iris_pca, columns = pca_columns) irisDF_pca[&#39;target&#39;] = iris.target irisDF_pca.head(3) . pca_component_1 pca_component_2 target . 0 -2.264703 | 0.480027 | 0 | . 1 -2.080961 | -0.674134 | 0 | . 2 -2.364229 | -0.341908 | 0 | . 만들어진 PCA 특성 값으로 데이터 프레임을 만들었습니다. . markers = [&#39;^&#39;,&#39;s&#39;,&#39;o&#39;] for i, marker in enumerate(markers): x_axis_data = irisDF_pca[irisDF[&#39;target&#39;] == i][&#39;pca_component_1&#39;] y_axis_data = irisDF_pca[irisDF[&#39;target&#39;] == i][&#39;pca_component_2&#39;] plt.scatter(x_axis_data, y_axis_data, marker = marker, label = iris.target_names[i]) plt.legend() plt.xlabel(&#39;pca_component_1&#39;) plt.ylabel(&#39;pca_component_2&#39;) plt.show() . 두 개의 pca 특성 값으로 노란색과 초록색 데이터 까지 분류가 가능해집니다. . 사실 두 개의 pca 특성 값에 네 개의 특성값이 섞여있다고 볼 수 있는데요. . 삼차원 이상에 데이터는 시각화 하기 힘들기 때문에 이렇게 시각화 할 수 있는것이 pca분석의 장점이라고 할 수 있습니다. . print(pca.explained_variance_ratio_) . [0.72962445 0.22850762] . explained_varianceratio 값은 변환 된 특성이 얼마나 변동을 설명하는 가를 보여쥽니다. . 두 개의 pca 특성이 약 95% 정도에 변동을 설명하고 있습니다. . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np rcf = RandomForestClassifier(random_state = 156) scores = cross_val_score(rcf, iris.data, iris.target, scoring = &#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도 :&#39;, scores) print(&#39;평균 정확도 :&#39;, np.mean(scores)) . 개별 정확도 : [0.98 0.94 0.96] 평균 정확도 : 0.96 . 기존 4차원 데이터를 랜덤포레스트 기법을 이용해서 검정했습니다. . 평균 정확도는 약 96%가 나옵니다. . pca_x = irisDF_pca[[&#39;pca_component_1&#39;,&#39;pca_component_2&#39;]] scores_pca = cross_val_score(rcf, pca_x, iris.target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도 :&#39;, scores_pca) print(&#39;평균 정확도 :&#39;, np.mean(scores_pca)) . 개별 정확도 : [0.88 0.88 0.88] 평균 정확도 : 0.88 . PCA기법으로 변환한 데이터를 통해 분석한 결과, 평균 정확도는 약 88%가 나옵니다. . 성능이 다소 감소했다고도 볼 수 있습니다. . 하지만 특성 수가 절반이 된 걸 생각해보면 원본 데이터의 특성을 상당부분 잘 유지하고 있다고도 볼 수 있습니다. . &#49888;&#50857;&#52852;&#46300; &#44256;&#44061; &#45936;&#51060;&#53552; &#49464;&#53944; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd df = pd.read_excel(&#39;/content/drive/MyDrive/credit_card.xls&#39;, header = 1, sheet_name=&#39;Data&#39;).iloc[0:,1:] print(df.shape) df.head(3) . (30000, 24) . LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | 3913 | 3102 | 689 | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | 2682 | 1725 | 2682 | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 29239 | 14027 | 13559 | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 24개의 특성과 3만개의 데이터가 있습니다. . df.rename(columns={&#39;PAY_0&#39;:&#39;PAY_1&#39;, &#39;default payment next month&#39;:&#39;default&#39;}, inplace=True) y_target = df[&#39;default&#39;] x_features = df.drop(&#39;default&#39;, axis = 1) . pay_0 다음 pay_2 칼럼이 있어서 pay_1로 이름 변경했습니다. . default.. 칼럼도 길어서 짧게 바꿨습니다. . import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline corr = x_features.corr() plt.figure(figsize = (14,14)) sns.heatmap(corr, annot=True, fmt = &#39;.1g&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f76d75da490&gt; . 상관계수 행렬을 관찰해본 결과 PAY 변수끼리, 또 BILL 변수 끼리 상관계수가 매우 높은 것을 알 수 있습니다. . 다중공선성 등 상당부분 문제가 있기 때문에 PCA 방법으로 조정해보겠습니다. . from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler cols_bill = [&#39;BILL_AMT&#39;+str(i) for i in range(1,7)] print(&#39;대상 속성명:&#39;, cols_bill) scaler = StandardScaler() df_cols_scaled = scaler.fit_transform(x_features[cols_bill]) pca = PCA(n_components = 2) pca.fit(df_cols_scaled) print(&#39;변동성:&#39;, pca.explained_variance_ratio_) . 대상 속성명: [&#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;] 변동성: [0.90555253 0.0509867 ] . 단 두 개의 pca 특성으로 변동성을 95프로이상 설명할 수 있습니다. . import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score rcf = RandomForestClassifier(n_estimators = 300, random_state = 156) scores = cross_val_score(rcf, x_features, y_target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도:&#39;, scores) print(&#39;평균 정확도:&#39;, np.mean(scores)) . 개별 정확도: [0.8083 0.8196 0.8232] 평균 정확도: 0.8170333333333333 . 원본 데이터를 그대로 적용했을 때 정확도 입니다. . scaler = StandardScaler() df_scaled = scaler.fit_transform(x_features) pca = PCA(n_components = 6) df_pca = pca.fit_transform(df_scaled) scores_pca = cross_val_score(rcf, df_pca, y_target, scoring=&#39;accuracy&#39;, cv = 3) print(&#39;개별 정확도:&#39;, scores_pca) print(&#39;평균 정확도:&#39;, np.mean(scores_pca)) . 개별 정확도: [0.7924 0.7969 0.8012] 평균 정확도: 0.7968333333333334 . 전체 23개의 속성중 6개 속성만 이용했음에도 정확도가 원본 데이터 대비 크게 떨어지지 않습니다. . 이 기법은 최근 컴퓨터 비전 분야에 많이 쓰입니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/pca/scale/randomforest/2021/10/06/PythonMachine6_1.html",
            "relUrl": "/book/jupyter/guide/pca/scale/randomforest/2021/10/06/PythonMachine6_1.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "[머신러닝 가이드] 5-4 실전분석(자전거 대여 수요 예측)",
            "content": ". &#52880;&#44544;&#50640;&#49436; &#45936;&#51060;&#53552; &#51649;&#51217; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;0c820de52cea65ec11954012ef8b00d2&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ # Permission Warning이 발생하지 않도록 해줍니다. !chmod 600 ~/.kaggle/kaggle.json . ! kaggle competitions download -c bike-sharing-demand . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sampleSubmission.csv to /content 0% 0.00/140k [00:00&lt;?, ?B/s] 100% 140k/140k [00:00&lt;00:00, 50.8MB/s] Downloading test.csv to /content 0% 0.00/316k [00:00&lt;?, ?B/s] 100% 316k/316k [00:00&lt;00:00, 44.9MB/s] Downloading train.csv to /content 0% 0.00/633k [00:00&lt;?, ?B/s] 100% 633k/633k [00:00&lt;00:00, 40.4MB/s] . &#45936;&#51060;&#53552; &#46168;&#47084;&#48372;&#44592; &#48143; &#44032;&#44277; . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;, category = RuntimeWarning) bike_df = pd.read_csv(&#39;./train.csv&#39;) print(bike_df.shape) bike_df.head() . (10886, 12) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . 변수는 11개, 10886개 데이터가 있습니다. datetime변수는 가공이 필요합니다. . bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB . 결측값은 없습니다. . bike_df[&#39;datetime&#39;] = bike_df.datetime.apply(pd.to_datetime) bike_df[&#39;year&#39;] = bike_df.datetime.apply(lambda x : x.year) bike_df[&#39;month&#39;] = bike_df.datetime.apply(lambda x : x.month) bike_df[&#39;day&#39;] = bike_df.datetime.apply(lambda x : x.day) bike_df[&#39;hour&#39;] = bike_df.datetime.apply(lambda x : x.hour) bike_df.head(3) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | 2011 | 1 | 1 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | 2011 | 1 | 1 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | 2011 | 1 | 1 | 2 | . pd.to_datetime 함수를 통해 데이터 타임을 datetime으로 바꿨습니다. . datetime 데이터 타입은 year, month 등등으로 구분할 수 있습니다. . 이를 활용하여 년, 달, 날, 시간 변수로 각각 생성하였습니다. . drop_columns = [&#39;datetime&#39;, &#39;casual&#39;, &#39;registered&#39;] bike_df.drop(drop_columns, axis = 1, inplace = True) . datetime 변수는 분해를 했기 때문에 원본 변수가 필요 없어졌습니다. . casual + registered = count 변수 이므로 두 변수 모두 제외하겠습니다. . from sklearn.metrics import mean_squared_error, mean_absolute_error def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle def rmse(y, pred): return np.sqrt(mean_squared_error(y, pred)) def evaluate_regr(y, pred): rmsle_val = rmsle(y, pred) rmse_val = rmse(y, pred) mae_val = mean_absolute_error(y, pred) print(&#39;rmsle :&#39;, np.round(rmsle_val, 4), &#39;rmse :&#39;, np.round(rmse_val, 4), &#39;mse :&#39;, np.round(mae_val, 4)) . 이번 분석의 성능 평가 방법은 rmsle 이기 때문에 이를 구현했습니다. . &#52395;&#48264;&#51704; &#48516;&#49437; . from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso y_target = bike_df[&#39;count&#39;] x_features = bike_df.drop([&#39;count&#39;], axis = 1, inplace = False) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target, test_size = 0.3, random_state = 0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) evaluate_regr(y_test, pred) . rmsle : 1.1647 rmse : 140.8996 mse : 105.9244 . 실제 타겟 값이 대여 횟수임으로 지금 rmse 값은 매우 크다고 볼 수 있습니다. . def get_top_error_data(y_test, pred, n_tops = 5): result_df = pd.DataFrame(y_test.values, columns=[&#39;real_count&#39;]) result_df[&#39;predicted_count&#39;] = np.round(pred) result_df[&#39;diff&#39;] = np.abs(result_df[&#39;real_count&#39;] - result_df[&#39;predicted_count&#39;]) print(result_df.sort_values(&#39;diff&#39;, ascending= False)[:n_tops]) get_top_error_data(y_test, pred) . real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 . 실제값과 예측값이 가장 차이가 큰 5개 데이터를 출력했습니다. . 상당히 차이가 많이 나는걸 볼 수 있는데요. . 타겟값의 분포가 치우쳐 있는지 확인을 해볼 필요가 있겠습니다. . y_target.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c8fd090&gt; . 오른쪽 꼬리가 매우 두터운 형태임을 알 수 있습니다. . 이런 형태일 때 가장 자주 쓰이는 로그변환을 적용해보겠습니다. . y_log_transform = np.log1p(y_target) y_log_transform.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c792690&gt; . 정규분포와는 다소 차이가 있지만 변환 전보다 왜곡 정도가 많이 개선됐습니다. . y_target_log = np.log1p(y_target) x_train, x_test, y_train, y_test = train_test_split(x_features, y_target_log, test_size= 0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(x_train, y_train) pred = lr_reg.predict(x_test) y_test_exp = np.expm1(y_test) pred_exp = np.expm1(pred) evaluate_regr(y_test_exp, pred_exp) . rmsle : 1.0168 rmse : 162.5943 mse : 109.2862 . mse 값은 전보다 개선 되었지만 rmse 값은 더 증가하였습니다. . 무슨 이유일까요? . &#46160;&#48264;&#51704; &#48516;&#49437; . coef = pd.Series(lr_reg.coef_, index=x_features.columns) coef_sort = coef.sort_values(ascending = False) sns.barplot(x=coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c1f0990&gt; . 다른 값에 비해 year값이 높습니다. . year값은 년도인데 년도가 이렇게 큰 영향을 미치는 것을 일반적인 사실로 받아들이기 힘듭니다. . 이유를 추정해보자면 연도 변수의 값이 큰 점을 들 수 있습니다.(2011,2012) . 비슷한 이유로 범주형 변수로 변환할 필요가 있는 변수들을 원핫인코딩방식으로 변환하겠습니다. . x_features_ohe = pd.get_dummies(x_features, columns = [&#39;year&#39;, &#39;month&#39;,&#39;day&#39;,&#39;hour&#39;,&#39;holiday&#39;, &#39;workingday&#39;, &#39;season&#39;, &#39;weather&#39;]) x_features_ohe.shape . (10886, 73) . 원핫 인코딩 결과 열 개수가 73개로 크게 늘어났습니다. . x_train, x_test, y_train, y_test = train_test_split(x_features_ohe, y_target_log, test_size= 0.3, random_state=0) def get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = False): model.fit(x_train, y_train) pred = model.predict(x_test) if is_expm1: y_test = np.expm1(y_test) pred = np.expm1(pred) print(model.__class__.__name__) evaluate_regr(y_test, pred) lr_reg = LinearRegression() ridge_reg = Ridge(alpha = 10) lasso_reg = Lasso(alpha = 0.01) for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model, x_train, x_test, y_train, y_test, is_expm1 = True) . LinearRegression rmsle : 0.5896 rmse : 97.6878 mse : 63.3821 Ridge rmsle : 0.5901 rmse : 98.5286 mse : 63.8934 Lasso rmsle : 0.6348 rmse : 113.2188 mse : 72.8027 . 원핫 인코딩을 적용한 후 결과가 눈에 띄게 좋아졌습니다. . coef = pd.Series(lr_reg.coef_, index=x_features_ohe.columns) coef_sort = coef.sort_values(ascending = False)[:20] sns.barplot(x = coef_sort.values, y = coef_sort.index) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f853c772bd0&gt; . 회귀계수가 높은 피처 20개를 출력해보았습니다. . &#49464;&#48264;&#51704; &#48516;&#49437; . from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor rf_reg = RandomForestRegressor(n_estimators = 500) gbm_reg = GradientBoostingRegressor(n_estimators = 500) xgb_reg = XGBRegressor(n_estimaters = 500) lgbm_reg = LGBMRegressor(n_estimaters = 500) for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: get_model_predict(model, x_train.values, x_test.values, y_train.values, y_test.values, is_expm1=True) . RandomForestRegressor rmsle : 0.3549 rmse : 50.2976 mse : 31.1562 GradientBoostingRegressor rmsle : 0.3299 rmse : 53.3352 mse : 32.7448 [16:01:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor rmsle : 0.4828 rmse : 95.6137 mse : 59.2047 LGBMRegressor rmsle : 0.3315 rmse : 51.3807 mse : 31.8325 . 부스팅 모델을 사용하면 더 좋은 성능을 보일 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/datetime/scale/randomforest/boost/regression/2021/10/02/PythonMachine5_4.html",
            "relUrl": "/book/jupyter/guide/datetime/scale/randomforest/boost/regression/2021/10/02/PythonMachine5_4.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "[SSUDA] 자전거 수요 예측 모델",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle (1).json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;ksy1998&#34;,&#34;key&#34;:&#34;0c820de52cea65ec11954012ef8b00d2&#34;}&#39;} . kaggle.json 파일 선택합니다. . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c bike-sharing-demand . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading train.csv to /content 0% 0.00/633k [00:00&lt;?, ?B/s] 100% 633k/633k [00:00&lt;00:00, 42.8MB/s] Downloading sampleSubmission.csv to /content 0% 0.00/140k [00:00&lt;?, ?B/s] 100% 140k/140k [00:00&lt;00:00, 44.0MB/s] Downloading test.csv to /content 0% 0.00/316k [00:00&lt;?, ?B/s] 100% 316k/316k [00:00&lt;00:00, 41.9MB/s] . 캐글에서 복사한 코드에 느낌표만 붙여줍니다. . import sklearn import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import numpy as np from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge ,Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score from sklearn.feature_selection import VarianceThreshold from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures from sklearn.metrics import mean_squared_log_error as msle import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn.ensemble import GradientBoostingRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import GridSearchCV from xgboost import XGBRegressor %matplotlib inline . train=pd.read_csv(&#39;./train.csv&#39;) test=pd.read_csv(&#39;./test.csv&#39;) . &#45936;&#51060;&#53552; &#46168;&#47084;&#48372;&#44592; . train.head() . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . test.head() . datetime season holiday workingday weather temp atemp humidity windspeed . 0 2011-01-20 00:00:00 | 1 | 0 | 1 | 1 | 10.66 | 11.365 | 56 | 26.0027 | . 1 2011-01-20 01:00:00 | 1 | 0 | 1 | 1 | 10.66 | 13.635 | 56 | 0.0000 | . 2 2011-01-20 02:00:00 | 1 | 0 | 1 | 1 | 10.66 | 13.635 | 56 | 0.0000 | . 3 2011-01-20 03:00:00 | 1 | 0 | 1 | 1 | 10.66 | 12.880 | 56 | 11.0014 | . 4 2011-01-20 04:00:00 | 1 | 0 | 1 | 1 | 10.66 | 12.880 | 56 | 11.0014 | . 변수가 3개 차이 나는데, casual + registered = count 변수 입니다. . 테스트 데이터에서는 count를 맞추는것이 목적입니다. . train.drop([&#39;casual&#39;,&#39;registered&#39;],1,inplace=True) . 글쓴이는 이런 이유로 쿨하게 두 변수를 날렸습니다. . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 count 10886 non-null int64 dtypes: float64(3), int64(6), object(1) memory usage: 850.6+ KB . 데이터 10886개, count를 제외한 변수 개수 9개, 결측값은 없습니다. . train.describe() . season holiday workingday weather temp atemp humidity windspeed count . count 10886.000000 | 10886.000000 | 10886.000000 | 10886.000000 | 10886.00000 | 10886.000000 | 10886.000000 | 10886.000000 | 10886.000000 | . mean 2.506614 | 0.028569 | 0.680875 | 1.418427 | 20.23086 | 23.655084 | 61.886460 | 12.799395 | 191.574132 | . std 1.116174 | 0.166599 | 0.466159 | 0.633839 | 7.79159 | 8.474601 | 19.245033 | 8.164537 | 181.144454 | . min 1.000000 | 0.000000 | 0.000000 | 1.000000 | 0.82000 | 0.760000 | 0.000000 | 0.000000 | 1.000000 | . 25% 2.000000 | 0.000000 | 0.000000 | 1.000000 | 13.94000 | 16.665000 | 47.000000 | 7.001500 | 42.000000 | . 50% 3.000000 | 0.000000 | 1.000000 | 1.000000 | 20.50000 | 24.240000 | 62.000000 | 12.998000 | 145.000000 | . 75% 4.000000 | 0.000000 | 1.000000 | 2.000000 | 26.24000 | 31.060000 | 77.000000 | 16.997900 | 284.000000 | . max 4.000000 | 1.000000 | 1.000000 | 4.000000 | 41.00000 | 45.455000 | 100.000000 | 56.996900 | 977.000000 | . 최솟값 또는 최댓값에 이상한 값은 없습니다. . 또 값을 관찰해보면 season, holiday, workingday, weather 변수는 범주형 변수인 것을 알 수 있습니다. . 그리고 datetime 변수는 형태가 특수하여 이 표에 표현되지 않습니다. . datetime &#48320;&#49688;&#50640; &#45824;&#54644; . train[&#39;datetime&#39;] . 0 2011-01-01 00:00:00 1 2011-01-01 01:00:00 2 2011-01-01 02:00:00 3 2011-01-01 03:00:00 4 2011-01-01 04:00:00 ... 10881 2012-12-19 19:00:00 10882 2012-12-19 20:00:00 10883 2012-12-19 21:00:00 10884 2012-12-19 22:00:00 10885 2012-12-19 23:00:00 Name: datetime, Length: 10886, dtype: object . datetime 변수는 날짜 + 시간 변수 입니다. . 데이터 타입은 현재 object 타입인데요. 변경해보겠습니다. . train[&#39;datetime&#39;] = pd.to_datetime(train[&#39;datetime&#39;]) test[&#39;datetime&#39;] = pd.to_datetime(test[&#39;datetime&#39;]) train[&#39;Month&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).month test[&#39;Month&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).month train[&#39;Year&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).year test[&#39;Year&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).year train[&#39;WeekDay&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).weekday test[&#39;WeekDay&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).weekday train[&#39;Hour&#39;]=pd.DatetimeIndex(train[&#39;datetime&#39;]).hour test[&#39;Hour&#39;]=pd.DatetimeIndex(test[&#39;datetime&#39;]).hour train.head(3) . datetime season holiday workingday weather temp atemp humidity windspeed count Month Year WeekDay Hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 16 | 1 | 2011 | 5 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 40 | 1 | 2011 | 5 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 32 | 1 | 2011 | 5 | 2 | . 판다스에 to_* 함수를 통해 데이터 타입을 바꿀 수 있습니다. . 여기서는 to_datetime 함수로 &#39;datetime&#39; 데이터 형으로 바꿨습니다. . 이 데이터 형에 특징은 DatetimeIndex함수를 이용하여 연도/달/일 등을 쉽게 추출할 수 있다는 것 입니다. . &#45936;&#51060;&#53552; &#49548;&#44060; . season =&gt; 계절 변수, 1 : 봄, 2 : 여름, 3 : 가을, 4 : 겨울 . holiday =&gt; 휴일 변수, 날짜가 휴일이면 1 아니면 0 . workingday =&gt; 근무일 변수, 날짜가 주말도 휴일도 아니라면 1 . weather =&gt; 날씨 변수, 맑음이 1 폭우가 4. 흐릴수록 값이 점차 증가. . temp =&gt; 온도, atemp =&gt; 체감온도, humidity =&gt; 습도, windspeed =&gt; 풍속 . categorical_cols=[&#39;season&#39;,&#39;holiday&#39;,&#39;workingday&#39;,&#39;weather&#39;] numerical_cols=[&#39;temp&#39;,&#39;atemp&#39;,&#39;humidity&#39;,&#39;windspeed&#39;] label=&#39;count&#39; . 변수를 범주형 변수와 연속형 변수로 나눴습니다. . &#49884;&#44036; &#45936;&#51060;&#53552; &#44288;&#52769; . def encodetime(train,test,col,label): d3=train[[col,label]].groupby(col).mean() d3.sort_values(by=&#39;count&#39;,ascending=False) plt.scatter(x=d3.index,y=d3[&#39;count&#39;]) d3=d3.sort_values(by=&#39;count&#39;) d3[&#39;w&#39;]=np.arange(train[col].nunique()) #순서 dic=dict(zip(d3.index,d3[&#39;w&#39;])) train[col]=train[col].map(dic) test[col]=test[col].map(dic) . .nunique() =&gt; 유니크한 범주 개수 출력 . dict(zip()) =&gt; 두 시리즈 변수를 튜플로 묶고 딕셔러니 자료형으로 변환 . map =&gt; 주로 함수를 입력하는데, 여기서는 딕셔너리 키 값을 받을때 value 값을 반환해주는 함수로 사용 . 즉 이 함수는 col 변수로 변수가 입력되면 그 변수의 plot를 출력해주고 0~n 까지 레이블 인코딩을 해줍니다. . encodetime(train,test,&#39;Year&#39;,label) . 연도별로 차이가 있습니다. . encodetime(train,test,&#39;Month&#39;,label) . 날씨가 따뜻한 여름 부근에 확실히 값이 큽니다. . encodetime(train,test,&#39;Hour&#39;,label) . 야간시간에 값이 떨어집니다. . encodetime(train,test,&#39;WeekDay&#39;,label) . 0부터 월요일이므로 값이 많이 떨어지는 6은 일요일입니다. . &#48276;&#51452;&#54805; &#51088;&#47308; &#49884;&#44033;&#54868; . def boxplot(x,y,**kwargs): sns.boxplot(x=x,y=y) x=plt.xticks(rotation=90) f=pd.melt(train,id_vars=[&#39;count&#39;],value_vars=categorical_cols) g=sns.FacetGrid(f,col=&#39;variable&#39;,col_wrap=2,sharex=False) g.map(boxplot,&#39;value&#39;,&#39;count&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd4625d22d0&gt; . boxplot 모형입니다. count변수의 이상치가 변수에 상관없이 존재합니다. . 파이썬 문법 : 함수 인자 내 **kwargs에 대해서. . 단순하게 함수 인자 내 *이나 **이 보일 경우 C언어에서 사용하는 포인터 개념이 아닙니다. . 몇개의 인자를 보낼지 모를때 사용되며, **같은 경우 딕셔너리 형태일때 사용합니다. . 다만 이 코드는 너무 복잡해서 지금 실력에서 어떻게 해석을 못하겠습니다. . sns.pairplot(train[[*numerical_cols,&#39;count&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x7fd460faa110&gt; . *numerical_cols은 리스트를 해체한다고 이해하면 편할 것 같아요. . pairplot함수를 통해 연속형 변수 간에 산점도를 한 눈에 볼 수 있습니다. . f, ax = plt.subplots(figsize=(15, 15)) corr = train[[*numerical_cols,&#39;count&#39;]].corr() sns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd461781f50&gt; . heatmap 함수는 아까 본 산점도를 상관계수 버전으로 보여줍니다. . square는 셀을 정사각형으로 출력하는 것, annot은 셀 안에 숫자를 출력해주는 것을 의미합니다. . 여기서는 체감온도와 실제 온도 간 상관계수가 엄청 높은 것이 눈에 띄네요. . f, ax = plt.subplots(figsize=(15, 15)) corr = train.corr() sns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd4602b2650&gt; . 조금 더 확장해서 모든 변수간 상관계수를 살펴보았습니다. . &#48152;&#51025;&#48320;&#49688; &#48320;&#54872; . sns.displot(train[label] , kde=True, height=8.27, aspect=11.7/8.27) sns.displot(np.log(train[label]) , kde=True, height=8.27, aspect=11.7/8.27) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd46015a090&gt; . count 변수를 로그변환 해보았는데요. . 로그변환 전 우측 꼬리가 긴 그래프였는데 변환 후 상대적으로 더 정규분포에 가까워졌습니다. . 다만 왼쪽 꼬리가 다소 길어져 변환 정도를 조절할 필요가 있겠습니다. . def trans(x,l1=0.3,l2=0): if l1!=0: return ((x+l2)**l1-1)/l1 else: return np.log(x+l2) def rev_trans(x,l1=0.3,l2=0): return (x*l1+1)**(1/l1)-l2 z=train[label].apply(trans) sns.displot(z , kde=True, height=8.27, aspect=11.7/8.27) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd45feda3d0&gt; . l2가 0일때 이는 람다가 l1인 box-cox 변환입니다. . 이 변환은 정규분포가 아닌 값을 정규분포형태로 변환합니다. . 그림을 확인해보면 이전 대비 그래프가 확연히 정규분포 형태에 가까운 것을 알 수 있습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . x=train.drop([&#39;count&#39;,&#39;datetime&#39;,&#39;atemp&#39;],1) xtest=test.drop([&#39;datetime&#39;,&#39;atemp&#39;],1) y=train[&#39;count&#39;] xt,xv,yt,yv=train_test_split(x,y,test_size=0.2,random_state=101) . 변수이름을 조금 대충 만들었네요. . 보면서 생각난 점은 데이터 분석 프로젝트롤 여러명이서 할 경우 변수명 통일이 상당히 중요하다는 점 입니다. . def redun(x): return x def mk_model_RF(xt1,xv1,yt,yv,md=None,func1=redun,func2=redun,mss=2,n_est=100,al=0): ytt=yt.apply(func1) yvt=yv.apply(func2) model=RandomForestRegressor(max_depth=md, random_state=0,min_samples_split=mss,n_estimators=n_est,ccp_alpha=al) model.fit(xt1,ytt) ypt=np.apply_along_axis(func2,arr=model.predict(xt1),axis=0) ypv=np.apply_along_axis(func2,arr=model.predict(xv1),axis=0) print(&#39;training r2:&#39;,r2_score(yt,ypt)) print(&#39;Validation r2:&#39;,r2_score(yv,ypv)) print(&#39;training rmsle:&#39;,np.sqrt(msle(yt,ypt))) print(&#39;validation rmsle:&#39;,np.sqrt(msle(yv,ypv))) return model . 함수를 조금 복잡하게 만들었는데 함수 한 개만 소개하겠습니다. . np.apply_along_axis 함수는 인자가 (함수, 어레이, 행/열 여부) 입니다. . apply와 비슷한 역할의 함수인데 넘파이 함수라서 실행속도가 엄청 빠릅니다. . mk_model_RF(xt,xv,yt,yv) . training r2: 0.9922090269471024 Validation r2: 0.9338288545809785 training rmsle: 0.16871805329660905 validation rmsle: 0.379570233664069 . RandomForestRegressor(bootstrap=True, ccp_alpha=0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=0, verbose=0, warm_start=False) . 랜덤 포레스트 방법이네요. . count 변수에 box-cox변환을 하지 않은 결과입니다. . mk_model_RF(xt,xv,yt,yv,func1=trans,func2=rev_trans,n_est=400,md=20) . training r2: 0.9916973216662878 Validation r2: 0.9338712883052291 training rmsle: 0.12704646646266962 validation rmsle: 0.34592422107196 . RandomForestRegressor(bootstrap=True, ccp_alpha=0, criterion=&#39;mse&#39;, max_depth=20, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None, oob_score=False, random_state=0, verbose=0, warm_start=False) . box-cox 변환을 한 결과가 조금 더 좋은 것 같습니다. . def mk_model_xgb(xt,xv,yt,yv,func1=redun,func2=redun,lr=1,min_child_weight =25,colsample_bytree = 0.8,md=None): model =XGBRegressor( colsample_bytree = colsample_bytree, learning_rate = lr,min_child_weight =min_child_weight, max_depth=md ) ytt=yt.apply(func1) model.fit(xt,ytt) ypt=np.apply_along_axis(func2,arr=model.predict(xt),axis=0) ypv=np.apply_along_axis(func2,arr=model.predict(xv),axis=0) print(&#39;training r2:&#39;,r2_score(yt,ypt)) print(&#39;Validation r2:&#39;,r2_score(yv,ypv)) print(&#39;training rmsle:&#39;,np.sqrt(msle(yt,ypt))) print(&#39;validation rmsle:&#39;,np.sqrt(msle(yv,ypv))) return model . mk_model_xgb(xt,xv,yt,yv,func1=trans,func2=rev_trans,lr=0.2,min_child_weight =20,colsample_bytree = 0.8,md=20) . [14:54:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. training r2: 0.9945096180099459 Validation r2: 0.9504273424389956 training rmsle: 0.10444065614264901 validation rmsle: 0.3133324307194445 . XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8, gamma=0, importance_type=&#39;gain&#39;, learning_rate=0.2, max_delta_step=0, max_depth=20, min_child_weight=20, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective=&#39;reg:linear&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) . xgboost 사용 결과 값이 더 좋게 나옵니다. box-cox 변환을 한 결과입니다. . model=XGBRegressor(colsample_bytree = 0.8, learning_rate = 0.2,min_child_weight =20, max_depth=20).fit(x,y.apply(trans)) . [14:55:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . 그래서 box-cox변환 후 xgboost를 사용해 모델을 적합시켰습니다. . &#48516;&#49437; &#44208;&#44284; &#51228;&#52636; . yp=np.apply_along_axis(rev_trans,arr=model.predict(xtest),axis=0) . plt.hist(yp) . (array([2543., 1497., 1057., 606., 367., 184., 125., 69., 34., 11.]), array([7.3024821e-01, 9.8740685e+01, 1.9675111e+02, 2.9476157e+02, 3.9277197e+02, 4.9078241e+02, 5.8879285e+02, 6.8680328e+02, 7.8481372e+02, 8.8282416e+02, 9.8083459e+02], dtype=float32), &lt;a list of 10 Patch objects&gt;) . test[&#39;count&#39;]=yp test[[&#39;datetime&#39;, &#39;count&#39;]].to_csv(&#39;./submission.csv&#39;, index=False) !kaggle competitions submit -c bike-sharing-demand -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 188k/188k [00:01&lt;00:00, 106kB/s] Successfully submitted to Bike Sharing Demand . 별도에 작업 없이 캐글과 연동하여 바로 제출할 수 있습니다. . &#45712;&#45184;&#51216; . 우선 이번 데이터 분석은 저번보다 열심히 한 것 같네요. . 중점으로 두었던 것은 제가 시각화 부분이 많이 부족해서 이 부분 공부해보려고 이번 코드 골랐습니다. . 잊고 있었던 box-cox 정규분포 변환에 대해 다시 떠올리게 되었던 것도 큰 수확인것 같네요. . 이 사람이 코드 설명을 크게 한 것이 없어 찾아보느라도 고생한 것 같습니다. . 결과는 0.41정도로 상위권은 아니지만 노력 대비 어느정도 성과가 있습니다. . 이번 코드 리뷰를 통해서 많이 배운것 같습니다. . 대회 출처 : https://www.kaggle.com/c/bike-sharing-demand . 코드 출처 : https://www.kaggle.com/muhammedmamdouhsalah/bike-sharing .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/randomforest/scale/boxcox/xgboost/regression/2021/10/01/kagglessu3.html",
            "relUrl": "/ssuda/jupyter/kaggle/randomforest/scale/boxcox/xgboost/regression/2021/10/01/kagglessu3.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "[머신러닝 가이드] 5-3 다양한 회귀",
            "content": ". &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression cancer = load_breast_cancer() . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split scaler = StandardScaler() data_scaled = scaler.fit_transform(cancer.data) x_train, x_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size = 0.3, random_state = 0) . 평균이 0, 분산이 1인 정규분포 형태로 형 변환을 했습니다. . 로지스틱 회귀기법은 선형 회귀 방식에 응용으로 데이터의 정규분포도에 영향을 많이 받습니다. . from sklearn.metrics import accuracy_score, roc_auc_score import numpy as np lr_clf = LogisticRegression() lr_clf.fit(x_train, y_train) lr_preds = lr_clf.predict(x_test) print(&#39;정확도 :&#39;, np.round(accuracy_score(y_test, lr_preds), 4)) print(&#39;roc 커브 :&#39;, np.round(roc_auc_score(y_test, lr_preds), 4)) . 정확도 : 0.9766 roc 커브 : 0.9716 . from sklearn.model_selection import GridSearchCV params = {&#39;penalty&#39; : [&#39;l2&#39;, &#39;l1&#39;], &#39;C&#39; : [0.01, 0.1, 1, 5, 10]} grid_clf = GridSearchCV(lr_clf, param_grid = params, scoring = &#39;accuracy&#39;, cv = 3) grid_clf.fit(data_scaled, cancer.target) print(&#39;최적 파라미터 : &#39;, grid_clf.best_params_, &#39;최적 평균 정확도&#39;, grid_clf.best_score_) . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터 : {&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;} 최적 평균 정확도 0.975392184164114 . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . 최적 파라미터는 l2 규제로(릿지 회귀) c가(알파의 역수) 1일때 입니다. . &#53944;&#47532; &#44592;&#48152; &#54924;&#44480; &#47784;&#45944; . from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor import pandas as pd import numpy as np boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) rf = RandomForestRegressor(random_state = 0, n_estimators = 1000) neg_mse_scores = cross_val_score(rf, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse score : &#39;, np.round(neg_mse_scores, 4)) print(&#39;rmse score : &#39;, np.round(rmse_scores, 4)) print(&#39;평균 rmse score : &#39;, np.round(avg_rmse, 4)) . mse score : [ -7.933 -13.0584 -20.5278 -46.3057 -18.8008] rmse score : [2.8166 3.6136 4.5308 6.8048 4.336 ] 평균 rmse score : 4.4204 . 랜덤 포레스트 회귀 입니다. 평균 rmse 값은 4.42로 꽤 좋은 수치 입니다. . def get_model_cv_prediction(model, x_data, y_target): neg_mse_scores = cross_val_score(model, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(model.__class__.__name__) print(&#39;평균 rmse : &#39;, np.round(avg_rmse, 4)) . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor dt_reg = DecisionTreeRegressor(random_state = 0, max_depth = 4) rf_reg = RandomForestRegressor(random_state = 0, n_estimators = 1000) gb_reg = GradientBoostingRegressor(random_state = 0, n_estimators = 1000) xgb_reg = XGBRegressor(random_state = 0, n_estimators = 1000) lgb_reg = LGBMRegressor(random_state = 0, n_estimators = 1000) models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] for model in models: get_model_cv_prediction(model, x_data, y_target) . DecisionTreeRegressor 평균 rmse : 6.2377 RandomForestRegressor 평균 rmse : 4.4204 GradientBoostingRegressor 평균 rmse : 4.2692 [13:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [13:18:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. XGBRegressor 평균 rmse : 4.0889 LGBMRegressor 평균 rmse : 4.6464 . 여러 모델을 테스트 해보았습니다. . xgb부스팅 모델의 성능이 가장 우수하게 나왔습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/logistic/randomforest/boost/regression/2021/09/30/PythonMachine5_3.html",
            "relUrl": "/book/jupyter/guide/logistic/randomforest/boost/regression/2021/09/30/PythonMachine5_3.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "[머신러닝 가이드] 5-2 선형회귀응용",
            "content": ". &#45796;&#54637; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures import numpy as np x = np.arange(4).reshape(2,2) # 행 부터 숫자 채워짐 print(&#39;일차 단항식 계수 피처: n&#39;, x) poly = PolynomialFeatures(degree = 2) poly.fit(x) poly_ftr = poly.transform(x) print(&#39;변환된 2차 다항식 계수 피처: n&#39;, poly_ftr) . 일차 단항식 계수 피처: [[0 1] [2 3]] 변환된 2차 다항식 계수 피처: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] . 2차 다항계수는 [1, x1, x2, x1^2, x1x2, x2^2] 로 구성되어 있습니다. . def polynomial_func(x): y = 1 + 2 * x[:,0] + 3 * x[:,0] **2 + 4 * x[:,1] **3 return y y = polynomial_func(x) . from sklearn.linear_model import LinearRegression poly_ftr = PolynomialFeatures(degree = 3).fit_transform(x) print(&#39;3차 다항식 계수 feature: n&#39;, poly_ftr) model = LinearRegression() model.fit(poly_ftr, y) print(&#39;회귀 계수 n&#39;, np.round(model.coef_,2)) print(&#39;회귀 shape&#39;, model.coef_.shape) . 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] 회귀 shape (10,) . poly함수로 다항식 계수를 생성한 뒤 단순 선형 회귀 함수에 대입해줍니다. . 원하는 값인 [1,2,0,3,0,0,0,0,0,4] 와 다소 차이가 있긴 합니다. . &#47551;&#51648; &#54924;&#44480; . from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline import numpy as np model = Pipeline([(&#39;poly&#39;, PolynomialFeatures(degree=3)), (&#39;linear&#39;, LinearRegression())]) model = model.fit(x,y) print(&#39;회귀 계수 n&#39;, np.round(model.named_steps[&#39;linear&#39;].coef_,2)) . 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] . 파이프 라인 함수로 다항식으로에 변환과 선형 회귀를 한번에 한 모습입니다. . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score from sklearn.datasets import load_boston import pandas as pd boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) ridge = Ridge(alpha = 10) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-11.42 -24.29 -28.14 -74.6 -28.52] rmse scores [3.38 4.93 5.31 8.64 5.34] 평균 rmse score: 5.52 . 단순 선형회귀 모델 rmse 평균값이 5.84로 릿지 회귀가 더 좋은 퍼포먼스를 보입니다. . alphas = [0,0.1,1,10,100] for alpha in alphas: ridge = Ridge(alpha=alpha) neg_mse_scores = cross_val_score(ridge, x_data, y_target, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 5.8287 alpha 값 0.1 일때 평균 rmse : 5.7885 alpha 값 1 일때 평균 rmse : 5.6526 alpha 값 10 일때 평균 rmse : 5.5182 alpha 값 100 일때 평균 rmse : 5.3296 . alpha 값이 100일때가 가장 값이 좋습니다. . import matplotlib.pyplot as plt import seaborn as sns fig, axs = plt.subplots(figsize= (18,6), nrows = 1, ncols = 5) coeff_df = pd.DataFrame() for pos, alpha in enumerate(alphas): ridge = Ridge(alpha = alpha) ridge.fit(x_data, y_target) coeff = pd.Series(data=ridge.coef_, index = x_data.columns) colname = &#39;alpha:&#39;+str(alpha) coeff_df[colname] = coeff coeff = coeff.sort_values(ascending = False) axs[pos].set_title(colname) axs[pos].set_xlim(-3, 6) sns.barplot(x=coeff.values, y = coeff.index, ax = axs[pos]) plt.show() . 알파 값이 커지면(=규제가 세지면) 회귀계수 값이 전반적으로 작아집니다. . 다만 릿지 회귀에 경우 회귀 계수를 0으로 만들지는 않습니다. . &#46972;&#50136; &#54924;&#44480; . from sklearn.linear_model import Lasso, ElasticNet def get_linear_reg_eval(model_name, params = None, x_data_n = None, y_target_n = None, verbose= True, return_coeff = True): coeff_df = pd.DataFrame() if verbose : print(model_name) for param in params: if model_name ==&#39;Ridge&#39; : model = Ridge(alpha = param) elif model_name ==&#39;Lasso&#39; : model = Lasso(alpha = param) elif model_name ==&#39;ElasticNet&#39; : model = ElasticNet(alpha = param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, x_data_n, y_target_n, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores)) print(&#39;alpha &#39;, param, &#39;일때 평균 rmse:&#39;, np.round(avg_rmse,2)) model.fit(x_data_n, y_target_n) if return_coeff: coeff = pd.Series(data=model.coef_, index = x_data_n.columns) colname = &#39;alpha:&#39;+str(param) coeff_df[colname] = coeff return coeff_df . lasso_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;Lasso&#39;,params=lasso_alphas, x_data_n = x_data, y_target_n= y_target) . Lasso alpha 0.07 일때 평균 rmse: 5.61 alpha 0.1 일때 평균 rmse: 5.62 alpha 0.5 일때 평균 rmse: 5.67 alpha 1 일때 평균 rmse: 5.78 alpha 3 일때 평균 rmse: 6.19 . 알파 값이 0.07일때 최고 성능을 보여줍니다. . 앞서 한 릿지보다는 성능이 떨어지지만, 단순 선형 회귀 모델보다 값이 크므로 쓰임새가 있습니다. . sort_column = &#39;alpha:&#39;+str(lasso_alphas[0]) coeff_lasso_df.sort_values(by = sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.789725 | 3.703202 | 2.498212 | 0.949811 | 0.000000 | . CHAS 1.434343 | 0.955190 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.270936 | 0.274707 | 0.277451 | 0.264206 | 0.061864 | . ZN 0.049059 | 0.049211 | 0.049544 | 0.049165 | 0.037231 | . B 0.010248 | 0.010249 | 0.009469 | 0.008247 | 0.006510 | . NOX -0.000000 | -0.000000 | -0.000000 | -0.000000 | 0.000000 | . AGE -0.011706 | -0.010037 | 0.003604 | 0.020910 | 0.042495 | . TAX -0.014290 | -0.014570 | -0.015442 | -0.015212 | -0.008602 | . INDUS -0.042120 | -0.036619 | -0.005253 | -0.000000 | -0.000000 | . CRIM -0.098193 | -0.097894 | -0.083289 | -0.063437 | -0.000000 | . LSTAT -0.560431 | -0.568769 | -0.656290 | -0.761115 | -0.807679 | . PTRATIO -0.765107 | -0.770654 | -0.758752 | -0.722966 | -0.265072 | . DIS -1.176583 | -1.160538 | -0.936605 | -0.668790 | -0.000000 | . 계수가 0인것이 보입니다. 알파값이 커질수록 회귀 계수가 0인 것이 늘어납니다. . &#50648;&#46972;&#49828;&#54001; &#54924;&#44480; . 다음은 엘라스틱 회귀 입니다. 쉽게 라쏘회귀 + 릿지 회귀로 볼 수 있습니다. . 라쏘 회귀에 경우 서로 상관관계가 높은 피처가 있으면 중요 피처를 제외하고 모두 회귀계수를 0으로 만듭니다. . 이를 다소 완화해주기 위한 목적으로 만들어졌습니다. 다만 수행시간이 다소 깁니다. . 여기서 알파는 알파1 + 알파 2 이며, l1_ratio는 말 그대로 l1규제(라쏘) 비율입니다. . elastic_alphas = [0.07,0.1,0.5,1,3] coeff_lasso_df = get_linear_reg_eval(&#39;ElasticNet&#39;, params=elastic_alphas, x_data_n= x_data, y_target_n= y_target) . ElasticNet alpha 0.07 일때 평균 rmse: 5.54 alpha 0.1 일때 평균 rmse: 5.53 alpha 0.5 일때 평균 rmse: 5.47 alpha 1 일때 평균 rmse: 5.6 alpha 3 일때 평균 rmse: 6.07 . 알파값이 0.5일때 가장 좋은 예측 성능을 보여줍니다. . sort_column = &#39;alpha:&#39;+str(elastic_alphas[0]) coeff_lasso_df.sort_values(by= sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.574162 | 3.414154 | 1.918419 | 0.938789 | 0.000000 | . CHAS 1.330724 | 0.979706 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.278880 | 0.283443 | 0.300761 | 0.289299 | 0.146846 | . ZN 0.050107 | 0.050617 | 0.052878 | 0.052136 | 0.038268 | . B 0.010122 | 0.010067 | 0.009114 | 0.008320 | 0.007020 | . AGE -0.010116 | -0.008276 | 0.007760 | 0.020348 | 0.043446 | . TAX -0.014522 | -0.014814 | -0.016046 | -0.016218 | -0.011417 | . INDUS -0.044855 | -0.042719 | -0.023252 | -0.000000 | -0.000000 | . CRIM -0.099468 | -0.099213 | -0.089070 | -0.073577 | -0.019058 | . NOX -0.175072 | -0.000000 | -0.000000 | -0.000000 | -0.000000 | . LSTAT -0.574822 | -0.587702 | -0.693861 | -0.760457 | -0.800368 | . PTRATIO -0.779498 | -0.784725 | -0.790969 | -0.738672 | -0.423065 | . DIS -1.189438 | -1.173647 | -0.975902 | -0.725174 | -0.031208 | . 라쏘모델에 비해 회귀계수를 0으로 만드는 개수가 다소 줄었습니다. . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#51012; &#50948;&#54620; &#45936;&#51060;&#53552; &#48320;&#54872; . 선형 회귀에서 중요한 것 중 하나가 데이터 분포도의 정규화 입니다. . 특히 타깃값의 분포가 정규분포가 아닌 왜곡(skew)된 분포는 예측 성능에 부정적입니다. . 따라서 선형 회귀 모델을 적용하기 전 먼저 데이터 스케일링/정규화 작업을 수행해주어야 합니다. . from sklearn.preprocessing import StandardScaler, MinMaxScaler def get_scaled_data(method=&#39;None&#39;, p_degree = None, input_data = None): if method == &#39;Standard&#39;: scaled_data = StandardScaler().fit_transform(input_data) elif method == &#39;MinMax&#39;: scaled_data = MinMaxScaler().fit_transform(input_data) if method == &#39;Log&#39;: scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data . alphas = [0.1, 1, 10, 100] scale_methods=[(None, None), (&#39;Standard&#39;, None), (&#39;Standard&#39;,2), (&#39;MinMax&#39;,None), (&#39;MinMax&#39;, 2), (&#39;Log&#39;, None)] for scale_method in scale_methods: x_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=x_data) print(&#39; n 변환유형:&#39;, scale_method[0], &#39;, Polynomial Degree:&#39;, scale_method[1]) get_linear_reg_eval(&#39;Ridge&#39;, params = alphas, x_data_n=x_data_scaled, y_target_n= y_target, verbose=False, return_coeff = False) . 변환유형: None , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: Standard , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: MinMax , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 5.79 alpha 1 일때 평균 rmse: 5.65 alpha 10 일때 평균 rmse: 5.52 alpha 100 일때 평균 rmse: 5.33 변환유형: MinMax , Polynomial Degree: 2 alpha 0.1 일때 평균 rmse: 9.14 alpha 1 일때 평균 rmse: 8.94 alpha 10 일때 평균 rmse: 10.56 alpha 100 일때 평균 rmse: 10.57 변환유형: Log , Polynomial Degree: None alpha 0.1 일때 평균 rmse: 4.77 alpha 1 일때 평균 rmse: 4.68 alpha 10 일때 평균 rmse: 4.84 alpha 100 일때 평균 rmse: 6.24 . log 변환이 다른 변환에 비해 성능이 뛰어난 걸 볼 수 있습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/poly/ridge/lasso/regression/2021/09/25/PythonMachine5_2.html",
            "relUrl": "/book/jupyter/guide/poly/ridge/lasso/regression/2021/09/25/PythonMachine5_2.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "[SSUDA] 심장병 데이터 분석",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import pandas as pd data = pd.read_csv(&quot;/content/drive/MyDrive/heart.csv&quot;) . Verson 1. &#49900;&#54540;&#54620; &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; &#47784;&#54805; . &#45936;&#51060;&#53552; &#51060;&#54644; . df = data.copy() df.head() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . 디폴트 값은 5입니다. . df.columns . Index([&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalachh&#39;, &#39;exng&#39;, &#39;oldpeak&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;, &#39;output&#39;], dtype=&#39;object&#39;) . df.columns.values.tolist() . [&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalachh&#39;, &#39;exng&#39;, &#39;oldpeak&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;, &#39;output&#39;] . 컬럼은 이런 방식으로 확인할 수 있습니다. . 밑에 DataFrame.columns.values.tolist() 함수는 컬럼 추출 중 가장 런타임이 빠르다고 합니다. . print(&#39;Shape is&#39;,df.shape) . Shape is (303, 14) . 303개 데이터, 14개 특성값이 있습니다. . df.isnull().sum() . age 0 sex 0 cp 0 trtbps 0 chol 0 fbs 0 restecg 0 thalachh 0 exng 0 oldpeak 0 slp 0 caa 0 thall 0 output 0 dtype: int64 . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trtbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalachh 303 non-null int64 8 exng 303 non-null int64 9 oldpeak 303 non-null float64 10 slp 303 non-null int64 11 caa 303 non-null int64 12 thall 303 non-null int64 13 output 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . 글쓴이는 윗방식으로 null값 유무를 체크했습니다. . 그러나 df.info() 방식이 여러가지 정보를 같이 줘 더 효율적입니다. . df.describe() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | . mean 54.366337 | 0.683168 | 0.966997 | 131.623762 | 246.264026 | 0.148515 | 0.528053 | 149.646865 | 0.326733 | 1.039604 | 1.399340 | 0.729373 | 2.313531 | 0.544554 | . std 9.082101 | 0.466011 | 1.032052 | 17.538143 | 51.830751 | 0.356198 | 0.525860 | 22.905161 | 0.469794 | 1.161075 | 0.616226 | 1.022606 | 0.612277 | 0.498835 | . min 29.000000 | 0.000000 | 0.000000 | 94.000000 | 126.000000 | 0.000000 | 0.000000 | 71.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 47.500000 | 0.000000 | 0.000000 | 120.000000 | 211.000000 | 0.000000 | 0.000000 | 133.500000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 55.000000 | 1.000000 | 1.000000 | 130.000000 | 240.000000 | 0.000000 | 1.000000 | 153.000000 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 61.000000 | 1.000000 | 2.000000 | 140.000000 | 274.500000 | 0.000000 | 1.000000 | 166.000000 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 77.000000 | 1.000000 | 3.000000 | 200.000000 | 564.000000 | 1.000000 | 2.000000 | 202.000000 | 1.000000 | 6.200000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . 데이터를 보면 어느정도 스케일링이 필요하다는 것을 알 수 있습니다. . &#53945;&#49457; &#49828;&#52992;&#51068;&#47553; . df[&#39;age&#39;] = df[&#39;age&#39;]/max(df[&#39;age&#39;]) df[&#39;cp&#39;] = df[&#39;cp&#39;]/max(df[&#39;cp&#39;]) df[&#39;trtbps&#39;] = df[&#39;trtbps&#39;]/max(df[&#39;trtbps&#39;]) df[&#39;chol&#39;] = df[&#39;chol&#39;]/max(df[&#39;chol&#39;]) df[&#39;thalachh&#39;] = df[&#39;thalachh&#39;]/max(df[&#39;thalachh&#39;]) . df.describe() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | . mean 0.706056 | 0.683168 | 0.322332 | 0.658119 | 0.436638 | 0.148515 | 0.528053 | 0.740826 | 0.326733 | 1.039604 | 1.399340 | 0.729373 | 2.313531 | 0.544554 | . std 0.117949 | 0.466011 | 0.344017 | 0.087691 | 0.091898 | 0.356198 | 0.525860 | 0.113392 | 0.469794 | 1.161075 | 0.616226 | 1.022606 | 0.612277 | 0.498835 | . min 0.376623 | 0.000000 | 0.000000 | 0.470000 | 0.223404 | 0.000000 | 0.000000 | 0.351485 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 0.616883 | 0.000000 | 0.000000 | 0.600000 | 0.374113 | 0.000000 | 0.000000 | 0.660891 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 0.714286 | 1.000000 | 0.333333 | 0.650000 | 0.425532 | 0.000000 | 1.000000 | 0.757426 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 0.792208 | 1.000000 | 0.666667 | 0.700000 | 0.486702 | 0.000000 | 1.000000 | 0.821782 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 2.000000 | 1.000000 | 1.000000 | 6.200000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . 이전과 달리 특성 스케일이 확실히 비슷해졌습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553; . from sklearn.model_selection import train_test_split #splitting data into training data and testing data X_train, X_test, y_train, y_test = train_test_split( df.drop([&#39;output&#39;], axis=1), df.output, test_size= 0.2, # 20% test data &amp; 80% train data random_state=0, stratify=df.output ) . stratify 속성 =&gt; y값의 공평한 분배를 위해 사용하는 속성입니다. . from sklearn.linear_model import LogisticRegression clf = LogisticRegression() clf.fit(X_train, y_train) from sklearn.metrics import accuracy_score Y_pred = clf.predict(X_test) acc=accuracy_score(y_test, Y_pred) print(&#39;Accuracy is&#39;,round(acc,2)*100,&#39;%&#39;) . Accuracy is 89.0 % . 로지스틱 회귀 모형을 별다른 튜닝 없이 사용했습니다. . 정확도 측면에서만 보면 캐글에 있는 다른 코드와 별반 다르지 않습니다. . Verson 2. &#49900;&#54540;&#54620; &#46373;&#47084;&#45789; &#47784;&#54805; . &#45936;&#51060;&#53552; &#51060;&#54644;2 . df = data.copy() df.output.value_counts() . 1 165 0 138 Name: output, dtype: int64 . 이전 모델에서 생략(?)된 부분인거 같은데 1과 0 값의 비율이 조금 차이가 있습니다. . df.corr().abs()[&#39;output&#39;].sort_values(ascending = False) . output 1.000000 exng 0.436757 cp 0.433798 oldpeak 0.430696 thalachh 0.421741 caa 0.391724 slp 0.345877 thall 0.344029 sex 0.280937 age 0.225439 trtbps 0.144931 restecg 0.137230 chol 0.085239 fbs 0.028046 Name: output, dtype: float64 . Y값과의 상관계수가 어느정도 되는지 확인해보았습니다. . &#45936;&#51060;&#53552; &#47784;&#45944;&#47553;2 . from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score X = df.drop(&#39;output&#39;, axis = 1) y = df[&#39;output&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) X_train.shape . (242, 13) . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 여기서는 StandardScaler를 사용해 스케일링을 했습니다. . 평균 0, 분산 1로 조정합니다. 이 스케일링은 이상치가 있을때 잘 작용하지 않을 수 있습니다. . from tensorflow import keras model = keras.Sequential( [ keras.layers.Dense( 256, activation=&quot;relu&quot;, input_shape=[13] ), keras.layers.Dense(515, activation=&quot;relu&quot;), keras.layers.Dropout(0.3), keras.layers.Dense(50, activation=&quot;relu&quot;), keras.layers.Dropout(0.3), keras.layers.Dense(1, activation=&quot;sigmoid&quot;), ] ) model.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_20 (Dense) (None, 256) 3584 _________________________________________________________________ dense_21 (Dense) (None, 515) 132355 _________________________________________________________________ dropout_10 (Dropout) (None, 515) 0 _________________________________________________________________ dense_22 (Dense) (None, 50) 25800 _________________________________________________________________ dropout_11 (Dropout) (None, 50) 0 _________________________________________________________________ dense_23 (Dense) (None, 1) 51 ================================================================= Total params: 161,790 Trainable params: 161,790 Non-trainable params: 0 _________________________________________________________________ . 활성화 함수로 제일 많이 사용하는 relu와 sigmoid함수를 사용했습니다. . relu함수 : 입력이 양수일 경우 그대로 반환, 음수일경우 0으로 만듭니다. . sigmoid함수 : 1 / (1 + e^z) 함수. 값을 0에서 1 사이로 변환합니다. . 첫번째 구간에 아웃풋 값을 256개 주었는데, 변수값이 13개임으로 모수가 14개입니다. . 그래서 256*14 = 3584개 파라미터가 나오게 된 것입니다. . 중간에 있는 드롭아웃은 일정 비율만큼 뉴런을 랜덤하게 꺼서 과대적합을 막는 역할을 합니다. . model.compile(optimizer = &#39;Adam&#39;, loss = &#39;binary_crossentropy&#39;, metrics = [&#39;binary_accuracy&#39;]) early_stopping = keras.callbacks.EarlyStopping( patience = 20, min_delta = 0.001, restore_best_weights =True ) history = model.fit( X_train, y_train, validation_data=(X_test, y_test), batch_size=15, epochs=50, callbacks = [early_stopping], verbose=1, ) . Epoch 1/50 17/17 [==============================] - 1s 15ms/step - loss: 0.5500 - binary_accuracy: 0.7190 - val_loss: 0.3814 - val_binary_accuracy: 0.8852 Epoch 2/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3823 - binary_accuracy: 0.8347 - val_loss: 0.3797 - val_binary_accuracy: 0.8852 Epoch 3/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3354 - binary_accuracy: 0.8719 - val_loss: 0.4391 - val_binary_accuracy: 0.8197 Epoch 4/50 17/17 [==============================] - 0s 6ms/step - loss: 0.3017 - binary_accuracy: 0.8802 - val_loss: 0.4147 - val_binary_accuracy: 0.8689 Epoch 5/50 17/17 [==============================] - 0s 6ms/step - loss: 0.2589 - binary_accuracy: 0.9091 - val_loss: 0.4388 - val_binary_accuracy: 0.8689 Epoch 6/50 17/17 [==============================] - 0s 6ms/step - loss: 0.2579 - binary_accuracy: 0.9256 - val_loss: 0.4795 - val_binary_accuracy: 0.8525 Epoch 7/50 17/17 [==============================] - 0s 7ms/step - loss: 0.2019 - binary_accuracy: 0.9256 - val_loss: 0.4895 - val_binary_accuracy: 0.8689 Epoch 8/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1889 - binary_accuracy: 0.9298 - val_loss: 0.5359 - val_binary_accuracy: 0.8361 Epoch 9/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1887 - binary_accuracy: 0.9215 - val_loss: 0.5324 - val_binary_accuracy: 0.8525 Epoch 10/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1578 - binary_accuracy: 0.9545 - val_loss: 0.5441 - val_binary_accuracy: 0.8689 Epoch 11/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1686 - binary_accuracy: 0.9215 - val_loss: 0.6338 - val_binary_accuracy: 0.8689 Epoch 12/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1448 - binary_accuracy: 0.9504 - val_loss: 0.6872 - val_binary_accuracy: 0.8197 Epoch 13/50 17/17 [==============================] - 0s 6ms/step - loss: 0.1065 - binary_accuracy: 0.9628 - val_loss: 0.7682 - val_binary_accuracy: 0.8197 Epoch 14/50 17/17 [==============================] - 0s 7ms/step - loss: 0.0879 - binary_accuracy: 0.9835 - val_loss: 0.8583 - val_binary_accuracy: 0.8197 Epoch 15/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0877 - binary_accuracy: 0.9711 - val_loss: 0.9300 - val_binary_accuracy: 0.8361 Epoch 16/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0688 - binary_accuracy: 0.9835 - val_loss: 0.9281 - val_binary_accuracy: 0.8361 Epoch 17/50 17/17 [==============================] - 0s 8ms/step - loss: 0.0615 - binary_accuracy: 0.9835 - val_loss: 0.9688 - val_binary_accuracy: 0.8361 Epoch 18/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0496 - binary_accuracy: 0.9835 - val_loss: 1.0818 - val_binary_accuracy: 0.8197 Epoch 19/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0915 - binary_accuracy: 0.9628 - val_loss: 1.3326 - val_binary_accuracy: 0.8525 Epoch 20/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0953 - binary_accuracy: 0.9669 - val_loss: 1.1602 - val_binary_accuracy: 0.8525 Epoch 21/50 17/17 [==============================] - 0s 7ms/step - loss: 0.0366 - binary_accuracy: 0.9959 - val_loss: 1.1617 - val_binary_accuracy: 0.8525 Epoch 22/50 17/17 [==============================] - 0s 6ms/step - loss: 0.0407 - binary_accuracy: 0.9876 - val_loss: 1.2300 - val_binary_accuracy: 0.8361 . model.evaluate(X_test, y_test) . 2/2 [==============================] - 0s 7ms/step - loss: 0.3797 - binary_accuracy: 0.8852 . [0.3796648383140564, 0.8852459192276001] . predictions =(model.predict(X_test)&gt;0.5).astype(&quot;int32&quot;) from sklearn.metrics import classification_report, confusion_matrix, accuracy_score accuracy_score(y_test, predictions) . 0.8852459016393442 . 아까 결과와 비슷한 수치를 보입니다. . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.00 0.00 0.00 29 1 0.52 1.00 0.69 32 accuracy 0.52 61 macro avg 0.26 0.50 0.34 61 weighted avg 0.28 0.52 0.36 61 . classification_report 함수가 상당히 유용한 걸 알 수있습니다. . 한번에 정밀도, 재현율, f1-score 값 까지 보여줍니다. . &#45712;&#45184;&#51216; . 분류에 기본적인 로지스틱 회귀모형과 단순한 딥러닝 코드를 따라해봤습니다. . 특히 딥러닝 부분에 경우 정말 기본적인 것밖에 몰라 코드 해석에 시간이 많이 걸렸네요. . 여러가지로 코드를 만져가며 느낀점은 이번 데이터에 경우 스케일링이 많이 중요한 것 같습니다. . 스케일링 종류에 따라서 정확도 값이 크게 변하는 것을 관찰했습니다. . 특히 트리기반 부스팅 모델이 아니라 더 그런 것 같습니다. . 너무 복잡한 모델을 급하게 이해하기 보다, 이해할 수 있는 모델을 관찰하며 데이터 분석은 어떤 과정으로 하는가를 살펴봤습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/logistic/scale/keras/regression/2021/09/24/kagglessu2.html",
            "relUrl": "/ssuda/jupyter/kaggle/logistic/scale/keras/regression/2021/09/24/kagglessu2.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "[머신러닝 가이드] 5-1 단순선형회귀",
            "content": ". &#44221;&#49324;&#54616;&#44053;&#48277; . import numpy as np import matplotlib.pyplot as plt %matplotlib inline np.random.seed(8) x = 2 * np.random.randn(100,1) y = 6 + 4 * x + np.random.randn(100,1) plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x7f6bcf5bb850&gt; . y = 4x + 6 근사 . np.random.randn =&gt; 표준정규분포에서 값 생성. 100,1 은 값 행렬 형식 선언 입니다. . def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y-y_pred)) / N return cost . 편차 제곱 평균을 계산해주는 함수. . np.square =&gt; 제곱 해주는 함수 . def get_weight_updates(w1, w0, x, y, learning_rate = 0.01): N = len(y) # w1, w0 동일한 행렬 크기를 갖는 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) #np.dot 행렬의 곱 y_pred = np.dot(x, w1.T) + w0 diff = y - y_pred w0_factors = np.ones((N, 1)) w1_update = -(2/N) * learning_rate * (np.dot(x.T, diff)) w0_update = -(2/N) * learning_rate * (np.dot(w0_factors.T, diff)) return w1_update, w0_update . 편미분한 w1, w0값을 이용해서 w0, w1값을 지속적으로 업데이트 해줍니다 . np.zeros_like(w1) =&gt; w1값과 같은 형태에 값은 0인 행렬 생성 . np.dot(,) =&gt; 행렬 연산 . def gradient_descent_steps(x,y, iters = 10000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, x, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . 위 두 함수를 통해 w1, w0 값을 지속적으로 업데이트 하여 최적에 값에 도달하게 합니다. . w1, w0 = gradient_descent_steps(x,y, 1000) print(&#39;w1 :&#39;, np.round(w1[0,0],4), &#39;w0 :&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱평균:&#39;, np.round(get_cost(y, y_pred),4)) . w1 : 3.9974 w0 : 5.9649 편차제곱평균: 1.1967 . plt.scatter(x,y) plt.plot(x,y_pred) . [&lt;matplotlib.lines.Line2D at 0x7f6bcf10f110&gt;] . 경사하강법을 이용해 회귀선이 잘 만들어졌습니다. . 다만 데이터에 개수가 100개보다 훨씬 많아지면 전체데이터로 계수를 업데이트 하지 못합니다. . 그 때문에 실전에서는 대부분 (미니배치)확률적 경사 하강법을 이용합니다. . 이 방식은 전체 데이터가 아닌 일부 데이터로 계수를 업데이트 하기 때문에 속도가 상대적으로 빠릅니다. . 이를 구현해보겠습니다. . def stochastic_gradient_descent_steps(x,y,batch_size = 10, iters = 1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index = 0 for ind in range(iters): np.random.seed(ind) stochastic_random_index = np.random.permutation(x.shape[0]) sample_x = x[stochastic_random_index[0:batch_size]] sample_y = y[stochastic_random_index[0:batch_size]] w1_update, w0_update = get_weight_updates(w1, w0, sample_x, sample_y) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . np.random.permutation(x.shape[0]) =&gt; 주어진 데이터를 셔플해서 출력함 . 앞 함수와 바뀐 부분은 x, y를 샘플링해서 넣는다는 점 입니다. . w1, w0 = stochastic_gradient_descent_steps(x,y,iters=1000) print(&#39;w1:&#39;, np.round(w1[0,0],3), &#39;w0:&#39;, np.round(w0[0,0],4)) y_pred = w1[0,0] * x + w0 print(&#39;편차제곱 평균:&#39;, np.round(get_cost(y,y_pred),4)) . w1: 4.006 w0: 5.9135 편차제곱 평균: 1.1996 . 편차제곱 평균 값이 전체 x,y를 투입했을때와 큰 차이가 없습니다. . 그러므로 계산 속도가 훨씬 빠른 미니배치 경사하강법을 많이 사용합니다. . &#45800;&#49692; &#49440;&#54805; &#54924;&#44480;(&#48372;&#49828;&#53556; &#51452;&#53469; &#44032;&#44201;) . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;보스턴 데이터 세트 크기:&#39;, bostonDF.shape) bostonDF.head() . 보스턴 데이터 세트 크기: (506, 14) . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . 사이킷런에 내장되어있는 보스턴 주택 데이터를 불러왔습니다. . bostonDF.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 CRIM 506 non-null float64 1 ZN 506 non-null float64 2 INDUS 506 non-null float64 3 CHAS 506 non-null float64 4 NOX 506 non-null float64 5 RM 506 non-null float64 6 AGE 506 non-null float64 7 DIS 506 non-null float64 8 RAD 506 non-null float64 9 TAX 506 non-null float64 10 PTRATIO 506 non-null float64 11 B 506 non-null float64 12 LSTAT 506 non-null float64 13 PRICE 506 non-null float64 dtypes: float64(14) memory usage: 55.5 KB . 결측값은 없으며 모든 피처가 float 형 입니다. . fig, axs = plt.subplots(figsize=(16,8), ncols = 4, nrows = 2) lm_features = [&#39;RM&#39;,&#39;ZN&#39;, &#39;INDUS&#39;,&#39;NOX&#39;,&#39;AGE&#39;,&#39;PTRATIO&#39;,&#39;LSTAT&#39;,&#39;RAD&#39;] for i, feature in enumerate(lm_features): row = int(i/4) col = i%4 sns.regplot(x=feature, y=&#39;PRICE&#39;, data=bostonDF, ax=axs[row][col]) . sns.regplot(x,y) =&gt; x,y 산점도와 함께 회귀직선을 그려줌. . plt.subplots(ncols = , nrows= ) 여러개의 그림을 그릴 수 있게 해줌. . RM과 LSTAT 변수가 가장 PRICE 변수와 연관성이 있어보입니다. . from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score y_target = bostonDF[&#39;PRICE&#39;] x_data = bostonDF.drop([&#39;PRICE&#39;], axis = 1, inplace=False) x_train, x_test, y_train, y_test = train_test_split(x_data, y_target, test_size = 0.3, random_state = 156) lr = LinearRegression() lr.fit(x_train, y_train) y_preds = lr.predict(x_test) mse = mean_squared_error(y_test, y_preds) rmse = np.sqrt(mse) print(&#39;mse :&#39;, np.round(mse,4), &#39;, rmse :&#39;, np.round(rmse, 4)) print(&#39;결정계수:&#39;, np.round(r2_score(y_test, y_preds), 4)) . mse : 17.2969 , rmse : 4.159 결정계수: 0.7572 . 모델을 어느정도 설명해 준 모습입니다. . print(&#39;절편 값:&#39;,lr.intercept_) print(&#39;회귀 계수값:&#39;, np.round(lr.coef_,1)) . 절편 값: 40.995595172164755 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] . coeff = pd.Series(data=np.round(lr.coef_, 1), index = x_data.columns) coeff.sort_values(ascending=False) . RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 B 0.0 TAX -0.0 AGE 0.0 INDUS 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 . 변수 이름과 추정 회귀 계수를 맵핑 시킨 모습입니다. . NOX 변수의 계수 값이 크게 작아보입니다. . from sklearn.model_selection import cross_val_score neg_mse_scores = cross_val_score(lr, x_data, y_target, scoring=&#39;neg_mean_squared_error&#39;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;mse scores&#39;, np.round(neg_mse_scores,2)) print(&#39;rmse scores&#39;, np.round(rmse_scores, 2)) print(&#39;평균 rmse score:&#39;, np.round(avg_rmse,2)) . mse scores [-12.46 -26.05 -33.07 -80.76 -33.31] rmse scores [3.53 5.1 5.75 8.99 5.77] 평균 rmse score: 5.83 . 5개의 폴드 세트를 이용한 교차검증 입니다. . scoring = &#39;neg_mean_squared_error&#39; 같은 경우 보통 모델 평가를 위한 값이 커야 좋은 값인데, mse 값은 작아야 좋습니다. . 그러므로 음수를 붙여서 보정해준다고 생각하면 좋습니다. . 다음에는 다항회귀, 릿지/라쏘 회귀 부분을 공부하겠습니다. .",
            "url": "https://ksy1526.github.io/myblog/book/jupyter/guide/math/regression/2021/09/20/PythonMachine5_1.html",
            "relUrl": "/book/jupyter/guide/math/regression/2021/09/20/PythonMachine5_1.html",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "[SSUDA] 주택 가격 예측",
            "content": ". https://www.kaggle.com/c/house-prices-advanced-regression-techniques . 캐글에 있는 주택 가격 예측 데이터 분석입니다. . 부스팅 모델들이 튜닝하는데 시간이 걸리기 때문에 좀 더 간단한 선형 회귀 모델을 사용하겠습니다. . 분류 관련 공부를 조금 해본 경험으로, 회귀에 기본인 선형 회귀모델을 이번 데이터를 이용해 공부해보겠습니다. . 이번 분석에 핵심 포인트는 숫자 변수 대부분이 치우쳐 있으므로 숫자 변수를 log_transform하는 것입니다. . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; &#48143; &#46168;&#47084;&#48372;&#44592; . import pandas as pd import numpy as np import seaborn as sns import matplotlib import matplotlib.pyplot as plt from scipy.stats import skew from scipy.stats.stats import pearsonr %config InlineBackend.figure_format = &#39;retina&#39; #set &#39;png&#39; here when working on notebook %matplotlib inline . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . train = pd.read_csv(&quot;/content/drive/MyDrive/house/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/house/test.csv&quot;) . train.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating ... CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | ... | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | ... | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | ... | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | ... | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | ... | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 81 columns): # Column Non-Null Count Dtype -- -- 0 Id 1460 non-null int64 1 MSSubClass 1460 non-null int64 2 MSZoning 1460 non-null object 3 LotFrontage 1201 non-null float64 4 LotArea 1460 non-null int64 5 Street 1460 non-null object 6 Alley 91 non-null object 7 LotShape 1460 non-null object 8 LandContour 1460 non-null object 9 Utilities 1460 non-null object 10 LotConfig 1460 non-null object 11 LandSlope 1460 non-null object 12 Neighborhood 1460 non-null object 13 Condition1 1460 non-null object 14 Condition2 1460 non-null object 15 BldgType 1460 non-null object 16 HouseStyle 1460 non-null object 17 OverallQual 1460 non-null int64 18 OverallCond 1460 non-null int64 19 YearBuilt 1460 non-null int64 20 YearRemodAdd 1460 non-null int64 21 RoofStyle 1460 non-null object 22 RoofMatl 1460 non-null object 23 Exterior1st 1460 non-null object 24 Exterior2nd 1460 non-null object 25 MasVnrType 1452 non-null object 26 MasVnrArea 1452 non-null float64 27 ExterQual 1460 non-null object 28 ExterCond 1460 non-null object 29 Foundation 1460 non-null object 30 BsmtQual 1423 non-null object 31 BsmtCond 1423 non-null object 32 BsmtExposure 1422 non-null object 33 BsmtFinType1 1423 non-null object 34 BsmtFinSF1 1460 non-null int64 35 BsmtFinType2 1422 non-null object 36 BsmtFinSF2 1460 non-null int64 37 BsmtUnfSF 1460 non-null int64 38 TotalBsmtSF 1460 non-null int64 39 Heating 1460 non-null object 40 HeatingQC 1460 non-null object 41 CentralAir 1460 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1460 non-null int64 44 2ndFlrSF 1460 non-null int64 45 LowQualFinSF 1460 non-null int64 46 GrLivArea 1460 non-null int64 47 BsmtFullBath 1460 non-null int64 48 BsmtHalfBath 1460 non-null int64 49 FullBath 1460 non-null int64 50 HalfBath 1460 non-null int64 51 BedroomAbvGr 1460 non-null int64 52 KitchenAbvGr 1460 non-null int64 53 KitchenQual 1460 non-null object 54 TotRmsAbvGrd 1460 non-null int64 55 Functional 1460 non-null object 56 Fireplaces 1460 non-null int64 57 FireplaceQu 770 non-null object 58 GarageType 1379 non-null object 59 GarageYrBlt 1379 non-null float64 60 GarageFinish 1379 non-null object 61 GarageCars 1460 non-null int64 62 GarageArea 1460 non-null int64 63 GarageQual 1379 non-null object 64 GarageCond 1379 non-null object 65 PavedDrive 1460 non-null object 66 WoodDeckSF 1460 non-null int64 67 OpenPorchSF 1460 non-null int64 68 EnclosedPorch 1460 non-null int64 69 3SsnPorch 1460 non-null int64 70 ScreenPorch 1460 non-null int64 71 PoolArea 1460 non-null int64 72 PoolQC 7 non-null object 73 Fence 281 non-null object 74 MiscFeature 54 non-null object 75 MiscVal 1460 non-null int64 76 MoSold 1460 non-null int64 77 YrSold 1460 non-null int64 78 SaleType 1460 non-null object 79 SaleCondition 1460 non-null object 80 SalePrice 1460 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 924.0+ KB . all_data = pd.concat((train.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;], test.loc[:,&#39;MSSubClass&#39;:&#39;SaleCondition&#39;])) . id(고유번호)와 설명변수를 뺀 나머지 변수들을 전처리를 위해 all_data 변수로 합쳐주었습니다. . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 이 코드의 데이터 전처리는 화려하지 않습니다. 기본에 충실합니다. . 다음 3가지로 요약할 수 있습니다. . 로그(기능 + 1)를 사용하여 오른쪽으로 꼬리가 긴 그래프를 변환합니다. 그러면 어느정도 정규화됩니다. | 범주형 형상에 대한 더미 변수 생성 | 숫자 결측값(NaN)을 각 열의 평균으로 바꾸기 | 설명변수를 로그변환 해보기 . matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 6.0) prices = pd.DataFrame({&quot;price&quot;:train[&quot;SalePrice&quot;], &quot;log(price + 1)&quot;:np.log1p(train[&quot;SalePrice&quot;])}) prices.hist() . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1089890&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3be1052b10&gt;]], dtype=object) . 로그변환 전 우측 꼬리가 두터운 느낌이였는데 잘 정규화 된 모습입니다. . all_data.dtypes . MSSubClass int64 MSZoning object LotFrontage float64 LotArea int64 Street object ... MiscVal int64 MoSold int64 YrSold int64 SaleType object SaleCondition object Length: 79, dtype: object . train[&quot;SalePrice&quot;] = np.log1p(train[&quot;SalePrice&quot;]) numeric_feats = all_data.dtypes[all_data.dtypes != &quot;object&quot;].index . all_data.dtypes =&gt; 데이터 타입 나열. 여기서 인덱스는 변수이름이기 때문에 이런 방식으로 쉽게 추출. . skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) skewed_feats = skewed_feats[skewed_feats &gt; 0.75] skewed_feats = skewed_feats.index all_data[skewed_feats] = np.log1p(all_data[skewed_feats]) . shew = 왜도 값을 나타네는 함수. 왜도란 그래프가 비 대칭적인 모양인 것 . shew값이 큰 양수값이면 오른쪽으로 긴 꼬리를 가지는 분포를 가집니다. . 그러므로 shew값을 기준으로 로그변환을 할 변수를 찾을 수 있습니다. . 참고로 apply 함수는 파이썬 데이터 프레임에 적용하는 함수인데, 원하는 함수를 적용하고 싶을때 사용합니다. . 이때 apply 기본인자는 axis = 0이므로 열을 기준으로 함수를 적용합니다. . all_data = pd.get_dummies(all_data) all_data.head(5) . MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold MSZoning_C (all) MSZoning_FV MSZoning_RH MSZoning_RL ... GarageFinish_Unf GarageQual_Ex GarageQual_Fa GarageQual_Gd GarageQual_Po GarageQual_TA GarageCond_Ex GarageCond_Fa GarageCond_Gd GarageCond_Po GarageCond_TA PavedDrive_N PavedDrive_P PavedDrive_Y PoolQC_Ex PoolQC_Fa PoolQC_Gd Fence_GdPrv Fence_GdWo Fence_MnPrv Fence_MnWw MiscFeature_Gar2 MiscFeature_Othr MiscFeature_Shed MiscFeature_TenC SaleType_COD SaleType_CWD SaleType_Con SaleType_ConLD SaleType_ConLI SaleType_ConLw SaleType_New SaleType_Oth SaleType_WD SaleCondition_Abnorml SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Family SaleCondition_Normal SaleCondition_Partial . 0 4.110874 | 4.189655 | 9.042040 | 7 | 5 | 2003 | 2003 | 5.283204 | 6.561031 | 0.0 | 5.017280 | 6.753438 | 6.753438 | 6.751101 | 0.0 | 7.444833 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 8 | 0 | 2003.0 | 2.0 | 548.0 | 0.000000 | 4.127134 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 1 3.044522 | 4.394449 | 9.169623 | 6 | 8 | 1976 | 1976 | 0.000000 | 6.886532 | 0.0 | 5.652489 | 7.141245 | 7.141245 | 0.000000 | 0.0 | 7.141245 | 0.0 | 0.693147 | 2 | 0 | 3 | 0.693147 | 6 | 1 | 1976.0 | 2.0 | 460.0 | 5.700444 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 5 | 2007 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 4.110874 | 4.234107 | 9.328212 | 7 | 5 | 2001 | 2002 | 5.093750 | 6.188264 | 0.0 | 6.075346 | 6.825460 | 6.825460 | 6.765039 | 0.0 | 7.488294 | 1.0 | 0.000000 | 2 | 1 | 3 | 0.693147 | 6 | 1 | 2001.0 | 2.0 | 608.0 | 0.000000 | 3.761200 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 9 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 3 4.262680 | 4.110874 | 9.164401 | 7 | 5 | 1915 | 1970 | 0.000000 | 5.379897 | 0.0 | 6.293419 | 6.629363 | 6.869014 | 6.629363 | 0.0 | 7.448916 | 1.0 | 0.000000 | 1 | 0 | 3 | 0.693147 | 7 | 1 | 1998.0 | 3.0 | 642.0 | 0.000000 | 3.583519 | 5.609472 | 0.0 | 0.0 | 0.0 | 0.0 | 2 | 2006 | 0 | 0 | 0 | 1 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 4 4.110874 | 4.442651 | 9.565284 | 8 | 5 | 2000 | 2000 | 5.860786 | 6.486161 | 0.0 | 6.196444 | 7.044033 | 7.044033 | 6.960348 | 0.0 | 7.695758 | 1.0 | 0.000000 | 2 | 1 | 4 | 0.693147 | 9 | 1 | 2000.0 | 3.0 | 836.0 | 5.262690 | 4.442651 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 12 | 2008 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 288 columns . get_dummies 함수로 모든 object형 값이 원핫인코딩 됐습니다. . 저번에 프로젝트 할 때 변수를 하나하나 입력했던 것이 생각나는데 더 편한 방식을 알게 되었습니다. . all_data = all_data.fillna(all_data.mean()) . 결측값이 있을때 각 열의 평균값으로 대체하는 일반적인 방식입니다. . 윗 코드와 마찬가지로 저번 프로젝트에서 열마다 함수를 돌려 사용했는데 더 편한 방식을 알게 됐습니다. . X_train = all_data[:train.shape[0]] X_test = all_data[train.shape[0]:] y = train.SalePrice . 저번 프로젝트에서 트레인, 테스트 데이터에 각각 전처리를 적용했습니다. . 하지만 이 방법처럼 all_data로 묶고 한번에 전처리 하는 방식이 깔끔한 것 같습니다. . &#47551;&#51648; &#47784;&#45944; . 선형 회귀 모델 적합을 하겠습니다. . 이때 라쏘, 릿지 방법을 모두 사용해서 최적의 rmse 값을 찾겠습니다. . from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso from sklearn.model_selection import cross_val_score def rmse_cv(model): rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)) return(rmse) . cross_val_score 함수는 교차 검증 후 정확도를 리스트로 보여줍니다. . 여기서 cv = 5 이기 때문에 5-fold로 교차검증 하게 됩니다. . model_ridge = Ridge() . 릿지 모델의 주요 파라미터는 알파입니다. . 알파값이 높아지면 규제가 심해지고 과적합을 방지해줍니다. . 다만 너무 많이 높아지면 과소적합이 되기 때문에 적절한 값을 찾아야합니다. . alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas] . 다양한 알파값을 릿지 함수에 적용시켰습니다. . 여기서 [값 for alpha in alphas] 는 for루프를 리스트 내에서 돌리는 것 입니다. . cv_ridge = pd.Series(cv_ridge, index = alphas) cv_ridge.plot(title = &quot;Validation - Just Do It&quot;) plt.xlabel(&quot;alpha&quot;) plt.ylabel(&quot;rmse&quot;) . Text(0, 0.5, &#39;rmse&#39;) . 시리즈에 plot를 하면 그래프가 생깁니다. . 이때 x축은 인덱스, y축은 본 값이 들어갑니다. . 알파값이 10일때 rmse값이 최소로, 알파는 10을 쓰는 것이 좋겠습니다. . 보통 규제하는 변수와 예측도를 측정하는 값간에 그래프는 U자형태가 잘 나옵니다. . 그 이유는 규제가 약할때와 쌜 때 각각 과소적합, 과적합이 일어나 예측도를 측정하는 값이 커지기 때문입니다. . cv_ridge.min() . 0.1273373466867076 . 최적의 rmse값은 0.1273입니다. . &#46972;&#50136; &#47784;&#45944; . 이번엔 라쏘 모델입니다. . 라쏘 모델은 릿지 모델과 다르게 영향력이 작은 변수의 계수를 0으로 만듭니다. . 변수 선택 과정까지 한번에 할 수 있다는 것이 장점입니다. . model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y) . LassoCV 함수로 여러가지 알파값을 동시에 검정할 수 있습니다. . model_lasso.alpha_ . 0.0005 . rmse_cv(model_lasso).mean() . 0.12256735885048142 . 라쏘 모델이 rmse 값이 훨씬 낮아서 좋습니다. . 라쏘 모델을 사용하겠습니다. . coef = pd.Series(model_lasso.coef_, index = X_train.columns) . 회귀 모델.coef_ =&gt; 계수를 컬럼순으로 보여줍니다. . print(&quot;Lasso picked &quot; + str(sum(coef != 0)) + &quot; variables and eliminated the other &quot; + str(sum(coef == 0)) + &quot; variables&quot;) . Lasso picked 110 variables and eliminated the other 178 variables . 110개 변수는 선택되었고 178개 변수는 계수가 0, 즉 선택하지 않은 변수들입니다. . coef . MSSubClass -0.007480 LotFrontage 0.000000 LotArea 0.071826 OverallQual 0.053160 OverallCond 0.043027 ... SaleCondition_AdjLand 0.000000 SaleCondition_Alloca -0.000000 SaleCondition_Family -0.007925 SaleCondition_Normal 0.019666 SaleCondition_Partial 0.000000 Length: 288, dtype: float64 . imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)]) matplotlib.rcParams[&#39;figure.figsize&#39;] = (8.0, 10.0) imp_coef.plot(kind = &quot;barh&quot;) plt.title(&quot;Coefficients in the Lasso Model&quot;) . Text(0.5, 1.0, &#39;Coefficients in the Lasso Model&#39;) . sort_values() 함수는 범주형 변수의 히스토그램을 아는데 유용한 함수입니다. . 여기선 정렬기능으로 사용했는데, 정렬기능으로도 충분히 우수한 것을 보여줬습니다. . 정렬된 값 상위 10개, 하위 10개를 시각화했는데, 이 변수들이 핵심 변수입니다. . 왜냐하면 계수의 절대값이 큰 값이기 때문입니다. . 양의 값으로 가장 큰 GrLivArea변수는 면적으로 주택가격에 당연히 큰 영향을 끼칩니다. . matplotlib.rcParams[&#39;figure.figsize&#39;] = (6.0, 6.0) preds = pd.DataFrame({&quot;preds&quot;:model_lasso.predict(X_train), &quot;true&quot;:y}) preds[&quot;residuals&quot;] = preds[&quot;true&quot;] - preds[&quot;preds&quot;] preds.plot(x = &quot;preds&quot;, y = &quot;residuals&quot;,kind = &quot;scatter&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3bd2529490&gt; . 잔차 그림도 큰 이상이 없습니다. . model_lasso = Lasso(alpha = 0.0005).fit(X_train, y) pred = model_lasso.predict(X_test) pred2 = np.exp(pred) - 1 X_test[&#39;SalePrice&#39;] = pred2 X_test[&#39;Id&#39;] = test[&#39;Id&#39;] final = X_test[[&#39;Id&#39;,&#39;SalePrice&#39;]] final.to_csv(&#39;/content/drive/MyDrive/houselasso2.csv&#39;,encoding=&#39;UTF-8&#39;, index=False) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . 알파값 0.0005인 라쏘 모델로 모델을 적합시키고 그 모델로 예측 파일을 만들었습니다. .",
            "url": "https://ksy1526.github.io/myblog/ssuda/jupyter/kaggle/ridge/lasso/regression/2021/09/15/kagglessu1.html",
            "relUrl": "/ssuda/jupyter/kaggle/ridge/lasso/regression/2021/09/15/kagglessu1.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ksy1526.github.io/myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ksy1526.github.io/myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "안녕하세요. 김성연이라고 합니다. . 저는 현재 통계학과 재학중이며 머신러닝/딥러닝 관련 공부하고 있습니다. . 이 블로그는 제가 하는 공부를 기록하는 곳인데요. 방문하시는 분에게도 도움이 되었으면 좋겠습니다. . 구체적으로 캐글 내 인기 코드를 제 시각에 맞추서 리뷰하거나, 머신러닝/딥러닝 책 예제 코드를 따라하며 공부하고 있습니다. . SSUDA는 ‘우리들의 데이터 수다’ 모임으로 교내 소모임입니다. 주로 캐글 코드 리뷰를 업로드 합니다. . 머신러닝 가이드는 ‘파이썬 머신러닝 완벽가이드’ 교제 내 예제 코드를 따라한 것을 업로드 합니다. (https://book.naver.com/bookdb/book_detail.nhn?bid=16238302) . Do it 자연어는 ‘Do it! BERT와 GPT로 배우는 자연어 처리’ 교제 내 예제 코드를 따라한 것을 업로드 합니다. (https://book.naver.com/bookdb/book_detail.nhn?bid=16238302) . 시계열분석은 ‘실전 시계열 분석’ 교제 내 예제 코드를 따라한 것을 업로드 합니다. (https://book.naver.com/bookdb/book_detail.naver?bid=18563552) . DACON, Kaggle은 제가 데이콘이나 캐글 대회에 실제로 참가할때 사용한 코드를 업로드 합니다. . 혹시 코드가 이상하거나 질문 있으시면, 댓글이나 제 메일(zan05134@naver.com) 으로 연락 부탁드립니다. . 감사합니다. .",
          "url": "https://ksy1526.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ksy1526.github.io/myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}